{"id": "2c8726af-f2ee-42d6-85e1-193b060b7137", "fitness": 0.08433228037419432, "name": "MADE", "description": "A Memetic Adaptive Differential Evolution (MADE) algorithm combining differential evolution with adaptive mutation and local search to efficiently explore and exploit the search space.", "code": "import numpy as np\n\nclass MADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Base population size\n        self.F = 0.8  # Differential weight\n        self.CR = 0.7  # Crossover probability\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = self.population_size\n\n        while evals < self.budget:\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Adaptive Mutation\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi / 2)\n            # Local Search using Simplex Method\n            if evals < self.budget:\n                local_search_solution = self.simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n        return self.best_solution, self.best_fitness\n\n    def simplex_search(self, func, solution, lb, ub):\n        # Simplex method as a local search strategy\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20):  # Limit the number of local iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 0, "feedback": "The algorithm MADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08433 with standard deviation 0.04713.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.051085235160171494, 0.0539525194867686, 0.05433196575830357, 0.1500518984920266, 0.14863060798052097, 0.15237923122956998, 0.06466361508013008, 0.04271813079991993, 0.04117731938033764]}}
{"id": "0240b38a-96ce-4e3d-9d3d-d6b1229a5d82", "fitness": 0.08563631505245756, "name": "EMADE", "description": "Enhanced Memetic Adaptive Differential Evolution (EMADE) with dynamic population sizing and improved local search using an adaptive Nelder-Mead method for better convergence and exploration of the search space.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.8\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi)\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 1, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08564 with standard deviation 0.05107.", "error": "", "parent_ids": ["2c8726af-f2ee-42d6-85e1-193b060b7137"], "operator": null, "metadata": {"aucs": [0.04812470446086803, 0.0539525194867686, 0.05433196575830357, 0.1724002929195868, 0.14863060798052097, 0.14961866500454968, 0.05994696613684325, 0.04271813079991993, 0.041002982924757125]}}
{"id": "4fa714a6-27b5-4977-893d-f6857c2085ea", "fitness": 0.08320099395219904, "name": "RefinedEMADE", "description": "Introducing a dynamic crossover strategy and self-adaptive mutation factor in EMADE for enhanced exploration and exploitation in black box optimization.", "code": "import numpy as np\n\nclass RefinedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F_min = 0.4\n        self.F_max = 0.9\n        self.CR_min = 0.5\n        self.CR_max = 1.0\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = np.random.uniform(self.F_min, self.F_max)\n                mutant = np.clip(a + F * (b - c), lb, ub)\n\n                CR = self.CR_min + (self.CR_max - self.CR_min) * (1 - evals / self.budget)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 2, "feedback": "The algorithm RefinedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08320 with standard deviation 0.04584.", "error": "", "parent_ids": ["0240b38a-96ce-4e3d-9d3d-d6b1229a5d82"], "operator": null, "metadata": {"aucs": [0.05623911319643826, 0.060775502633721334, 0.05369843881675462, 0.14707444872650677, 0.16132900632482472, 0.13237352149334303, 0.04434979257385929, 0.04665156093927836, 0.046317560865065]}}
{"id": "aa045bff-6d08-4c3d-932d-27114f3a7974", "fitness": -Infinity, "name": "ImprovedEMADE", "description": "Enhanced Memetic Adaptive Differential Evolution (EMADE) with adaptive dynamic population and crossover rates, combined with an improved exploration strategy through a stochastic Lévy flight mutation.", "code": "import numpy as np\n\nclass ImprovedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n        self.beta = 1.5  # Parameter for Lévy flight\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            F = 0.5 + 0.5 * np.random.rand()  # Dynamic F\n            CR = 0.3 + 0.7 * np.random.rand()  # Dynamic CR\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Stochastic Lévy flight mutation for exploration\n            if np.random.rand() < 0.1:\n                flight = self.levy_flight(lb, ub)\n                candidate = np.clip(self.best_solution + flight * (ub - lb), lb, ub)\n                candidate_fitness = func(candidate)\n                evals += 1\n                if candidate_fitness < self.best_fitness:\n                    self.best_fitness = candidate_fitness\n                    self.best_solution = candidate\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def levy_flight(self, lb, ub):\n        sigma_u = (np.gamma(1 + self.beta) * np.sin(np.pi * self.beta / 2) /\n                   (np.gamma((1 + self.beta) / 2) * self.beta * 2**((self.beta - 1) / 2)))**(1 / self.beta)\n        u = np.random.normal(0, sigma_u, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / np.abs(v)**(1 / self.beta)\n        return step * 0.01 * (ub - lb)", "configspace": "", "generation": 3, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_ids": ["0240b38a-96ce-4e3d-9d3d-d6b1229a5d82"], "operator": null, "metadata": {}}
{"id": "45a896e0-af97-4475-bff7-9717d0a9b86a", "fitness": 0.08250306924779675, "name": "EMADEPlus", "description": "Enhanced Memetic Adaptive Differential Evolution (EMADE+) with adaptive crossover rates and diversity-based dynamic population control for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass EMADEPlus:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.8\n        self.CR = 0.9  # Start with a higher crossover rate\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Update crossover rate based on diversity\n            diversity = np.std(pop, axis=0).mean()\n            self.CR = 0.9 - 0.5 * (1 - diversity / np.std(ub - lb))\n\n            # Adjust mutation factor dynamically\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi)\n\n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing with diversity consideration\n            if evals / self.budget > 0.5 or diversity < 0.1:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 4, "feedback": "The algorithm EMADEPlus got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08250 with standard deviation 0.04183.", "error": "", "parent_ids": ["0240b38a-96ce-4e3d-9d3d-d6b1229a5d82"], "operator": null, "metadata": {"aucs": [0.04929058807585451, 0.0633455628252525, 0.05880528930618034, 0.13678845324660038, 0.1417323819347328, 0.14425136970912678, 0.04144260709547187, 0.059467452575411306, 0.04740391846154024]}}
{"id": "b1d8a693-492c-462a-a0df-3f9f5bae3aef", "fitness": 0.08544220017133229, "name": "AMDE", "description": "Advanced Memetic Differential Evolution (AMDE) with dynamic exploration-exploitation balance and adaptive simplex local search for enhanced convergence.", "code": "import numpy as np\n\nclass AMDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.8\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.cos(evals / self.budget * np.pi)  # Improved oscillation for exploration-exploitation\n\n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.25:  # Earlier reduction in population size for faster convergence\n                pop_size = max(10, int(self.initial_population_size * (1 - 0.75 * (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(30 + int(10 * np.cos(np.pi * len(solution) / self.dim))):  # Changed the oscillation pattern\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 5, "feedback": "The algorithm AMDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08544 with standard deviation 0.04884.", "error": "", "parent_ids": ["0240b38a-96ce-4e3d-9d3d-d6b1229a5d82"], "operator": null, "metadata": {"aucs": [0.06779825015825502, 0.0539525194867686, 0.05433196575830357, 0.16252090855819257, 0.14863060798052097, 0.14957204147799041, 0.0424906365248412, 0.04271813079991993, 0.04696474079719837]}}
{"id": "ebbfcd2c-96df-4471-a8b1-2d7c158ce2e5", "fitness": 0.08557074507633892, "name": "EMADE", "description": "Slightly adjust the scaling factor `F` oscillation to improve exploration and exploitation balance in EMADE.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.8\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.35 * np.sin(evals / self.budget * np.pi)  # Adjusted oscillation amplitude\n\n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 6, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08557 with standard deviation 0.04918.", "error": "", "parent_ids": ["0240b38a-96ce-4e3d-9d3d-d6b1229a5d82"], "operator": null, "metadata": {"aucs": [0.04812470446086803, 0.0539525194867686, 0.05433196575830357, 0.1617059513810869, 0.14863060798052097, 0.15237923122956998, 0.06729061166525518, 0.04271813079991993, 0.041002982924757125]}}
{"id": "9cc00a50-9b55-45ac-8941-c080a758b331", "fitness": 0.08563631505245756, "name": "EMADE", "description": "Enhanced Memetic Adaptive Differential Evolution (EMADE) with dynamic exploration-exploitation balance using a phase-based adaptive strategy and improved local search with dynamic simplex resizing for efficient convergence. ", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.8\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        phase_threshold = self.budget // 4\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi)\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub, phase_threshold, evals)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub, phase_threshold, evals):\n        n = len(solution)\n        iteration_factor = 20 if evals < phase_threshold else 10\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(iteration_factor + int(5 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations dynamically\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 7, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08564 with standard deviation 0.05107.", "error": "", "parent_ids": ["0240b38a-96ce-4e3d-9d3d-d6b1229a5d82"], "operator": null, "metadata": {"aucs": [0.04812470446086803, 0.0539525194867686, 0.05433196575830357, 0.1724002929195868, 0.14863060798052097, 0.14961866500454968, 0.05994696613684325, 0.04271813079991993, 0.041002982924757125]}}
{"id": "48c18536-722d-4767-8548-8851d2875838", "fitness": 0.09465974865135207, "name": "EMADE", "description": "Slightly adjust the mutation factor `F` range to improve exploration-exploitation balance.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9  # Changed from 0.8 to 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi)\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 8, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09466 with standard deviation 0.04700.", "error": "", "parent_ids": ["0240b38a-96ce-4e3d-9d3d-d6b1229a5d82"], "operator": null, "metadata": {"aucs": [0.09977307915045863, 0.05705932751449039, 0.05866079952959369, 0.1580746303584749, 0.16090049155743358, 0.15374217789040534, 0.07147978790459675, 0.0458303098821734, 0.046417134074541955]}}
{"id": "e5924ef0-d4a5-45af-bf5b-c921fa311725", "fitness": 0.08362158758716272, "name": "EMADE", "description": "Introduce oscillating crossover probability to enhance adaptability.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9  # Changed from 0.8 to 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < (0.5 + 0.5 * np.sin(evals / self.budget * np.pi))  # Oscillating CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi)\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 9, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08362 with standard deviation 0.04795.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.05630617986560382, 0.04913330992010523, 0.04998857114475885, 0.15481551897409818, 0.14341058990970712, 0.1547729158866289, 0.044849561313122566, 0.042177264733537, 0.05714037653690285]}}
{"id": "593f03b0-a22f-4d86-ac8d-75d7ebb52362", "fitness": 0.09465974865135207, "name": "EMADE", "description": "Introduce adaptive crossover rate based on progress towards the budget to improve exploration-exploitation balance.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9  # Changed from 0.8 to 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi)\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n            \n            self.CR = 0.4 + 0.3 * np.cos(evals / self.budget * np.pi)  # Change\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 10, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09466 with standard deviation 0.04700.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.09977307915045863, 0.05705932751449039, 0.05866079952959369, 0.1580746303584749, 0.16090049155743358, 0.15374217789040534, 0.07147978790459675, 0.0458303098821734, 0.046417134074541955]}}
{"id": "535eb225-a964-4518-8fe0-9cc71919814c", "fitness": 0.09465974865135207, "name": "EMADE", "description": "Introduce a small adaptive adjustment to the crossover probability CR to enhance exploration-exploitation balance further.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi)\n            self.CR = 0.5 + 0.2 * np.cos(evals / self.budget * np.pi)  # Updated line\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 11, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09466 with standard deviation 0.04700.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.09977307915045863, 0.05705932751449039, 0.05866079952959369, 0.1580746303584749, 0.16090049155743358, 0.15374217789040534, 0.07147978790459675, 0.0458303098821734, 0.046417134074541955]}}
{"id": "3706d1a2-667f-40a9-8be2-4fd52bea6d6c", "fitness": 0.09465974865135207, "name": "EMADE", "description": "Introduce a random restart mechanism at 75% budget to enhance exploration.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9  # Changed from 0.8 to 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi)\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n            # Random Restart Mechanism\n            if evals / self.budget > 0.75 and evals % pop_size == 0:\n                pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n                fitness = np.array([func(ind) for ind in pop])\n                evals += pop_size\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 12, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09466 with standard deviation 0.04700.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.09977307915045863, 0.05705932751449039, 0.05866079952959369, 0.1580746303584749, 0.16090049155743358, 0.15374217789040534, 0.07147978790459675, 0.0458303098821734, 0.046417134074541955]}}
{"id": "ea1f2c37-e691-4e37-9c39-c7743d617561", "fitness": 0.08140832063452529, "name": "EMADEPlus", "description": "Introduce adaptive mutation factor and local search based on success rate to enhance convergence speed and robustness.", "code": "import numpy as np\n\nclass EMADEPlus:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n        self.successful_mutations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                self.F = 0.5 + 0.5 * (self.successful_mutations / max(1, evals))  # Adaptive mutation factor\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    self.successful_mutations += 1\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals < self.budget and self.successful_mutations > pop_size * 0.1:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 13, "feedback": "The algorithm EMADEPlus got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08141 with standard deviation 0.05123.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.05072960919149827, 0.045386324226960206, 0.044003781987153934, 0.1470347308547474, 0.16030097944030064, 0.15307178674884658, 0.04085540909325258, 0.03800581198342434, 0.05328645218454364]}}
{"id": "de8ca166-b452-4cd3-b954-bf4ce3101d86", "fitness": 0.09232822917691577, "name": "EMADE", "description": "Slightly adjust the crossover rate `CR` range to enhance genetic diversity.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.8  # Changed from 0.7 to 0.8\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi)\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 14, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09233 with standard deviation 0.04680.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.05958679800594713, 0.06630166786022729, 0.06280463179654217, 0.15318586234743703, 0.16527800779094914, 0.155779233921149, 0.06306032167987241, 0.05101271404676411, 0.053944825143353636]}}
{"id": "bd842c77-dc55-4ac3-b69d-b13320c15872", "fitness": 0.07779880752390828, "name": "EnhancedEMADE", "description": "Enhance EMADE by integrating a dynamic adaptive differential evolution strategy, refining mutation and crossover rates based on population diversity for improved convergence.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.5\n        self.CR = 0.9\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))  # Calculate population diversity\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                self.F = 0.5 + 0.3 * (1 - diversity)  # Adjust F based on diversity\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover_rate = self.CR * (1 - diversity)  # Adjust CR based on diversity\n                crossover = np.random.rand(self.dim) < crossover_rate\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 15, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07780 with standard deviation 0.04272.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.0425474989114355, 0.0639710962811898, 0.05436247038445641, 0.13238042883882783, 0.14557479553529917, 0.13308101288403928, 0.03396929408297744, 0.049534841551740705, 0.04476782924520839]}}
{"id": "8d5a4e77-80e6-4355-98a5-a1e5ee2571c4", "fitness": 0.09232822917691577, "name": "EMADE", "description": "Slightly enhance the crossover rate `CR` to improve solution diversity and convergence speed.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.8  # Changed from 0.7 to 0.8\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi)\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 16, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09233 with standard deviation 0.04680.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.05958679800594713, 0.06630166786022729, 0.06280463179654217, 0.15318586234743703, 0.16527800779094914, 0.155779233921149, 0.06306032167987241, 0.05101271404676411, 0.053944825143353636]}}
{"id": "767bc5f5-7240-4932-bf5e-615a91de031e", "fitness": 0.08722440398663338, "name": "EnhancedEMADE", "description": "Introduce multi-scale mutation rates and adaptive crossover based on population diversity for balanced exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9  # Adjust mutation factor\n        self.CR = 0.7  # Crossover rate\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            diversity = np.std(pop, axis=0).mean()\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.adaptive_crossover_rate(diversity)\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi) * (1 + diversity)\n\n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_crossover_rate(self, diversity):\n        return 0.5 + 0.2 * np.tanh(diversity - 0.5)\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08722 with standard deviation 0.04747.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.04960613074523734, 0.05705932751449039, 0.05866079952959369, 0.14624353925180134, 0.16090049155743358, 0.15374217789040534, 0.06655972543402333, 0.0458303098821734, 0.046417134074541955]}}
{"id": "1ffb4fe0-7d40-4e1d-b15b-4fb238c9024e", "fitness": 0.08963249491838982, "name": "EMADE", "description": "Slightly adjust crossover rate `CR` to enhance solution diversity.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.75  # Changed from 0.7 to 0.75\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi)\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 18, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08963 with standard deviation 0.04862.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.05291853516376388, 0.06630166786022729, 0.06280463179654217, 0.1523204529904203, 0.16527800779094914, 0.155779233921149, 0.04633238555233876, 0.05101271404676411, 0.053944825143353636]}}
{"id": "beaa366e-4c1f-41e7-b57e-efe8525ec93d", "fitness": 0.08448375886306791, "name": "EMADE", "description": "Introduce adaptive crossover rate and integrate Levy flight for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n        self.levy_exponent = 1.5\n        self.levy_scale = 0.01\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            dynamic_CR = self.CR * (1 - evals / self.budget)\n            \n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                crossover = np.random.rand(self.dim) < dynamic_CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi)\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n            \n            if evals < self.budget:\n                levy_solution = self.levy_flight(self.best_solution, lb, ub)\n                levy_fitness = func(levy_solution)\n                evals += 1\n                if levy_fitness < self.best_fitness:\n                    self.best_fitness = levy_fitness\n                    self.best_solution = levy_solution\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]\n\n    def levy_flight(self, current_pos, lb, ub):\n        step = np.random.standard_cauchy(size=self.dim) * self.levy_scale\n        step = step / np.abs(step)**(1/self.levy_exponent)\n        new_pos = np.clip(current_pos + step, lb, ub)\n        return new_pos", "configspace": "", "generation": 19, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08448 with standard deviation 0.05051.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.04398825881884383, 0.0570516489791667, 0.05865787018701185, 0.1517607092869212, 0.16087572330415234, 0.15372600093018074, 0.042051598935573975, 0.045827665603979884, 0.04641435372178071]}}
{"id": "e7eaeeeb-9ee7-49ac-8e3a-76afa6378694", "fitness": 0.0903955036179773, "name": "EMADE", "description": "Introduce a decay factor to the crossover rate (CR) for better convergence over iterations.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9  # Changed from 0.8 to 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            # Change: Apply a decay factor to crossover rate\n            self.CR *= 0.99  \n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi)\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 20, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09040 with standard deviation 0.05095.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.054603879079254125, 0.05705932751449039, 0.058660841840336375, 0.17069179822379388, 0.16090049155743358, 0.15374144396230338, 0.06565431136326627, 0.0458303098821734, 0.046417129138744295]}}
{"id": "30466180-f58d-431f-b5b3-9ed4cb56db0e", "fitness": 0.08809524794133179, "name": "EMADE", "description": "Use a dynamic crossover rate (CR) instead of a fixed one to balance exploration and exploitation better.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9  # Changed from 0.8 to 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < (0.6 + 0.4 * np.sin(evals / self.budget * np.pi))  # Changed from self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi)\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 21, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08810 with standard deviation 0.04632.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.05032437005854451, 0.06657530723661287, 0.0577391535534294, 0.14816482595782354, 0.15951243271146764, 0.15122394481875445, 0.06237944341579327, 0.0500631640592043, 0.04687458966035618]}}
{"id": "cf4b5dd3-ef63-4f9c-86a7-3417062815ad", "fitness": 0.08287020394279826, "name": "EMADE", "description": "Introduce a feedback-driven dynamic adaptation of mutation factor `F` and crossover rate `CR` based on fitness improvement to enhance convergence.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n        self.F = 0.9\n        self.CR = 0.7\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            improved = False\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    improved = True\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if improved:\n                self.F = min(1.0, self.F + 0.05)\n                self.CR = max(0.1, self.CR - 0.05)\n            else:\n                self.F = max(0.5, self.F - 0.05)\n                self.CR = min(0.9, self.CR + 0.05)\n\n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 22, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08287 with standard deviation 0.04756.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.04146310017634258, 0.06500779203024876, 0.05866079952959369, 0.1310002653277279, 0.16090049155743358, 0.15374217789040534, 0.04280976501671707, 0.0458303098821734, 0.046417134074541955]}}
{"id": "3e034ca7-905c-4ada-8c61-76059718194b", "fitness": 0.09465974865135207, "name": "EMADE", "description": "Slightly refine the crossover strategy to improve convergence speed.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9  # Changed from 0.8 to 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < (self.CR + 0.05 * np.sin(evals / self.budget * np.pi))\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi)\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 23, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09466 with standard deviation 0.04700.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.09977307915045863, 0.05705932751449039, 0.05866079952959369, 0.1580746303584749, 0.16090049155743358, 0.15374217789040534, 0.07147978790459675, 0.0458303098821734, 0.046417134074541955]}}
{"id": "611b2546-5139-48de-ab2d-f6e4aaee8439", "fitness": 0.08500942122024985, "name": "EMADE", "description": "Introduce a small random perturbation to the mutation factor `F` to slightly enhance diversity in the search process.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9  # Changed from 0.8 to 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi) + np.random.uniform(-0.05, 0.05)  # Slight random perturbation\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 24, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08501 with standard deviation 0.04978.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.043994693035576016, 0.05705932751449039, 0.05866079952959369, 0.15033487362038234, 0.16090049155743358, 0.15374217789040534, 0.04459136391633234, 0.0458303098821734, 0.0499707540358616]}}
{"id": "7e67eba4-379b-4bb1-bc6d-7971324ce1b7", "fitness": 0.09232822917691577, "name": "EMADE", "description": "Adjust the crossover rate `CR` to 0.8 to enhance the diversity of the population and exploration.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9  # Changed from 0.8 to 0.9\n        self.CR = 0.8  # Adjusted from 0.7 to 0.8\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi)\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 25, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09233 with standard deviation 0.04680.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.05958679800594713, 0.06630166786022729, 0.06280463179654217, 0.15318586234743703, 0.16527800779094914, 0.155779233921149, 0.06306032167987241, 0.05101271404676411, 0.053944825143353636]}}
{"id": "1b833b92-7c01-4402-bdfa-3299c57c2905", "fitness": 0.08733619724798322, "name": "EnhancedEMADE", "description": "Enhance dynamic population sizing and adaptive mutation factor to better balance exploration and exploitation throughout the optimization process.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            # Update mutation factor dynamically based on progress\n            self.F = 0.5 + 0.4 * np.cos(evals / self.budget * np.pi)\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # More aggressive dynamic population sizing based on evaluations\n            pop_size = max(5, int(self.initial_population_size * (1 - 0.5 * (evals / self.budget)**2)))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(15 + int(5 * np.sin(np.pi * len(solution) / self.dim))):\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 26, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08734 with standard deviation 0.04500.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.0730459291018829, 0.06041246506916387, 0.060222430257335535, 0.1396038594413921, 0.156886887200134, 0.15254925086511373, 0.05275517183341272, 0.043571864513341274, 0.04697791695007292]}}
{"id": "2f83e1e6-1b38-4a56-a5bd-24e7e9b6973c", "fitness": 0.07943739125216602, "name": "EMADE", "description": "Introduce a small random perturbation to the trial vector to enhance exploration.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9  # Changed from 0.8 to 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                # Introduce small random perturbation\n                trial += np.random.normal(0, 0.01, self.dim)  \n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.3 * np.sin(evals / self.budget * np.pi)\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 27, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07944 with standard deviation 0.05035.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.05159249258585996, 0.052128928793562346, 0.04225252867867235, 0.16672499944733832, 0.1466138244725097, 0.13529844963758153, 0.03987562332222039, 0.04367969723080789, 0.03676997710094165]}}
{"id": "eea568a5-0549-4d74-998d-411d6bcfc877", "fitness": 0.1191575030428755, "name": "EMADE", "description": "Implement a non-linear dynamic scaling factor for `F` to improve convergence by changing its computation formula.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9  # Changed from 0.8 to 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.4 * np.sin((evals / self.budget)**2 * np.pi)  # Changed the scaling factor formula\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 28, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11916 with standard deviation 0.07726.", "error": "", "parent_ids": ["48c18536-722d-4767-8548-8851d2875838"], "operator": null, "metadata": {"aucs": [0.28727972392793966, 0.06152466716632432, 0.05866079952959369, 0.17740850877457914, 0.16090049155743358, 0.15374217789040534, 0.08065371458288861, 0.0458303098821734, 0.046417134074541955]}}
{"id": "07ce4e51-4d51-4b66-ad1f-9a1ebe8ea1ee", "fitness": 0.08636262844410493, "name": "RefinedEMADE", "description": "Implement a modified dynamic scaling factor and adaptive population perturbation for enhanced convergence.", "code": "import numpy as np\n\nclass RefinedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                # New scaling factor varying with evaluations\n                self.F = 0.5 + 0.4 * np.sin(2 * np.pi * evals / (self.budget * 1.5))\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Adaptive Population Perturbation\n            if evals / self.budget > 0.5:\n                perturb_factor = 1 - (evals / self.budget)\n                pop += np.random.normal(0, perturb_factor, pop.shape)\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n\n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 29, "feedback": "The algorithm RefinedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08636 with standard deviation 0.04143.", "error": "", "parent_ids": ["eea568a5-0549-4d74-998d-411d6bcfc877"], "operator": null, "metadata": {"aucs": [0.06623798216301346, 0.053527608736328114, 0.07234949820466585, 0.13633694326138268, 0.14051562112205307, 0.1536651755253967, 0.05747299788773885, 0.040781424159958446, 0.056376404936407165]}}
{"id": "99666301-76f8-4fd0-b140-4afa475bbe8f", "fitness": 0.08263582971021738, "name": "EnhancedEMADE", "description": "Introduce a stochastic exploration component with dynamic adjustment to balance exploration and exploitation and improve solution diversity.  ", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                perturbation = np.random.normal(0, 0.1, self.dim)\n                mutant = np.clip(a + self.F * (b - c) + perturbation, lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.4 * np.sin((evals / self.budget)**2 * np.pi) + 0.1 * np.random.rand()\n\n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 30, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08264 with standard deviation 0.04628.", "error": "", "parent_ids": ["eea568a5-0549-4d74-998d-411d6bcfc877"], "operator": null, "metadata": {"aucs": [0.061604713707666625, 0.06101117795837996, 0.04932829078636436, 0.1524559193790257, 0.1485740245840973, 0.14082248035156386, 0.04396922747490917, 0.044677932522168784, 0.04127870062778072]}}
{"id": "bd971721-d14e-494e-846f-d9b3e9c105f7", "fitness": 0.07736647651001059, "name": "EMADE", "description": "Introduce a small random perturbation to the best solution before performing local search to potentially escape local optima.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9  # Changed from 0.8 to 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.4 * np.sin((evals / self.budget)**2 * np.pi)  # Changed the scaling factor formula\n            \n            if evals < self.budget:\n                perturbed_solution = np.clip(self.best_solution + np.random.uniform(-0.1, 0.1, self.dim), lb, ub)  # Small perturbation\n                local_search_solution = self.adaptive_simplex_search(func, perturbed_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 31, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07737 with standard deviation 0.04334.", "error": "", "parent_ids": ["eea568a5-0549-4d74-998d-411d6bcfc877"], "operator": null, "metadata": {"aucs": [0.05579413196613481, 0.05348925093261625, 0.057963828742121026, 0.13293135668534017, 0.14019111849600951, 0.14017925717536062, 0.03822851552414164, 0.038543981828663254, 0.03897684723970807]}}
{"id": "6891d33d-9bf0-43a4-9ca2-1d91aec69ae9", "fitness": 0.08974754587401713, "name": "EMADE", "description": "Introduce an adaptive mutation factor and local search jump distance to enhance exploration and exploitation balance in the black-box optimization process.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = self.F * (1 - evals / self.budget)  # Adaptive Mutation Factor\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.4 * np.sin((evals / self.budget)**2 * np.pi)\n\n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub, evals / self.budget)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub, progress):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        jump_distance = 0.05 * (1 - progress)  # Adaptive jump distance\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + jump_distance * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 32, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08975 with standard deviation 0.04913.", "error": "", "parent_ids": ["eea568a5-0549-4d74-998d-411d6bcfc877"], "operator": null, "metadata": {"aucs": [0.050872205027862716, 0.0721926249872813, 0.06448759432364726, 0.143351172096719, 0.16602680183716434, 0.16391926487042274, 0.04251434579920821, 0.05519598932636294, 0.04916791459748571]}}
{"id": "36110582-6d51-4891-94e6-20634a1f49e0", "fitness": 0.0793157349545274, "name": "EMADEEnhanced", "description": "Enhance convergence with adaptive mutation and crossover rates based on population diversity measures.", "code": "import numpy as np\n\nclass EMADEEnhanced:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))  # Calculate diversity of the population\n            self.F = 0.5 + 0.4 * np.sin((evals / self.budget)**2 * np.pi) * diversity\n            self.CR = 0.9 - 0.8 * np.sin((evals / self.budget) * np.pi) * (1 - diversity)\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n\n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 33, "feedback": "The algorithm EMADEEnhanced got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07932 with standard deviation 0.04699.", "error": "", "parent_ids": ["eea568a5-0549-4d74-998d-411d6bcfc877"], "operator": null, "metadata": {"aucs": [0.03650180488719956, 0.04832239465283128, 0.06591437818929491, 0.12804845301186885, 0.13967845801081025, 0.16272705462021275, 0.03236062623485836, 0.04765218471069366, 0.052636260272976965]}}
{"id": "f52166b6-dee5-40ba-afb1-b618f5393931", "fitness": 0.08592843999330703, "name": "EMADE", "description": "Adjust the crossover rate dynamically based on the evaluation progress to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.4 * np.sin((evals / self.budget)**2 * np.pi)\n            self.CR = 0.9 - 0.3 * (evals / self.budget)  # Adjust crossover rate dynamically\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 34, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08593 with standard deviation 0.04611.", "error": "", "parent_ids": ["eea568a5-0549-4d74-998d-411d6bcfc877"], "operator": null, "metadata": {"aucs": [0.05919872815370719, 0.06552447267177264, 0.05968706528870438, 0.13506390850976036, 0.16090049155743358, 0.15374217789040534, 0.04699167191126441, 0.0458303098821734, 0.046417134074541955]}}
{"id": "63cc43f3-e210-4df0-8968-e1daf82158b1", "fitness": 0.08287377254612871, "name": "EMADE", "description": "Improve the crossover strategy by introducing a non-uniform crossover probability to enhance exploration.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9  # Changed from 0.8 to 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                # Change: Implement non-uniform crossover probability\n                crossover = np.random.rand(self.dim) < (self.CR * (1 - evals / self.budget))\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.4 * np.sin((evals / self.budget)**2 * np.pi)  # Changed the scaling factor formula\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 35, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08287 with standard deviation 0.04793.", "error": "", "parent_ids": ["eea568a5-0549-4d74-998d-411d6bcfc877"], "operator": null, "metadata": {"aucs": [0.041686297044701726, 0.06182586229945375, 0.058660841840336375, 0.13336243450882046, 0.16090049155743358, 0.15374144396230338, 0.04031438180018543, 0.0458303098821734, 0.04954189001975029]}}
{"id": "ff6f4f4a-1200-4082-802d-cebaebedad3e", "fitness": 0.1191575030428755, "name": "EMADE", "description": "Introduce non-linear variation in crossover rate (CR) to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9  # Changed from 0.8 to 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.4 * np.sin((evals / self.budget)**2 * np.pi)  # Changed the scaling factor formula\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n            self.CR = 0.5 + 0.2 * np.cos((evals / self.budget)**2 * np.pi)  # Introduced non-linear variation for CR\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 36, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11916 with standard deviation 0.07726.", "error": "", "parent_ids": ["eea568a5-0549-4d74-998d-411d6bcfc877"], "operator": null, "metadata": {"aucs": [0.28727972392793966, 0.06152466716632432, 0.05866079952959369, 0.17740850877457914, 0.16090049155743358, 0.15374217789040534, 0.08065371458288861, 0.0458303098821734, 0.046417134074541955]}}
{"id": "5d7e4922-2d75-4449-855c-610af6740e52", "fitness": 0.09101612729984497, "name": "EMADE", "description": "Enhance mutation strategy by incorporating a random scaling factor to diversify search.", "code": "import numpy as np\n\nclass EMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9  # Changed from 0.8 to 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = self.F * np.random.uniform(0.8, 1.2)  # Introduced random scaling factor\n                mutant = np.clip(a + F_dynamic * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.4 * np.sin((evals / self.budget)**2 * np.pi)  # Changed the scaling factor formula\n            \n            if evals < self.budget:\n                local_search_solution = self.adaptive_simplex_search(func, self.best_solution, lb, ub)\n                local_search_fitness = func(local_search_solution)\n                evals += 1\n                if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_solution = local_search_solution\n\n            # Dynamic Population Sizing\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_simplex_search(self, func, solution, lb, ub):\n        n = len(solution)\n        simplex = np.zeros((n + 1, n))\n        simplex[0] = solution\n        for i in range(1, n + 1):\n            y = np.copy(solution)\n            y[i - 1] = y[i - 1] + 0.05 * (ub[i - 1] - lb[i - 1])\n            simplex[i] = y\n\n        for _ in range(20 + int(10 * np.sin(np.pi * len(solution) / self.dim))):  # Adapt number of iterations\n            f_values = np.array([func(x) for x in simplex])\n            i_h = np.argmax(f_values)\n            i_l = np.argmin(f_values)\n            \n            centroid = np.mean(simplex[[i for i in range(n + 1) if i != i_h]], axis=0)\n            x_r = np.clip(centroid + (centroid - simplex[i_h]), lb, ub)\n            f_xr = func(x_r)\n\n            if f_xr < f_values[i_l]:\n                x_e = np.clip(centroid + 2 * (centroid - simplex[i_h]), lb, ub)\n                f_xe = func(x_e)\n                if f_xe < f_xr:\n                    simplex[i_h] = x_e\n                else:\n                    simplex[i_h] = x_r\n            elif f_xr < f_values[i_h]:\n                simplex[i_h] = x_r\n            else:\n                for j in range(n + 1):\n                    if j != i_l:\n                        simplex[j] = simplex[i_l] + 0.5 * (simplex[j] - simplex[i_l])\n\n        return simplex[i_l]", "configspace": "", "generation": 37, "feedback": "The algorithm EMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09102 with standard deviation 0.04792.", "error": "", "parent_ids": ["eea568a5-0549-4d74-998d-411d6bcfc877"], "operator": null, "metadata": {"aucs": [0.07860008730864176, 0.0573928118493221, 0.04968291468262209, 0.1711506535573203, 0.1502322735454681, 0.1498958040050784, 0.0647194021751828, 0.04608713856645297, 0.05138406000851625]}}
{"id": "e3bf30c9-fe9e-46b4-8c77-ac557bdfec50", "fitness": 0.140229143981793, "name": "EnhancedEMADE", "description": "Integrate adaptive control of crossover rate and apply neighborhood-based perturbation to enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                self.CR = 0.5 + 0.5 * np.cos(np.pi * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.4 * np.sin((evals / self.budget)**2 * np.pi)\n\n            if evals < self.budget:\n                neighbors = self.get_neighbors(pop, self.best_solution, lb, ub)\n                for neighbor in neighbors:\n                    ns_fitness = func(neighbor)\n                    evals += 1\n                    if ns_fitness < self.best_fitness:\n                        self.best_fitness = ns_fitness\n                        self.best_solution = neighbor\n\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def get_neighbors(self, pop, solution, lb, ub):\n        epsilon = 0.1 * (ub - lb)\n        neighbors = [np.clip(solution + np.random.uniform(-epsilon, epsilon), lb, ub) for _ in range(5)]\n        return neighbors", "configspace": "", "generation": 38, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.14023 with standard deviation 0.05406.", "error": "", "parent_ids": ["eea568a5-0549-4d74-998d-411d6bcfc877"], "operator": null, "metadata": {"aucs": [0.08199642299717225, 0.24122634649586638, 0.08241998192612376, 0.1677851738052717, 0.1687377065180864, 0.1721642090399601, 0.17258881896182632, 0.09705129048691985, 0.07809234560491041]}}
{"id": "e67b9ce3-ad5c-4c8d-a83b-9698dd517a2d", "fitness": -Infinity, "name": "EnhancedEMADE_Improved", "description": "Introduce adaptive population size and chaotic initial distribution to improve exploration-exploitation dynamics.", "code": "import numpy as np\n\nclass EnhancedEMADE_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = self.initialize_population(lb, ub, pop_size)\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                self.CR = 0.5 + 0.5 * np.cos(np.pi * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.4 * np.sin((evals / self.budget)**2 * np.pi)\n\n            if evals < self.budget:\n                neighbors = self.get_neighbors(pop, self.best_solution, lb, ub)\n                for neighbor in neighbors:\n                    ns_fitness = func(neighbor)\n                    evals += 1\n                    if ns_fitness < self.best_fitness:\n                        self.best_fitness = ns_fitness\n                        self.best_solution = neighbor\n\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n                pop = self.adapt_population(pop, fitness, pop_size, lb, ub)\n\n        return self.best_solution, self.best_fitness\n\n    def initialize_population(self, lb, ub, pop_size):\n        chaos = np.random.rand(pop_size, self.dim)\n        pop = lb + (ub - lb) * chaos\n        return pop\n\n    def get_neighbors(self, pop, solution, lb, ub):\n        epsilon = 0.1 * (ub - lb)\n        neighbors = [np.clip(solution + np.random.uniform(-epsilon, epsilon), lb, ub) for _ in range(5)]\n        return neighbors\n\n    def adapt_population(self, pop, fitness, new_pop_size, lb, ub):\n        sorted_indices = np.argsort(fitness)\n        pop = pop[sorted_indices[:new_pop_size]]\n        while len(pop) < new_pop_size:\n            new_individual = np.random.uniform(lb, ub, self.dim)\n            pop = np.vstack([pop, new_individual])\n        return pop", "configspace": "", "generation": 39, "feedback": "An exception occurred: IndexError('index 17 is out of bounds for axis 0 with size 10').", "error": "IndexError('index 17 is out of bounds for axis 0 with size 10')", "parent_ids": ["e3bf30c9-fe9e-46b4-8c77-ac557bdfec50"], "operator": null, "metadata": {}}
{"id": "a3ac88ad-4264-4514-a85b-219bb6dd0212", "fitness": 0.10046365803649543, "name": "EnhancedEMADE", "description": "Refine the mutation strategy with adaptive scaling factor to improve solution diversity and convergence.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F_adaptive = 0.5 + 0.5 * np.sin(np.pi * (fitness[i] / self.best_fitness))  # Adaptive scaling factor\n                mutant = np.clip(a + F_adaptive * (b - c), lb, ub)  # Use adaptive F\n\n                self.CR = 0.5 + 0.5 * np.cos(np.pi * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            self.F = 0.5 + 0.4 * np.sin((evals / self.budget)**2 * np.pi)\n\n            if evals < self.budget:\n                neighbors = self.get_neighbors(pop, self.best_solution, lb, ub)\n                for neighbor in neighbors:\n                    ns_fitness = func(neighbor)\n                    evals += 1\n                    if ns_fitness < self.best_fitness:\n                        self.best_fitness = ns_fitness\n                        self.best_solution = neighbor\n\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget))))\n\n        return self.best_solution, self.best_fitness\n\n    def get_neighbors(self, pop, solution, lb, ub):\n        epsilon = 0.1 * (ub - lb)\n        neighbors = [np.clip(solution + np.random.uniform(-epsilon, epsilon), lb, ub) for _ in range(5)]\n        return neighbors", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10046 with standard deviation 0.04657.", "error": "", "parent_ids": ["e3bf30c9-fe9e-46b4-8c77-ac557bdfec50"], "operator": null, "metadata": {"aucs": [0.08826028702728039, 0.08248115441066006, 0.06576328174607338, 0.16963558851213345, 0.16569797850688395, 0.1583649330753979, 0.055689072440377685, 0.06274407884577038, 0.05553654776388173]}}
{"id": "278388d7-3eba-43a5-807c-1796e4c9c604", "fitness": 0.1692097309202512, "name": "EnhancedEMADE", "description": "Introduce adaptive mutation scaling and employ convergence-based population resizing.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                self.F = 0.5 + 0.4 * np.cos((evals / self.budget) * np.pi)  # Changed line\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                self.CR = 0.5 + 0.5 * np.cos(np.pi * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals < self.budget:\n                neighbors = self.get_neighbors(pop, self.best_solution, lb, ub)\n                for neighbor in neighbors:\n                    ns_fitness = func(neighbor)\n                    evals += 1\n                    if ns_fitness < self.best_fitness:\n                        self.best_fitness = ns_fitness\n                        self.best_solution = neighbor\n\n            if evals / self.budget > 0.5:\n                pop_size = max(10, int(self.initial_population_size * (1 - (evals / self.budget)**1.2)))  # Changed line\n\n        return self.best_solution, self.best_fitness\n\n    def get_neighbors(self, pop, solution, lb, ub):\n        epsilon = 0.1 * (ub - lb)\n        neighbors = [np.clip(solution + np.random.uniform(-epsilon, epsilon), lb, ub) for _ in range(5)]\n        return neighbors", "configspace": "", "generation": 41, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.16921 with standard deviation 0.09074.", "error": "", "parent_ids": ["e3bf30c9-fe9e-46b4-8c77-ac557bdfec50"], "operator": null, "metadata": {"aucs": [0.31469794340126434, 0.32895186614966676, 0.11855091524523631, 0.16985870243451362, 0.17563350835558045, 0.17864701849399578, 0.07715932482483101, 0.09034453628907557, 0.06904376308809712]}}
{"id": "0f93854e-40a0-422f-b400-986e5883b58a", "fitness": 0.2019657556626644, "name": "EnhancedEMADE", "description": "Enhance exploration and exploitation balance with dynamic population size and localized search.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                self.F = 0.5 + 0.4 * np.cos((evals / self.budget) * np.pi)\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                self.CR = 0.5 + 0.5 * np.cos(np.pi * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals < self.budget:\n                neighbors = self.get_neighbors(pop, self.best_solution, lb, ub)\n                for neighbor in neighbors:\n                    ns_fitness = func(neighbor)\n                    evals += 1\n                    if ns_fitness < self.best_fitness:\n                        self.best_fitness = ns_fitness\n                        self.best_solution = neighbor\n\n            if evals / self.budget > 0.3:  # Changed line\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.5)))  # Changed line\n\n            if np.random.rand() < 0.1:  # Changed line\n                new_individual = np.random.uniform(lb, ub, self.dim)  # Changed line\n                new_fitness = func(new_individual)  # Changed line\n                evals += 1  # Changed line\n                if new_fitness < self.best_fitness:  # Changed line\n                    self.best_fitness = new_fitness  # Changed line\n                    self.best_solution = new_individual  # Changed line\n\n        return self.best_solution, self.best_fitness\n\n    def get_neighbors(self, pop, solution, lb, ub):\n        epsilon = 0.1 * (ub - lb)\n        neighbors = [np.clip(solution + np.random.uniform(-epsilon, epsilon), lb, ub) for _ in range(5)]\n        return neighbors", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20197 with standard deviation 0.08434.", "error": "", "parent_ids": ["278388d7-3eba-43a5-807c-1796e4c9c604"], "operator": null, "metadata": {"aucs": [0.20574894680231393, 0.17759357802493514, 0.08548609211132885, 0.17509442516873874, 0.1724135752192515, 0.2571659715078618, 0.1690676851097418, 0.40894865789179813, 0.16617286912800955]}}
{"id": "6d7ec696-d998-4725-ac1a-348ebbb74bf6", "fitness": 0.1911513283093235, "name": "AdaptiveFuzzyEMADE", "description": "Adaptive Differential Evolution with Fuzzy Logic for dynamic parameter tuning and localized search.", "code": "import numpy as np\n\nclass AdaptiveFuzzyEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                self.F = self.fuzzy_F(evals, self.budget)\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                self.CR = self.fuzzy_CR(evals, self.budget)\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals < self.budget:\n                neighbors = self.get_neighbors(pop, self.best_solution, lb, ub)\n                for neighbor in neighbors:\n                    ns_fitness = func(neighbor)\n                    evals += 1\n                    if ns_fitness < self.best_fitness:\n                        self.best_fitness = ns_fitness\n                        self.best_solution = neighbor\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness\n\n    def get_neighbors(self, pop, solution, lb, ub):\n        epsilon = 0.1 * (ub - lb)\n        neighbors = [np.clip(solution + np.random.uniform(-epsilon, epsilon), lb, ub) for _ in range(5)]\n        return neighbors\n\n    def fuzzy_F(self, evals, budget):\n        progress = evals / budget\n        if progress < 0.3:\n            return 0.9\n        elif progress < 0.7:\n            return 0.5 + 0.4 * np.cos(progress * np.pi)\n        else:\n            return 0.2\n\n    def fuzzy_CR(self, evals, budget):\n        progress = evals / budget\n        if progress < 0.3:\n            return 0.9\n        elif progress < 0.7:\n            return 0.5 + 0.5 * np.cos(np.pi * progress)\n        else:\n            return 0.3", "configspace": "", "generation": 43, "feedback": "The algorithm AdaptiveFuzzyEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19115 with standard deviation 0.08237.", "error": "", "parent_ids": ["0f93854e-40a0-422f-b400-986e5883b58a"], "operator": null, "metadata": {"aucs": [0.19826515884735352, 0.0876976314090796, 0.22061923823382934, 0.16320518127235717, 0.1669464952408758, 0.16425684673832697, 0.38017827088463396, 0.09649610080368665, 0.24269703135376863]}}
{"id": "87e773f4-6f46-430d-b71c-a1a2fd28a270", "fitness": 0.10339301028773448, "name": "AdaptiveEMADE", "description": "Introduce adaptive learning rates and a self-adjusting search pattern to enhance convergence speed and solution accuracy.", "code": "import numpy as np\n\nclass AdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                self.F = 0.5 + 0.4 * np.sin((evals / self.budget) * np.pi)\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                self.CR = 0.5 + 0.5 * np.sin(np.pi * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals < self.budget:\n                neighbors = self.get_neighbors(pop, self.best_solution, lb, ub)\n                for neighbor in neighbors:\n                    ns_fitness = func(neighbor)\n                    evals += 1\n                    if ns_fitness < self.best_fitness:\n                        self.best_fitness = ns_fitness\n                        self.best_solution = neighbor\n\n            if evals / self.budget > 0.3:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.5)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n            if evals / self.budget > 0.5:\n                self.CR = 0.3 + 0.7 * np.cos((evals / self.budget) * np.pi)\n\n            if np.random.rand() < 0.05:\n                self.dynamic_exploration(lb, ub, func, evals)\n\n        return self.best_solution, self.best_fitness\n\n    def get_neighbors(self, pop, solution, lb, ub):\n        epsilon = 0.1 * (ub - lb)\n        neighbors = [np.clip(solution + np.random.uniform(-epsilon, epsilon), lb, ub) for _ in range(5)]\n        return neighbors\n\n    def dynamic_exploration(self, lb, ub, func, evals):\n        candidate = np.random.uniform(lb, ub, self.dim)\n        candidate_fitness = func(candidate)\n        evals += 1\n        if candidate_fitness < self.best_fitness:\n            self.best_fitness = candidate_fitness\n            self.best_solution = candidate", "configspace": "", "generation": 44, "feedback": "The algorithm AdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10339 with standard deviation 0.04272.", "error": "", "parent_ids": ["0f93854e-40a0-422f-b400-986e5883b58a"], "operator": null, "metadata": {"aucs": [0.13060024031112438, 0.07313279247790883, 0.06962928946074853, 0.1655765357366058, 0.15234163682596713, 0.1517220171665098, 0.06077380635363572, 0.06575238913722559, 0.0610083851198846]}}
{"id": "c35c52f8-c15c-44c0-9d68-5b5569efcd54", "fitness": 0.17088497365124494, "name": "EnhancedEMADE", "description": "Adjust dynamic population size based on budget usage for better exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                self.F = 0.5 + 0.4 * np.cos((evals / self.budget) * np.pi)\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                self.CR = 0.5 + 0.5 * np.cos(np.pi * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals < self.budget:\n                neighbors = self.get_neighbors(pop, self.best_solution, lb, ub)\n                for neighbor in neighbors:\n                    ns_fitness = func(neighbor)\n                    evals += 1\n                    if ns_fitness < self.best_fitness:\n                        self.best_fitness = ns_fitness\n                        self.best_solution = neighbor\n\n            if evals / self.budget > 0.3:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.4)))  # Changed line\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness\n\n    def get_neighbors(self, pop, solution, lb, ub):\n        epsilon = 0.1 * (ub - lb)\n        neighbors = [np.clip(solution + np.random.uniform(-epsilon, epsilon), lb, ub) for _ in range(5)]\n        return neighbors", "configspace": "", "generation": 45, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17088 with standard deviation 0.06296.", "error": "", "parent_ids": ["0f93854e-40a0-422f-b400-986e5883b58a"], "operator": null, "metadata": {"aucs": [0.11050299059750579, 0.18722927514691967, 0.08471807307643608, 0.17313454410303075, 0.18993508152752914, 0.1873415032040816, 0.2638162258621979, 0.25743686719181247, 0.08385020215169126]}}
{"id": "5983f2e2-4b93-4b67-9cdc-055066052398", "fitness": 0.2019657556626644, "name": "EnhancedEMADE", "description": "Introduce adaptive mutation factor scaling for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                self.F = 0.5 + 0.4 * np.cos((evals / self.budget) * np.pi)\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                self.CR = 0.5 + 0.5 * np.cos(np.pi * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals < self.budget:\n                neighbors = self.get_neighbors(pop, self.best_solution, lb, ub)\n                for neighbor in neighbors:\n                    ns_fitness = func(neighbor)\n                    evals += 1\n                    if ns_fitness < self.best_fitness:\n                        self.best_fitness = ns_fitness\n                        self.best_solution = neighbor\n\n            if evals / self.budget > 0.3:  \n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.5)))  \n\n            if np.random.rand() < 0.1:  \n                new_individual = np.random.uniform(lb, ub, self.dim)  \n                new_fitness = func(new_individual)  \n                evals += 1  \n                if new_fitness < self.best_fitness:  \n                    self.best_fitness = new_fitness  \n                    self.best_solution = new_individual  \n\n            # Change: Adaptive mutation factor scaling\n            self.F = 0.5 + 0.4 * np.sin((evals / self.budget) * np.pi)\n\n        return self.best_solution, self.best_fitness\n\n    def get_neighbors(self, pop, solution, lb, ub):\n        epsilon = 0.1 * (ub - lb)\n        neighbors = [np.clip(solution + np.random.uniform(-epsilon, epsilon), lb, ub) for _ in range(5)]\n        return neighbors", "configspace": "", "generation": 46, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20197 with standard deviation 0.08434.", "error": "", "parent_ids": ["0f93854e-40a0-422f-b400-986e5883b58a"], "operator": null, "metadata": {"aucs": [0.20574894680231393, 0.17759357802493514, 0.08548609211132885, 0.17509442516873874, 0.1724135752192515, 0.2571659715078618, 0.1690676851097418, 0.40894865789179813, 0.16617286912800955]}}
{"id": "ea4a6804-3204-47d0-8f1b-e67d9ab53e9b", "fitness": 0.09621712501446032, "name": "EnhancedEMADEv2", "description": "Adapt exploration-exploitation balance using dynamic strategies with self-adaptive mutation factor and differential learning.", "code": "import numpy as np\n\nclass EnhancedEMADEv2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                self.F = 0.5 + 0.4 * np.sin((evals / self.budget) * np.pi)\n\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                self.CR = 0.5 + 0.5 * np.sin(np.pi * (evals / self.budget))\n                \n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals < self.budget:\n                enhanced_neighbors = self.get_enhanced_neighbors(pop, self.best_solution, lb, ub)\n                for neighbor in enhanced_neighbors:\n                    ns_fitness = func(neighbor)\n                    evals += 1\n                    if ns_fitness < self.best_fitness:\n                        self.best_fitness = ns_fitness\n                        self.best_solution = neighbor\n\n            if evals / self.budget > 0.3:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.7)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness\n\n    def get_enhanced_neighbors(self, pop, solution, lb, ub):\n        epsilon = 0.1 * (ub - lb)\n        differential_learning_rate = np.random.uniform(0.1, 0.5)\n        neighbors = [np.clip(solution + np.random.uniform(-epsilon, epsilon) * differential_learning_rate, lb, ub) for _ in range(10)]\n        return neighbors", "configspace": "", "generation": 47, "feedback": "The algorithm EnhancedEMADEv2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09622 with standard deviation 0.04087.", "error": "", "parent_ids": ["0f93854e-40a0-422f-b400-986e5883b58a"], "operator": null, "metadata": {"aucs": [0.06684961904808584, 0.07806031335181096, 0.0680464793394574, 0.15185963437356198, 0.16094931000604296, 0.14733023502466247, 0.05951920929001908, 0.07201430766711725, 0.061325017029384976]}}
{"id": "4bf335a4-d3f7-481b-b21f-5462edc35033", "fitness": 0.15924072935232103, "name": "AdaptiveMultiFidelityEMADE", "description": "Introduce adaptive learning rates and multi-fidelity evaluations to balance exploration and exploitation dynamically.", "code": "import numpy as np\n\nclass AdaptiveMultiFidelityEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                dynamic_F = 0.5 + 0.4 * np.cos((evals / self.budget) * np.pi)\n                mutant = np.clip(a + dynamic_F * (b - c), lb, ub)\n\n                dynamic_CR = 0.5 + 0.5 * np.cos(np.pi * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < dynamic_CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                if evals % 5 == 0:  # Apply multi-fidelity evaluation periodically\n                    trial_fitness = func(trial) * np.random.uniform(0.8, 1.2)\n                else:\n                    trial_fitness = func(trial)\n\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals < self.budget:\n                neighbors = self.get_neighbors(pop, self.best_solution, lb, ub)\n                for neighbor in neighbors:\n                    ns_fitness = func(neighbor)\n                    evals += 1\n                    if ns_fitness < self.best_fitness:\n                        self.best_fitness = ns_fitness\n                        self.best_solution = neighbor\n\n            if evals / self.budget > 0.3:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.5)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness\n\n    def get_neighbors(self, pop, solution, lb, ub):\n        epsilon = 0.1 * (ub - lb)\n        neighbors = [np.clip(solution + np.random.uniform(-epsilon, epsilon), lb, ub) for _ in range(5)]\n        return neighbors", "configspace": "", "generation": 48, "feedback": "The algorithm AdaptiveMultiFidelityEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.15924 with standard deviation 0.08911.", "error": "", "parent_ids": ["0f93854e-40a0-422f-b400-986e5883b58a"], "operator": null, "metadata": {"aucs": [0.35807996872051573, 0.09800635447646622, 0.07705341347015782, 0.16836309473265598, 0.17769906924245382, 0.1655681858375938, 0.23859413765211035, 0.0897603353751073, 0.06004200466382836]}}
{"id": "b97a50e0-3755-450c-9b25-46e9067010d9", "fitness": 0.1156972492212675, "name": "RefinedEMADE", "description": "Introduce adaptive mutation scaling and stochastic gradient refinement to enhance convergence toward the global optimum.", "code": "import numpy as np\n\nclass RefinedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                # Adaptive mutation factor\n                F = 0.5 + 0.4 * np.sin((evals / self.budget) * np.pi)\n                mutant = np.clip(a + F * (b - c), lb, ub)\n\n                # Adaptive crossover rate\n                CR = 0.6 + 0.4 * (1 - np.exp(-3 * evals / self.budget))\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals < self.budget:\n                neighbors = self.get_neighbors(pop, self.best_solution, lb, ub)\n                for neighbor in neighbors:\n                    ns_fitness = func(neighbor)\n                    evals += 1\n                    if ns_fitness < self.best_fitness:\n                        self.best_fitness = ns_fitness\n                        self.best_solution = neighbor\n\n            if evals / self.budget > 0.3:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.5)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n            # Stochastic gradient refinement\n            if np.random.rand() < 0.05 and evals < self.budget:\n                refined_individual = self.stochastic_gradient_refinement(self.best_solution, func, lb, ub)\n                refined_fitness = func(refined_individual)\n                evals += 1\n                if refined_fitness < self.best_fitness:\n                    self.best_fitness = refined_fitness\n                    self.best_solution = refined_individual\n\n        return self.best_solution, self.best_fitness\n\n    def get_neighbors(self, pop, solution, lb, ub):\n        epsilon = 0.1 * (ub - lb)\n        neighbors = [np.clip(solution + np.random.uniform(-epsilon, epsilon), lb, ub) for _ in range(5)]\n        return neighbors\n\n    def stochastic_gradient_refinement(self, solution, func, lb, ub):\n        learning_rate = 0.01\n        gradient = np.random.normal(0, 1, self.dim)\n        refined_solution = solution - learning_rate * gradient\n        return np.clip(refined_solution, lb, ub)", "configspace": "", "generation": 49, "feedback": "The algorithm RefinedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11570 with standard deviation 0.05214.", "error": "", "parent_ids": ["0f93854e-40a0-422f-b400-986e5883b58a"], "operator": null, "metadata": {"aucs": [0.0857636117501942, 0.21770302308358525, 0.08204071493896181, 0.1556429383941461, 0.14930137878247807, 0.15489217322050874, 0.061118372309707514, 0.07531042749445072, 0.0595026030173752]}}
{"id": "b14c4a51-ad80-49e8-b2a7-38a1703a3c74", "fitness": 0.2377780205960356, "name": "EnhancedEMADE", "description": "Implement adaptive mutation scaling for enhanced exploration efficacy.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.F = 0.9\n        self.CR = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                self.F = 0.5 + 0.4 * np.cos((evals / self.budget) * np.pi)\n                mutant = np.clip(a + self.F * (b - c) * (0.8 + 0.2 * np.random.rand()), lb, ub)  # Changed line\n\n                self.CR = 0.5 + 0.5 * np.cos(np.pi * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals < self.budget:\n                neighbors = self.get_neighbors(pop, self.best_solution, lb, ub)\n                for neighbor in neighbors:\n                    ns_fitness = func(neighbor)\n                    evals += 1\n                    if ns_fitness < self.best_fitness:\n                        self.best_fitness = ns_fitness\n                        self.best_solution = neighbor\n\n            if evals / self.budget > 0.3:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.5)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness\n\n    def get_neighbors(self, pop, solution, lb, ub):\n        epsilon = 0.1 * (ub - lb)\n        neighbors = [np.clip(solution + np.random.uniform(-epsilon, epsilon), lb, ub) for _ in range(5)]\n        return neighbors", "configspace": "", "generation": 50, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.23778 with standard deviation 0.08818.", "error": "", "parent_ids": ["0f93854e-40a0-422f-b400-986e5883b58a"], "operator": null, "metadata": {"aucs": [0.3645229793625052, 0.0820421053046888, 0.18794874146519402, 0.1808474532602845, 0.1671527945253286, 0.3375327122303665, 0.2591107844020134, 0.23087053281328418, 0.32997408200065526]}}
{"id": "d994329c-e63b-4b92-b89b-b377e97e49c1", "fitness": 0.37584563377138824, "name": "RefinedEMADE", "description": "Utilize dynamic mutation scaling and adaptive population resizing for improved search adaptability and convergence.", "code": "import numpy as np\n\nclass RefinedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Dynamic mutation scaling\n                F = 0.5 + 0.4 * np.cos((evals / self.budget) * np.pi)\n                mutant = np.clip(a + F * (b - c), lb, ub)\n\n                # Adaptive crossover\n                CR = 0.5 + 0.5 * np.cos(np.pi * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Adaptive population resizing\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.2)))\n\n            # Random exploration\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 51, "feedback": "The algorithm RefinedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.37585 with standard deviation 0.04371.", "error": "", "parent_ids": ["b14c4a51-ad80-49e8-b2a7-38a1703a3c74"], "operator": null, "metadata": {"aucs": [0.28011068504189707, 0.4153731305082362, 0.42693877004160474, 0.39457097303964395, 0.3372241605653534, 0.3671554258083831, 0.3913048771454375, 0.35728035806748304, 0.41265232372445526]}}
{"id": "1aa94434-dc10-484b-91af-f82a16054482", "fitness": 0.3797153301592099, "name": "RefinedEMADE", "description": "Integrate multi-strategy mutation and adaptive learning rate to enhance exploration and convergence.", "code": "import numpy as np\n\nclass RefinedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Multi-strategy mutation\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                # Adaptive learning rate\n                CR = 0.5 + 0.3 * np.cos(3.14 * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Adaptive population resizing\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.2)))\n\n            # Random exploration\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 52, "feedback": "The algorithm RefinedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.37972 with standard deviation 0.05474.", "error": "", "parent_ids": ["d994329c-e63b-4b92-b89b-b377e97e49c1"], "operator": null, "metadata": {"aucs": [0.3408847683411066, 0.41398421251115747, 0.44071606777909367, 0.42959125517281294, 0.3301005109791266, 0.3022715559822846, 0.3989965395485193, 0.44842681111471905, 0.31246625000406825]}}
{"id": "062180ff-9460-484d-9d34-7d8a3559cd93", "fitness": 0.22184038115219945, "name": "RefinedEMADE", "description": "Enhance exploration and exploitation by incorporating a dynamic crossover strategy and chaotic maps for diversity.", "code": "import numpy as np\n\nclass RefinedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n        self.chaotic_sequence = self.init_chaotic_sequence()\n\n    def init_chaotic_sequence(self):\n        x = 0.7  # Initial value for the chaotic sequence\n        sequence = []\n        for _ in range(self.budget):\n            x = 4 * x * (1 - x)  # Logistic map\n            sequence.append(x)\n        return sequence\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Multi-strategy mutation\n                if self.chaotic_sequence[evals % len(self.chaotic_sequence)] < 0.5:\n                    mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                # Dynamic crossover strategy\n                CR = 0.5 + (0.5 - self.chaotic_sequence[evals % len(self.chaotic_sequence)]) * np.cos(3.14 * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Adaptive population resizing\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.2)))\n\n            # Random exploration\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 53, "feedback": "The algorithm RefinedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22184 with standard deviation 0.08420.", "error": "", "parent_ids": ["1aa94434-dc10-484b-91af-f82a16054482"], "operator": null, "metadata": {"aucs": [0.21381008413066582, 0.3516895168479013, 0.2759380031965485, 0.18059630760223544, 0.17596684652368366, 0.17131278305240583, 0.35361265435636724, 0.0831085982184917, 0.19052863644149565]}}
{"id": "9bd6102b-2520-4835-8479-6b2205617190", "fitness": 0.1993763769418926, "name": "RefinedEMADE", "description": "Enhance exploration and exploitation balance by refining mutation strategy and introducing dynamic crossover.", "code": "import numpy as np\n\nclass RefinedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Multi-strategy mutation\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + 0.9 * (b - c), lb, ub)  # Changed\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.6 * ((b + c) / 2 - d), lb, ub)  # Changed\n\n                # Adaptive learning rate\n                CR = 0.3 + 0.4 * np.sin(3.14 * (evals / self.budget))  # Changed\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Adaptive population resizing\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.2)))\n\n            # Random exploration\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 54, "feedback": "The algorithm RefinedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19938 with standard deviation 0.04790.", "error": "", "parent_ids": ["1aa94434-dc10-484b-91af-f82a16054482"], "operator": null, "metadata": {"aucs": [0.2298055913061312, 0.11786555750800254, 0.19747041688474654, 0.16224466561702855, 0.16783828415273716, 0.18770485729055597, 0.28774983251504327, 0.25141204317774124, 0.19229614402504713]}}
{"id": "840cf0c2-b79b-431c-b946-73afa5ca0161", "fitness": 0.2611524349570198, "name": "RefinedEMADE", "description": "Enhance diversity and convergence speed by incorporating Lévy flights and tournament selection.", "code": "import numpy as np\n\nclass RefinedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Multi-strategy mutation with Lévy flights\n                if np.random.rand() < 0.5:\n                    step = np.random.standard_cauchy(self.dim)\n                    mutant = np.clip(a + 0.8 * (b - c) + step, lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n                \n                # Adaptive learning rate\n                CR = 0.5 + 0.3 * np.cos(3.14 * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Tournament selection for diversity\n            tournament_size = 3\n            for _ in range(tournament_size):\n                participants = np.random.choice(pop_size, tournament_size)\n                best_idx = participants[np.argmin(fitness[participants])]\n                pop[np.random.randint(pop_size)] = pop[best_idx]\n\n            # Adaptive population resizing\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.2)))\n\n            # Random exploration\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 55, "feedback": "The algorithm RefinedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26115 with standard deviation 0.14144.", "error": "", "parent_ids": ["1aa94434-dc10-484b-91af-f82a16054482"], "operator": null, "metadata": {"aucs": [0.2577091342562746, 0.24754941204172587, 0.07333352334461929, 0.1768506567690119, 0.17658641664902952, 0.21069102354182057, 0.559423853657075, 0.19988901320804353, 0.44833888114557796]}}
{"id": "a1c3ca68-3732-4385-a35a-0c1d30ac3448", "fitness": 0.3467874944117944, "name": "EnhancedEMADE", "description": "Enhance exploration and exploitation by incorporating an adaptive differential evolution with dynamic regrouping and local search.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive differential mutation with dynamic regrouping\n                F = 0.5 + 0.3 * np.sin((2 * np.pi * evals) / self.budget)\n                mutant = np.clip(a + F * (b - c), lb, ub)\n\n                # Adaptive crossover\n                CR = 0.5 + 0.3 * np.cos(np.pi * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Dynamic population resizing\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.2)))\n\n            # Periodic local search\n            if evals % (self.budget // 10) == 0:\n                for j in range(pop_size):\n                    local_trial = np.clip(pop[j] + np.random.normal(0, 0.1, self.dim), lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[j]:\n                        pop[j] = local_trial\n                        fitness[j] = local_fitness\n                    if local_fitness < self.best_fitness:\n                        self.best_fitness = local_fitness\n                        self.best_solution = local_trial\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 56, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.34679 with standard deviation 0.07648.", "error": "", "parent_ids": ["1aa94434-dc10-484b-91af-f82a16054482"], "operator": null, "metadata": {"aucs": [0.32242750982912294, 0.40560969400051716, 0.4429329653512143, 0.18647512502911567, 0.2650868239877887, 0.3617755443815338, 0.4223978518985333, 0.37729677470568646, 0.3370851605226374]}}
{"id": "e41fb4c4-9d02-464c-9c50-de9c19968d76", "fitness": 0.31371983187883007, "name": "EnhancedEMADE", "description": "Enhance exploration and convergence by integrating variable neighborhood search and adaptive population strategies.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                # Multi-strategy mutation with variable neighborhood search\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d) + 0.1 * np.random.uniform(-1, 1, self.dim), lb, ub)\n\n                # Adaptive learning rate\n                CR = 0.5 + 0.3 * np.cos(3.14 * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Adaptive population resizing with increased variability\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - ((evals / self.budget)**1.5))))\n                pop = pop[:pop_size]\n                fitness = fitness[:pop_size]\n\n            # Random exploration with diversity\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 57, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31372 with standard deviation 0.08076.", "error": "", "parent_ids": ["1aa94434-dc10-484b-91af-f82a16054482"], "operator": null, "metadata": {"aucs": [0.33852542200670177, 0.3836133059660203, 0.36748809274163274, 0.19333496904066083, 0.3225701589256753, 0.2237385235204219, 0.20702972235440198, 0.3517646610692057, 0.4354136312847503]}}
{"id": "d609d700-d300-4fc7-beb1-11c9cfdca845", "fitness": 0.34249455810713036, "name": "EnhancedEMADE", "description": "Enhanced multi-strategy search with fitness-based population resizing and random reinitialization for improved convergence and diversification.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                # Enhanced multi-strategy mutation\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                elif np.random.rand() < 0.3:\n                    mutant = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                # Adaptive learning rate\n                CR = 0.5 + 0.3 * np.cos(3.14 * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Fitness-based population resizing\n            if evals / self.budget > 0.2:\n                avg_fitness = np.mean(fitness)\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.2) * (avg_fitness / (avg_fitness + 1))))\n\n            # Random reinitialization for diversification\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < fitness[np.argmax(fitness)]:\n                    replace_idx = np.argmax(fitness)\n                    pop[replace_idx] = new_individual\n                    fitness[replace_idx] = new_fitness\n\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 58, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.34249 with standard deviation 0.08522.", "error": "", "parent_ids": ["1aa94434-dc10-484b-91af-f82a16054482"], "operator": null, "metadata": {"aucs": [0.37955181830897555, 0.4809536186808324, 0.3629920125822308, 0.41348410890669707, 0.1707876024480387, 0.2766738660116689, 0.28540914247245763, 0.3871414175033009, 0.325457436049972]}}
{"id": "f99bb54d-3568-4ac5-bb7c-c34308d587f4", "fitness": 0.17870441172189278, "name": "DynamicEMADE", "description": "Introduce dynamic population management and diversity-enhancing perturbation to improve convergence and robustness.", "code": "import numpy as np\n\nclass DynamicEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Enhanced multi-strategy mutation\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                # Diversity-enhancing perturbation\n                perturbation = np.random.normal(0, 0.1, self.dim)\n                mutant = np.clip(mutant + perturbation, lb, ub)\n\n                # Adaptive learning rate with annealing schedule\n                CR = 0.5 + 0.3 * np.cos(3.14 * (evals / self.budget)) * (0.9**(evals/self.budget))\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Dynamic population resizing\n            pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.3)))\n\n            # Additional random exploration with decreasing probability\n            if np.random.rand() < 0.1 * (1 - evals/self.budget):\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 59, "feedback": "The algorithm DynamicEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17870 with standard deviation 0.07417.", "error": "", "parent_ids": ["1aa94434-dc10-484b-91af-f82a16054482"], "operator": null, "metadata": {"aucs": [0.20735806897318154, 0.12067355098602683, 0.3472167570463601, 0.1651096811871785, 0.16804510974704634, 0.17185830146162984, 0.23372191038341705, 0.10849317163388339, 0.08586315407831124]}}
{"id": "c1441d81-4605-4c58-9209-758bb777bd6d", "fitness": 0.31492369388102137, "name": "RefinedEMADE", "description": "Enhance the exploration phase by adding an additional diversity mechanism.", "code": "import numpy as np\n\nclass RefinedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Multi-strategy mutation\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                # Adaptive learning rate\n                CR = 0.5 + 0.3 * np.cos(3.14 * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Adaptive population resizing\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.2)))\n\n            # Random exploration\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n            # Add diversity by introducing mutation on best solution\n            if np.random.rand() < 0.05:  # New line added\n                perturbation = np.random.normal(0, 0.1, self.dim)  # New line added\n                best_mutant = np.clip(self.best_solution + perturbation, lb, ub)  # New line added\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 60, "feedback": "The algorithm RefinedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31492 with standard deviation 0.04462.", "error": "", "parent_ids": ["1aa94434-dc10-484b-91af-f82a16054482"], "operator": null, "metadata": {"aucs": [0.3684638134514683, 0.33988658065792665, 0.33608992168677543, 0.3056485585715488, 0.2753760888236729, 0.22774815961312544, 0.3805165511738462, 0.30155619420202584, 0.2990273767488032]}}
{"id": "f84bc297-faa5-4027-82bd-a236461d9812", "fitness": 0.15623144357580288, "name": "RefinedEMADE", "description": "Integrate multi-strategy mutation and adaptive learning rate to enhance exploration and convergence, with increased crossover probability to improve solution diversity.", "code": "import numpy as np\n\nclass RefinedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Multi-strategy mutation\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                # Adaptive learning rate\n                CR = 0.7 + 0.3 * np.cos(3.14 * (evals / self.budget))  # Changed line: Increased the base CR from 0.5 to 0.7\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Adaptive population resizing\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.2)))\n\n            # Random exploration\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 61, "feedback": "The algorithm RefinedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.15623 with standard deviation 0.08917.", "error": "", "parent_ids": ["1aa94434-dc10-484b-91af-f82a16054482"], "operator": null, "metadata": {"aucs": [0.27878483555003053, 0.33376516375875265, 0.07213446123218303, 0.1538387611813321, 0.16205698807833457, 0.1617312851184015, 0.07032320671498393, 0.1071307396177198, 0.06631755093048786]}}
{"id": "45675575-f63d-476e-a0bf-009c557664e1", "fitness": 0.3078007507608876, "name": "RefinedEMADE_V2", "description": "RefinedEMADE_V2 enhances convergence by incorporating a dynamic mutation strategy and an elite preservation mechanism.", "code": "import numpy as np\n\nclass RefinedEMADE_V2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n        elite = pop[np.argmin(fitness)]\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Dynamic mutation strategy\n                F = 0.5 + 0.5 * np.random.rand()\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                # Adaptive learning rate\n                CR = 0.5 + 0.3 * np.cos(3.14 * (evals / self.budget))\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Adaptive population resizing\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.2)))\n\n            # Elite preservation\n            if self.best_fitness < func(elite):\n                elite = self.best_solution\n\n            # Random exploration\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 62, "feedback": "The algorithm RefinedEMADE_V2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30780 with standard deviation 0.09553.", "error": "", "parent_ids": ["1aa94434-dc10-484b-91af-f82a16054482"], "operator": null, "metadata": {"aucs": [0.4453434785202992, 0.4111294539539142, 0.30712748906718756, 0.18668321389804843, 0.22487737223356608, 0.20099898881331635, 0.43651891839712853, 0.25294620554463476, 0.304581636419893]}}
{"id": "44c6a910-c752-44b8-bd71-8c8c69b46556", "fitness": 0.42134572221135536, "name": "EnhancedEMADE", "description": "Employ dynamic population adaptation and self-adaptive crossover rates to enhance exploration-exploitation balance in optimization.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n        crossover_rates = np.random.uniform(0.3, 0.9, pop_size)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                CR = crossover_rates[i]\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    crossover_rates[i] = CR + 0.1 * (1 - CR)  # Reward successful crossover rate\n                else:\n                    crossover_rates[i] = CR - 0.1 * CR  # Punish unsuccessful crossover rate\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.5)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 63, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42135 with standard deviation 0.11781.", "error": "", "parent_ids": ["1aa94434-dc10-484b-91af-f82a16054482"], "operator": null, "metadata": {"aucs": [0.4784945969899338, 0.553820379863588, 0.46948521760537754, 0.20103222855218228, 0.3729313569253101, 0.27989062215190785, 0.38820867530125336, 0.589934181807687, 0.4583142407049585]}}
{"id": "333ef45f-e61f-4eb4-9c61-64a2a5f570c2", "fitness": 0.31752141397573147, "name": "EnhancedEMADE", "description": "Introduce adaptive mutation strategies and integrate population diversity to enhance the exploration-exploitation balance in optimization.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n        crossover_rates = np.random.uniform(0.3, 0.9, pop_size)\n        mutation_rates = np.random.uniform(0.5, 1.0, pop_size)  # New mutation rates\n\n        while evals < self.budget:\n            avg_fitness = np.mean(fitness)  # Added for diversity measurement\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + mutation_rates[i] * (b - c), lb, ub)  # Adaptive mutation\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                CR = crossover_rates[i]\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    crossover_rates[i] = CR + 0.1 * (1 - CR)\n                    mutation_rates[i] = min(1.0, mutation_rates[i] + 0.1)  # Reward mutation adjustment\n                else:\n                    crossover_rates[i] = CR - 0.1 * CR\n                    mutation_rates[i] = max(0.5, mutation_rates[i] - 0.1)  # Punish mutation adjustment\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n            if np.random.rand() < 0.1 and np.mean(fitness) > avg_fitness:  # Increase diversity\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < np.max(fitness):\n                    worst_idx = np.argmax(fitness)\n                    pop[worst_idx] = new_individual\n                    fitness[worst_idx] = new_fitness\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 64, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31752 with standard deviation 0.09938.", "error": "", "parent_ids": ["44c6a910-c752-44b8-bd71-8c8c69b46556"], "operator": null, "metadata": {"aucs": [0.2536405821634762, 0.4524979476316884, 0.257079471590997, 0.17399294705173585, 0.18007664705019844, 0.324704650562061, 0.397409253811406, 0.41975727827445575, 0.39853394764556516]}}
{"id": "be16ae36-d17d-498d-97e6-f3c29b4f0f01", "fitness": 0.20575114950896822, "name": "RefinedEMADE", "description": "Integrate adaptive differential evolution with opposition-based learning to enhance convergence speed and solution accuracy in black-box optimization.", "code": "import numpy as np\n\nclass RefinedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def opposition_based_learning(self, individual, lb, ub):\n        return lb + ub - individual\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n        crossover_rates = np.random.uniform(0.3, 0.9, pop_size)\n        F = np.random.uniform(0.5, 0.9, pop_size)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F[i] * (b - c), lb, ub)\n\n                if np.random.rand() < 0.5:\n                    mutant = self.opposition_based_learning(mutant, lb, ub)\n\n                CR = crossover_rates[i]\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    crossover_rates[i] = CR + 0.1 * (1 - CR)  # Reward successful crossover rate\n                    F[i] = F[i] + 0.1 * (1 - F[i])\n                else:\n                    crossover_rates[i] = CR - 0.1 * CR  # Punish unsuccessful crossover rate\n                    F[i] = F[i] - 0.1 * F[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.5)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 65, "feedback": "The algorithm RefinedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20575 with standard deviation 0.04743.", "error": "", "parent_ids": ["44c6a910-c752-44b8-bd71-8c8c69b46556"], "operator": null, "metadata": {"aucs": [0.22657984702508982, 0.2182241531496656, 0.1848586882402019, 0.19995787676517418, 0.19459730267842712, 0.302270958344956, 0.23533983204839926, 0.17218629921809425, 0.11774538811070578]}}
{"id": "143ff150-d298-42fa-8592-e790f840848e", "fitness": 0.6395268279176398, "name": "EnhancedEMADE", "description": "Incorporate adaptive mutation scaling factor and enhance diversity through Gaussian perturbation in EnhancedEMADE.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n        crossover_rates = np.random.uniform(0.3, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                # Use adaptive mutation scaling\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                CR = crossover_rates[i]\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                # Introduce Gaussian perturbation for diversity\n                if np.random.rand() < 0.1:\n                    trial += np.random.normal(0, 0.1, self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    crossover_rates[i] = CR + 0.1 * (1 - CR)  # Reward successful crossover rate\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)  # Adapt mutation scale\n                else:\n                    crossover_rates[i] = CR - 0.1 * CR  # Punish unsuccessful crossover rate\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.5)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 66, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.63953 with standard deviation 0.04141.", "error": "", "parent_ids": ["44c6a910-c752-44b8-bd71-8c8c69b46556"], "operator": null, "metadata": {"aucs": [0.5778946726295919, 0.6367118995737271, 0.7088097589995211, 0.6466949183450076, 0.6915071958939982, 0.6501555726945085, 0.579230512932833, 0.6219145650723564, 0.6428223551172141]}}
{"id": "d3101bfc-5935-470a-9736-839d5b3f2042", "fitness": 0.37855234700639767, "name": "EnhancedEMADE", "description": "Introduce a dynamic mutation scaling factor based on the iteration progress in EnhancedEMADE.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n        crossover_rates = np.random.uniform(0.3, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                # Use adaptive mutation scaling\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                CR = crossover_rates[i]\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                # Introduce Gaussian perturbation for diversity\n                if np.random.rand() < 0.1:\n                    trial += np.random.normal(0, 0.1, self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    crossover_rates[i] = CR + 0.1 * (1 - CR)  # Reward successful crossover rate\n                    mutation_scale = 0.5 + 0.5 * ((self.budget - evals) / self.budget)  # Adapt mutation scale based on progress\n                else:\n                    crossover_rates[i] = CR - 0.1 * CR  # Punish unsuccessful crossover rate\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.5)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.37855 with standard deviation 0.04923.", "error": "", "parent_ids": ["143ff150-d298-42fa-8592-e790f840848e"], "operator": null, "metadata": {"aucs": [0.43474661595471453, 0.3639470863043699, 0.3968291779657348, 0.4094521308989685, 0.3511937795328657, 0.26043964329287883, 0.4220948579386876, 0.3999313313240874, 0.36833649984527206]}}
{"id": "8bc2723e-9185-4213-ac1a-c0251776c09e", "fitness": 0.10412099027721568, "name": "RefinedEnhancedEMADE", "description": "Improve EnhancedEMADE by introducing adaptive population resizing and elite selection to refine exploitation and exploration balance.", "code": "import numpy as np\n\nclass RefinedEnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n        crossover_rates = np.random.uniform(0.3, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                # Adaptive mutation scaling\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                CR = crossover_rates[i]\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                # Gaussian perturbation for diversity\n                if np.random.rand() < 0.1:\n                    trial += np.random.normal(0, 0.1, self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    crossover_rates[i] = CR + 0.1 * (1 - CR)\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)\n                else:\n                    crossover_rates[i] = CR - 0.1 * CR\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Adaptive population resizing and elite selection\n            if evals / self.budget > 0.2:\n                elite_count = max(1, int(0.1 * pop_size))\n                elite_indices = np.argsort(fitness)[:elite_count]\n                elite_pop = pop[elite_indices]\n                elite_fitness = fitness[elite_indices]\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.5)))\n                pop = np.random.uniform(lb, ub, (pop_size - elite_count, self.dim))\n                pop = np.vstack((elite_pop, pop))\n                fitness = np.array([func(ind) for ind in pop[elite_count:]])\n                fitness = np.concatenate((elite_fitness, fitness))\n                evals += (pop_size - elite_count)\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 68, "feedback": "The algorithm RefinedEnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10412 with standard deviation 0.04477.", "error": "", "parent_ids": ["143ff150-d298-42fa-8592-e790f840848e"], "operator": null, "metadata": {"aucs": [0.06997720219530257, 0.07716429399468927, 0.08506233300358679, 0.16534892059196493, 0.17985316115406114, 0.15206289424164832, 0.05620261535518789, 0.08241994057806445, 0.06899755138043573]}}
{"id": "c1934eab-6cfb-4d9f-b961-8c67207f507a", "fitness": -Infinity, "name": "EnhancedEMADE", "description": "Introduce adaptive population resizing and dynamic crossover rate adjustment to enhance convergence in EnhancedEMADE.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n        crossover_rates = np.random.uniform(0.3, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                CR = crossover_rates[i]\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                if np.random.rand() < 0.1:\n                    trial += np.random.normal(0, 0.1, self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    crossover_rates[i] = CR + 0.1 * (1 - CR)\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)\n                else:\n                    crossover_rates[i] = CR - 0.1 * CR\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.5)))\n                if evals / self.budget > 0.6:\n                    crossover_rates = np.clip(crossover_rates + np.random.normal(0, 0.05, pop_size), 0.1, 0.9)\n\n            if np.random.rand() < 0.05:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 69, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (20,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (20,) (10,) ')", "parent_ids": ["143ff150-d298-42fa-8592-e790f840848e"], "operator": null, "metadata": {}}
{"id": "4f77606e-d0f3-42c2-853e-01ba53d31a29", "fitness": 0.41738432424771005, "name": "AdaptiveCollaborativeEMADE", "description": "Introduce collaborative differential mutation and adaptive probabilistic learning to enhance convergence and exploration in AdaptiveCollaborativeEMADE.", "code": "import numpy as np\n\nclass AdaptiveCollaborativeEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n        crossover_rates = np.random.uniform(0.3, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n\n                # Collaborative differential mutation\n                if np.random.rand() < 0.5:\n                    mutant = a + mutation_scale * (b - c) + np.random.normal(0, 0.1, self.dim)\n                else:\n                    d, e = pop[np.random.choice(indices, 2, replace=False)]\n                    mutant = a + 0.5 * ((b + c) / 2 - d + (d - e))\n\n                mutant = np.clip(mutant, lb, ub)\n                \n                CR = crossover_rates[i]\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                # Adaptive probabilistic learning\n                if np.random.rand() < 0.1:\n                    learning_rate = np.random.uniform(0.01, 0.1)\n                    trial += learning_rate * np.random.normal(0, 0.1, self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    crossover_rates[i] = CR + 0.1 * (1 - CR)\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)\n                else:\n                    crossover_rates[i] = CR - 0.1 * CR\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.5)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 70, "feedback": "The algorithm AdaptiveCollaborativeEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41738 with standard deviation 0.08955.", "error": "", "parent_ids": ["143ff150-d298-42fa-8592-e790f840848e"], "operator": null, "metadata": {"aucs": [0.5148790652872197, 0.48339299070961184, 0.4658122383857958, 0.3253288851774755, 0.5546196706159393, 0.3350661087147897, 0.27935654594447656, 0.3661903382923245, 0.4318130751017575]}}
{"id": "5fc65f3e-caaf-4db8-ad82-6c16e63bdc6b", "fitness": 0.6395268279176398, "name": "RefinedEnhancedEMADE", "description": "Introduce dynamic population resizing and adaptive crossover rates to improve convergence in EnhancedEMADE.", "code": "import numpy as np\n\nclass RefinedEnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n        crossover_rates = np.random.uniform(0.3, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n\n                # Use adaptive mutation scaling\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                CR = crossover_rates[i]\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                # Introduce Gaussian perturbation for diversity\n                if np.random.rand() < 0.1:\n                    trial += np.random.normal(0, 0.1, self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    crossover_rates[i] = CR + 0.1 * (1 - CR)  # Reward successful crossover rate\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)  # Adapt mutation scale\n                else:\n                    crossover_rates[i] = CR - 0.1 * CR  # Punish unsuccessful crossover rate\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Dynamic population resizing based on convergence rate\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.5)))\n                # Resize population\n                if len(pop) > pop_size:\n                    pop = pop[:pop_size]\n                    fitness = fitness[:pop_size]\n                    crossover_rates = crossover_rates[:pop_size]\n\n            # Introduce exploration through random individuals\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 71, "feedback": "The algorithm RefinedEnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.63953 with standard deviation 0.04141.", "error": "", "parent_ids": ["143ff150-d298-42fa-8592-e790f840848e"], "operator": null, "metadata": {"aucs": [0.5778946726295919, 0.6367118995737271, 0.7088097589995211, 0.6466949183450076, 0.6915071958939982, 0.6501555726945085, 0.579230512932833, 0.6219145650723564, 0.6428223551172141]}}
{"id": "9dc38dc5-5b37-4401-afe0-8f8170c86198", "fitness": 0.593314694899879, "name": "EnhancedEMADE", "description": "Introduce self-adaptive learning mechanisms and elitism to EnhancedEMADE for improved convergence and solution quality.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n        crossover_rates = np.random.uniform(0.3, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n        \n        # Initialize adaptive learning rates\n        alpha, beta = 0.1, 0.05\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                # Use adaptive mutation scaling\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                CR = crossover_rates[i]\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                # Introduce Gaussian perturbation for diversity\n                if np.random.rand() < 0.1:\n                    trial += np.random.normal(0, 0.1, self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    crossover_rates[i] = CR + alpha * (1 - CR)  # Reward successful crossover rate\n                    mutation_scale = mutation_scale * (1 - beta) + beta * np.abs(fitness[i] - trial_fitness)  # Adapt mutation scale\n                else:\n                    crossover_rates[i] = CR * (1 - alpha)  # Punish unsuccessful crossover rate\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Implement elitism by preserving the best individual\n            elite_index = np.argmin(fitness)\n            if fitness[elite_index] < self.best_fitness:\n                self.best_fitness = fitness[elite_index]\n                self.best_solution = pop[elite_index]\n\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.5)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 72, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.59331 with standard deviation 0.04234.", "error": "", "parent_ids": ["143ff150-d298-42fa-8592-e790f840848e"], "operator": null, "metadata": {"aucs": [0.6383966725518275, 0.5634529691531178, 0.5211364213209637, 0.5956843579450231, 0.5928150338433245, 0.5592821853086865, 0.5940335078004376, 0.6757858862455932, 0.599245219929937]}}
{"id": "5f1e197a-220e-49c7-b30d-2a0ebd7bead4", "fitness": 0.40461106963177784, "name": "EnhancedEMADE", "description": "Enhance adaptability by modifying the mutation scale adjustment mechanism to account for fitness variance.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n        crossover_rates = np.random.uniform(0.3, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                # Use adaptive mutation scaling\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                CR = crossover_rates[i]\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                # Introduce Gaussian perturbation for diversity\n                if np.random.rand() < 0.1:\n                    trial += np.random.normal(0, 0.1, self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    crossover_rates[i] = CR + 0.1 * (1 - CR)  # Reward successful crossover rate\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.std(fitness)  # Adapt mutation scale\n                else:\n                    crossover_rates[i] = CR - 0.1 * CR  # Punish unsuccessful crossover rate\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.5)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 73, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40461 with standard deviation 0.09399.", "error": "", "parent_ids": ["143ff150-d298-42fa-8592-e790f840848e"], "operator": null, "metadata": {"aucs": [0.5196917082683068, 0.4225948424144381, 0.5028765483186571, 0.2172017536130263, 0.4052493327868295, 0.29713560651155047, 0.49002578265984353, 0.42551307705797103, 0.36121097505537736]}}
{"id": "68bfddd5-ab77-478d-9cd0-1de4780b2980", "fitness": 0.35359350106363485, "name": "EnhancedEMADE", "description": "Integrate dynamic population resizing and feedback-adaptive mutation mechanism in EnhancedEMADE for better exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n        crossover_rates = np.random.uniform(0.3, 0.9, pop_size)\n        mutation_scales = np.random.uniform(0.5, 1.0, pop_size)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n\n                # Use feedback-adaptive mutation\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + mutation_scales[i] * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                CR = crossover_rates[i]\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                # Introduce Gaussian perturbation for diversity\n                if np.random.rand() < 0.1:\n                    trial += np.random.normal(0, 0.1, self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    crossover_rates[i] = CR + 0.1 * (1 - CR)  # Reward successful crossover rate\n                    mutation_scales[i] *= 0.9  # Reduce mutation scale for successful trials\n                else:\n                    crossover_rates[i] = CR - 0.1 * CR  # Punish unsuccessful crossover rate\n                    mutation_scales[i] = min(1.0, mutation_scales[i] * 1.1)  # Increase mutation scale for exploration\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Dynamic population resizing based on progress\n            progress = evals / self.budget\n            if progress > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - progress**1.5)))\n\n            # Introduce new random individual occasionally\n            if np.random.rand() < 0.05:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35359 with standard deviation 0.07508.", "error": "", "parent_ids": ["143ff150-d298-42fa-8592-e790f840848e"], "operator": null, "metadata": {"aucs": [0.43409489759411424, 0.35383098848060013, 0.365669898087188, 0.3570665148982193, 0.27928368807150405, 0.19789308096291114, 0.32345097494901154, 0.43560185806794405, 0.4354496084612216]}}
{"id": "4862e955-c97c-4d55-b5ae-56c186322d63", "fitness": 0.5224593166650909, "name": "EnhancedEMADE", "description": "Incorporate competitive selection pressure by dynamically adjusting population size based on fitness diversity.", "code": "import numpy as np\n\nclass EnhancedEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n        crossover_rates = np.random.uniform(0.3, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                # Use adaptive mutation scaling\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                CR = crossover_rates[i]\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                # Introduce Gaussian perturbation for diversity\n                if np.random.rand() < 0.1:\n                    trial += np.random.normal(0, 0.1, self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    crossover_rates[i] = CR + 0.1 * (1 - CR)  # Reward successful crossover rate\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)  # Adapt mutation scale\n                else:\n                    crossover_rates[i] = CR - 0.1 * CR  # Punish unsuccessful crossover rate\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals / self.budget > 0.2:\n                fitness_variance = np.var(fitness)\n                pop_size = max(5, int(self.initial_population_size / (1 + fitness_variance)))  # Adjust population size based on fitness diversity\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 75, "feedback": "The algorithm EnhancedEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.52246 with standard deviation 0.25350.", "error": "", "parent_ids": ["143ff150-d298-42fa-8592-e790f840848e"], "operator": null, "metadata": {"aucs": [0.1468928342686031, 0.6664801157898184, 0.7228460493267282, 0.7173448828590494, 0.6556364528493741, 0.6460669581577702, 0.2597615210085795, 0.7781797574062962, 0.10892527831959942]}}
{"id": "1e0798dc-6663-4a60-af3c-663557a96bcf", "fitness": 0.6593977565303747, "name": "AdaptiveEMADE", "description": "AdaptiveEMADE: Enhance adaptability and convergence by incorporating differential learning rates and dynamic crossover based on fitness variance.", "code": "import numpy as np\n\nclass AdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        # Introduce differential learning rates\n        learning_rates = np.random.uniform(0.4, 1.0, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                # Dynamic crossover based on fitness variance\n                fitness_variance = np.var(fitness)\n                CR = np.clip(0.5 * (1 - fitness_variance / (fitness_variance + 1)), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                # Gaussian perturbation for diversity\n                if np.random.rand() < 0.1:\n                    trial += np.random.normal(0, 0.1, self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])  # Reward successful learning rate\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)  # Adapt mutation scale\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]  # Punish unsuccessful learning rate\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.5)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 76, "feedback": "The algorithm AdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.65940 with standard deviation 0.03172.", "error": "", "parent_ids": ["143ff150-d298-42fa-8592-e790f840848e"], "operator": null, "metadata": {"aucs": [0.7056866470442812, 0.6466904845367482, 0.6405133176768003, 0.6361683111288561, 0.689641442605488, 0.6131444799237401, 0.7092248051918559, 0.6467261010449133, 0.646784219620689]}}
{"id": "aa54d57d-035e-49e4-9bd0-734d17aad0eb", "fitness": 0.6662116935301104, "name": "AdaptiveEMADE", "description": "Enhance differential evolution by improving dynamic crossover rate adaptation based on the progress ratio.", "code": "import numpy as np\n\nclass AdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        # Introduce differential learning rates\n        learning_rates = np.random.uniform(0.4, 1.0, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n                else:\n                    d = pop[np.random.choice(indices)]\n                    mutant = np.clip(a + 0.5 * ((b + c) / 2 - d), lb, ub)\n\n                # Dynamic crossover based on fitness variance and progress ratio\n                fitness_variance = np.var(fitness)\n                progress_ratio = evals / self.budget\n                CR = np.clip(0.5 * (1 - fitness_variance / (fitness_variance + 1)) * (1 - progress_ratio), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                # Gaussian perturbation for diversity\n                if np.random.rand() < 0.1:\n                    trial += np.random.normal(0, 0.1, self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])  # Reward successful learning rate\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)  # Adapt mutation scale\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]  # Punish unsuccessful learning rate\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.5)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 77, "feedback": "The algorithm AdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.66621 with standard deviation 0.02956.", "error": "", "parent_ids": ["1e0798dc-6663-4a60-af3c-663557a96bcf"], "operator": null, "metadata": {"aucs": [0.7056866470442812, 0.6466904845367482, 0.6405133176768003, 0.6615568689941023, 0.7055196883238601, 0.6332031093377438, 0.7092248051918559, 0.6467261010449133, 0.646784219620689]}}
{"id": "ac181759-db8b-4b47-9e15-a650961305d7", "fitness": 0.3877740582104772, "name": "EnhancedAdaptiveEMADE", "description": "Improve the adaptive differential evolution by integrating a strategic mutation rate adaptation mechanism and incorporating competition-based learning for enhanced exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        # Strategic mutation rates\n        mutation_rates = np.random.uniform(0.6, 1.0, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n\n                # Competitive learning for mutation\n                if fitness[i] < np.median(fitness):\n                    mutation_rate = np.clip(mutation_rates[i] + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    mutation_rate = np.clip(mutation_rates[i] - np.random.normal(0, 0.05), 0.4, 1.0)\n\n                mutant = np.clip(a + mutation_rate * (b - c), lb, ub)\n\n                # Dynamic crossover based on progress and diversity\n                progress_ratio = evals / self.budget\n                diversity = np.std(pop, axis=0).mean()\n                CR = np.clip(0.3 + 0.4 * (1 - diversity / (diversity + 1)) * (1 - progress_ratio), 0.2, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                # Random Gaussian perturbation\n                if np.random.rand() < 0.1:\n                    trial += np.random.normal(0, 0.05 * (1 - progress_ratio), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    mutation_rates[i] = mutation_rate\n                    if trial_fitness < self.best_fitness:\n                        self.best_fitness = trial_fitness\n                        self.best_solution = trial\n\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget)**1.2)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedAdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.38777 with standard deviation 0.07947.", "error": "", "parent_ids": ["aa54d57d-035e-49e4-9bd0-734d17aad0eb"], "operator": null, "metadata": {"aucs": [0.3335615226804921, 0.4659068483966555, 0.4604721670793982, 0.3125154298781886, 0.23208961510996573, 0.3474840397328437, 0.46852946995457756, 0.4290365889536357, 0.44037084210853794]}}
{"id": "f9f4da56-85d7-40b3-baad-482fbfeda03c", "fitness": 0.7020293376233268, "name": "EnhancedAdaptiveEMADE", "description": "Enhance differential evolution by incorporating adaptive learning rates based on success history and dynamic population size adjustment.", "code": "import numpy as np\n\nclass EnhancedAdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        # Use adaptive learning rates initialized to moderate values\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n\n                # Dynamic crossover based on fitness variance and progress ratio\n                fitness_variance = np.var(fitness)\n                progress_ratio = evals / self.budget\n                CR = np.clip(0.6 * (1 - fitness_variance / (fitness_variance + 1 + 1e-9)) * (1 - progress_ratio), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                # Introduce adaptive Gaussian perturbation for diversity\n                if np.random.rand() < 0.15:\n                    trial += np.random.normal(0, 0.1 * (1 - progress_ratio), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n            \n            # Dynamic adjustment of population size\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n\n            # Periodically introduce new random individuals\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 79, "feedback": "The algorithm EnhancedAdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.70203 with standard deviation 0.04605.", "error": "", "parent_ids": ["aa54d57d-035e-49e4-9bd0-734d17aad0eb"], "operator": null, "metadata": {"aucs": [0.6417528994756019, 0.6991299705439942, 0.7453870655434292, 0.7730217434913567, 0.7048836038620793, 0.6940564014038471, 0.628662119647429, 0.7523877567257929, 0.6789824779164104]}}
{"id": "dfc24e84-db19-4b69-8aa4-8db3ec00a736", "fitness": 0.120964393844209, "name": "RefinedAdaptiveEMADE", "description": "Enhance differential evolution by integrating historical fitness trends for strategic mutation and crossover rates, while dynamically balancing exploration and exploitation.", "code": "import numpy as np\n\nclass RefinedAdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        # Adaptive parameters\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n        historical_fitness_trend = np.zeros(pop_size)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n\n                # Adjust crossover rate based on historical fitness trends\n                recent_fitness_improvement = np.mean(historical_fitness_trend)\n                CR = np.clip(0.4 + 0.5 * (1 - recent_fitness_improvement), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                # Introduce adaptive Gaussian perturbation\n                if np.random.rand() < 0.1:\n                    trial += np.random.normal(0, 0.05 * (1 - evals / self.budget), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                fitness_diff = fitness[i] - trial_fitness\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness_diff)\n                    historical_fitness_trend[i] = 0.9 * historical_fitness_trend[i] + 0.1 * fitness_diff\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n                    historical_fitness_trend[i] = 0.9 * historical_fitness_trend[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n            \n            # Dynamic adjustment of population size\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n\n            # Periodically introduce new random individuals\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 80, "feedback": "The algorithm RefinedAdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12096 with standard deviation 0.02741.", "error": "", "parent_ids": ["f9f4da56-85d7-40b3-baad-482fbfeda03c"], "operator": null, "metadata": {"aucs": [0.12367047974355616, 0.09794810030870238, 0.1174800462097576, 0.16049519125761946, 0.15556452520227482, 0.15415741553405626, 0.09198554818326088, 0.09022731754026991, 0.09715092061838337]}}
{"id": "ee7a7873-aa82-4005-a510-add6f3b7723b", "fitness": -Infinity, "name": "EnhancedCrowdingEMADE", "description": "Introduce an adaptive crowding distance mechanism to improve exploration-exploitation balance and maintain diversity in the differential evolution process.", "code": "import numpy as np\n\nclass EnhancedCrowdingEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n\n                fitness_variance = np.var(fitness)\n                progress_ratio = evals / self.budget\n                CR = np.clip(0.6 * (1 - fitness_variance / (fitness_variance + 1 + 1e-9)) * (1 - progress_ratio), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if np.random.rand() < 0.15:\n                    trial += np.random.normal(0, 0.1 * (1 - progress_ratio), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n            \n            pop, fitness = self.adaptive_crowding_distance(pop, fitness, lb, ub, func)\n            \n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness\n\n    def adaptive_crowding_distance(self, pop, fitness, lb, ub, func):\n        crowding_dist = np.zeros(len(pop))\n        for i in range(len(pop)):\n            for j in range(len(pop)):\n                if i != j:\n                    crowding_dist[i] += np.linalg.norm(pop[i] - pop[j])\n        \n        elite_indices = np.argsort(fitness)[:len(pop)//2]\n        crowding_dist[elite_indices] = np.inf\n        \n        new_pop = pop[elite_indices]\n        new_fitness = fitness[elite_indices]\n\n        while len(new_pop) < self.initial_population_size:\n            if np.random.rand() < 0.7:\n                random_individual = np.random.uniform(lb, ub, self.dim)\n                random_fitness = func(random_individual)\n                new_pop = np.vstack((new_pop, random_individual))\n                new_fitness = np.append(new_fitness, random_fitness)\n            else:\n                selected = np.random.choice(elite_indices)\n                mutated = np.clip(new_pop[selected] + np.random.normal(0, 0.1, self.dim), lb, ub)\n                mutated_fitness = func(mutated)\n                new_pop = np.vstack((new_pop, mutated))\n                new_fitness = np.append(new_fitness, mutated_fitness)\n        \n        return new_pop, new_fitness", "configspace": "", "generation": 81, "feedback": "An exception occurred: IndexError('index 16 is out of bounds for axis 0 with size 11').", "error": "IndexError('index 16 is out of bounds for axis 0 with size 11')", "parent_ids": ["f9f4da56-85d7-40b3-baad-482fbfeda03c"], "operator": null, "metadata": {}}
{"id": "5d94442f-857c-48ae-82cb-96f31742855c", "fitness": 0.10283437678087343, "name": "RefinedAdaptiveEMADE", "description": "Refine adaptive differential evolution by incorporating dynamic scaling factors based on population diversity and adaptive crossover probability for enhanced exploration and exploitation balance.", "code": "import numpy as np\n\nclass RefinedAdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        # Adaptive parameters\n        mutation_scale = 0.8\n        crossover_prob = 0.8\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n\n                # Compute diversity-based scaling factor\n                diversity = np.mean(np.linalg.norm(pop - np.mean(pop, axis=0), axis=1))\n                dynamic_scale = mutation_scale * (1 + 0.1 * np.random.randn() * diversity)\n\n                mutant = np.clip(a + dynamic_scale * (b - c), lb, ub)\n\n                # Adaptive crossover probability\n                progress_ratio = evals / self.budget\n                CR = crossover_prob * (1 - progress_ratio)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                # Adaptive Gaussian perturbation for exploration\n                if np.random.rand() < 0.15:\n                    trial += np.random.normal(0, 0.1 * (1 - progress_ratio), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    mutation_scale *= 0.9 + 0.1 * (1 - trial_fitness / (fitness[i] + 1e-9))\n                else:\n                    mutation_scale *= 1.2\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            # Dynamic adjustment of population size\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - np.sqrt(evals / self.budget))))\n                pop = pop[:pop_size]\n                fitness = fitness[:pop_size]\n\n            # Periodically introduce new random individuals\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 82, "feedback": "The algorithm RefinedAdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10283 with standard deviation 0.03485.", "error": "", "parent_ids": ["f9f4da56-85d7-40b3-baad-482fbfeda03c"], "operator": null, "metadata": {"aucs": [0.07494505631724313, 0.09588645952946839, 0.07551878680668778, 0.14595673586336344, 0.1533669995217487, 0.15402237018306852, 0.06974235309962062, 0.07910719827245516, 0.07696343143420503]}}
{"id": "a62fb5de-b671-4f9e-b2ad-0003d8de4127", "fitness": 0.6542808676754781, "name": "EnhancedAdaptiveEMADE", "description": "Enhance differential evolution by incorporating adaptive learning rates based on success history and dynamic population size adjustment, with periodic mutation scale recalibration.", "code": "import numpy as np\n\nclass EnhancedAdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n\n                fitness_variance = np.var(fitness)\n                progress_ratio = evals / self.budget\n                CR = np.clip(0.6 * (1 - fitness_variance / (fitness_variance + 1 + 1e-9)) * (1 - progress_ratio), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if np.random.rand() < 0.15:\n                    trial += np.random.normal(0, 0.1 * (1 - progress_ratio), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n            \n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n            \n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n            \n            # Introduce periodic mutation scale recalibration\n            if evals % (self.budget // 10) == 0:\n                mutation_scale = np.random.uniform(0.5, 1.0)\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedAdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.65428 with standard deviation 0.05801.", "error": "", "parent_ids": ["f9f4da56-85d7-40b3-baad-482fbfeda03c"], "operator": null, "metadata": {"aucs": [0.6417528994756019, 0.6350773395095421, 0.7453870655434292, 0.7730217434913567, 0.5982174885342169, 0.6140558217930117, 0.628662119647429, 0.6397134418908305, 0.6126398891938855]}}
{"id": "5f67e181-3f52-4932-8fd8-791ae023e0be", "fitness": 0.2831176002409339, "name": "RefinedAdaptiveEMADE", "description": "Improve the adaptability of the algorithm by integrating self-adaptive mutation scales and enhancing exploration through diversity preservation.", "code": "import numpy as np\n\nclass RefinedAdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n        self.mutation_scale_as = np.array([0.5] * self.initial_population_size)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n\n                # Self-adaptive mutation scale\n                self.mutation_scale_as[i] = np.clip(self.mutation_scale_as[i] * (1.0 + np.random.normal(0.0, 0.1)), 0.1, 1.0)\n                mutant = np.clip(a + self.mutation_scale_as[i] * (b - c), lb, ub)\n\n                fitness_variance = np.var(fitness)\n                progress_ratio = evals / self.budget\n                CR = np.clip(0.6 * (1 - fitness_variance / (fitness_variance + 1 + 1e-9)) * (1 - progress_ratio), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                if np.random.rand() < 0.15:\n                    trial += np.random.normal(0, 0.1 * (1 - progress_ratio), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 84, "feedback": "The algorithm RefinedAdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28312 with standard deviation 0.08754.", "error": "", "parent_ids": ["f9f4da56-85d7-40b3-baad-482fbfeda03c"], "operator": null, "metadata": {"aucs": [0.2715429456575047, 0.3304245880394805, 0.44710488725314435, 0.18081071881885458, 0.20706439926698428, 0.20137034420280786, 0.3047869458614826, 0.3906615523718896, 0.21429202069625664]}}
{"id": "13e2a1fc-17f5-4415-9fbf-5b210fa4ad2e", "fitness": 0.3069468667146612, "name": "EnhancedAdaptiveEMADE", "description": "Introduce adaptive mutation scale based on fitness variance to enhance exploration.", "code": "import numpy as np\n\nclass EnhancedAdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        # Use adaptive learning rates initialized to moderate values\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n\n                # Dynamic crossover based on fitness variance and progress ratio\n                fitness_variance = np.var(fitness)\n                progress_ratio = evals / self.budget\n                CR = np.clip(0.6 * (1 - fitness_variance / (fitness_variance + 1 + 1e-9)) * (1 - progress_ratio), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                # Introduce adaptive Gaussian perturbation for diversity\n                if np.random.rand() < 0.15:\n                    trial += np.random.normal(0, 0.1 * (1 - progress_ratio), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * fitness_variance  # Change made here\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n            \n            # Dynamic adjustment of population size\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n\n            # Periodically introduce new random individuals\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedAdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30695 with standard deviation 0.11299.", "error": "", "parent_ids": ["f9f4da56-85d7-40b3-baad-482fbfeda03c"], "operator": null, "metadata": {"aucs": [0.4341561463469944, 0.4404712602608445, 0.24111652943812978, 0.1763401075602279, 0.17319026198505694, 0.18063725886844428, 0.4508334948280236, 0.3753957878328913, 0.29038095331133873]}}
{"id": "7ef86cd1-b4d4-4780-9829-c30400ec752d", "fitness": 0.4284981479222687, "name": "EnhancedAdaptiveEMADERefined", "description": "Improve adaptive differential evolution by incorporating multi-stage mutation strategies and environmental adaptability to dynamically balance exploration and exploitation.  ", "code": "import numpy as np\n\nclass EnhancedAdaptiveEMADERefined:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                # Multi-stage mutation strategy\n                if np.random.rand() < 0.5:\n                    # Differential mutation\n                    mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n                else:\n                    # Current-to-best/1 mutation\n                    d = pop[np.argmin(fitness)]\n                    mutant = np.clip(a + mutation_scale * (d - a) + mutation_scale * (b - c), lb, ub)\n\n                fitness_variance = np.var(fitness)\n                progress_ratio = evals / self.budget\n                CR = np.clip(0.6 * (1 - fitness_variance / (fitness_variance + 1 + 1e-9)) * (1 - progress_ratio), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                # Adaptive Gaussian perturbation with environmental adaptability\n                trial += np.random.normal(0, 0.1 * (1 - progress_ratio), self.dim) * (1 - fitness_variance / (fitness_variance + 1 + 1e-9))\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n            \n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 86, "feedback": "The algorithm EnhancedAdaptiveEMADERefined got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42850 with standard deviation 0.22814.", "error": "", "parent_ids": ["f9f4da56-85d7-40b3-baad-482fbfeda03c"], "operator": null, "metadata": {"aucs": [0.6205335941601406, 0.5322532909810871, 0.7138233370227898, 0.258970332798083, 0.20974857436152172, 0.15632075652422073, 0.6250268787660342, 0.1022988330775293, 0.637507733609012]}}
{"id": "daf69ec6-c79a-48bf-97a4-528e7d7bd94a", "fitness": 0.6803253854127234, "name": "EnhancedAdaptiveEMADE", "description": "Refine EnhancedAdaptiveEMADE by adding a strategic re-evaluation step for underperforming individuals to improve convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        # Use adaptive learning rates initialized to moderate values\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n\n                # Dynamic crossover based on fitness variance and progress ratio\n                fitness_variance = np.var(fitness)\n                progress_ratio = evals / self.budget\n                CR = np.clip(0.6 * (1 - fitness_variance / (fitness_variance + 1 + 1e-9)) * (1 - progress_ratio), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                # Introduce adaptive Gaussian perturbation for diversity\n                if np.random.rand() < 0.15:\n                    trial += np.random.normal(0, 0.1 * (1 - progress_ratio), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n            \n            # Dynamic adjustment of population size\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n\n            # Periodically introduce new random individuals\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n                \n            # Re-evaluate underperforming individuals\n            if evals < self.budget and np.random.rand() < 0.05:\n                idx = np.argmax(fitness)\n                new_fitness = func(pop[idx])\n                evals += 1\n                if new_fitness < fitness[idx]:\n                    fitness[idx] = new_fitness\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedAdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.68033 with standard deviation 0.15984.", "error": "", "parent_ids": ["f9f4da56-85d7-40b3-baad-482fbfeda03c"], "operator": null, "metadata": {"aucs": [0.7107514311169272, 0.7193030489208709, 0.7586423377745113, 0.7606829993289527, 0.23208156312240036, 0.7410651115205928, 0.7041361114076004, 0.7644527169900768, 0.7318131485325774]}}
{"id": "4d81aadf-cb66-4238-9eab-f6e722698ce3", "fitness": 0.7306691697791132, "name": "EnhancedAdaptiveEMADE", "description": "Improve EnhancedAdaptiveEMADE by introducing adaptive mutation scale based on overall population fitness improvement.", "code": "import numpy as np\n\nclass EnhancedAdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        # Use adaptive learning rates initialized to moderate values\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            initial_population_fitness = np.mean(fitness)  # Track initial population fitness\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n\n                # Dynamic crossover based on fitness variance and progress ratio\n                fitness_variance = np.var(fitness)\n                progress_ratio = evals / self.budget\n                CR = np.clip(0.6 * (1 - fitness_variance / (fitness_variance + 1 + 1e-9)) * (1 - progress_ratio), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                # Introduce adaptive Gaussian perturbation for diversity\n                if np.random.rand() < 0.15:\n                    trial += np.random.normal(0, 0.1 * (1 - progress_ratio), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n            \n            # Adapt mutation scale based on population fitness improvement\n            current_population_fitness = np.mean(fitness)\n            if current_population_fitness < initial_population_fitness:\n                mutation_scale *= 1.05\n            else:\n                mutation_scale *= 0.95\n\n            # Dynamic adjustment of population size\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n\n            # Periodically introduce new random individuals\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 88, "feedback": "The algorithm EnhancedAdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.73067 with standard deviation 0.05646.", "error": "", "parent_ids": ["f9f4da56-85d7-40b3-baad-482fbfeda03c"], "operator": null, "metadata": {"aucs": [0.7999786786683205, 0.7404570513385105, 0.761837263843547, 0.7398236366407507, 0.772972448257793, 0.7545147985996341, 0.59280632174899, 0.6960691316325258, 0.7175631972819472]}}
{"id": "f85a11c9-794d-47ca-b9c5-9ef61e30fe48", "fitness": 0.09006056601424749, "name": "EnhancedAdaptiveEMADE", "description": "Introduce a dynamic adaptive crossover strategy based on mutation success rates and population diversity to improve convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        # Initial learning rates and mutation scale\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        success_rates = np.zeros(pop_size)  # Track success rates for adaptive crossover\n        diversity_measure = lambda pop: np.std(pop, axis=0).mean()  # Population diversity\n\n        while evals < self.budget:\n            initial_population_fitness = np.mean(fitness)\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n\n                # Dynamic crossover based on success rate and diversity\n                CR = 0.3 + 0.7 * success_rates[i] + 0.1 * diversity_measure(pop)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                # Adaptive Gaussian perturbation\n                if np.random.rand() < 0.15:\n                    trial += np.random.normal(0, 0.1 * (1 - evals / self.budget), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)\n                    success_rates[i] = success_rates[i] * 0.9 + 0.1  # Update success rate\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n                    success_rates[i] = success_rates[i] * 0.9  # Decrease success rate\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n            \n            # Adapt mutation scale based on population fitness improvement\n            current_population_fitness = np.mean(fitness)\n            if current_population_fitness < initial_population_fitness:\n                mutation_scale *= 1.05\n            else:\n                mutation_scale *= 0.95\n\n            # Dynamic adjustment of population size\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n\n            # Periodically introduce new random individuals\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 89, "feedback": "The algorithm EnhancedAdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09006 with standard deviation 0.04909.", "error": "", "parent_ids": ["4d81aadf-cb66-4238-9eab-f6e722698ce3"], "operator": null, "metadata": {"aucs": [0.05387114155090289, 0.06554765781607441, 0.06818124938199788, 0.15360082803847297, 0.14533359923668232, 0.17486281942343163, 0.040620850402697695, 0.05219454015543423, 0.05633240812253337]}}
{"id": "ff5cb514-86bd-4cae-8e23-df9c6c1b68fd", "fitness": 0.5943984277025861, "name": "EnhancedAdaptiveEMADE", "description": "Enhance adaptability in EnhancedAdaptiveEMADE by incorporating fitness improvement tracking and refined mutation-perturbation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            initial_population_fitness = np.mean(fitness)\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n\n                fitness_variance = np.var(fitness)\n                progress_ratio = evals / self.budget\n                CR = np.clip(0.6 * (1 - fitness_variance / (fitness_variance + 1)), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                if np.random.rand() < 0.2:  # Adjusted for increased perturbation frequency\n                    trial += np.random.normal(0, 0.1 * (1 - progress_ratio), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                    mutation_scale = mutation_scale * 0.85 + 0.15 * np.abs(fitness[i] - trial_fitness)  # Adjusted learning of mutation scale\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            current_population_fitness = np.mean(fitness)\n            if current_population_fitness < initial_population_fitness:\n                mutation_scale *= 1.07  # Increased scaling for better adaptability\n            else:\n                mutation_scale *= 0.93  # Increased scaling for better adaptability\n\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n\n            if np.random.rand() < 0.15:  # Increased frequency of new individuals\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 90, "feedback": "The algorithm EnhancedAdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.59440 with standard deviation 0.25954.", "error": "", "parent_ids": ["4d81aadf-cb66-4238-9eab-f6e722698ce3"], "operator": null, "metadata": {"aucs": [0.12654465697744255, 0.7554012714816731, 0.6524967612116765, 0.7116893048582398, 0.788531662794105, 0.784084247644922, 0.10262039672688228, 0.7054083407712781, 0.7228092068570546]}}
{"id": "21c10601-d826-475a-aee0-7fbcef96b9b4", "fitness": 0.6955894256923416, "name": "RefinedEnhancedAdaptiveEMADE", "description": "Implement a multi-strategy approach by combining adaptive mutation with a diversity-preserving mechanism that dynamically adjusts the exploration-exploitation balance.", "code": "import numpy as np\n\nclass RefinedEnhancedAdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        # Use adaptive learning rates initialized to moderate values\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            initial_population_fitness = np.mean(fitness)\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n\n                fitness_variance = np.var(fitness)\n                progress_ratio = evals / self.budget\n                CR = np.clip(0.6 * (1 - fitness_variance / (fitness_variance + 1 + 1e-9)) * (1 - progress_ratio), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if np.random.rand() < 0.15 + 0.1 * (1 - progress_ratio):  # Increased perturbation probability\n                    trial += np.random.normal(0, 0.1 * (1 - progress_ratio), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n            \n            current_population_fitness = np.mean(fitness)\n            if current_population_fitness < initial_population_fitness:\n                mutation_scale *= 1.05\n            else:\n                mutation_scale *= 0.95\n\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n            # Diversity-preserving mechanism\n            if np.random.rand() < 0.05:\n                random_idx = np.random.randint(0, pop_size)\n                pop[random_idx] = np.random.uniform(lb, ub, self.dim)\n                fitness[random_idx] = func(pop[random_idx])\n                evals += 1\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 91, "feedback": "The algorithm RefinedEnhancedAdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.69559 with standard deviation 0.03668.", "error": "", "parent_ids": ["4d81aadf-cb66-4238-9eab-f6e722698ce3"], "operator": null, "metadata": {"aucs": [0.6844990704382523, 0.7024051223468633, 0.7014059662212574, 0.7263994956165866, 0.6557558569962971, 0.7628225386205635, 0.63347375539655, 0.7189851258816671, 0.674557899713037]}}
{"id": "eb316f44-6baa-4381-9669-e478a648416f", "fitness": 0.3603460335372952, "name": "EnhancedAdaptiveEMADEPlus", "description": "EnhancedAdaptiveEMADE+ introduces adaptive mutation scale with elitism and competition, dynamically leveraging the best and worst solutions for accelerated convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveEMADEPlus:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            initial_population_fitness = np.mean(fitness)\n            best_idx = np.argmin(fitness)\n            worst_idx = np.argmax(fitness)\n            \n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n\n                # Use elitism and competition with best and worst individuals\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(pop[best_idx] + mutation_scale * (b - c), lb, ub)\n                else:\n                    mutant = np.clip(a + mutation_scale * (pop[worst_idx] - c), lb, ub)\n\n                fitness_variance = np.var(fitness)\n                progress_ratio = evals / self.budget\n                CR = np.clip(0.6 * (1 - fitness_variance / (fitness_variance + 1 + 1e-9)) * (1 - progress_ratio), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n\n                if np.random.rand() < 0.15:\n                    trial += np.random.normal(0, 0.1 * (1 - progress_ratio), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n\n            current_population_fitness = np.mean(fitness)\n            if current_population_fitness < initial_population_fitness:\n                mutation_scale *= 1.05\n            else:\n                mutation_scale *= 0.95\n\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedAdaptiveEMADEPlus got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.36035 with standard deviation 0.27929.", "error": "", "parent_ids": ["4d81aadf-cb66-4238-9eab-f6e722698ce3"], "operator": null, "metadata": {"aucs": [0.7843000589483918, 0.7385858893374194, 0.7259325689010534, 0.16133117592281843, 0.22086030144561508, 0.24816258355255594, 0.08889548274275794, 0.15913607448813905, 0.11591016649690622]}}
{"id": "473a3c22-bbc9-43fa-afb8-ec2271df175e", "fitness": 0.7291899603874504, "name": "EnhancedAdaptiveEMADE", "description": "Enhance EnhancedAdaptiveEMADE by tuning the crossover rate (CR) adaptation formula for better performance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        # Use adaptive learning rates initialized to moderate values\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            initial_population_fitness = np.mean(fitness)  # Track initial population fitness\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n\n                # Dynamic crossover based on fitness variance and progress ratio (modified)\n                fitness_variance = np.var(fitness)\n                progress_ratio = evals / self.budget\n                CR = np.clip(0.7 * (1 - fitness_variance / (fitness_variance + 1 + 1e-9)) * (1 - progress_ratio), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                # Introduce adaptive Gaussian perturbation for diversity\n                if np.random.rand() < 0.15:\n                    trial += np.random.normal(0, 0.1 * (1 - progress_ratio), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n            \n            # Adapt mutation scale based on population fitness improvement\n            current_population_fitness = np.mean(fitness)\n            if current_population_fitness < initial_population_fitness:\n                mutation_scale *= 1.05\n            else:\n                mutation_scale *= 0.95\n\n            # Dynamic adjustment of population size\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n\n            # Periodically introduce new random individuals\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 93, "feedback": "The algorithm EnhancedAdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.72919 with standard deviation 0.05723.", "error": "", "parent_ids": ["4d81aadf-cb66-4238-9eab-f6e722698ce3"], "operator": null, "metadata": {"aucs": [0.7999786786683205, 0.7404570513385105, 0.761837263843547, 0.7478089041654923, 0.7843293398485673, 0.7218597549591534, 0.59280632174899, 0.6960691316325258, 0.7175631972819472]}}
{"id": "329b5f32-2004-4856-95b3-9ad1519810d9", "fitness": 0.30398909442769856, "name": "EnhancedAdaptiveEMADE", "description": "Improved AdaptiveEMADE by introducing self-adaptive mutation scale and adaptive crossover strategy based on dynamic population diversity and fitness improvement.", "code": "import numpy as np\n\nclass EnhancedAdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        # Use adaptive learning rates initialized to moderate values\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0, pop_size)  # Make mutation scale self-adaptive\n\n        while evals < self.budget:\n            initial_population_fitness = np.mean(fitness)  # Track initial population fitness\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                mutant = np.clip(a + mutation_scale[i] * (b - c), lb, ub)\n\n                # Dynamic crossover based on dynamic population diversity and progress ratio\n                diversity = np.mean(np.std(pop, axis=0)) / (ub - lb).mean()\n                progress_ratio = evals / self.budget\n                CR = np.clip(0.7 * (1 - diversity) * (1 - progress_ratio), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                # Introduce adaptive Gaussian perturbation for diversity\n                if np.random.rand() < 0.15:\n                    trial += np.random.normal(0, 0.1 * (1 - progress_ratio), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                    mutation_scale[i] = mutation_scale[i] * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)  # Self-adaptive mutation\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n            \n            # Adapt mutation scale based on population fitness improvement\n            current_population_fitness = np.mean(fitness)\n            if current_population_fitness < initial_population_fitness:\n                mutation_scale *= 1.05\n            else:\n                mutation_scale *= 0.95\n\n            # Dynamic adjustment of population size\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n\n            # Periodically introduce new random individuals\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 94, "feedback": "The algorithm EnhancedAdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30399 with standard deviation 0.07895.", "error": "", "parent_ids": ["4d81aadf-cb66-4238-9eab-f6e722698ce3"], "operator": null, "metadata": {"aucs": [0.3806513495765006, 0.3605278982693154, 0.33045557476319776, 0.3980020660164406, 0.2671294812538775, 0.2750319155804015, 0.24742482623210205, 0.34666001704040117, 0.13001872111705037]}}
{"id": "8b786ffd-9127-4b9d-b52f-0e016656119b", "fitness": 0.49303617153228735, "name": "EnhancedAdaptiveEMADE", "description": "Enhance EnhancedAdaptiveEMADE by incorporating periodic memory-based elite solutions to guide mutation, improving convergence by learning from historically best solutions.", "code": "import numpy as np\n\nclass EnhancedAdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n        self.elite_solutions = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            initial_population_fitness = np.mean(fitness)\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                # Use elite solution to enhance mutation\n                if self.elite_solutions and np.random.rand() < 0.2:\n                    elite = self.elite_solutions[np.random.randint(len(self.elite_solutions))]\n                    mutant = np.clip(a + mutation_scale * (b - c) + 0.1 * (elite - a), lb, ub)\n                else:\n                    mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n\n                fitness_variance = np.var(fitness)\n                progress_ratio = evals / self.budget\n                CR = np.clip(0.6 * (1 - fitness_variance / (fitness_variance + 1 + 1e-9)) * (1 - progress_ratio), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if np.random.rand() < 0.15:\n                    trial += np.random.normal(0, 0.1 * (1 - progress_ratio), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n                    if len(self.elite_solutions) < 5 or trial_fitness < max([func(elite) for elite in self.elite_solutions]):\n                        self.elite_solutions.append(trial)\n                        if len(self.elite_solutions) > 5:\n                            worst_elite = max(self.elite_solutions, key=func)\n                            self.elite_solutions.remove(worst_elite)\n\n            current_population_fitness = np.mean(fitness)\n            if current_population_fitness < initial_population_fitness:\n                mutation_scale *= 1.05\n            else:\n                mutation_scale *= 0.95\n\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n                    if len(self.elite_solutions) < 5 or new_fitness < max([func(elite) for elite in self.elite_solutions]):\n                        self.elite_solutions.append(new_individual)\n                        if len(self.elite_solutions) > 5:\n                            worst_elite = max(self.elite_solutions, key=func)\n                            self.elite_solutions.remove(worst_elite)\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 95, "feedback": "The algorithm EnhancedAdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.49304 with standard deviation 0.22464.", "error": "", "parent_ids": ["4d81aadf-cb66-4238-9eab-f6e722698ce3"], "operator": null, "metadata": {"aucs": [0.5100834905837104, 0.11224376536864034, 0.6916227767908769, 0.6598047632641932, 0.6592134743540892, 0.6608103554262128, 0.08554693456983942, 0.4361957620135163, 0.6218042214195079]}}
{"id": "a81d2644-7b95-45a9-8bd2-c92b8ce19afe", "fitness": 0.5563822727538607, "name": "EnhancedAdaptiveEMADEv2", "description": "Enhance EnhancedAdaptiveEMADE by integrating a self-adaptive mutation scale and dynamically adjusted crossover probability based on population diversity and performance trends.", "code": "import numpy as np\n\nclass EnhancedAdaptiveEMADEv2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n        \n        mutation_scale = np.random.uniform(0.5, 1.0)\n        adaptive_scale = 0.5\n        \n        while evals < self.budget:\n            initial_population_fitness = np.mean(fitness)\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n                \n                # Adaptive crossover probability based on diversity and progress\n                diversity = np.std(pop, axis=0).mean()\n                progress_ratio = evals / self.budget\n                CR = np.clip(0.6 * (1 - diversity / (diversity + 1e-9)) * (1 - progress_ratio), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if np.random.rand() < 0.15:\n                    trial += np.random.normal(0, 0.1 * (1 - progress_ratio), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    adaptive_scale = min(1.0, adaptive_scale + 0.05)\n                else:\n                    adaptive_scale = max(0.1, adaptive_scale - 0.05)\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n            \n            # Update mutation scale based on fitness improvement and adaptive scaling\n            current_population_fitness = np.mean(fitness)\n            if current_population_fitness < initial_population_fitness:\n                mutation_scale *= 1.05 * adaptive_scale\n            else:\n                mutation_scale *= 0.95 * adaptive_scale\n\n            # Dynamic adjustment of population size\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n\n            # Periodically introduce new random individuals\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 96, "feedback": "The algorithm EnhancedAdaptiveEMADEv2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.55638 with standard deviation 0.28832.", "error": "", "parent_ids": ["4d81aadf-cb66-4238-9eab-f6e722698ce3"], "operator": null, "metadata": {"aucs": [0.12027339241097146, 0.7311251793647512, 0.7587977125785517, 0.1882879531342, 0.8049011922306651, 0.7811883776036046, 0.14281983027773992, 0.7500305096537591, 0.7300163075305022]}}
{"id": "64303644-a036-4266-905f-993285d5ab19", "fitness": 0.40865436704067737, "name": "EnhancedAdaptiveEMADE", "description": "Introduce a population fitness-based mutation and crossover adjustment strategy to improve convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            initial_population_fitness = np.mean(fitness)\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                # Update mutation scale based on the best fitness improvement\n                if np.random.rand() < 0.2:\n                    mutation_scale = np.clip(mutation_scale + 0.05 * (1 - fitness[i] / self.best_fitness), 0.1, 1.0)\n                \n                mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n                \n                fitness_variance = np.var(fitness)\n                progress_ratio = evals / self.budget\n                CR = np.clip(0.6 * (1 - fitness_variance / (fitness_variance + 1 + 1e-9)) * (1 - progress_ratio), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if np.random.rand() < 0.15:\n                    trial += np.random.normal(0, 0.1 * (1 - progress_ratio), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n            \n            current_population_fitness = np.mean(fitness)\n            if current_population_fitness < initial_population_fitness:\n                mutation_scale *= 1.05\n            else:\n                mutation_scale *= 0.95\n\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 97, "feedback": "The algorithm EnhancedAdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40865 with standard deviation 0.08599.", "error": "", "parent_ids": ["4d81aadf-cb66-4238-9eab-f6e722698ce3"], "operator": null, "metadata": {"aucs": [0.4794242199894264, 0.555711209031768, 0.43832459511639765, 0.2764561120203134, 0.4913421975157003, 0.3414329807605627, 0.41957391664031396, 0.3328984650908875, 0.34272560720072576]}}
{"id": "0a20bea5-0d14-4f7c-baba-75d3e22adcf7", "fitness": 0.6691650164944247, "name": "EnhancedAdaptiveEMADE", "description": "Integrate a dynamic feedback mechanism to adapt crossover rates and mutation scales based on population diversity and convergence speed in EnhancedAdaptiveEMADE for enhanced exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        # Use adaptive learning rates initialized to moderate values\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            initial_population_fitness = np.mean(fitness)  # Track initial population fitness\n            diversity = np.mean(np.std(pop, axis=0))  # Measure population diversity\n            convergence_speed = np.abs(initial_population_fitness - np.mean(fitness))  # Measure convergence speed\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n\n                # Dynamic crossover based on population diversity and convergence speed\n                CR = np.clip(0.6 * (1 - diversity / (diversity + 1 + 1e-9)) * (1 - evals / self.budget), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                # Introduce adaptive Gaussian perturbation for diversity\n                if np.random.rand() < 0.15:\n                    trial += np.random.normal(0, 0.1 * (1 - evals / self.budget), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n            \n            # Adapt mutation scale based on population fitness improvement and convergence speed\n            current_population_fitness = np.mean(fitness)\n            if current_population_fitness < initial_population_fitness:\n                mutation_scale *= (1.05 + 0.1 * convergence_speed)\n            else:\n                mutation_scale *= (0.95 - 0.1 * convergence_speed)\n\n            # Dynamic adjustment of population size\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.3)))\n\n            # Periodically introduce new random individuals\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 98, "feedback": "The algorithm EnhancedAdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.66917 with standard deviation 0.16645.", "error": "", "parent_ids": ["4d81aadf-cb66-4238-9eab-f6e722698ce3"], "operator": null, "metadata": {"aucs": [0.7999786786683205, 0.7404570513385105, 0.761837263843547, 0.22833939262896297, 0.7903319315788535, 0.6951021797281646, 0.59280632174899, 0.6960691316325258, 0.7175631972819472]}}
{"id": "a297d01c-e13e-433d-b243-66e85ad82909", "fitness": 0.7316418265624036, "name": "EnhancedAdaptiveEMADE", "description": "Enhance EnhancedAdaptiveEMADE by incorporating a self-adaptive population size and dynamic learning rate adjustment based on exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveEMADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_population_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        evals = pop_size\n\n        learning_rates = np.random.uniform(0.6, 0.9, pop_size)\n        mutation_scale = np.random.uniform(0.5, 1.0)\n\n        while evals < self.budget:\n            initial_population_fitness = np.mean(fitness)\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                mutant = np.clip(a + mutation_scale * (b - c), lb, ub)\n\n                fitness_variance = np.var(fitness)\n                progress_ratio = evals / self.budget\n                CR = np.clip(0.6 * (1 - fitness_variance / (fitness_variance + 1 + 1e-9)) * (1 - progress_ratio), 0.3, 0.9)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if np.random.rand() < 0.15:\n                    trial += np.random.normal(0, 0.1 * (1 - progress_ratio), self.dim)\n\n                trial_fitness = func(trial)\n                evals += 1\n                if evals >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    learning_rates[i] = learning_rates[i] + 0.1 * (1 - learning_rates[i])\n                    mutation_scale = mutation_scale * 0.9 + 0.1 * np.abs(fitness[i] - trial_fitness)\n                else:\n                    learning_rates[i] = learning_rates[i] - 0.1 * learning_rates[i]\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial\n            \n            current_population_fitness = np.mean(fitness)\n            if current_population_fitness < initial_population_fitness:\n                mutation_scale *= 1.05\n            else:\n                mutation_scale *= 0.95\n\n            if evals / self.budget > 0.2:\n                pop_size = max(5, int(self.initial_population_size * (1 - (evals / self.budget) ** 1.5)))\n\n            if np.random.rand() < 0.1:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_individual\n\n        return self.best_solution, self.best_fitness", "configspace": "", "generation": 99, "feedback": "The algorithm EnhancedAdaptiveEMADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.73164 with standard deviation 0.04529.", "error": "", "parent_ids": ["4d81aadf-cb66-4238-9eab-f6e722698ce3"], "operator": null, "metadata": {"aucs": [0.790571324918562, 0.7347904132792221, 0.7295580940034654, 0.7286785176117838, 0.7840524860839568, 0.7518599321564639, 0.6251082833604964, 0.7102058807076699, 0.7299515069400129]}}
