{"id": "bfdeb49c-f58c-480d-a24d-452e86fb0895", "fitness": 0.08798405912211615, "name": "APSO_GP", "description": "Adaptive Particle Swarm Optimization with Gaussian Perturbation (APSO-GP) that balances exploration and exploitation dynamically using a Gaussian perturbation mechanism.", "code": "import numpy as np\n\nclass APSO_GP:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)  # Population size scales with budget\n        self.inertia = 0.7\n        self.cognitive = 1.5\n        self.social = 1.5\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n        \n        global_best_position = None\n        global_best_score = np.inf\n        \n        for _ in range(self.budget // self.pop_size):\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            # Update velocities and positions\n            r1, r2 = np.random.rand(), np.random.rand()\n            for i in range(self.pop_size):\n                velocities[i] = (self.inertia * velocities[i]\n                                 + self.cognitive * r1 * (personal_best_positions[i] - particles[i])\n                                 + self.social * r2 * (global_best_position - particles[i]))\n                \n                particles[i] += velocities[i]\n                \n                # Apply Gaussian perturbation to enhance exploration\n                if np.random.rand() < 0.1:  # 10% chance to perturb\n                    particles[i] = np.clip(particles[i] + np.random.normal(0, 0.1, self.dim), lb, ub)\n                else:\n                    particles[i] = np.clip(particles[i], lb, ub)\n        \n        return global_best_position, global_best_score", "configspace": "", "generation": 0, "feedback": "The algorithm APSO_GP got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08798 with standard deviation 0.04246.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.06418690474424982, 0.05635221686593994, 0.05671368541798427, 0.15087237071174997, 0.14585094254110864, 0.1455656261474868, 0.06877262720010058, 0.05762160517482362, 0.04592055329560174]}}
{"id": "42a798a6-b091-4da0-ab45-bcfead76222a", "fitness": 0.08777648743733682, "name": "APSO_GP", "description": "Enhanced APSO_GP with increased Gaussian perturbation probability to improve the exploration phase.", "code": "import numpy as np\n\nclass APSO_GP:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)  # Population size scales with budget\n        self.inertia = 0.7\n        self.cognitive = 1.5\n        self.social = 1.5\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n        \n        global_best_position = None\n        global_best_score = np.inf\n        \n        for _ in range(self.budget // self.pop_size):\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            # Update velocities and positions\n            r1, r2 = np.random.rand(), np.random.rand()\n            for i in range(self.pop_size):\n                velocities[i] = (self.inertia * velocities[i]\n                                 + self.cognitive * r1 * (personal_best_positions[i] - particles[i])\n                                 + self.social * r2 * (global_best_position - particles[i]))\n                \n                particles[i] += velocities[i]\n                \n                # Apply Gaussian perturbation to enhance exploration\n                if np.random.rand() < 0.2:  # 20% chance to perturb (increased from 10%)\n                    particles[i] = np.clip(particles[i] + np.random.normal(0, 0.1, self.dim), lb, ub)\n                else:\n                    particles[i] = np.clip(particles[i], lb, ub)\n        \n        return global_best_position, global_best_score", "configspace": "", "generation": 1, "feedback": "The algorithm APSO_GP got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08778 with standard deviation 0.04386.", "error": "", "parent_ids": ["bfdeb49c-f58c-480d-a24d-452e86fb0895"], "operator": null, "metadata": {"aucs": [0.060277291366301, 0.06525688304643762, 0.0561379877073902, 0.1551395146000476, 0.15111918132422242, 0.14182557528303108, 0.048996903119107316, 0.057457819023227, 0.05377723146626712]}}
{"id": "2d59e6c2-8d3e-449f-983d-3ce756d49ced", "fitness": 0.08585072334828363, "name": "Enhanced_APSO_GP", "description": "Enhanced APSO-GP with dynamic inertia adjustment and adaptive Gaussian perturbation for improved convergence and solution quality.", "code": "import numpy as np\n\nclass Enhanced_APSO_GP:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)  # Population size scales with budget\n        self.inertia = 0.9  # Start with higher inertia\n        self.cognitive = 1.5\n        self.social = 1.5\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n        \n        global_best_position = None\n        global_best_score = np.inf\n        \n        for iteration in range(self.budget // self.pop_size):\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            # Dynamically adjust inertia\n            self.inertia = 0.9 - 0.5 * (iteration / (self.budget // self.pop_size))\n            \n            # Update velocities and positions\n            r1, r2 = np.random.rand(), np.random.rand()\n            for i in range(self.pop_size):\n                velocities[i] = (self.inertia * velocities[i]\n                                 + self.cognitive * r1 * (personal_best_positions[i] - particles[i])\n                                 + self.social * r2 * (global_best_position - particles[i]))\n                \n                particles[i] += velocities[i]\n                \n                # Adaptive Gaussian perturbation\n                perturbation_probability = 0.1 + 0.1 * (iteration / (self.budget // self.pop_size))\n                if np.random.rand() < perturbation_probability:\n                    particles[i] = np.clip(particles[i] + np.random.normal(0, 0.1, self.dim), lb, ub)\n                else:\n                    particles[i] = np.clip(particles[i], lb, ub)\n        \n        return global_best_position, global_best_score", "configspace": "", "generation": 2, "feedback": "The algorithm Enhanced_APSO_GP got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08585 with standard deviation 0.04145.", "error": "", "parent_ids": ["bfdeb49c-f58c-480d-a24d-452e86fb0895"], "operator": null, "metadata": {"aucs": [0.06437464471496479, 0.06127003272746401, 0.052386396647835176, 0.15131166042039146, 0.141205513058956, 0.13903294596263682, 0.05976337544553112, 0.058108077559825944, 0.04520386359694728]}}
{"id": "faee1fe9-690d-4dfc-ba89-8a9b22a4843c", "fitness": 0.08361531450541876, "name": "APSO_ANL", "description": "Enhanced APSO with Adaptive Neighborhood Learning (APSO-ANL) introduces dynamic learning from neighboring particles to improve convergence.", "code": "import numpy as np\n\nclass APSO_ANL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)  # Population size scales with budget\n        self.inertia = 0.7\n        self.cognitive = 1.5\n        self.social = 1.5\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n        \n        global_best_position = None\n        global_best_score = np.inf\n        \n        for _ in range(self.budget // self.pop_size):\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            # Update velocities and positions\n            r1, r2 = np.random.rand(), np.random.rand()\n            for i in range(self.pop_size):\n                velocities[i] = (self.inertia * velocities[i]\n                                 + self.cognitive * r1 * (personal_best_positions[i] - particles[i])\n                                 + self.social * r2 * (global_best_position - particles[i]))\n\n                # Learning from neighbor particles\n                neighbor_index = (i + 1) % self.pop_size  \n                velocities[i] += 0.1 * (personal_best_positions[neighbor_index] - particles[i])\n                \n                particles[i] += velocities[i]\n                \n                # Apply Gaussian perturbation to enhance exploration\n                if np.random.rand() < 0.1:  # 10% chance to perturb\n                    particles[i] = np.clip(particles[i] + np.random.normal(0, 0.1, self.dim), lb, ub)\n                else:\n                    particles[i] = np.clip(particles[i], lb, ub)\n        \n        return global_best_position, global_best_score", "configspace": "", "generation": 3, "feedback": "The algorithm APSO_ANL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08362 with standard deviation 0.04197.", "error": "", "parent_ids": ["bfdeb49c-f58c-480d-a24d-452e86fb0895"], "operator": null, "metadata": {"aucs": [0.05520779467072545, 0.056277044752212624, 0.055374807302148166, 0.14160254846193365, 0.14228657948288737, 0.14454412099060632, 0.05831037607485179, 0.04709678959668617, 0.05183776921671723]}}
{"id": "e9996a7d-6638-439d-874a-0543d0e06468", "fitness": 0.0880471337701095, "name": "EPSO_DSL", "description": "Enhanced Particle Swarm Optimization with Dynamic Social Learning (EPSO-DSL) introduces a dynamically adjusted social component to prevent premature convergence and incorporates adaptive velocity scaling for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass EPSO_DSL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia = 0.7\n        self.cognitive = 1.5\n        self.social_min = 1.0\n        self.social_max = 3.0\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n        \n        global_best_position = None\n        global_best_score = np.inf\n        \n        for iteration in range(self.budget // self.pop_size):\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n            \n            social = self.social_min + (self.social_max - self.social_min) * np.exp(-iteration / (self.budget / self.pop_size))\n            \n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (self.inertia * velocities\n                          + self.cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n            \n            if np.random.rand() < 0.2:  # 20% chance to apply Gaussian perturbation\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n        \n        return global_best_position, global_best_score", "configspace": "", "generation": 4, "feedback": "The algorithm EPSO_DSL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08805 with standard deviation 0.03843.", "error": "", "parent_ids": ["bfdeb49c-f58c-480d-a24d-452e86fb0895"], "operator": null, "metadata": {"aucs": [0.06307090580106112, 0.07065145761780911, 0.06375611217578381, 0.14392507972630186, 0.14420400818778178, 0.1371701395368945, 0.05631626265314815, 0.06416484010596823, 0.04916539812623699]}}
{"id": "8965a707-3b26-45d8-a59c-5b8639de30aa", "fitness": 0.08659286505539035, "name": "EPSO_DSL", "description": "Enhanced Particle Swarm Optimization with Dynamic Social Learning (EPSO-DSL+) improves the adaptive social component with nonlinear velocity damping and variable mutation for better convergence.", "code": "import numpy as np\n\nclass EPSO_DSL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia = 0.7\n        self.cognitive = 1.5\n        self.social_min = 1.0\n        self.social_max = 3.0\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n        \n        global_best_position = None\n        global_best_score = np.inf\n        \n        for iteration in range(self.budget // self.pop_size):\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n            \n            social = self.social_min + (self.social_max - self.social_min) * np.exp(-iteration / (self.budget / self.pop_size))\n            \n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            damping_factor = 0.9 - (iteration / (self.budget / self.pop_size) * 0.5)  # Nonlinear damping\n            velocities = (damping_factor * velocities\n                          + self.cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n            \n            particles += velocities\n            \n            if np.random.rand() < 0.3:  # 30% chance to apply Gaussian perturbation\n                perturbation = np.random.normal(0, 0.05, (self.pop_size, self.dim))  # Reduced perturbation scale\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n        \n        return global_best_position, global_best_score", "configspace": "", "generation": 5, "feedback": "The algorithm EPSO_DSL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08659 with standard deviation 0.03937.", "error": "", "parent_ids": ["e9996a7d-6638-439d-874a-0543d0e06468"], "operator": null, "metadata": {"aucs": [0.058603173159841315, 0.05598815735150098, 0.06906100788311731, 0.1391374570525038, 0.14034702490706397, 0.14584840964223256, 0.057759568409828654, 0.04956289819540394, 0.0630280888970206]}}
{"id": "4aadd256-c638-4886-a5e8-b2164ff50140", "fitness": 0.0864149908179344, "name": "AI_MEPSO", "description": "Adaptive Inertia and Mutation Enhanced Particle Swarm Optimization (AI-MEPSO) leverages time-varying inertia and mutation strategies to balance exploration and exploitation for improved convergence.", "code": "import numpy as np\n\nclass AI_MEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.social_min = 1.0\n        self.social_max = 3.0\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n        \n        global_best_position = None\n        global_best_score = np.inf\n        \n        iteration = 0\n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n            \n            w = self.w_max - ((self.w_max - self.w_min) * (iteration / (self.budget // self.pop_size)))\n            social = self.social_min + (self.social_max - self.social_min) * np.exp(-iteration / (self.budget / self.pop_size))\n            \n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (w * velocities\n                          + self.c1 * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n            \n            if self.evaluations / self.budget < 0.5 and np.random.rand() < 0.3:  # 30% chance for mutation in early stages\n                mutation = np.random.normal(0, 0.05, (self.pop_size, self.dim))\n                particles = np.clip(particles + mutation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n            iteration += 1\n        \n        return global_best_position, global_best_score", "configspace": "", "generation": 6, "feedback": "The algorithm AI_MEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08641 with standard deviation 0.03911.", "error": "", "parent_ids": ["e9996a7d-6638-439d-874a-0543d0e06468"], "operator": null, "metadata": {"aucs": [0.0579837354089493, 0.05598815735150098, 0.06948228735388562, 0.1391374570525038, 0.14034702490706397, 0.14417159032588156, 0.05779811898126552, 0.04932149518143025, 0.06350505079892854]}}
{"id": "3440642d-d175-47f2-83c0-ff9a297db6d6", "fitness": 0.09184488847377098, "name": "AQ_PSO", "description": "Adaptive Quantum-PSO (AQ-PSO) enhances EPSO_DSL by integrating a quantum-behavior-inspired update mechanism to improve convergence speed and accuracy, adapting positions based on probability amplitudes for refined exploration and exploitation.", "code": "import numpy as np\n\nclass AQ_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia = 0.7\n        self.cognitive = 1.5\n        self.social_min = 1.0\n        self.social_max = 3.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1  # Probability of entering quantum behavior\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n        \n        global_best_position = None\n        global_best_score = np.inf\n        \n        for iteration in range(self.budget // self.pop_size):\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n            \n            social = self.social_min + (self.social_max - self.social_min) * np.exp(-iteration / (self.budget / self.pop_size))\n            \n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (self.inertia * velocities\n                          + self.cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n            \n            particles += velocities\n            \n            if np.random.rand() < self.quantum_prob:  # Apply quantum behavior\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n            \n            # Gaussian perturbation or boundary check\n            if np.random.rand() < 0.2:  \n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n        \n        return global_best_position, global_best_score", "configspace": "", "generation": 7, "feedback": "The algorithm AQ_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09184 with standard deviation 0.03878.", "error": "", "parent_ids": ["e9996a7d-6638-439d-874a-0543d0e06468"], "operator": null, "metadata": {"aucs": [0.07439815568365771, 0.06885626867842964, 0.0690446437398099, 0.14424820883247103, 0.14823344108571568, 0.14534912975948433, 0.06794461073680613, 0.054025776860459396, 0.05450376088710507]}}
{"id": "6367bbba-8f56-435d-b432-55162ffd49b1", "fitness": 0.09122689856910551, "name": "AQ_PSO", "description": "A refined AQ-PSO with adaptive quantum probability based on iteration, enhancing exploration in early stages and focusing on exploitation later.", "code": "import numpy as np\n\nclass AQ_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia = 0.7\n        self.cognitive = 1.5\n        self.social_min = 1.0\n        self.social_max = 3.0\n        self.evaluations = 0\n        self.quantum_prob_base = 0.1  # Base probability of entering quantum behavior\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n        \n        global_best_position = None\n        global_best_score = np.inf\n        \n        for iteration in range(self.budget // self.pop_size):\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n            \n            social = self.social_min + (self.social_max - self.social_min) * np.exp(-iteration / (self.budget / self.pop_size))\n            \n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (self.inertia * velocities\n                          + self.cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n            \n            particles += velocities\n            \n            quantum_prob = self.quantum_prob_base * (1 - iteration / (self.budget // self.pop_size))  # Adaptive probability\n            if np.random.rand() < quantum_prob:  # Apply quantum behavior\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n            \n            # Gaussian perturbation or boundary check\n            if np.random.rand() < 0.2:  \n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n        \n        return global_best_position, global_best_score", "configspace": "", "generation": 8, "feedback": "The algorithm AQ_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09123 with standard deviation 0.03886.", "error": "", "parent_ids": ["3440642d-d175-47f2-83c0-ff9a297db6d6"], "operator": null, "metadata": {"aucs": [0.07280556644338654, 0.06932024161755723, 0.0690446437398099, 0.14459287611853333, 0.14807904903600233, 0.14387979771383008, 0.06470770852240582, 0.054025776860459396, 0.05458642706996497]}}
{"id": "e3fa6e44-4ac7-4ff6-b1b2-2bdf23f0bad5", "fitness": 0.09081962115217096, "name": "QE_PSO", "description": "Quantum Enhanced-PSO (QE-PSO) introduces adaptive inertia and dynamic quantum probability to enhance convergence by balancing exploration and exploitation more effectively.", "code": "import numpy as np\n\nclass QE_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.cognitive = 1.5\n        self.social_min = 1.0\n        self.social_max = 3.0\n        self.evaluations = 0\n        self.quantum_prob_min = 0.1\n        self.quantum_prob_max = 0.3\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n        \n        global_best_position = None\n        global_best_score = np.inf\n        \n        for iteration in range(self.budget // self.pop_size):\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n            \n            social = self.social_min + (self.social_max - self.social_min) * np.exp(-iteration / (self.budget / self.pop_size))\n            inertia = self.inertia_max - (self.inertia_max - self.inertia_min) * (iteration / (self.budget / self.pop_size))\n            quantum_prob = self.quantum_prob_min + (self.quantum_prob_max - self.quantum_prob_min) * (iteration / (self.budget / self.pop_size))\n            \n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + self.cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n            \n            particles += velocities\n            \n            if np.random.rand() < quantum_prob:  # Apply quantum behavior\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n            \n            # Gaussian perturbation or boundary check\n            if np.random.rand() < 0.2:  \n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n        \n        return global_best_position, global_best_score", "configspace": "", "generation": 9, "feedback": "The algorithm QE_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09082 with standard deviation 0.03715.", "error": "", "parent_ids": ["3440642d-d175-47f2-83c0-ff9a297db6d6"], "operator": null, "metadata": {"aucs": [0.06133543826328114, 0.07530024173188266, 0.06407067580524572, 0.1412577267269557, 0.1432747698140706, 0.14385064485254406, 0.06177847497827582, 0.0710883108904442, 0.05542030730683878]}}
{"id": "b71bf991-6722-4f6a-8f9a-e6dc7a4363b3", "fitness": 0.08936232520916933, "name": "Enhanced_AQ_PSO", "description": "Enhanced AQ-PSO with adaptive quantum behavior, dynamic velocity scaling, and improved exploration mechanisms to increase convergence speed and solution accuracy.", "code": "import numpy as np\n\nclass Enhanced_AQ_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(60, budget // 8)  # Increase population size\n        self.inertia = 0.6  # Adjusted for better momentum control\n        self.cognitive = 2.0  # Enhanced influence of personal best\n        self.social_min = 0.5  # Broadened social range for exploration\n        self.social_max = 2.5\n        self.evaluations = 0\n        self.quantum_prob = 0.15  # Increased probability for quantum behavior\n        self.delta_scale = 0.9  # Dynamic scaling for velocity step size\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n        \n        global_best_position = None\n        global_best_score = np.inf\n        \n        for iteration in range(self.budget // self.pop_size):\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n            \n            social = self.social_min + (self.social_max - self.social_min) * np.exp(-iteration / (self.budget / self.pop_size))\n            \n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (self.inertia * velocities\n                          + self.cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n            \n            particles += velocities * self.delta_scale  # Scale velocity for dynamic adjustment\n            \n            if np.random.rand() < self.quantum_prob:  # Apply quantum behavior\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(3)  # Adjusted spread factor\n            \n            if np.random.rand() < 0.3:  # Increase perturbation probability\n                perturbation = np.random.normal(0, 0.15, (self.pop_size, self.dim))  # Enhance perturbation effect\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n        \n        return global_best_position, global_best_score", "configspace": "", "generation": 10, "feedback": "The algorithm Enhanced_AQ_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08936 with standard deviation 0.03818.", "error": "", "parent_ids": ["3440642d-d175-47f2-83c0-ff9a297db6d6"], "operator": null, "metadata": {"aucs": [0.07676884154775643, 0.05984881934807784, 0.06261466561996454, 0.13986073747115557, 0.1460460632394356, 0.14149721773147006, 0.0689420723888231, 0.05101866094384511, 0.05766384859199569]}}
{"id": "529021a1-7723-4566-87d9-d1d40382d5aa", "fitness": 0.08817344786773332, "name": "EAQ_PSO", "description": "Enhanced Adaptive Quantum-PSO (EAQ_PSO) introduces dynamic inertia and Gaussian perturbation based on iteration count to balance exploration and exploitation for better convergence.", "code": "import numpy as np\n\nclass EAQ_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_start = 0.9\n        self.inertia_end = 0.4\n        self.cognitive = 1.5\n        self.social_min = 1.0\n        self.social_max = 3.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1  # Probability of entering quantum behavior\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n        \n        global_best_position = None\n        global_best_score = np.inf\n        \n        for iteration in range(self.budget // self.pop_size):\n            inertia = self.inertia_end + (self.inertia_start - self.inertia_end) * (1 - iteration / (self.budget / self.pop_size))\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n            \n            social = self.social_min + (self.social_max - self.social_min) * np.exp(-iteration / (self.budget / self.pop_size))\n            \n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + self.cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n            \n            particles += velocities\n            \n            if np.random.rand() < self.quantum_prob:  # Apply quantum behavior\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n            \n            # Gaussian perturbation or boundary check\n            if np.random.rand() < 0.2:  \n                perturbation = np.random.normal(0, 0.1 * (1 - iteration / (self.budget / self.pop_size)), (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n        \n        return global_best_position, global_best_score", "configspace": "", "generation": 11, "feedback": "The algorithm EAQ_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08817 with standard deviation 0.03843.", "error": "", "parent_ids": ["3440642d-d175-47f2-83c0-ff9a297db6d6"], "operator": null, "metadata": {"aucs": [0.06047609274628862, 0.06853006291795571, 0.06154862846730469, 0.14180280832551306, 0.14093656825921952, 0.1441517573044795, 0.058970564024370065, 0.06233024277552113, 0.05481430598894754]}}
{"id": "000e293e-0ac7-43a1-b452-44f6c0bf88e1", "fitness": 0.08903042878528315, "name": "QE_APSO", "description": "Quantum-Enhanced Adaptive PSO (QE-APSO) introduces a dynamic quantum amplitude adaptation and stochastic turbulence mechanism to further optimize exploration and exploitation balance, enhancing convergence speed and solution accuracy.", "code": "import numpy as np\n\nclass QE_APSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia = 0.7\n        self.cognitive = 1.5\n        self.social_min = 1.0\n        self.social_max = 3.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n        self.turbulence_prob = 0.05\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n        \n        global_best_position = None\n        global_best_score = np.inf\n        \n        for iteration in range(self.budget // self.pop_size):\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n            \n            social = self.social_min + (self.social_max - self.social_min) * np.exp(-iteration / (self.budget / self.pop_size))\n            \n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (self.inertia * velocities\n                          + self.cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n            \n            particles += velocities\n            \n            if np.random.rand() < self.quantum_prob:  # Apply quantum behavior\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                quantum_amplitude = np.random.uniform(0.5, 1.5)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta * quantum_amplitude / np.sqrt(2)\n            \n            if np.random.rand() < self.turbulence_prob:  # Apply stochastic turbulence\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles += perturbation\n            \n            particles = np.clip(particles, lb, ub)\n        \n        return global_best_position, global_best_score", "configspace": "", "generation": 12, "feedback": "The algorithm QE_APSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08903 with standard deviation 0.03954.", "error": "", "parent_ids": ["3440642d-d175-47f2-83c0-ff9a297db6d6"], "operator": null, "metadata": {"aucs": [0.06304971208844956, 0.06799656550820632, 0.06745840566321293, 0.14475749461354537, 0.14182792566912472, 0.14713080027970327, 0.05902208818291754, 0.055906767162264615, 0.05412409990012401]}}
{"id": "c8e46721-3a45-4d2f-9077-50a9bdb8b1b1", "fitness": 0.0844012692755637, "name": "QIA_PSO", "description": "Quantum-Inspired Adaptive PSO (QIA-PSO) augments AQ-PSO by introducing adaptive inertia and quantum-inspired tunneling to enhance dynamic exploration and exploitation balance, boosting convergence speed and robustness.", "code": "import numpy as np\n\nclass QIA_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_max = 0.9\n        self.inertia_min = 0.4\n        self.cognitive = 1.5\n        self.social_min = 1.0\n        self.social_max = 3.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1  # Probability of entering quantum behavior\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n        \n        global_best_position = None\n        global_best_score = np.inf\n        \n        for iteration in range(self.budget // self.pop_size):\n            inertia = self.inertia_max - ((self.inertia_max - self.inertia_min) * (iteration / (self.budget / self.pop_size)))\n            \n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n            \n            social = self.social_min + (self.social_max - self.social_min) * np.exp(-iteration / (self.budget / self.pop_size))\n            \n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + self.cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n            \n            particles += velocities\n            \n            if np.random.rand() < self.quantum_prob:  # Apply quantum behavior\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n            \n            # Quantum tunneling for escaping local minima\n            if np.random.rand() < 0.02:\n                particles += np.random.normal(0, 0.5, (self.pop_size, self.dim))\n            \n            # Gaussian perturbation or boundary check\n            if np.random.rand() < 0.2:  \n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n        \n        return global_best_position, global_best_score", "configspace": "", "generation": 13, "feedback": "The algorithm QIA_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08440 with standard deviation 0.03734.", "error": "", "parent_ids": ["3440642d-d175-47f2-83c0-ff9a297db6d6"], "operator": null, "metadata": {"aucs": [0.059003380564264796, 0.0630149300441335, 0.05753654791889473, 0.14042226320410556, 0.13454528543906707, 0.13614615974728572, 0.05913387423469185, 0.05785740384051663, 0.0519515784871134]}}
{"id": "0cc02cd1-3255-44b4-86e5-441269d77abf", "fitness": 0.09279393718444001, "name": "QED_PSO", "description": "Quantum-Enhanced Dynamic Adaptive PSO (QED-PSO) introduces dynamic parameter adaptation and enhanced quantum behavior to improve exploration-exploitation balance, leading to faster and more accurate convergence.", "code": "import numpy as np\n\nclass QED_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 14, "feedback": "The algorithm QED_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09279 with standard deviation 0.03834.", "error": "", "parent_ids": ["3440642d-d175-47f2-83c0-ff9a297db6d6"], "operator": null, "metadata": {"aucs": [0.06465775006976027, 0.09246153914624688, 0.06059311027900216, 0.14937228779964284, 0.14393391182610504, 0.14162606887988094, 0.06178123650769962, 0.0685826372049605, 0.052136892946661906]}}
{"id": "418fd87d-d1ac-488d-9459-8a278475f3c2", "fitness": 0.08889497298412051, "name": "QA_PSO", "description": "Quantum Adaptive PSO (QA-PSO) introduces adaptive quantum perturbation and dynamic diversity control to maintain population diversity and enhance convergence speed and accuracy.", "code": "import numpy as np\n\nclass QA_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n        self.diversity_threshold = 0.1\n        self.diversity_control_factor = 0.5\n\n    def diversity(self, particles):\n        return np.mean(np.std(particles, axis=0))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            current_diversity = self.diversity(particles)\n            if current_diversity < self.diversity_threshold:\n                diversity_boost = np.random.uniform(-self.diversity_control_factor, self.diversity_control_factor, (self.pop_size, self.dim))\n                particles += diversity_boost\n\n            particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 15, "feedback": "The algorithm QA_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08889 with standard deviation 0.03852.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06543821877949585, 0.06786794963417098, 0.061697465969359544, 0.1407090308032729, 0.14214389377447834, 0.1460501036281594, 0.0644004650936657, 0.061249585885216895, 0.05049804328926499]}}
{"id": "12d31dd3-37b8-418f-bfc9-ee0212c2fe77", "fitness": 0.09152882166304943, "name": "QED_DEPSO", "description": "Quantum-Enhanced Dynamic Adaptive PSO with Differential Evolution (QED-DEPSO) integrates differential evolution for enhanced diversity and convergence speed, refining particle positions alongside quantum mechanisms.", "code": "import numpy as np\n\nclass QED_DEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n        self.DE_prob = 0.3\n        self.F = 0.5\n        self.CR = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < self.DE_prob:\n                for i in range(self.pop_size):\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    a, b, c = particles[indices]\n                    mutant = np.clip(a + self.F * (b - c), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, particles[i])\n                    if self.evaluations < self.budget:\n                        trial_score = func(trial)\n                        self.evaluations += 1\n                        if trial_score < personal_best_scores[i]:\n                            personal_best_scores[i] = trial_score\n                            particles[i] = trial\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 16, "feedback": "The algorithm QED_DEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09153 with standard deviation 0.03780.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.07014153712312365, 0.07481290453759315, 0.05656910865343656, 0.14690343397750671, 0.1479867160802547, 0.1378547920819887, 0.056198715420544665, 0.06393009417912987, 0.06936209291386686]}}
{"id": "3c22e681-06bc-4208-81be-e5a8f4d35c34", "fitness": 0.08899169129115121, "name": "QED_PSO", "description": "Introduce adaptive quantum probability modulation and enhance velocity update to refine exploration-exploitation balance.", "code": "import numpy as np\n\nclass QED_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob_init = 0.1\n        self.quantum_prob_final = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n            quantum_prob = self.quantum_prob_init * (1 - progress) + self.quantum_prob_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles)\n                          + np.random.uniform(-0.1, 0.1, (self.pop_size, self.dim)))  # Enhanced velocity update\n\n            particles += velocities\n\n            if np.random.rand() < quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 17, "feedback": "The algorithm QED_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08899 with standard deviation 0.04086.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.07601366340187177, 0.06178018350477654, 0.056610973422778654, 0.15730835297506574, 0.13846256612997632, 0.1400856332609045, 0.06923920001359829, 0.05394889940546321, 0.047475749505925924]}}
{"id": "b789ae9e-1ec1-47ce-97b5-a208c04fe1f2", "fitness": 0.09177366599437348, "name": "QED_PSO_DQP", "description": "Quantum-Enhanced Dynamic Adaptive PSO (QED-PSO) with Dynamic Quantum Probability (DQP) introduces adaptive quantum transitions based on convergence progress to further enhance the exploration-exploitation balance, allowing for superior adaptation to the landscape.", "code": "import numpy as np\n\nclass QED_PSO_DQP:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob_init = 0.1\n        self.quantum_prob_final = 0.3\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n            quantum_prob = self.quantum_prob_init * (1 - progress) + self.quantum_prob_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 18, "feedback": "The algorithm QED_PSO_DQP got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09177 with standard deviation 0.03823.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.061378748067386635, 0.08418319566487109, 0.06294601643505982, 0.14937228779964284, 0.14376212773007324, 0.14074371588704393, 0.06128278191387271, 0.06870716550090716, 0.05358695495050392]}}
{"id": "2ef7c708-0f4b-4c20-aa61-848447dccec5", "fitness": 0.08808490737528735, "name": "AQI_PSO", "description": "Adaptive Quantum-Inspired PSO (AQI-PSO) enhances the QED-PSO by introducing adaptive quantum probabilities and chaos-driven perturbations for improved global exploration and convergence.", "code": "import numpy as np\n\nclass AQI_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob_init = 0.1\n        self.quantum_prob_final = 0.3  # Increase over time for more exploration\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n            quantum_prob = self.quantum_prob_init * (1 - progress) + self.quantum_prob_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            # Chaos-driven perturbation with logistic map\n            perturbation_factor = 4 * progress * (1 - progress)  # Logistic map\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, perturbation_factor, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 19, "feedback": "The algorithm AQI_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08808 with standard deviation 0.04031.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06269784496863795, 0.06867555223231558, 0.060628970895607814, 0.14937228779964284, 0.1431148906563806, 0.14148886522463933, 0.06128278191387271, 0.05282700807494756, 0.05267596461154167]}}
{"id": "af80a85c-cbb5-491d-87e2-f2cc6345eaf5", "fitness": 0.0919657585634053, "name": "QED_PSO", "description": "Quantum-Enhanced Dynamic Adaptive PSO with Adaptive Quantum Probability adjusts quantum behavior probability based on convergence progress, enhancing exploration and exploitation.", "code": "import numpy as np\n\nclass QED_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob_base = 0.1  # Base probability for quantum behavior\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            current_quantum_prob = self.quantum_prob_base + 0.2 * (1 - progress)  # Adjust quantum probability\n            if np.random.rand() < current_quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 20, "feedback": "The algorithm QED_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09197 with standard deviation 0.03715.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06404712919007349, 0.07708786197604145, 0.059531714280552483, 0.144393490445893, 0.14376212773007324, 0.14382807438724765, 0.06777817590172097, 0.06905940394464039, 0.05820384921440502]}}
{"id": "d3efc3bb-165d-4928-9663-2c80f32d64af", "fitness": 0.09050499422734493, "name": "QED_PSO", "description": "Improved QED-PSO with adaptive quantum probability and enhanced perturbation strategy.", "code": "import numpy as np\n\nclass QED_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            self.quantum_prob = 0.1 * (1 - progress) + 0.3 * progress  # Adaptive quantum probability\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.2, (self.pop_size, self.dim))  # Enhanced perturbation\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 21, "feedback": "The algorithm QED_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09050 with standard deviation 0.03957.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06187393931025342, 0.0719242686970879, 0.05969612292247328, 0.14937228779964284, 0.14729714167611596, 0.1410810460409151, 0.06128278191387271, 0.06931493172707037, 0.05270242795867275]}}
{"id": "4634a89b-0bc0-4423-8aa0-200ea0d539bd", "fitness": 0.08927938046822372, "name": "QED_PSO", "description": "Quantum-Enhanced Dynamic Adaptive PSO (QED-PSO) with Lvy Flight and Adaptive Quantum Probability balances exploration and exploitation more effectively to achieve better convergence.", "code": "import numpy as np\n\nclass QED_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob_init = 0.1\n        self.quantum_prob_final = 0.3  # Increased final quantum probability\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        def levy_flight(Lambda):\n            sigma = (np.math.gamma(1 + Lambda) * np.sin(np.pi * Lambda / 2) / \n                     (np.math.gamma((1 + Lambda) / 2) * Lambda * 2**((Lambda - 1) / 2)))**(1 / Lambda)\n            u = np.random.normal(0, sigma, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            step = u / abs(v)**(1 / Lambda)\n            return 0.01 * step\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n            quantum_prob = self.quantum_prob_init * (1 - progress) + self.quantum_prob_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = levy_flight(1.5)  # Lvy flight step\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 22, "feedback": "The algorithm QED_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08928 with standard deviation 0.03949.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.0621934490227406, 0.06766605259812275, 0.059506690499837145, 0.14937228779964284, 0.1440077081605171, 0.1409777276561085, 0.06128278191387271, 0.06554330256071605, 0.05296442400245571]}}
{"id": "5b8fa646-310a-4b7a-b114-21f037206928", "fitness": 0.09279393718444001, "name": "QED_PSO", "description": "Introducing a dynamic learning factor adjustment to enhance the balance between exploration and exploitation in QED-PSO.", "code": "import numpy as np\n\nclass QED_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 23, "feedback": "The algorithm QED_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09279 with standard deviation 0.03834.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06465775006976027, 0.09246153914624688, 0.06059311027900216, 0.14937228779964284, 0.14393391182610504, 0.14162606887988094, 0.06178123650769962, 0.0685826372049605, 0.052136892946661906]}}
{"id": "ee84039c-22b9-4827-a9fc-1037261e2e66", "fitness": 0.08883688440032916, "name": "D_QEAPSO", "description": "Dynamic Quantum-Enhanced Adaptive PSO (D-QEAPSO) leverages a novel multi-phase exploration-exploitation strategy with adaptive quantum jumps to boost convergence speed and solution accuracy in diverse search spaces.", "code": "import numpy as np\n\nclass D_QEAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob_base = 0.1\n        self.quantum_prob_adaptive = 0.2\n        self.perturbation_prob = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n            quantum_prob = self.quantum_prob_base + self.quantum_prob_adaptive * (1 - progress)\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            adaptive_quantum_jump = np.random.rand(self.pop_size) < quantum_prob\n            if adaptive_quantum_jump.any():\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                quantum_particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n                particles[adaptive_quantum_jump] = quantum_particles[adaptive_quantum_jump]\n\n            if np.random.rand() < self.perturbation_prob:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 24, "feedback": "The algorithm D_QEAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08884 with standard deviation 0.04119.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06776686845507573, 0.0579921703429237, 0.06474678931743372, 0.14714988439777676, 0.14463485196560422, 0.14853970289063434, 0.05895289873894671, 0.051897781449982694, 0.05785101204458454]}}
{"id": "0e54614d-3a2c-4148-83b2-cb65c7e49ace", "fitness": 0.09210491926783301, "name": "QED_PSO_AQP", "description": "Quantum-Enhanced Dynamic Adaptive PSO with Adaptive Quantum Probability (QED-PSO-AQP) improves exploration-exploitation through dynamic parameter adaptation and an adaptive quantum probability based on convergence rate for more precise optimization.", "code": "import numpy as np\n\nclass QED_PSO_AQP:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob_base = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n        last_global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            # Adaptive quantum probability based on the rate of convergence\n            if last_global_best_score != np.inf:\n                improvement_rate = (last_global_best_score - global_best_score) / abs(last_global_best_score)\n                quantum_prob = self.quantum_prob_base + max(0, (0.2 - improvement_rate))\n            else:\n                quantum_prob = self.quantum_prob_base\n\n            if np.random.rand() < quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n            last_global_best_score = global_best_score\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 25, "feedback": "The algorithm QED_PSO_AQP got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09210 with standard deviation 0.03833.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06327469644083317, 0.07873778196758907, 0.06024503962444128, 0.14937228779964284, 0.14376212773007324, 0.14382807438724765, 0.06128278191387271, 0.07014508649034545, 0.05829639705645173]}}
{"id": "628c68ef-3ce8-45bf-9a6c-7ea580d91567", "fitness": 0.09003731130120647, "name": "QCH_PSO", "description": "Quantum-Cohort Hybrid PSO (QCH-PSO) integrates cohort intelligence and quantum-inspired adjustments to dynamically adapt exploration and exploitation, encouraging diverse paths toward global optima for enhanced convergence accuracy.", "code": "import numpy as np\n\nclass QCH_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n        self.cohort_size = max(3, self.pop_size // 5)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            cohort_indices = np.random.choice(self.pop_size, self.cohort_size, replace=False)\n            cohort_best_position = personal_best_positions[cohort_indices[np.argmin(personal_best_scores[cohort_indices])]]\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n            for i in range(self.pop_size):\n                particles[i] = particles[i] + np.random.normal(0, 0.01, self.dim) * (cohort_best_position - particles[i])\n                particles[i] = np.clip(particles[i], lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 26, "feedback": "The algorithm QCH_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09004 with standard deviation 0.03750.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06790992475107216, 0.07050256050001846, 0.06672319326812182, 0.14270397265040535, 0.14269272046826942, 0.14287195769359085, 0.0602073842966836, 0.06059883041950409, 0.05612525766319254]}}
{"id": "a35adc46-9293-4d59-bf0f-35fe963df834", "fitness": 0.09056904628652004, "name": "QED_PSO_DPQP", "description": "Quantum-Enhanced Dynamic Adaptive PSO with Dual-Phase Quantum Perturbation (QED_PSO_DPQP) introduces a dual-phase quantum perturbation mechanism for enhanced diversity and convergence speed.", "code": "import numpy as np\n\nclass QED_PSO_DPQP:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n        self.perturbation_phase = 0.05\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < self.perturbation_phase:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 27, "feedback": "The algorithm QED_PSO_DPQP got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09057 with standard deviation 0.03889.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06378525239919852, 0.06971705797548222, 0.06037181733555408, 0.14937228779964284, 0.14491755900556336, 0.1408228340139447, 0.061753319255029804, 0.0714741922932638, 0.052907096501001116]}}
{"id": "5007fe36-414a-4d84-80b5-f7ef8bb67409", "fitness": 0.08972422739204987, "name": "QED_PSO_plus", "description": "QED_PSO+ introduces adaptive quantum probability and enhanced diversity mechanisms to improve convergence speed and accuracy.", "code": "import numpy as np\n\nclass QED_PSO_plus:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob_init = 0.1\n        self.quantum_prob_final = 0.3\n        self.local_search_prob = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n            quantum_prob = self.quantum_prob_init * (1 - progress) + self.quantum_prob_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < self.local_search_prob:\n                perturbation = np.random.normal(0, 0.05 + 0.1 * progress, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 28, "feedback": "The algorithm QED_PSO_plus got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08972 with standard deviation 0.04094.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.061538133132244655, 0.06867793153683732, 0.06095154220072607, 0.1494559649033933, 0.1501013463779186, 0.14245717884819553, 0.06128278191387271, 0.05904421295235929, 0.05400895466290134]}}
{"id": "8b803f3c-0a87-4b09-be75-4942673d466f", "fitness": 0.08985578125769328, "name": "QED_PSO_SAM", "description": "Quantum-Enhanced Dynamic Adaptive PSO with Self-Adaptive Mutation (QED_PSO_SAM) adds a self-adaptive mutation mechanism to enhance diversity in the population, improving convergence by preventing premature stagnation.", "code": "import numpy as np\n\nclass QED_PSO_SAM:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n        self.mutation_rate = 0.05\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < self.mutation_rate:\n                mutation_strength = (ub - lb) * 0.1 * (1 - progress)\n                mutation = np.random.normal(0, mutation_strength, (self.pop_size, self.dim))\n                particles += mutation\n\n            particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 29, "feedback": "The algorithm QED_PSO_SAM got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08986 with standard deviation 0.03976.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06378525239919852, 0.06658205231867198, 0.06111776347608444, 0.14937228779964284, 0.14751367313725272, 0.14031118288168432, 0.06261620351537056, 0.06518014756915447, 0.052223468222179714]}}
{"id": "3d0d983e-4393-42af-9231-e0c9d1fce02e", "fitness": 0.09177366599437348, "name": "QED_PSO", "description": "Enhanced QED_PSO by incorporating adaptive quantum probability for improved convergence.", "code": "import numpy as np\n\nclass QED_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob_init = 0.1  # Changed from self.quantum_prob\n        self.quantum_prob_final = 0.3\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n            quantum_prob = self.quantum_prob_init * (1 - progress) + self.quantum_prob_final * progress\n            \n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < quantum_prob:  # Updated usage of adaptive quantum_prob\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 30, "feedback": "The algorithm QED_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09177 with standard deviation 0.03823.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.061378748067386635, 0.08418319566487109, 0.06294601643505982, 0.14937228779964284, 0.14376212773007324, 0.14074371588704393, 0.06128278191387271, 0.06870716550090716, 0.05358695495050392]}}
{"id": "b4dd4a82-ce36-4d56-9d0e-7bf886d39ff7", "fitness": 0.0871796284204196, "name": "AQSO", "description": "Adaptive Quantum-Swarm Optimization (AQSO) introduces multi-swarm dynamics with local search enhancement and quantum-inspired transitions for improved convergence and robustness.", "code": "import numpy as np\n\nclass AQSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(100, budget // 20)  # Increased swarm size\n        self.sub_swarm_size = self.pop_size // 2  # Splitting into sub-swarms\n        self.inertia_init = 0.8\n        self.inertia_final = 0.3\n        self.cognitive_init = 1.5\n        self.cognitive_final = 0.5\n        self.social_init = 2.5\n        self.social_final = 1.5\n        self.evaluations = 0\n        self.quantum_prob = 0.15  # Adjusted quantum probability\n        self.local_search_prob = 0.25  # Introduced local search probability\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < self.local_search_prob:  # Enhanced local search\n                for j in range(self.sub_swarm_size):\n                    perturb = np.random.normal(0, 0.05, self.dim)\n                    particles[j] = np.clip(particles[j] + perturb, lb, ub)\n                for j in range(self.sub_swarm_size):\n                    perturb = np.random.normal(0, 0.05, self.dim)\n                    particles[j + self.sub_swarm_size] = np.clip(particles[j + self.sub_swarm_size] + perturb, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 31, "feedback": "The algorithm AQSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08718 with standard deviation 0.03920.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06708026715201432, 0.06047488606390572, 0.06242288777265592, 0.14233962541215395, 0.1415038739333292, 0.1430332228715805, 0.0590469984102705, 0.04972791703505541, 0.05898697713281087]}}
{"id": "86d2d2fd-cc76-4619-87bd-5fedf2362354", "fitness": 0.08292398438256705, "name": "QE_PSO", "description": "Quantum-Enhanced Particle Swarm Optimization (QE-PSO) refines dynamic adaptation by introducing adaptive quantum jumps and a novel chaotic perturbation strategy to enhance convergence speed and accuracy.", "code": "import numpy as np\n\nclass QE_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(60, budget // 10)  # Increased population size for better diversity\n        self.inertia_init = 0.8  # Adjusted inertia weight to maintain balance\n        self.inertia_final = 0.3\n        self.cognitive_init = 1.5  # Adjusted cognitive and social factors\n        self.cognitive_final = 0.5\n        self.social_init = 1.5\n        self.social_final = 0.5\n        self.evaluations = 0\n        self.quantum_prob = 0.15  # Increased quantum probability for better exploration\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.3:  # Introduced chaotic perturbation\n                perturbation = np.random.standard_cauchy((self.pop_size, self.dim)) * 0.05\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 32, "feedback": "The algorithm QE_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08292 with standard deviation 0.04034.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.05868787921917584, 0.06291544865069243, 0.052231224943181, 0.13474255012732772, 0.13818613814950165, 0.1449625284734033, 0.0544196354165678, 0.058987794177197905, 0.04118266028605588]}}
{"id": "2928746d-3a6d-4e12-bae6-8cc4c79d14f5", "fitness": 0.09177366599437348, "name": "QED_PSO", "description": "Introduced adaptive quantum probability to dynamically balance exploration and exploitation.", "code": "import numpy as np\n\nclass QED_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob_init = 0.1  # renamed for clarity\n        self.quantum_prob_final = 0.3  # new parameter\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n            quantum_prob = self.quantum_prob_init * (1 - progress) + self.quantum_prob_final * progress  # adaptive quantum probability\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < quantum_prob:  # use new adaptive quantum probability\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 33, "feedback": "The algorithm QED_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09177 with standard deviation 0.03823.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.061378748067386635, 0.08418319566487109, 0.06294601643505982, 0.14937228779964284, 0.14376212773007324, 0.14074371588704393, 0.06128278191387271, 0.06870716550090716, 0.05358695495050392]}}
{"id": "8909791d-12ba-4a92-9e10-59fadf6783ef", "fitness": 0.09271153588563807, "name": "QED_PSO_AdaptiveMutation", "description": "Quantum-Enhanced Dynamic Adaptive PSO with Adaptive Mutation introduces adaptive mutation rates alongside dynamic parameter adaptation to improve exploration-exploitation balance and convergence speed.", "code": "import numpy as np\n\nclass QED_PSO_AdaptiveMutation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n        self.mutation_rate_init = 0.2\n        self.mutation_rate_final = 0.05\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n            mutation_rate = self.mutation_rate_init * (1 - progress) + self.mutation_rate_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < mutation_rate:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 34, "feedback": "The algorithm QED_PSO_AdaptiveMutation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09271 with standard deviation 0.03809.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06465775006976027, 0.09246153914624688, 0.06011172326671288, 0.14937228779964284, 0.14376212773007324, 0.14053601559723328, 0.06178123650769962, 0.0685826372049605, 0.05313850564841316]}}
{"id": "dc0c9d15-5845-4be3-a7b1-ce1f40f507c8", "fitness": 0.08998558219145462, "name": "QED_PSO", "description": "Enhanced QED-PSO with adaptive quantum probability and refined perturbation to improve convergence and diversity.", "code": "import numpy as np\n\nclass QED_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n        self.perturbation_std = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            adaptive_quantum_prob = self.quantum_prob + 0.05 * (1 - progress)  # Adaptive quantum probability\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, self.perturbation_std * (1 - progress), (self.pop_size, self.dim))  # Refined perturbation\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 35, "feedback": "The algorithm QED_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08999 with standard deviation 0.03960.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.061623242877061224, 0.08163449291279001, 0.05985792371905507, 0.14937228779964284, 0.14388561594645, 0.14165211404038403, 0.06128278191387271, 0.054576546787424185, 0.05598523372641151]}}
{"id": "be0d5cd1-7490-439f-abf8-f1a527f88fa2", "fitness": 0.09236025924294504, "name": "QED_PSO_DF", "description": "Quantum-Enhanced Dynamic Adaptive PSO with Diversity and Feedback Mechanism (QED-PSO-DF) integrates a diversity preservation strategy and feedback-driven parameter adjustment for more robust exploration and exploitation balance, improving convergence reliability and speed.", "code": "import numpy as np\n\nclass QED_PSO_DF:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n        self.diversity_threshold = 0.2\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            # Introduce diversity preservation\n            diversity_metric = np.mean(np.std(particles, axis=0))\n            if diversity_metric < self.diversity_threshold:\n                velocities += np.random.normal(0, 0.1, velocities.shape)\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            # Adjust particle perturbation frequency based on feedback\n            if np.random.rand() < (0.2 + 0.1 * (global_best_score / np.max(personal_best_scores + 1e-10))):\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 36, "feedback": "The algorithm QED_PSO_DF got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09236 with standard deviation 0.03830.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06465775006976027, 0.09246153914624688, 0.05973098352611883, 0.14937228779964284, 0.1439256030549857, 0.14031118288168432, 0.06178123650769962, 0.06534176845514783, 0.05365998174521902]}}
{"id": "1687616f-e7a2-49b9-b835-9ff936eb8c03", "fitness": 0.09107348445516351, "name": "QED_PSO", "description": "Refined QED-PSO with an adaptive perturbation mechanism to enhance convergence precision.", "code": "import numpy as np\n\nclass QED_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            perturbation_magnitude = 0.1 * (1 - progress)  # Adaptive perturbation magnitude\n            perturbation = np.random.normal(0, perturbation_magnitude, (self.pop_size, self.dim))\n            particles = np.clip(particles + perturbation, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 37, "feedback": "The algorithm QED_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09107 with standard deviation 0.04067.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.0711771330612061, 0.058849359094004905, 0.06033713176940758, 0.15113539721139924, 0.14689907744611597, 0.1468580868262951, 0.06398368348788741, 0.05613015182798353, 0.0642913393721718]}}
{"id": "f63e0e5a-92aa-476b-af34-41cfbd4ca524", "fitness": 0.08816712759184982, "name": "QED_PSO_AQP", "description": "Quantum-Enhanced Dynamic Adaptive PSO with Anisotropic Quantum Perturbation (QED-PSO-AQP) introduces anisotropic quantum perturbation to enhance local search capabilities and improve convergence reliability in complex landscapes.", "code": "import numpy as np\n\nclass QED_PSO_AQP:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.2  # Modified probability for quantum perturbation\n        self.anisotropic_factor = 0.5  # Anisotropic scaling factor\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                anisotropic_delta = delta * np.random.uniform(-self.anisotropic_factor, self.anisotropic_factor, (self.pop_size, self.dim))\n                particles = center + anisotropic_delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 38, "feedback": "The algorithm QED_PSO_AQP got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08817 with standard deviation 0.04193.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.056600706682899404, 0.07327303754856274, 0.05931408888668466, 0.14937228779964284, 0.1501815004246827, 0.14031118288168432, 0.04750361800017966, 0.06313911281526075, 0.05380861328705133]}}
{"id": "821a37eb-e5df-4ad7-9bbc-5c4dbf12911f", "fitness": 0.09172912722421948, "name": "QED_PSO", "description": "Improved QED-PSO with adaptive quantum probability based on particle diversity to enhance exploration.", "code": "import numpy as np\n\nclass QED_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n            \n            diversity = np.mean(np.std(particles, axis=0))  # Calculate diversity\n            self.quantum_prob = 0.1 + 0.3 * (diversity / (ub - lb).mean())  # Update quantum_prob adaptively\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 39, "feedback": "The algorithm QED_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09173 with standard deviation 0.03882.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06327469644083317, 0.08422975028518553, 0.05945461129580554, 0.14937228779964284, 0.14376212773007324, 0.1429218120196204, 0.06128278191387271, 0.06901384547135403, 0.052250232061587853]}}
{"id": "b6e0d76d-b279-46fd-a14d-757b1e1e71f5", "fitness": -Infinity, "name": "QDAPSO_AQP", "description": "Quantum-Enhanced Dynamic Adaptive PSO with Adaptive Quantum Probability (QDAPSO-AQP) incorporates adaptive quantum probability adjustment and chaotic perturbation for enhanced convergence rates and solution quality.", "code": "import numpy as np\n\nclass QDAPSO_AQP:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n            self.quantum_prob = 0.1 + 0.1 * np.sin(2 * np.pi * iteration / 10)\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.3:\n                perturbation = np.random.normal(0, 0.1 * np.sin(2 * np.pi * iteration / 10), (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 40, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {}}
{"id": "3d1c564f-54b4-4c9a-a767-28af0ed59b12", "fitness": 0.0882984626480154, "name": "AQE_DN_PSO", "description": "Adaptive Quantum-Enhanced PSO with Dynamic Neighborhoods (AQE-DN-PSO) incorporates dynamic neighborhood structures and advanced quantum perturbation to improve convergence speed and solution accuracy in complex landscapes.", "code": "import numpy as np\n\nclass AQE_DN_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n        \n        global_best_position = None\n        global_best_score = np.inf\n\n        neighborhood_size = min(5, self.pop_size // 5)  # dynamic neighborhood size\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                for i in range(self.pop_size):\n                    neighborhood_indices = np.random.choice(self.pop_size, neighborhood_size, replace=False)\n                    neighborhood_best_position = personal_best_positions[neighborhood_indices[np.argmin(personal_best_scores[neighborhood_indices])]]\n                    center = (personal_best_positions[i] + neighborhood_best_position) / 2\n                    delta = np.abs(neighborhood_best_position - particles[i])\n                    particles[i] = center + np.random.uniform(-1, 1, self.dim) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 41, "feedback": "The algorithm AQE_DN_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08830 with standard deviation 0.03986.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06361434589492543, 0.06708012423810561, 0.05912740427834762, 0.15214688305164747, 0.13914999243398085, 0.1413541162574663, 0.05655176525622252, 0.06261659171883172, 0.05304494070261112]}}
{"id": "26d9dfa6-2a38-4c3c-a578-10c807bb83a2", "fitness": 0.09177366599437348, "name": "QED_PSO", "description": "Introducing a dynamic quantum probability adjustment in QED-PSO to enhance adaptability during optimization.", "code": "import numpy as np\n\nclass QED_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob_init = 0.1\n        self.quantum_prob_final = 0.3  # Dynamically adjust quantum probability\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n            quantum_prob = self.quantum_prob_init * (1 - progress) + self.quantum_prob_final * progress  # Dynamic adjustment\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 42, "feedback": "The algorithm QED_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09177 with standard deviation 0.03823.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.061378748067386635, 0.08418319566487109, 0.06294601643505982, 0.14937228779964284, 0.14376212773007324, 0.14074371588704393, 0.06128278191387271, 0.06870716550090716, 0.05358695495050392]}}
{"id": "a06f0845-bc18-41cd-a462-a78126978568", "fitness": 0.09194509919442313, "name": "QED_PSO_AQP", "description": "Quantum-Enhanced Dynamic Adaptive PSO with Adaptive Quantum Probability (QED-PSO-AQP) enhances QED-PSO by dynamically adjusting quantum probability based on convergence rate, improving the exploration-exploitation balance and convergence speed.", "code": "import numpy as np\n\nclass QED_PSO_AQP:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.init_quantum_prob = 0.1\n        self.quantum_prob = self.init_quantum_prob\n        self.convergence_threshold = 0.01\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n        last_global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            # Adjust quantum probability based on convergence\n            if abs(last_global_best_score - global_best_score) < self.convergence_threshold:\n                self.quantum_prob = min(0.3, self.quantum_prob + 0.05)\n            else:\n                self.quantum_prob = self.init_quantum_prob\n\n            last_global_best_score = global_best_score\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 43, "feedback": "The algorithm QED_PSO_AQP got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09195 with standard deviation 0.03880.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06327469644083317, 0.08585854302844276, 0.05942000073454934, 0.14937228779964284, 0.14376212773007324, 0.1429218120196204, 0.06128278191387271, 0.07014508649034545, 0.05146855659242833]}}
{"id": "0869b68f-ee44-4f48-ab29-637c6914a775", "fitness": 0.09279393718444001, "name": "QED_PSO", "description": "Improved dynamic adaptation in QED-PSO with enhanced quantum behavior for better convergence efficiency.", "code": "import numpy as np\n\nclass QED_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 44, "feedback": "The algorithm QED_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09279 with standard deviation 0.03834.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06465775006976027, 0.09246153914624688, 0.06059311027900216, 0.14937228779964284, 0.14393391182610504, 0.14162606887988094, 0.06178123650769962, 0.0685826372049605, 0.052136892946661906]}}
{"id": "95ade4b3-ca2a-4fec-8870-3db6a1315650", "fitness": 0.09177366599437348, "name": "QED_PSO", "description": "Enhanced Quantum-Enhanced Dynamic Adaptive PSO (QED-PSO) with adaptive quantum probability for improved convergence accuracy.", "code": "import numpy as np\n\nclass QED_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob_init = 0.1\n        self.quantum_prob_final = 0.3\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n            quantum_prob = self.quantum_prob_init * (1 - progress) + self.quantum_prob_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 45, "feedback": "The algorithm QED_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09177 with standard deviation 0.03823.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.061378748067386635, 0.08418319566487109, 0.06294601643505982, 0.14937228779964284, 0.14376212773007324, 0.14074371588704393, 0.06128278191387271, 0.06870716550090716, 0.05358695495050392]}}
{"id": "84652166-dae2-40d6-b828-2f6a30119cc4", "fitness": 0.09045941183270281, "name": "QEDAE_PSO", "description": "Quantum-Enhanced Dynamic Adaptive PSO with Improved Exploration-Exploitation Mechanism (QEDAE-PSO) introduces adaptive mechanisms for quantum and social components to enhance convergence speed and accuracy.", "code": "import numpy as np\n\nclass QEDAE_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.adaptive_quantum_prob = 0.1\n        self.adaptive_social_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            adaptive_quantum_prob = self.adaptive_quantum_prob * (1 - progress)\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            adaptive_social_prob = self.adaptive_social_prob * progress\n            if np.random.rand() < adaptive_social_prob:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 46, "feedback": "The algorithm QEDAE_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09046 with standard deviation 0.03836.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06553075435898204, 0.07032297510163876, 0.06080672660820041, 0.1495185937655329, 0.14270483135015377, 0.14056467960118724, 0.0693587075869121, 0.06021784363048954, 0.05510959449122854]}}
{"id": "88bd726a-13e8-45e2-9160-186826433e2f", "fitness": 0.09176858278927388, "name": "QED_PSO_NLI", "description": "Quantum-Enhanced Dynamic Adaptive PSO with Non-linear Inertia (QED-PSO-NLI) introduces a non-linear inertia adjustment and adaptive swarm shrinking to enhance convergence speed and exploitation capability.", "code": "import numpy as np\n\nclass QED_PSO_NLI:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress**2) + self.inertia_final * progress**2\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n            # Adaptive swarm shrinking\n            if iteration % 5 == 0 and self.pop_size > 5:\n                self.pop_size -= 1\n                particles = particles[:self.pop_size]\n                velocities = velocities[:self.pop_size]\n                personal_best_positions = personal_best_positions[:self.pop_size]\n                personal_best_scores = personal_best_scores[:self.pop_size]\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 47, "feedback": "The algorithm QED_PSO_NLI got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09177 with standard deviation 0.03865.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06416475106687758, 0.06846819966807727, 0.0653938302133018, 0.1490465236985804, 0.14407958875707128, 0.14501973489254005, 0.055574116794753214, 0.07264181323481522, 0.06152868677744816]}}
{"id": "c736186e-b8b8-49ed-8e38-5015b67c5040", "fitness": 0.09189171358901, "name": "QED_PSO", "description": "Enhanced QED-PSO with increased quantum probability for improved exploration and convergence.", "code": "import numpy as np\n\nclass QED_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.15  # Change 1: Adjusted quantum probability from 0.1 to 0.15\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 48, "feedback": "The algorithm QED_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09189 with standard deviation 0.03815.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06220992836149042, 0.08422975028518553, 0.06294601643505982, 0.14937228779964284, 0.14376212773007324, 0.14074371588704393, 0.06128278191387271, 0.06901384547135403, 0.053464968417367476]}}
{"id": "f475ea3a-0645-4023-a723-6b4a44aed4b0", "fitness": 0.09279393718444001, "name": "QED_PSO_DR", "description": "\"Quantum-Enhanced Dynamic Adaptive PSO with Diversity-Driven Particle Regeneration (QED_PSO_DR) introduces diversity-driven strategies to regenerate particles and enhance exploration, improving convergence speed and robustness.\"", "code": "import numpy as np\n\nclass QED_PSO_DR:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.inertia_init = 0.9\n        self.inertia_final = 0.4\n        self.cognitive_init = 2.0\n        self.cognitive_final = 1.0\n        self.social_init = 2.0\n        self.social_final = 1.0\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n        self.diversity_threshold = 1e-3\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(self.pop_size, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        for iteration in range(self.budget // self.pop_size):\n            progress = iteration / (self.budget / self.pop_size)\n            inertia = self.inertia_init * (1 - progress) + self.inertia_final * progress\n            cognitive = self.cognitive_init * (1 - progress) + self.cognitive_final * progress\n            social = self.social_init * (1 - progress) + self.social_final * progress\n\n            for i in range(self.pop_size):\n                if self.evaluations < self.budget:\n                    score = func(particles[i])\n                    self.evaluations += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = particles[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = particles[i]\n                else:\n                    break\n\n            # Calculate diversity\n            diversity = np.std(particles)\n\n            # Regenerate particles if diversity is below threshold\n            if diversity < self.diversity_threshold:\n                regeneration_indices = np.random.choice(self.pop_size, self.pop_size // 4, replace=False)\n                particles[regeneration_indices] = np.random.uniform(lb, ub, (len(regeneration_indices), self.dim))\n                velocities[regeneration_indices] = np.random.uniform(-1, 1, (len(regeneration_indices), self.dim))\n\n            r1, r2 = np.random.rand(self.pop_size, self.dim), np.random.rand(self.pop_size, self.dim)\n            velocities = (inertia * velocities\n                          + cognitive * r1 * (personal_best_positions - particles)\n                          + social * r2 * (global_best_position - particles))\n\n            particles += velocities\n\n            if np.random.rand() < self.quantum_prob:\n                center = (personal_best_positions + global_best_position) / 2\n                delta = np.abs(global_best_position - particles)\n                particles = center + np.random.uniform(-1, 1, (self.pop_size, self.dim)) * delta / np.sqrt(2)\n\n            if np.random.rand() < 0.2:\n                perturbation = np.random.normal(0, 0.1, (self.pop_size, self.dim))\n                particles = np.clip(particles + perturbation, lb, ub)\n            else:\n                particles = np.clip(particles, lb, ub)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 49, "feedback": "The algorithm QED_PSO_DR got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09279 with standard deviation 0.03834.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.06465775006976027, 0.09246153914624688, 0.06059311027900216, 0.14937228779964284, 0.14393391182610504, 0.14162606887988094, 0.06178123650769962, 0.0685826372049605, 0.052136892946661906]}}
{"id": "6f5d56cc-5b7a-4fcb-a2b8-659a5f69c3ed", "fitness": 0.23962926402227092, "name": "QED_HS", "description": "Quantum-Enhanced Adaptive Dynamic Harmony Search (QED-HS) combines harmony search with quantum-enhanced principles and adaptive parameter tuning for improved convergence and diversity in complex search spaces.", "code": "import numpy as np\n\nclass QED_HS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.9  # Harmony memory consideration rate\n        self.hmcr_final = 0.8\n        self.par_init = 0.4  # Pitch adjustment rate\n        self.par_final = 0.1\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.full(self.hms, np.inf)\n\n        best_harmony = None\n        best_score = np.inf\n\n        for i in range(self.hms):\n            score = func(harmony_memory[i])\n            self.evaluations += 1\n            harmony_scores[i] = score\n            if score < best_score:\n                best_score = score\n                best_harmony = harmony_memory[i]\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init * (1 - progress) + self.hmcr_final * progress\n            par = self.par_init * (1 - progress) + self.par_final * progress\n\n            new_harmony = np.zeros(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < hmcr:\n                    idx = np.random.randint(0, self.hms)\n                    new_harmony[j] = harmony_memory[idx, j]\n                    if np.random.rand() < par:\n                        new_harmony[j] += np.random.uniform(-0.1, 0.1) * (ub[j] - lb[j])\n                        new_harmony[j] = np.clip(new_harmony[j], lb[j], ub[j])\n                else:\n                    new_harmony[j] = np.random.uniform(lb[j], ub[j])\n\n            if np.random.rand() < self.quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(0, self.hms)])\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / np.sqrt(2)\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx] = new_harmony\n                harmony_scores[worst_idx] = score\n\n        return best_harmony, best_score", "configspace": "", "generation": 50, "feedback": "The algorithm QED_HS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.23963 with standard deviation 0.13596.", "error": "", "parent_ids": ["0cc02cd1-3255-44b4-86e5-441269d77abf"], "operator": null, "metadata": {"aucs": [0.21707175744146678, 0.525554113129153, 0.44262644824961717, 0.18760041977763053, 0.19399358335949435, 0.1954337629742734, 0.11768297617972745, 0.11865615106126881, 0.15804416402780685]}}
{"id": "dfec306c-0527-4823-a3a0-ddbd7ff11d0f", "fitness": 0.2980277413513672, "name": "QED_HS", "description": "Enhanced QED_HS with adaptive quantum probability and dynamic delta factor for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass QED_HS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.9  # Harmony memory consideration rate\n        self.hmcr_final = 0.8\n        self.par_init = 0.4  # Pitch adjustment rate\n        self.par_final = 0.1\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.full(self.hms, np.inf)\n\n        best_harmony = None\n        best_score = np.inf\n\n        for i in range(self.hms):\n            score = func(harmony_memory[i])\n            self.evaluations += 1\n            harmony_scores[i] = score\n            if score < best_score:\n                best_score = score\n                best_harmony = harmony_memory[i]\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init * (1 - progress) + self.hmcr_final * progress\n            par = self.par_init * (1 - progress) + self.par_final * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress)\n\n            new_harmony = np.zeros(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < hmcr:\n                    idx = np.random.randint(0, self.hms)\n                    new_harmony[j] = harmony_memory[idx, j]\n                    if np.random.rand() < par:\n                        new_harmony[j] += np.random.uniform(-0.1, 0.1) * (ub[j] - lb[j])\n                        new_harmony[j] = np.clip(new_harmony[j], lb[j], ub[j])\n                else:\n                    new_harmony[j] = np.random.uniform(lb[j], ub[j])\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(0, self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx] = new_harmony\n                harmony_scores[worst_idx] = score\n\n        return best_harmony, best_score", "configspace": "", "generation": 51, "feedback": "The algorithm QED_HS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29803 with standard deviation 0.10930.", "error": "", "parent_ids": ["6f5d56cc-5b7a-4fcb-a2b8-659a5f69c3ed"], "operator": null, "metadata": {"aucs": [0.28228627605759804, 0.5010986330988398, 0.32910082384603856, 0.18516232033356583, 0.29255449058644534, 0.31878439280264736, 0.22611205188532668, 0.42441478275870015, 0.12273590079314323]}}
{"id": "7b441418-2d45-41cc-8524-a862e5aaff4a", "fitness": 0.31279016873317367, "name": "QED_HS", "description": "Enhanced QED_HS with modified adaptive quantum probability for better convergence in exploration and exploitation phases.", "code": "import numpy as np\n\nclass QED_HS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.9  # Harmony memory consideration rate\n        self.hmcr_final = 0.8\n        self.par_init = 0.4  # Pitch adjustment rate\n        self.par_final = 0.1\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.full(self.hms, np.inf)\n\n        best_harmony = None\n        best_score = np.inf\n\n        for i in range(self.hms):\n            score = func(harmony_memory[i])\n            self.evaluations += 1\n            harmony_scores[i] = score\n            if score < best_score:\n                best_score = score\n                best_harmony = harmony_memory[i]\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init * (1 - progress) + self.hmcr_final * progress\n            par = self.par_init * (1 - progress) + self.par_final * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.05 * progress\n\n            new_harmony = np.zeros(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < hmcr:\n                    idx = np.random.randint(0, self.hms)\n                    new_harmony[j] = harmony_memory[idx, j]\n                    if np.random.rand() < par:\n                        new_harmony[j] += np.random.uniform(-0.1, 0.1) * (ub[j] - lb[j])\n                        new_harmony[j] = np.clip(new_harmony[j], lb[j], ub[j])\n                else:\n                    new_harmony[j] = np.random.uniform(lb[j], ub[j])\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(0, self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx] = new_harmony\n                harmony_scores[worst_idx] = score\n\n        return best_harmony, best_score", "configspace": "", "generation": 52, "feedback": "The algorithm QED_HS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31279 with standard deviation 0.12758.", "error": "", "parent_ids": ["dfec306c-0527-4823-a3a0-ddbd7ff11d0f"], "operator": null, "metadata": {"aucs": [0.45202224713336925, 0.23920896706002392, 0.46911078501787773, 0.2717258793532005, 0.1945409890597667, 0.4687272947500639, 0.09643217836987727, 0.38397324110879727, 0.23936993674558682]}}
{"id": "30208339-5742-4864-a38c-443e0fd5ec31", "fitness": 0.08829417196826801, "name": "AdaptiveQED_HS", "description": "Adaptive Enhanced QED_HS with dynamic learning rates and diversity preservation for improved convergence and robustness.", "code": "import numpy as np\n\nclass AdaptiveQED_HS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr = 0.9  # Initial Harmony memory consideration rate\n        self.par = 0.4  # Initial Pitch adjustment rate\n        self.evaluations = 0\n        self.alpha = 0.99  # Learning rate decay factor\n        self.quantum_prob = 0.1\n\n    def update_parameters(self, progress):\n        self.hmcr = self.hmcr * (self.alpha ** progress)\n        self.par = self.par * (self.alpha ** progress)\n        self.quantum_prob = self.quantum_prob * (self.alpha ** progress) + 0.05 * (1 - self.alpha ** progress)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.full(self.hms, np.inf)\n\n        best_harmony = None\n        best_score = np.inf\n\n        for i in range(self.hms):\n            score = func(harmony_memory[i])\n            self.evaluations += 1\n            harmony_scores[i] = score\n            if score < best_score:\n                best_score = score\n                best_harmony = harmony_memory[i]\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            self.update_parameters(progress)\n\n            new_harmony = np.zeros(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.hmcr:\n                    idx = np.random.randint(0, self.hms)\n                    new_harmony[j] = harmony_memory[idx, j]\n                    if np.random.rand() < self.par:\n                        new_harmony[j] += np.random.uniform(-0.1, 0.1) * (ub[j] - lb[j])\n                        new_harmony[j] = np.clip(new_harmony[j], lb[j], ub[j])\n                else:\n                    new_harmony[j] = np.random.uniform(lb[j], ub[j])\n\n            if np.random.rand() < self.quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(0, self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx] = new_harmony\n                harmony_scores[worst_idx] = score\n\n        return best_harmony, best_score", "configspace": "", "generation": 53, "feedback": "The algorithm AdaptiveQED_HS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08829 with standard deviation 0.04432.", "error": "", "parent_ids": ["7b441418-2d45-41cc-8524-a862e5aaff4a"], "operator": null, "metadata": {"aucs": [0.06815394619687243, 0.06251770997439854, 0.05606440701361637, 0.15271766530979913, 0.1464851555037855, 0.1522634438101501, 0.04864414687560181, 0.05510177134552241, 0.052699301684665856]}}
{"id": "3f2ba3ec-e3b5-4fff-a46e-40d78058e946", "fitness": 0.22410694322893615, "name": "QED_HS", "description": "Enhanced QED_HS with dynamic harmony memory consideration and pitch adjustment rates for improved convergence.", "code": "import numpy as np\n\nclass QED_HS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Harmony memory consideration rate\n        self.hmcr_final = 0.85\n        self.par_init = 0.45  # Pitch adjustment rate\n        self.par_final = 0.15\n        self.evaluations = 0\n        self.quantum_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.full(self.hms, np.inf)\n\n        best_harmony = None\n        best_score = np.inf\n\n        for i in range(self.hms):\n            score = func(harmony_memory[i])\n            self.evaluations += 1\n            harmony_scores[i] = score\n            if score < best_score:\n                best_score = score\n                best_harmony = harmony_memory[i]\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init * (1 - progress) + self.hmcr_final * progress\n            par = self.par_init * (1 - progress) + self.par_final * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.05 * progress\n\n            new_harmony = np.zeros(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < hmcr:\n                    idx = np.random.randint(0, self.hms)\n                    new_harmony[j] = harmony_memory[idx, j]\n                    if np.random.rand() < par:\n                        new_harmony[j] += np.random.uniform(-0.1, 0.1) * (ub[j] - lb[j])\n                        new_harmony[j] = np.clip(new_harmony[j], lb[j], ub[j])\n                else:\n                    new_harmony[j] = np.random.uniform(lb[j], ub[j])\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(0, self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx] = new_harmony\n                harmony_scores[worst_idx] = score\n\n        return best_harmony, best_score", "configspace": "", "generation": 54, "feedback": "The algorithm QED_HS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22411 with standard deviation 0.07780.", "error": "", "parent_ids": ["7b441418-2d45-41cc-8524-a862e5aaff4a"], "operator": null, "metadata": {"aucs": [0.17308280635216178, 0.3191423721336194, 0.26263198254687636, 0.3683863462731881, 0.2046646026633009, 0.22527720129227213, 0.1141091315682653, 0.21483365335639448, 0.13483439287434706]}}
{"id": "1c902210-1480-4fd1-8e94-683c7eb726a0", "fitness": 0.3372058439681098, "name": "QED_HS_Improved", "description": "Quantum Enhanced Dynamic Harmony Search with self-adaptive parameters for improved convergence and diversity balance.", "code": "import numpy as np\n\nclass QED_HS_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.9  # Harmony memory consideration rate\n        self.hmcr_final = 0.7\n        self.par_init = 0.4  # Pitch adjustment rate\n        self.par_final = 0.2\n        self.evaluations = 0\n        self.quantum_prob = 0.15\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.1 * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                new_harmony += np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 55, "feedback": "The algorithm QED_HS_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33721 with standard deviation 0.16680.", "error": "", "parent_ids": ["7b441418-2d45-41cc-8524-a862e5aaff4a"], "operator": null, "metadata": {"aucs": [0.10419999322559559, 0.34047683019008357, 0.6748679976212031, 0.22345076612912063, 0.4804082068069938, 0.3887072846713129, 0.1255707436136917, 0.3703511360858588, 0.32681963736912867]}}
{"id": "6f92e51f-518b-457b-8512-def52dda13c5", "fitness": 0.29904263431446904, "name": "QED_HS_Improved", "description": "Enhanced Quantum Harmony Search with Dynamic Adaptive Strategies for Improved Exploration-Exploitation Balance", "code": "import numpy as np\n\nclass QED_HS_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)\n        self.hmcr_init = 0.95  # Increased initial harmony memory consideration rate\n        self.hmcr_final = 0.6  # Lower final harmony memory consideration rate\n        self.par_init = 0.5  # Increased initial pitch adjustment rate\n        self.par_final = 0.1  # Lower final pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob = 0.2  # Increased initial quantum probability\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.15 * progress  # Adjusted quantum adaptation\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                new_harmony += np.random.normal(0, 0.05, self.dim) * (ub - lb)  # Changed perturbation method to normal\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 56, "feedback": "The algorithm QED_HS_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29904 with standard deviation 0.21005.", "error": "", "parent_ids": ["1c902210-1480-4fd1-8e94-683c7eb726a0"], "operator": null, "metadata": {"aucs": [0.40906570346281734, 0.10303343007313359, 0.45365601984700077, 0.4763508495269637, 0.7185978922534288, 0.19041577409456023, 0.11775170564146886, 0.11404594027554715, 0.10846639365530097]}}
{"id": "3b7f6d25-febe-4b3f-8494-c9aa908c2fc5", "fitness": 0.1283730771733773, "name": "QED_HS_Improved_Refined", "description": "Quantum Enhanced Dynamic Harmony Search with self-adaptive parameters and stochastic rank-based selection for better exploration-exploitation balance.", "code": "import numpy as np\n\nclass QED_HS_Improved_Refined:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.9  # Harmony memory consideration rate\n        self.hmcr_final = 0.7\n        self.par_init = 0.4  # Pitch adjustment rate\n        self.par_final = 0.2\n        self.evaluations = 0\n        self.quantum_prob = 0.15\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.1 * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                new_harmony += np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n\n            # Stochastic rank-based selection for replacement\n            ranks = np.argsort(harmony_scores)\n            selection_prob = (self.hms - ranks) / sum(self.hms - ranks)\n            selected_idx = np.random.choice(self.hms, p=selection_prob)\n\n            if score < harmony_scores[selected_idx]:\n                harmony_memory[selected_idx], harmony_scores[selected_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 57, "feedback": "The algorithm QED_HS_Improved_Refined got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12837 with standard deviation 0.03921.", "error": "", "parent_ids": ["1c902210-1480-4fd1-8e94-683c7eb726a0"], "operator": null, "metadata": {"aucs": [0.08841883800936479, 0.12453866777136102, 0.10257278634515365, 0.1698560929776004, 0.17419778364136473, 0.18208951386184258, 0.09303480640679018, 0.0718824071348696, 0.14876679841204865]}}
{"id": "c47d0ff7-9d28-46a4-b887-b52eaf9fe0c6", "fitness": 0.14602415654560075, "name": "QED_HS_Improved", "description": "Quantum Enhanced Dynamic Harmony Search with adaptive memory and selective local search for improved convergence and exploration.", "code": "import numpy as np\n\nclass QED_HS_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.85  # Adjusted initial Harmony memory consideration rate\n        self.hmcr_final = 0.65\n        self.par_init = 0.5  # Increased initial Pitch adjustment rate\n        self.par_final = 0.3\n        self.evaluations = 0\n        self.quantum_prob = 0.2  # Increased quantum probability\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.2 * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                new_harmony += np.random.uniform(-0.05, 0.05, self.dim) * (ub - lb)\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 58, "feedback": "The algorithm QED_HS_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.14602 with standard deviation 0.05605.", "error": "", "parent_ids": ["1c902210-1480-4fd1-8e94-683c7eb726a0"], "operator": null, "metadata": {"aucs": [0.10851115538119671, 0.09397912910892281, 0.17830060278617443, 0.1848724865388255, 0.16962576218639347, 0.22403812752823682, 0.06830260355006335, 0.0765337224804552, 0.21005381935013823]}}
{"id": "b5d88d47-c152-47a0-88cf-313b72e92e0e", "fitness": 0.31036595679660994, "name": "AQI_HS_Enhanced", "description": "Adaptive Quantum-Inspired Harmony Search with Noise Reduction and Enhanced Diversity for improved convergence reliability.", "code": "import numpy as np\n\nclass AQI_HS_Enhanced:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Increased initial Harmony memory consideration rate\n        self.hmcr_final = 0.65\n        self.par_init = 0.35  # Adjusted Pitch adjustment rate\n        self.par_final = 0.15\n        self.evaluations = 0\n        self.quantum_prob = 0.2\n        self.noise_reduction_prob = 0.1  # Probability of noise reduction\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.1 * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                noise = np.random.uniform(-0.05, 0.05, self.dim)  # Reduced noise range\n                new_harmony += noise * (ub - lb)\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + 2 * progress)  # Enhanced diversity factor\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            # Introduce noise reduction\n            if np.random.rand() < self.noise_reduction_prob:\n                new_harmony = np.clip(new_harmony - noise, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 59, "feedback": "The algorithm AQI_HS_Enhanced got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31037 with standard deviation 0.16676.", "error": "", "parent_ids": ["1c902210-1480-4fd1-8e94-683c7eb726a0"], "operator": null, "metadata": {"aucs": [0.5496686377908455, 0.13595355912666596, 0.5061207316232952, 0.3084510094033506, 0.1934016838519932, 0.534679846188133, 0.16676443867366142, 0.28548478045993, 0.11276892405161498]}}
{"id": "988bcbae-c73e-4707-b2d7-e4619e5dc4db", "fitness": 0.34441714712968224, "name": "QED_HS_Refined", "description": "Quantum Enhanced Dynamic Harmony Search with adaptive learning rate and strategic memory exploitation for superior convergence and solution diversity.", "code": "import numpy as np\n\nclass QED_HS_Refined:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.65  # Final Harmony memory consideration rate\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.1  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob = 0.2  # Initial Quantum probability\n        self.learning_rate = 0.1  # Adaptive learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.2 * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                new_harmony += np.random.normal(0, self.learning_rate, self.dim) * (ub - lb)\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.99)  # Reduce learning rate if improvement\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 60, "feedback": "The algorithm QED_HS_Refined got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.34442 with standard deviation 0.13839.", "error": "", "parent_ids": ["1c902210-1480-4fd1-8e94-683c7eb726a0"], "operator": null, "metadata": {"aucs": [0.5146021592922139, 0.42730198989814083, 0.40779602980379337, 0.28622658130995815, 0.34508151530372033, 0.5555247806445809, 0.202658670258973, 0.10753773634672525, 0.25302486130903445]}}
{"id": "1ba62f3f-f72c-4cf8-994b-58e02ba9a6b5", "fitness": 0.35661098447529643, "name": "Enhanced_QHS", "description": "Enhanced Quantum Harmony Search with dynamic exploration-exploitation balance and stochastic gradient-like information for refined convergence.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.65  # Final Harmony memory consideration rate\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.1  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob = 0.2  # Initial Quantum probability\n        self.learning_rate = 0.1  # Adaptive learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.2 * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.99)  # Reduce learning rate if improvement\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 61, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35661 with standard deviation 0.14264.", "error": "", "parent_ids": ["988bcbae-c73e-4707-b2d7-e4619e5dc4db"], "operator": null, "metadata": {"aucs": [0.421189011419398, 0.47373194837444976, 0.5883631498903152, 0.2018750118814866, 0.4159840896591942, 0.3577358471757902, 0.23069031674927787, 0.10264063148499492, 0.41728885364276125]}}
{"id": "f99a5105-f267-49f8-b8fe-2ec13d8c6ee1", "fitness": 0.30244151529102187, "name": "Enhanced_QHS", "description": "Enhanced Quantum Harmony Search with an improved adaptive quantum probability strategy for accelerated convergence.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.65  # Final Harmony memory consideration rate\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.1  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob = 0.2  # Initial Quantum probability\n        self.learning_rate = 0.1  # Adaptive learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress)**2 + 0.2 * progress  # Adjusted strategy\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.99)  # Reduce learning rate if improvement\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 62, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30244 with standard deviation 0.19430.", "error": "", "parent_ids": ["1ba62f3f-f72c-4cf8-994b-58e02ba9a6b5"], "operator": null, "metadata": {"aucs": [0.51956874877519, 0.13678491347996624, 0.11510857620696058, 0.2066204252649082, 0.22943853504827538, 0.22340817683038594, 0.5524968378853574, 0.10672412817472055, 0.631823295953432]}}
{"id": "9d4efd95-b5c7-4d8a-8d6e-73c4d191b5e8", "fitness": 0.39209748680043344, "name": "Enhanced_QHS", "description": "Minor adjustments in quantum probability and learning rate dynamics to enhance convergence efficiency.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.65  # Final Harmony memory consideration rate\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.1  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob = 0.25  # Adjusted Initial Quantum probability\n        self.learning_rate = 0.12  # Adjusted Adaptive learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.25 * progress  # Minor adjustment\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.99)  # Reduce learning rate if improvement\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 63, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.39210 with standard deviation 0.19705.", "error": "", "parent_ids": ["1ba62f3f-f72c-4cf8-994b-58e02ba9a6b5"], "operator": null, "metadata": {"aucs": [0.5958672146025801, 0.3358993230021061, 0.5132138424306744, 0.26556985484339735, 0.5517432203435275, 0.707478041418856, 0.30281858033805875, 0.09790752392576263, 0.15837978029893862]}}
{"id": "25966f4f-00ce-45cf-8219-ca5005511f9d", "fitness": 0.2810217774856908, "name": "Refined_QHS", "description": "Implement dynamic parameter adjustments and adaptive mutation strategies to improve convergence and exploration balance.", "code": "import numpy as np\n\nclass Refined_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.9  # Dynamic Harmony memory consideration rate\n        self.hmcr_final = 0.7\n        self.par_init = 0.4  # Dynamic Pitch adjustment rate\n        self.par_final = 0.15\n        self.evaluations = 0\n        self.quantum_prob_init = 0.3  # Increased initial Quantum probability\n        self.learning_rate = 0.1  # Adaptive learning rate\n        self.mutation_rate = 0.1  # Initial mutation rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob_init * (1 - progress) + 0.2 * progress\n            mutation_adaptive = self.mutation_rate * (1 - progress) + 0.05 * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < mutation_adaptive:\n                mutation_vector = np.random.normal(0, 1, self.dim)\n                new_harmony += self.learning_rate * mutation_vector\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.99)\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 64, "feedback": "The algorithm Refined_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28102 with standard deviation 0.12852.", "error": "", "parent_ids": ["9d4efd95-b5c7-4d8a-8d6e-73c4d191b5e8"], "operator": null, "metadata": {"aucs": [0.37130720875248346, 0.12309247559239334, 0.21801834516961927, 0.19867779941040187, 0.29985166969237775, 0.4335983217664934, 0.10543834515823713, 0.2739238591379547, 0.5052879726912565]}}
{"id": "85c859e9-008d-422d-ad05-47bb5f7b6411", "fitness": 0.3367759506740702, "name": "Enhanced_QHS", "description": "Fine-tune learning rate dynamics to enhance convergence and stability.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.65  # Final Harmony memory consideration rate\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.1  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob = 0.25  # Adjusted Initial Quantum probability\n        self.learning_rate = 0.12  # Adjusted Adaptive learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.25 * progress  # Minor adjustment\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.995)  # Reduce learning rate if improvement\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 65, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33678 with standard deviation 0.16512.", "error": "", "parent_ids": ["9d4efd95-b5c7-4d8a-8d6e-73c4d191b5e8"], "operator": null, "metadata": {"aucs": [0.4910295781544711, 0.6097621600758107, 0.41881352479223777, 0.18485430906109013, 0.22077251964610578, 0.3419293125039312, 0.49038971603410964, 0.1285200473725462, 0.14491238842632914]}}
{"id": "466276bb-c076-4dcb-9c08-76ac3b12f26b", "fitness": 0.2169290361142241, "name": "Enhanced_QHS_Adaptive", "description": "Introducing adaptive memory update and hybrid exploration strategy to enhance convergence in quantum-inspired harmony search.", "code": "import numpy as np\n\nclass Enhanced_QHS_Adaptive:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.65  # Final Harmony memory consideration rate\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.1  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob = 0.25  # Adjusted Initial Quantum probability\n        self.learning_rate = 0.12  # Adjusted Adaptive learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.25 * progress  # Minor adjustment\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            # Hybrid exploration strategy\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / (1 + progress)\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n                # Additional mutation for exploration\n                mutation = (ub - lb) * 0.1 * np.random.randn(self.dim)\n                new_harmony = np.clip(new_harmony + mutation, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.99)  # Reduce learning rate if improvement\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n                # Reactive adjustment of memory\n                if np.random.rand() < 0.5:\n                    worst_idx = np.argmax(harmony_scores)\n                    harmony_memory[worst_idx] = np.clip(np.mean(harmony_memory, axis=0) + \n                                                        (np.random.uniform(-1, 1, self.dim) * \n                                                         (ub - lb) * 0.1), lb, ub)\n\n        return best_harmony, best_score", "configspace": "", "generation": 66, "feedback": "The algorithm Enhanced_QHS_Adaptive got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.21693 with standard deviation 0.09489.", "error": "", "parent_ids": ["9d4efd95-b5c7-4d8a-8d6e-73c4d191b5e8"], "operator": null, "metadata": {"aucs": [0.27102712149196606, 0.12054142559135184, 0.37690813803370715, 0.19044620968024395, 0.2051774071553083, 0.20681954146946013, 0.12348464952189386, 0.1000010019245492, 0.35795583015953625]}}
{"id": "0a2b0f72-e274-4b69-a5dd-97e72adfa2d2", "fitness": 0.3901450382762234, "name": "Adaptive_Synergistic_QHS", "description": "Introduce adaptive perturbation with dynamic scaling and stochastic multi-swarm synergy to enhance solution diversity and convergence robustness.", "code": "import numpy as np\n\nclass Adaptive_Synergistic_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.65  # Final Harmony memory consideration rate\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.1  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob = 0.25  # Adjusted Initial Quantum probability\n        self.learning_rate = 0.12  # Adjusted Adaptive learning rate\n        self.dynamic_scaling_factor = 0.1  # Initial dynamic scaling factor\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.25 * progress\n            self.dynamic_scaling_factor = 0.1 + 0.9 * progress  # Increase scaling factor over time\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_estimation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_estimation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / (dynamic_delta_factor * self.dynamic_scaling_factor)\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.98)  # Further reduce learning rate if improvement\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 67, "feedback": "The algorithm Adaptive_Synergistic_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.39015 with standard deviation 0.10296.", "error": "", "parent_ids": ["9d4efd95-b5c7-4d8a-8d6e-73c4d191b5e8"], "operator": null, "metadata": {"aucs": [0.2809561546692412, 0.48870007803197657, 0.37374310304685765, 0.4066645129629112, 0.2048891683096511, 0.5671511881209521, 0.35595471194014083, 0.46666078091872165, 0.36658564648555825]}}
{"id": "ca35d4a0-a6c2-4262-b4a7-d6a098e92a8c", "fitness": 0.11042767371217319, "name": "Adaptive_QHS", "description": "Adaptive Quantum Harmony Search with Dynamic Momentum to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass Adaptive_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.65  # Final Harmony memory consideration rate\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.1  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob = 0.25  # Initial Quantum probability\n        self.learning_rate = 0.12  # Base learning rate\n        self.momentum = 0.9  # Initial momentum for adjustment\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        previous_harmony = np.zeros(self.dim)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.25 * progress  # Adjusted\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n            \n            # Apply dynamic momentum for exploration\n            momentum_term = self.momentum * (new_harmony - previous_harmony)\n            new_harmony += momentum_term\n            \n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n                \n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.99)  # Reduce learning rate if improvement\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n            previous_harmony = new_harmony\n\n        return best_harmony, best_score", "configspace": "", "generation": 68, "feedback": "The algorithm Adaptive_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11043 with standard deviation 0.03393.", "error": "", "parent_ids": ["9d4efd95-b5c7-4d8a-8d6e-73c4d191b5e8"], "operator": null, "metadata": {"aucs": [0.09314700913063156, 0.09629989281155626, 0.09464508426288332, 0.1582165317630888, 0.1501582149774593, 0.16298301353085554, 0.08003480496159243, 0.07167583485511098, 0.08668867711638051]}}
{"id": "b92430ab-3549-4d3e-ac5c-bd422f06ffb4", "fitness": 0.23465884453171965, "name": "Enhanced_QHS", "description": "Introduce dynamic adjustment of the harmony memory size and introduce a new quantum rotation factor to enhance exploration.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  \n        self.hmcr_final = 0.65  \n        self.par_init = 0.5  \n        self.par_final = 0.1  \n        self.evaluations = 0\n        self.quantum_prob = 0.25  \n        self.learning_rate = 0.12  \n        self.quantum_rotation = 0.1  # New quantum rotation factor\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.25 * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n                # Apply quantum rotation factor adjustment\n                new_harmony = new_harmony * (1 + self.quantum_rotation * np.random.uniform(-1, 1, self.dim))\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.99)\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 69, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.23466 with standard deviation 0.16321.", "error": "", "parent_ids": ["9d4efd95-b5c7-4d8a-8d6e-73c4d191b5e8"], "operator": null, "metadata": {"aucs": [0.11146829992861773, 0.5777291251377854, 0.4441425107362774, 0.2631214376801727, 0.20178017967902095, 0.23752656876349365, 0.08596493604020539, 0.09357429126252481, 0.09662225155737891]}}
{"id": "a8a57f6e-f73d-475e-980e-4181b1821fa4", "fitness": 0.4686039831176857, "name": "Enhanced_QHS", "description": "Improved balance between exploration and exploitation by refining learning dynamics and mutation probability.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.70  # Final Harmony memory consideration rate  # Change 1\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.05  # Final Pitch adjustment rate  # Change 2\n        self.evaluations = 0\n        self.quantum_prob = 0.20  # Adjusted Initial Quantum probability  # Change 3\n        self.learning_rate = 0.15  # Adjusted Adaptive learning rate  # Change 4\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.20 * progress  # Minor adjustment\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.98)  # Reduce learning rate if improvement  # Change 5\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 70, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.46860 with standard deviation 0.09980.", "error": "", "parent_ids": ["9d4efd95-b5c7-4d8a-8d6e-73c4d191b5e8"], "operator": null, "metadata": {"aucs": [0.4259899049220135, 0.531367413375461, 0.4758387970594624, 0.5640073408919167, 0.20731258726304258, 0.5366005313065947, 0.48247280311873253, 0.501670145103792, 0.49217632501815634]}}
{"id": "7a8a2676-3f9c-42c3-a91d-ee103cf4c15d", "fitness": 0.2108929390366049, "name": "Enhanced_QHS", "description": "Enhanced exploration and exploitation balance with dynamic memory allocation and adaptive quantum behavior.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  \n        self.hmcr_init = 0.95  \n        self.hmcr_final = 0.60  # Change 1: Adjusted final HMCR for better exploration\n        self.par_init = 0.5  \n        self.par_final = 0.02  # Change 2: Adjusted final PAR for more refined pitch adjustment\n        self.evaluations = 0\n        self.quantum_prob = 0.25  # Change 3: Increased initial Quantum probability for enhanced exploration\n        self.learning_rate = 0.18  # Change 4: Adjusted learning rate for better convergence control\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.25 * progress  # Change 5: Improved quantum adaptation\n\n            dynamic_memory_factor = int(self.hms * (0.5 + 0.5 * progress))  # Change 6: Dynamic memory allocation\n\n            new_harmony = np.array([harmony_memory[np.random.randint(dynamic_memory_factor), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(dynamic_memory_factor)])  # Change 7: Use dynamic memory factor\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.95)  # Change 8: Slightly increased decay rate of learning rate\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 71, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.21089 with standard deviation 0.19399.", "error": "", "parent_ids": ["a8a57f6e-f73d-475e-980e-4181b1821fa4"], "operator": null, "metadata": {"aucs": [0.21084838254359317, 0.1085435728300711, 0.7378962248768727, 0.18547747528098613, 0.21251933280922708, 0.1932136124371463, 0.06672474954203023, 0.08190521629645231, 0.100907884713065]}}
{"id": "0aefc52c-66fd-44cf-be1a-2bb1acc7bfcc", "fitness": 0.4686039831176857, "name": "Enhanced_QHS", "description": "Introduced a dynamic adjustment to the harmony memory size to enhance adaptability.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.70  # Final Harmony memory consideration rate  # Change 1\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.05  # Final Pitch adjustment rate  # Change 2\n        self.evaluations = 0\n        self.quantum_prob = 0.20  # Adjusted Initial Quantum probability  # Change 3\n        self.learning_rate = 0.15  # Adjusted Adaptive learning rate  # Change 4\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.hms = min(self.hms, max(1, int(self.hms * (1 - 0.1 * (self.evaluations / self.budget)))))  # Dynamic adjustment of HMS\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.20 * progress  # Minor adjustment\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.98)  # Reduce learning rate if improvement  # Change 5\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 72, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.46860 with standard deviation 0.09980.", "error": "", "parent_ids": ["a8a57f6e-f73d-475e-980e-4181b1821fa4"], "operator": null, "metadata": {"aucs": [0.4259899049220135, 0.531367413375461, 0.4758387970594624, 0.5640073408919167, 0.20731258726304258, 0.5366005313065947, 0.48247280311873253, 0.501670145103792, 0.49217632501815634]}}
{"id": "7a39c3c7-0505-4756-bf3b-4d4164a6ddf5", "fitness": 0.3932926171156684, "name": "Enhanced_QHS", "description": "Enhanced exploration and exploitation balance using adaptive harmony memory consideration rate and improved quantum dynamics.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.65  # Final Harmony memory consideration rate  # Change 1\n        self.par_init = 0.55  # Initial Pitch adjustment rate  # Change 2\n        self.par_final = 0.1  # Final Pitch adjustment rate  # Change 3\n        self.evaluations = 0\n        self.quantum_prob = 0.25  # Adjusted Initial Quantum probability  # Change 4\n        self.learning_rate = 0.12  # Adjusted Adaptive learning rate  # Change 5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * (progress ** 2)  # Change 6\n            par = self.par_init - (self.par_init - self.par_final) * (1 - np.cos(np.pi * progress)) / 2  # Change 7\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.25 * progress  # Change 8\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(3) * (1 + progress)  # Change 9\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.02, self.learning_rate * 0.97)  # Change 10\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 73, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.39329 with standard deviation 0.16364.", "error": "", "parent_ids": ["a8a57f6e-f73d-475e-980e-4181b1821fa4"], "operator": null, "metadata": {"aucs": [0.12081210219706218, 0.44925380978230944, 0.4700469900284918, 0.4518032404570381, 0.5937004882900345, 0.4677831218835471, 0.5184154867987558, 0.08875828412898756, 0.3790600304747894]}}
{"id": "7fdaf614-28c2-49eb-9e5c-781bdf5036b8", "fitness": 0.38976506761126856, "name": "Enhanced_QHS", "description": "Fine-tune the quantum probability dynamics for better adaptability during optimization.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.70  # Final Harmony memory consideration rate  # Change 1\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.05  # Final Pitch adjustment rate  # Change 2\n        self.evaluations = 0\n        self.quantum_prob = 0.20  # Adjusted Initial Quantum probability  # Change 3\n        self.learning_rate = 0.15  # Adjusted Adaptive learning rate  # Change 4\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress**2) + 0.20 * progress  # Minor adjustment\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.98)  # Reduce learning rate if improvement  # Change 5\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 74, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.38977 with standard deviation 0.12766.", "error": "", "parent_ids": ["a8a57f6e-f73d-475e-980e-4181b1821fa4"], "operator": null, "metadata": {"aucs": [0.48013627065546594, 0.2697660120943757, 0.46674736146739915, 0.5324198898644404, 0.22961031366207008, 0.3787027295574391, 0.416987854306353, 0.17931607219166013, 0.5541991047022135]}}
{"id": "d95c7440-84c8-48cf-8933-1e370bf06f63", "fitness": 0.259093511945601, "name": "Enhanced_QHS", "description": "Enhanced quantum-based harmony search with dynamic memory adaptation and improved convergence.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  \n        self.hmcr_init = 0.95  \n        self.hmcr_final = 0.65  # Change: Adjusted final harmony memory consideration rate\n        self.par_init = 0.5  \n        self.par_final = 0.10  # Change: Adjusted final pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob = 0.25  # Change: Increased initial quantum probability\n        self.learning_rate = 0.12  # Change: Adjusted adaptive learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.25 * progress  # Change: Minor adjustment\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.03, self.learning_rate * 0.97)  # Change: Reduce learning rate if improvement\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory = np.delete(harmony_memory, worst_idx, axis=0)  # Change: Remove worst harmony\n                harmony_scores = np.delete(harmony_scores, worst_idx)  # Change: Update scores\n                harmony_memory = np.vstack([harmony_memory, new_harmony])  # Change: Add new harmony\n                harmony_scores = np.append(harmony_scores, score)  # Change: Append new score\n\n        return best_harmony, best_score", "configspace": "", "generation": 75, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25909 with standard deviation 0.13900.", "error": "", "parent_ids": ["a8a57f6e-f73d-475e-980e-4181b1821fa4"], "operator": null, "metadata": {"aucs": [0.1334901166886453, 0.5610647769051096, 0.3702358801466863, 0.1912057937121524, 0.3316375706896303, 0.21853352007541393, 0.15641504440916543, 0.08112264865065566, 0.28813625623295036]}}
{"id": "0491341f-45b6-403e-9bf3-a96f3a98d80b", "fitness": 0.46096280624919295, "name": "Enhanced_QHS", "description": "Enhanced adaptive quantum dynamics with improved exploration control and learning rate adjustment for better optimization.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.70  # Final Harmony memory consideration rate\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.05  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob = 0.20  # Adjusted Initial Quantum probability\n        self.learning_rate = 0.15  # Adjusted Adaptive learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.25 * progress  # Change 1\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = 1.5 * (1 + progress)  # Change 2\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.97)  # Change 3\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 76, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.46096 with standard deviation 0.12815.", "error": "", "parent_ids": ["a8a57f6e-f73d-475e-980e-4181b1821fa4"], "operator": null, "metadata": {"aucs": [0.5851835403879916, 0.5182466499091432, 0.6011090947917783, 0.2892143123759148, 0.32772386012045474, 0.3151282418839393, 0.6549656204297559, 0.45299637848725127, 0.40409755785650714]}}
{"id": "4ce0a50f-7c53-4c0a-b4b4-64a6c5b2ebf7", "fitness": 0.29198358800383356, "name": "Enhanced_QHS_AdaptiveDiversity", "description": "Introduce adaptive diversity preservation with a dynamic opposition-based learning mechanism for enhanced exploration and improved exploitation balance.", "code": "import numpy as np\n\nclass Enhanced_QHS_AdaptiveDiversity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)\n        self.hmcr_init = 0.95\n        self.hmcr_final = 0.70\n        self.par_init = 0.5\n        self.par_final = 0.05\n        self.evaluations = 0\n        self.quantum_prob = 0.20\n        self.learning_rate = 0.15\n        self.diversity_factor_init = 0.1\n        self.diversity_factor_final = 0.3\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.20 * progress\n            diversity_factor = self.diversity_factor_init + (self.diversity_factor_final - self.diversity_factor_init) * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n            \n            # Opposition-based learning\n            if np.random.rand() < diversity_factor:\n                opposition = lb + ub - new_harmony\n                opposition_score = func(opposition)\n                self.evaluations += 1\n                if opposition_score < best_score:\n                    new_harmony, score = opposition, opposition_score\n                else:\n                    score = func(new_harmony)\n                    self.evaluations += 1\n            else:\n                score = func(new_harmony)\n                self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.98)\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 77, "feedback": "The algorithm Enhanced_QHS_AdaptiveDiversity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29198 with standard deviation 0.14976.", "error": "", "parent_ids": ["a8a57f6e-f73d-475e-980e-4181b1821fa4"], "operator": null, "metadata": {"aucs": [0.40337449520427926, 0.11191540762726304, 0.3897854409646996, 0.5138111641961352, 0.19273495819144115, 0.3428342948066704, 0.12637721406147917, 0.10365160957341635, 0.44336770740911824]}}
{"id": "e3785d27-7803-4d63-9740-4ad3c1f84e27", "fitness": 0.3278460052810255, "name": "Refined_QHS", "description": "Refined exploration and exploitation using dynamic harmony memory and adaptive pitch adjustments for enhanced convergence.", "code": "import numpy as np\n\nclass Refined_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)\n        self.hmcr_init = 0.95\n        self.hmcr_final = 0.65\n        self.par_init = 0.5\n        self.par_final = 0.05\n        self.evaluations = 0\n        self.quantum_prob = 0.25\n        self.learning_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.25 * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.97)\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 78, "feedback": "The algorithm Refined_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32785 with standard deviation 0.20584.", "error": "", "parent_ids": ["a8a57f6e-f73d-475e-980e-4181b1821fa4"], "operator": null, "metadata": {"aucs": [0.15268849595652656, 0.5958010040546844, 0.6704325244441516, 0.3865751449287076, 0.4592454291948186, 0.3492293139278366, 0.13539440789323232, 0.1009843927110835, 0.10026333441818847]}}
{"id": "88efe53a-d499-455b-9067-120bf57d56a3", "fitness": 0.27427846079041523, "name": "Adaptive_QHS", "description": "Adaptive Quantum Harmony Search with dynamic memory-based exploration and exploitation balance to enhance convergence.", "code": "import numpy as np\n\nclass Adaptive_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.65  # Final Harmony memory consideration rate\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.1  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob = 0.25  # Quantum probability\n        self.learning_rate = 0.1  # Initial Adaptive learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.25 * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * np.exp(progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.97)\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 79, "feedback": "The algorithm Adaptive_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27428 with standard deviation 0.11490.", "error": "", "parent_ids": ["a8a57f6e-f73d-475e-980e-4181b1821fa4"], "operator": null, "metadata": {"aucs": [0.19612342346899847, 0.491717080135334, 0.31420776228289904, 0.19486531109190774, 0.39062601231022487, 0.3226865281015042, 0.1808987708836921, 0.28428285179908797, 0.09309840704008865]}}
{"id": "126fe714-9b74-4dd2-93f8-c3e9b29e2726", "fitness": 0.4028145810241004, "name": "Enhanced_AQHS", "description": "Enhanced Adaptive Quantum Harmony Search (EAQHS) leveraging dynamic parameter tuning and variable environment perception for improved convergence.", "code": "import numpy as np\n\nclass Enhanced_AQHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)\n        self.hmcr_init = 0.95\n        self.hmcr_final = 0.70\n        self.par_init = 0.5\n        self.par_final = 0.05\n        self.evaluations = 0\n        self.quantum_prob = 0.20\n        self.learning_rate = 0.15\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.20 * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                similarity = 1 - np.abs((new_harmony - best_harmony) / (ub - lb)).mean()\n                effective_lr = self.learning_rate * (1 - similarity)  # Dynamic adjustment based on similarity\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += effective_lr * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.98)\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 80, "feedback": "The algorithm Enhanced_AQHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40281 with standard deviation 0.14105.", "error": "", "parent_ids": ["a8a57f6e-f73d-475e-980e-4181b1821fa4"], "operator": null, "metadata": {"aucs": [0.6671300406937969, 0.4674632210934542, 0.4461115351315299, 0.5213984605427013, 0.4134180394742881, 0.2021455923070703, 0.21230350302477263, 0.2914278056078726, 0.40393303134141756]}}
{"id": "374327c5-7200-482b-afbb-89dce848dfef", "fitness": 0.4638226077585943, "name": "Refined_QHS", "description": "Introduce dynamic memory consideration and adaptive scaling to enhance search diversity and convergence speed.", "code": "import numpy as np\n\nclass Refined_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)\n        self.hmcr_init = 0.95\n        self.hmcr_final = 0.70\n        self.par_init = 0.5\n        self.par_final = 0.05\n        self.evaluations = 0\n        self.quantum_prob = 0.20\n        self.learning_rate = 0.15\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * np.exp(-1.5 * progress)  # Dynamic adjustment\n\n            # Dynamic memory consideration based on population diversity\n            std_dev = np.std(harmony_memory, axis=0)\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress * std_dev.mean())  # Adaptive scaling\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.98)\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 81, "feedback": "The algorithm Refined_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.46382 with standard deviation 0.11009.", "error": "", "parent_ids": ["a8a57f6e-f73d-475e-980e-4181b1821fa4"], "operator": null, "metadata": {"aucs": [0.5185088533615007, 0.5040574745847808, 0.5739273234874185, 0.6362544067137931, 0.3416653657042428, 0.2742852519505118, 0.3594872883563429, 0.4748107658738904, 0.4914067397948677]}}
{"id": "a0110896-f066-4577-be25-e5b33e42740e", "fitness": 0.4686039831176857, "name": "Enhanced_QHS", "description": "Enhanced learning strategy by dynamically adapting mutation based on evaluation progress.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.70  # Final Harmony memory consideration rate  # Change 1\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.05  # Final Pitch adjustment rate  # Change 2\n        self.evaluations = 0\n        self.quantum_prob = 0.20  # Adjusted Initial Quantum probability  # Change 3\n        self.learning_rate = 0.15  # Adjusted Adaptive learning rate  # Change 4\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.20 * progress  # Minor adjustment\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.98)  # Reduce learning rate if improvement  # Change 5\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 82, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.46860 with standard deviation 0.09980.", "error": "", "parent_ids": ["a8a57f6e-f73d-475e-980e-4181b1821fa4"], "operator": null, "metadata": {"aucs": [0.4259899049220135, 0.531367413375461, 0.4758387970594624, 0.5640073408919167, 0.20731258726304258, 0.5366005313065947, 0.48247280311873253, 0.501670145103792, 0.49217632501815634]}}
{"id": "061dfc80-20ec-4ea1-a704-8ef4c93e5045", "fitness": 0.30184819865641516, "name": "Enhanced_QHS", "description": "Enhanced exploration-exploitation balance through an adaptive harmony memory and learning rate mechanism.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.65  # Final Harmony memory consideration rate  # Change 1\n        self.par_init = 0.6  # Initial Pitch adjustment rate  # Change 2\n        self.par_final = 0.1  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob = 0.15  # Adjusted Initial Quantum probability  # Change 3\n        self.learning_rate = 0.12  # Adjusted Adaptive learning rate  # Change 4\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.15 * progress  # Minor adjustment\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * np.random.uniform(-0.5, 0.5, self.dim)  # Change 5\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(1 + progress)  # Change 6\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.99)  # Reduce learning rate if improvement  # Change 7\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 83, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30185 with standard deviation 0.12059.", "error": "", "parent_ids": ["a8a57f6e-f73d-475e-980e-4181b1821fa4"], "operator": null, "metadata": {"aucs": [0.4574389676930486, 0.38910878370062973, 0.4689247218460548, 0.18271780448552133, 0.230902113645024, 0.2245148727292674, 0.34506384132111734, 0.32478685677514996, 0.0931758257119234]}}
{"id": "522fe2a2-424b-45c2-bb29-07501a3b7f85", "fitness": -Infinity, "name": "QuantumHarmonyOptimization", "description": "Quantum Harmony Optimization with Enhanced Diversity and Adaptive Differential Mutation for improved exploration and convergence.", "code": "import numpy as np\n\nclass QuantumHarmonyOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)\n        self.hmcr_init = 0.95\n        self.hmcr_final = 0.60  # Increased exploration over iterations\n        self.par_init = 0.6  # Higher initial pitch adjustment rate for more initial exploration\n        self.par_final = 0.05\n        self.evaluations = 0\n        self.quantum_prob = 0.25  # Higher initial quantum probability for more diverse solutions\n        self.learning_rate = 0.10  # Initial learning rate\n        self.diff_mutation_scale = 0.8  # Differential mutation scale factor\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.20 * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                diff_vector = np.random.choice(harmony_memory, 2, replace=False)\n                diff = self.diff_mutation_scale * (diff_vector[0] - diff_vector[1])\n                new_harmony += self.learning_rate * diff\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.98)\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 84, "feedback": "An exception occurred: ValueError('a must be 1-dimensional').", "error": "ValueError('a must be 1-dimensional')", "parent_ids": ["a8a57f6e-f73d-475e-980e-4181b1821fa4"], "operator": null, "metadata": {}}
{"id": "477b61e7-aa83-414d-86d1-b83e49994897", "fitness": 0.3066078326085564, "name": "Adaptive_QHS", "description": "Adaptive Quantum Harmony Search with dynamic parameter adjustment for enhanced convergence balancing exploration and exploitation.  ", "code": "import numpy as np\n\nclass Adaptive_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.65  # Final Harmony memory consideration rate\n        self.par_init = 0.4  # Initial Pitch adjustment rate\n        self.par_final = 0.1  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob = 0.25  # Initial Quantum probability\n        self.learning_rate = 0.2  # Initial Adaptive learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.25 * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = (1 + progress)**1.5\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.97)\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 85, "feedback": "The algorithm Adaptive_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30661 with standard deviation 0.17910.", "error": "", "parent_ids": ["a8a57f6e-f73d-475e-980e-4181b1821fa4"], "operator": null, "metadata": {"aucs": [0.5409300312921586, 0.5260782305024829, 0.15439401052071844, 0.2032181831501748, 0.22366034775424393, 0.23689318868366904, 0.14600816598035393, 0.13218734645235486, 0.5961009891408513]}}
{"id": "8ee9a17e-b7c7-4935-b03c-f2336a6f4696", "fitness": 0.4432024980499223, "name": "Adaptive_Diversity_QHS", "description": "Combine adaptive learning and diversity-driven exploration to enhance global and local search efficiency.", "code": "import numpy as np\n\nclass Adaptive_Diversity_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.65  # Final Harmony memory consideration rate\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.05  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob_init = 0.25  # Initial Quantum probability\n        self.learning_rate_init = 0.2  # Initial Adaptive learning rate\n        self.learning_rate_final = 0.05  # Final Adaptive learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            quantum_prob = self.quantum_prob_init * (1 - progress) + 0.1 * progress\n            learning_rate = self.learning_rate_init - (self.learning_rate_init - self.learning_rate_final) * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / np.sqrt(2)\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 86, "feedback": "The algorithm Adaptive_Diversity_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.44320 with standard deviation 0.15580.", "error": "", "parent_ids": ["a8a57f6e-f73d-475e-980e-4181b1821fa4"], "operator": null, "metadata": {"aucs": [0.40522152934242084, 0.6818559067091181, 0.2945360086302019, 0.4937928855473258, 0.20002203323159207, 0.6267196562808368, 0.5509430352506495, 0.2687876745937011, 0.4669437528634548]}}
{"id": "9463a154-9206-427c-a15a-f93e196a8847", "fitness": 0.36153819515571967, "name": "Adaptive_QHS", "description": "Adaptive Quantum Harmony Search with Dynamic Parameter Tuning to Balance Exploration and Exploitation Based on Search Progress.", "code": "import numpy as np\n\nclass Adaptive_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)\n        self.hmcr_init = 0.95\n        self.hmcr_final = 0.65  # Further reduced final HMCR to encourage exploration\n        self.par_init = 0.5\n        self.par_final = 0.1  # Slightly higher PAR to maintain exploration\n        self.quantum_prob = 0.25  # Increased initial Quantum probability\n        self.learning_rate = 0.2  # Higher initial learning rate for aggressive initial search\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress**2\n            par = self.par_init - (self.par_init - self.par_final) * progress**2\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress**2) + 0.15 * progress**2\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.97)\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 87, "feedback": "The algorithm Adaptive_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.36154 with standard deviation 0.17921.", "error": "", "parent_ids": ["a8a57f6e-f73d-475e-980e-4181b1821fa4"], "operator": null, "metadata": {"aucs": [0.5704874700176011, 0.48303448157062145, 0.6076928952341722, 0.18063762508924885, 0.5163651486625105, 0.22633728543594633, 0.18953820916681252, 0.10595323654639943, 0.3737974046781639]}}
{"id": "49a85422-f1f5-458f-8047-8a37bdb066ca", "fitness": 0.5144482717367311, "name": "Enhanced_QHS", "description": "Introduce a dynamic mutation step size to improve convergence rate and solution accuracy.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.70  # Final Harmony memory consideration rate  # Change 1\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.05  # Final Pitch adjustment rate  # Change 2\n        self.evaluations = 0\n        self.quantum_prob = 0.20  # Adjusted Initial Quantum probability  # Change 3\n        self.learning_rate = 0.15  # Adjusted Adaptive learning rate  # Change 4\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.20 * progress  # Minor adjustment\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                dynamic_step = (1 - progress) * 0.1  # Introduced dynamic mutation step size\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation * dynamic_step\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.98)  # Reduce learning rate if improvement  # Change 5\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 88, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.51445 with standard deviation 0.09937.", "error": "", "parent_ids": ["a8a57f6e-f73d-475e-980e-4181b1821fa4"], "operator": null, "metadata": {"aucs": [0.5352278547824648, 0.6505081631874098, 0.5800546012515149, 0.5654853704089594, 0.5549178520192799, 0.5764668765973906, 0.3334834624573395, 0.36322824573024204, 0.4706620191959787]}}
{"id": "014b753d-9c30-42c6-8f29-96436a84f130", "fitness": 0.08823334804374294, "name": "Enhanced_QHS", "description": "Implement an adaptive selection strategy and multi-parent recombination to enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10) \n        self.hmcr_init = 0.90  # Adjusted Initial Harmony memory consideration rate  \n        self.hmcr_final = 0.75  # Adjusted Final Harmony memory consideration rate  \n        self.par_init = 0.45  # Adjusted Initial Pitch adjustment rate\n        self.par_final = 0.15  # Adjusted Final Pitch adjustment rate  \n        self.evaluations = 0\n        self.quantum_prob = 0.25  # Adjusted Initial Quantum probability \n        self.learning_rate = 0.12  # Adjusted Adaptive learning rate  \n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.15 * progress  \n\n            parent_indices = np.random.choice(range(self.hms), size=2, replace=False)\n            new_harmony = np.mean(harmony_memory[parent_indices], axis=0)  \n            if np.random.rand() < hmcr:\n                for j in range(self.dim):\n                    if np.random.rand() >= 0.5:\n                        new_harmony[j] = harmony_memory[np.random.randint(self.hms), j]\n\n            if np.random.rand() < par:\n                dynamic_step = (1 - progress) * 0.1  \n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation * dynamic_step\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.98)  \n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 89, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08823 with standard deviation 0.04567.", "error": "", "parent_ids": ["49a85422-f1f5-458f-8047-8a37bdb066ca"], "operator": null, "metadata": {"aucs": [0.06135443558467968, 0.05818126570768978, 0.06472739037914255, 0.1474134465393091, 0.15294213416023905, 0.15663926508801962, 0.05159515409129334, 0.045314608403598466, 0.05593243243971491]}}
{"id": "c1f55c33-d415-49b6-94ec-1ed5b77ca013", "fitness": 0.4787479157387643, "name": "Enhanced_QHS", "description": "Integrate adaptive multi-population strategies to enhance exploration and exploitation dynamics.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)\n        self.hmcr_init = 0.95\n        self.hmcr_final = 0.70\n        self.par_init = 0.5\n        self.par_final = 0.05\n        self.evaluations = 0\n        self.quantum_prob = 0.20\n        self.learning_rate = 0.15\n        self.subpopulation_size = self.hms // 2  # Change 1: Introduce subpopulation size\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.20 * progress\n\n            # Select a subpopulation for enhanced exploration\n            subpopulation_indices = np.random.choice(self.hms, self.subpopulation_size, replace=False)  # Change 2\n            subpopulation = harmony_memory[subpopulation_indices]\n\n            new_harmony = np.array([subpopulation[np.random.randint(self.subpopulation_size), j]  # Change 3\n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                dynamic_step = (1 - progress) * 0.1\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation * dynamic_step\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.98)\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 90, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.47875 with standard deviation 0.15939.", "error": "", "parent_ids": ["49a85422-f1f5-458f-8047-8a37bdb066ca"], "operator": null, "metadata": {"aucs": [0.6155243155225496, 0.6642715359798244, 0.36286638837583596, 0.6710063485768932, 0.5350493326547208, 0.4245659072037007, 0.1640331639792474, 0.5259102565642197, 0.34550399279188704]}}
{"id": "5f8549d3-5b0d-4a99-b610-638802a3e35c", "fitness": 0.39883123770584794, "name": "Enhanced_QHS", "description": "Integrate an adaptive step decay mechanism to dynamically adjust learning rates and quantum probabilities, enhancing convergence efficiency.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.70  # Final Harmony memory consideration rate\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.05  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob = 0.25  # Change 1: Adjusted Initial Quantum probability\n        self.learning_rate = 0.15  # Initial Adaptive learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (0.8 + 0.2 * np.sin(progress * np.pi))  # Change 2: Dynamic adjustment\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                dynamic_step = (1 - progress) * 0.1\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation * dynamic_step\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.98)  # Reduce learning rate if improvement\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 91, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.39883 with standard deviation 0.12493.", "error": "", "parent_ids": ["49a85422-f1f5-458f-8047-8a37bdb066ca"], "operator": null, "metadata": {"aucs": [0.41532813411765457, 0.3546227894073618, 0.4998618220139983, 0.17854376355246948, 0.3871990163764568, 0.529205917482683, 0.5180230694221231, 0.2049833078791401, 0.501713319100744]}}
{"id": "8cc5ba8c-8182-49e3-bf8f-9619ecb43926", "fitness": 0.4824921705400311, "name": "Enhanced_QHS", "description": "Introduce a novel convergence acceleration mechanism and adaptive memory updating to balance exploration and exploitation effectively.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)\n        self.hmcr_init = 0.95\n        self.hmcr_final = 0.70\n        self.par_init = 0.5\n        self.par_final = 0.05\n        self.evaluations = 0\n        self.quantum_prob = 0.20\n        self.learning_rate = 0.15\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.20 * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                dynamic_step = (1 - progress) * 0.1\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation * dynamic_step\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(1.5) * (1 + progress)  # Adjusted dynamic delta factor\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.98)\n\n            worst_idx = np.argmax(harmony_scores)\n            adaptive_memory_update_threshold = 0.1 + 0.5 * progress  # New memory update threshold\n            if score < harmony_scores[worst_idx] or np.random.rand() < adaptive_memory_update_threshold:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 92, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.48249 with standard deviation 0.13216.", "error": "", "parent_ids": ["49a85422-f1f5-458f-8047-8a37bdb066ca"], "operator": null, "metadata": {"aucs": [0.5669569487101671, 0.434635934178099, 0.6114167316233298, 0.1847083965270464, 0.6107474391599943, 0.6159562340372979, 0.44829659314317305, 0.39941122607434776, 0.47030003140682475]}}
{"id": "71c752e3-7fb5-4e13-a1e7-af730bcbe507", "fitness": 0.5144482717367311, "name": "Enhanced_QHS_Adaptive", "description": "Implement an adaptive mutation control strategy to dynamically balance exploration and exploitation for better convergence.", "code": "import numpy as np\n\nclass Enhanced_QHS_Adaptive:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.70  # Final Harmony memory consideration rate\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.05  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob = 0.20  # Adjusted Initial Quantum probability\n        self.learning_rate = 0.15  # Adjusted Adaptive learning rate\n        self.mutation_amp = 0.1  # Initial mutation amplitude\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.20 * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                dynamic_step = self.mutation_amp * (1 - progress)  # Dynamic mutation amplitude\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation * dynamic_step\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.mutation_amp = max(0.05, self.mutation_amp * 0.98)  # Update mutation amplitude on improvement\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 93, "feedback": "The algorithm Enhanced_QHS_Adaptive got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.51445 with standard deviation 0.09937.", "error": "", "parent_ids": ["49a85422-f1f5-458f-8047-8a37bdb066ca"], "operator": null, "metadata": {"aucs": [0.5352278547824648, 0.6505081631874098, 0.5800546012515149, 0.5654853704089594, 0.5549178520192799, 0.5764668765973906, 0.3334834624573395, 0.36322824573024204, 0.4706620191959787]}}
{"id": "d9dc2903-f482-4ba0-b9e4-384e02b775ae", "fitness": 0.42665236670898715, "name": "Enhanced_QHS", "description": "Enhance diversity and exploration by introducing a variable quantum probability and adaptive learning rate for better convergence.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.70  # Final Harmony memory consideration rate\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.05  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob_init = 0.20  # Initial Quantum probability\n        self.quantum_prob_final = 0.10  # Final Quantum probability # Change 1\n        self.learning_rate_init = 0.15  # Initial Adaptive learning rate\n        self.learning_rate_final = 0.05  # Final Adaptive learning rate # Change 2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob_init - (self.quantum_prob_init - self.quantum_prob_final) * progress  # Change 3\n            adaptive_learning_rate = self.learning_rate_init - (self.learning_rate_init - self.learning_rate_final) * progress  # Change 4\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                dynamic_step = (1 - progress) * 0.1  # Existing dynamic mutation step size\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += adaptive_learning_rate * gradient_approximation * dynamic_step  # Change 5\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate_init = max(0.05, self.learning_rate_init * 0.98)  # Apply decay to initial learning rate # Change 6\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 94, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42665 with standard deviation 0.10026.", "error": "", "parent_ids": ["49a85422-f1f5-458f-8047-8a37bdb066ca"], "operator": null, "metadata": {"aucs": [0.5176988476899393, 0.5491402228155073, 0.4866701098078532, 0.32112419935489434, 0.2965194290591855, 0.3063219522615249, 0.4992961077772614, 0.3429598583890501, 0.5201405732256684]}}
{"id": "070d204c-2491-412c-86bd-76b891d6ff6d", "fitness": 0.42714541530637684, "name": "Enhanced_QHS", "description": "Incorporate adaptive harmony memory size and enhanced exploitation strategies to improve solution accuracy.", "code": "import numpy as np\n\nclass Enhanced_QHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95\n        self.hmcr_final = 0.75  # Change 1\n        self.par_init = 0.5\n        self.par_final = 0.10  # Change 2\n        self.evaluations = 0\n        self.quantum_prob = 0.25  # Change 3\n        self.learning_rate = 0.15\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.25 * progress  # Change 4\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                dynamic_step = (1 - progress) * 0.15  # Change 5\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation * dynamic_step\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.97)  # Change 6\n\n            if np.random.rand() < 0.1:  # Additional exploitation strategy, Change 7\n                local_exploration = (np.random.rand(self.dim) - 0.5) * (ub - lb) * 0.05\n                new_harmony += local_exploration\n                new_harmony = np.clip(new_harmony, lb, ub)\n                score = func(new_harmony)\n                self.evaluations += 1\n                if score < best_score:\n                    best_score = score\n                    best_harmony = new_harmony\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 95, "feedback": "The algorithm Enhanced_QHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42715 with standard deviation 0.07380.", "error": "", "parent_ids": ["49a85422-f1f5-458f-8047-8a37bdb066ca"], "operator": null, "metadata": {"aucs": [0.47949744187811993, 0.4990872412841795, 0.5176161137411406, 0.3786961645763267, 0.37488472018728114, 0.5177555112808352, 0.4199317770935974, 0.32210011367847646, 0.33473965403743466]}}
{"id": "27213c9c-8ad5-49f5-a20c-feeca0a6f389", "fitness": 0.4568700681262485, "name": "AdaptiveInertiaQHS", "description": "Introduce adaptive inertia weighting in quantum harmony search to balance exploration and exploitation dynamically.", "code": "import numpy as np\n\nclass AdaptiveInertiaQHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.70  # Final Harmony memory consideration rate\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.05  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob = 0.20  # Initial Quantum probability\n        self.learning_rate = 0.15  # Adaptive learning rate\n        self.inertia_weight_init = 0.9  # Initial inertia weight\n        self.inertia_weight_final = 0.4  # Final inertia weight\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob * (1 - progress) + 0.20 * progress\n\n            inertia_weight = self.inertia_weight_init - (self.inertia_weight_init - self.inertia_weight_final) * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                dynamic_step = (1 - progress) * 0.1\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation * dynamic_step\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = inertia_weight * new_harmony + (1 - inertia_weight) * best_harmony\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.98)\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 96, "feedback": "The algorithm AdaptiveInertiaQHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.45687 with standard deviation 0.13825.", "error": "", "parent_ids": ["49a85422-f1f5-458f-8047-8a37bdb066ca"], "operator": null, "metadata": {"aucs": [0.6370941820468291, 0.28208654140419187, 0.40839859118170996, 0.5610776360410077, 0.29307186947983466, 0.5356500550689604, 0.5586977048851547, 0.5749248575365551, 0.26082917549199347]}}
{"id": "190a0c33-8b65-41aa-b7e0-c01cfd7aef20", "fitness": 0.5192654133695513, "name": "Enhanced_QHS_Adaptive", "description": "Introduce adaptive dynamic mutation and crossover strategies to enhance exploration and exploitation balance during optimization.", "code": "import numpy as np\n\nclass Enhanced_QHS_Adaptive:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.70  # Final Harmony memory consideration rate\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.05  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob_init = 0.20  # Initial Quantum probability\n        self.quantum_prob_final = 0.05  # Final Quantum probability\n        self.learning_rate = 0.15\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob_init - (self.quantum_prob_init - self.quantum_prob_final) * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                dynamic_step = np.power((1 - progress), 2) * np.abs(ub - lb) * 0.1\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation * dynamic_step\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress * 0.5)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.98)\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 97, "feedback": "The algorithm Enhanced_QHS_Adaptive got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.51927 with standard deviation 0.10749.", "error": "", "parent_ids": ["49a85422-f1f5-458f-8047-8a37bdb066ca"], "operator": null, "metadata": {"aucs": [0.40639639405725636, 0.5886264323576339, 0.6066354075991365, 0.6117655790804208, 0.5704620682431898, 0.6343600349782388, 0.4611284627905794, 0.294297197912754, 0.499717143306753]}}
{"id": "727b9a45-e2e3-4ea1-b113-cfcb2d00f9ea", "fitness": 0.3755627696428423, "name": "Enhanced_QHS_Diverse", "description": "Introduce adaptive learning rate scaling and memory diversity reinforcement to enhance convergence speed and solution accuracy.  ", "code": "import numpy as np\n\nclass Enhanced_QHS_Diverse:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.70  # Final Harmony memory consideration rate\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.05  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob_init = 0.20  # Initial Quantum probability\n        self.quantum_prob_final = 0.05  # Final Quantum probability\n        self.learning_rate = 0.15\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob_init - (self.quantum_prob_init - self.quantum_prob_final) * progress\n\n            new_harmony = np.array([harmony_memory[np.random.randint(self.hms), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                dynamic_step = np.power((1 - progress), 2) * np.abs(ub - lb) * 0.1\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation * dynamic_step\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress * 0.5)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * (1 + progress * 0.02))  # Adaptive learning rate scaling\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n            # Enforce diversity by replacing worst solutions periodically\n            if self.evaluations % (self.hms // 2) == 0:\n                diversity_factor = np.random.uniform(lb, ub, self.dim)\n                for _ in range(self.hms // 5):\n                    replace_idx = np.argmax(harmony_scores)\n                    harmony_memory[replace_idx] = diversity_factor\n                    harmony_scores[replace_idx] = func(harmony_memory[replace_idx])\n\n        return best_harmony, best_score", "configspace": "", "generation": 98, "feedback": "The algorithm Enhanced_QHS_Diverse got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.37556 with standard deviation 0.11048.", "error": "", "parent_ids": ["190a0c33-8b65-41aa-b7e0-c01cfd7aef20"], "operator": null, "metadata": {"aucs": [0.5228674845617065, 0.586999586809112, 0.22187181866538663, 0.36973276793837173, 0.254687582567367, 0.33636168892998164, 0.3474338598549218, 0.40736835003916183, 0.3327417874195715]}}
{"id": "90f7c04c-1fca-4639-b3f7-ba7b5db306e2", "fitness": -Infinity, "name": "Enhanced_QHS_Adaptive", "description": "Fine-tune dynamic mutation scale and harmony selection to enhance convergence speed and solution quality.", "code": "import numpy as np\n\nclass Enhanced_QHS_Adaptive:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 10)  # Harmony memory size\n        self.hmcr_init = 0.95  # Initial Harmony memory consideration rate\n        self.hmcr_final = 0.70  # Final Harmony memory consideration rate\n        self.par_init = 0.5  # Initial Pitch adjustment rate\n        self.par_final = 0.05  # Final Pitch adjustment rate\n        self.evaluations = 0\n        self.quantum_prob_init = 0.20  # Initial Quantum probability\n        self.quantum_prob_final = 0.05  # Final Quantum probability\n        self.learning_rate = 0.15\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        harmony_scores = np.array([func(harmony_memory[i]) for i in range(self.hms)])\n        self.evaluations += self.hms\n\n        best_harmony = harmony_memory[np.argmin(harmony_scores)]\n        best_score = np.min(harmony_scores)\n\n        while self.evaluations < self.budget:\n            progress = self.evaluations / self.budget\n            hmcr = self.hmcr_init - (self.hmcr_init - self.hmcr_final) * progress\n            par = self.par_init - (self.par_init - self.par_final) * progress\n            adaptive_quantum_prob = self.quantum_prob_init - (self.quantum_prob_init - self.quantum_prob_final) * progress\n\n            new_harmony = np.array([harmony_memory[np.random.choice(range(self.hms), p=harmony_scores / harmony_scores.sum()), j] \n                                    if np.random.rand() < hmcr else np.random.uniform(lb[j], ub[j]) \n                                    for j in range(self.dim)])\n\n            if np.random.rand() < par:\n                dynamic_step = np.power((1 - progress), 2.5) * np.abs(ub - lb) * 0.1\n                gradient_approximation = (ub - lb) / self.dim * (-1 + 2 * np.random.rand(self.dim))\n                new_harmony += self.learning_rate * gradient_approximation * dynamic_step\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            if np.random.rand() < adaptive_quantum_prob:\n                center = (np.mean(harmony_memory, axis=0) + best_harmony) / 2\n                delta = np.abs(best_harmony - harmony_memory[np.random.randint(self.hms)])\n                dynamic_delta_factor = np.sqrt(2) * (1 + progress * 0.5)\n                new_harmony = center + np.random.uniform(-1, 1, self.dim) * delta / dynamic_delta_factor\n                new_harmony = np.clip(new_harmony, lb, ub)\n\n            score = func(new_harmony)\n            self.evaluations += 1\n\n            if score < best_score:\n                best_score = score\n                best_harmony = new_harmony\n                self.learning_rate = max(0.05, self.learning_rate * 0.98)\n\n            worst_idx = np.argmax(harmony_scores)\n            if score < harmony_scores[worst_idx]:\n                harmony_memory[worst_idx], harmony_scores[worst_idx] = new_harmony, score\n\n        return best_harmony, best_score", "configspace": "", "generation": 99, "feedback": "An exception occurred: ValueError('probabilities are not non-negative').", "error": "ValueError('probabilities are not non-negative')", "parent_ids": ["190a0c33-8b65-41aa-b7e0-c01cfd7aef20"], "operator": null, "metadata": {}}
