{"id": "f9f9841b-670c-4c2d-92ac-d185e96fe465", "fitness": 0.0730099495345508, "name": "AdaptiveGradientSearch", "description": "Adaptive Gradient Search (AGS) combines gradient estimation with adaptive step sizes to efficiently explore the search space within a limited budget.", "code": "import numpy as np\n\nclass AdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n        best_x = x\n        best_value = func(x)\n        self.evaluations += 1\n\n        step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n        \n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n                \n            x_new = x - step_size * grad\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < best_value:\n                best_value = value\n                best_x = x_new\n                step_size *= 1.2  # Increase step size if improving\n            else:\n                step_size *= 0.5  # Reduce step size if not improving\n\n            x = x_new\n\n        return best_x", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07301 with standard deviation 0.04920.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.03513204542247328, 0.02788700335476435, 0.1453488657779305, 0.0334917874598567, 0.035989949790798725, 0.019632968990594968, 0.11198835182313382, 0.10617283284518475, 0.14144574034622004]}}
{"id": "eed5deb7-0a67-4d38-b66b-e6cc733c927f", "fitness": 0.07583316415332467, "name": "ImprovedAdaptiveGradientSearch", "description": "Improved Adaptive Gradient Search (IAGS) enhances exploration by incorporating stochastic perturbations and dynamic learning rates for better convergence under a limited evaluation budget.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n        best_x = x\n        best_value = func(x)\n        self.evaluations += 1\n\n        step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n        \n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n            \n            # Introduce stochastic perturbation to improve exploration\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < best_value:\n                best_value = value\n                best_x = x_new\n                step_size *= 1.2  # Increase step size if improving\n                perturbation_strength *= 0.8  # Decrease perturbation as solution improves\n            else:\n                step_size *= 0.5  # Reduce step size if not improving\n                perturbation_strength *= 1.1  # Increase perturbation to escape local optima\n\n            x = x_new\n\n        return best_x", "configspace": "", "generation": 1, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07583 with standard deviation 0.04440.", "error": "", "parent_ids": ["f9f9841b-670c-4c2d-92ac-d185e96fe465"], "operator": null, "metadata": {"aucs": [0.039503681150160896, 0.033196035521429024, 0.1399665620357745, 0.0358003318974911, 0.04231797343565291, 0.03255048855762299, 0.11877165648865651, 0.11264628156468026, 0.12774546672845377]}}
{"id": "89b8d7a3-7220-41a3-b6ca-aeeeb27d458c", "fitness": 0.06210059116676087, "name": "EnhancedStochasticGradientDescent", "description": "The Enhanced Stochastic Gradient Descent (ESGD) introduces adaptive momentum and oscillating gradient steps to bolster fast convergence and exploration under tight evaluation constraints.", "code": "import numpy as np\n\nclass EnhancedStochasticGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.momentum = np.zeros(dim)\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n        best_x = x\n        best_value = func(x)\n        self.evaluations += 1\n\n        step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n        \n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n            \n            self.momentum = 0.9 * self.momentum + 0.1 * grad\n            x_new = x - step_size * self.momentum\n            x_new += np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < best_value:\n                best_value = value\n                best_x = x_new\n                step_size *= 1.1  # Increase step size if improving\n                perturbation_strength *= 0.9  # Decrease perturbation as solution improves\n            else:\n                step_size *= 0.7  # Reduce step size if not improving\n                perturbation_strength *= 1.2  # Increase perturbation to escape local optima\n\n            x = x_new\n\n        return best_x", "configspace": "", "generation": 2, "feedback": "The algorithm EnhancedStochasticGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06210 with standard deviation 0.04679.", "error": "", "parent_ids": ["eed5deb7-0a67-4d38-b66b-e6cc733c927f"], "operator": null, "metadata": {"aucs": [0.007476347228593805, 0.008667982104265337, 0.0985663706853096, 0.03598274624747044, 0.027017306722837198, 0.027398677279433903, 0.118757377070001, 0.1232562668007029, 0.11178224636223366]}}
{"id": "aa624c80-660a-4e76-9c41-00e858535e92", "fitness": 0.05942968710313913, "name": "AdaptiveGradientPerturbationSearch", "description": "Adaptive Gradient Perturbation Search (AGPS) enhances solution accuracy by employing adaptive momentum, stochastic perturbations, and a dynamic adjustment of both step size and inertia for robust convergence within evaluation constraints.", "code": "import numpy as np\n\nclass AdaptiveGradientPerturbationSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n        best_x = x\n        best_value = func(x)\n        self.evaluations += 1\n\n        step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n        momentum = np.zeros_like(x)\n        inertia = 0.9\n        \n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            # Adaptive momentum with stochastic perturbation\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            momentum = inertia * momentum - step_size * grad + perturbation\n            x_new = x + momentum\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < best_value:\n                best_value = value\n                best_x = x_new\n                step_size *= 1.1  # Increase step size slightly if improving\n                perturbation_strength *= 0.9  # Reduce perturbation as solution improves\n                inertia *= 0.95  # Decrease inertia to stabilize around minima\n            else:\n                step_size *= 0.7  # Reduce step size more significantly if not improving\n                perturbation_strength *= 1.2  # Increase perturbation to escape local optima\n                inertia = min(1.0, inertia * 1.05)  # Increase inertia slightly if stuck\n\n            x = x_new\n\n        return best_x", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveGradientPerturbationSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05943 with standard deviation 0.03977.", "error": "", "parent_ids": ["eed5deb7-0a67-4d38-b66b-e6cc733c927f"], "operator": null, "metadata": {"aucs": [0.024943253651208197, 0.023815201678582865, 0.05778449552610909, 0.03214317455557758, 0.03616649867314037, 0.01976845380363723, 0.11694875155062401, 0.12036019975940782, 0.10293715472996501]}}
{"id": "7949ad54-498c-4527-9427-833723804809", "fitness": 0.06472994782176011, "name": "ImprovedAdaptiveGradientSearch", "description": "Enhanced Adaptive Gradient Search (EAGS) improves convergence using adaptive perturbation scaling for better exploration.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n        best_x = x\n        best_value = func(x)\n        self.evaluations += 1\n\n        step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n        \n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n            \n            # Introduce stochastic perturbation to improve exploration\n            adaptive_scaling = np.random.uniform(0.8, 1.2)  # New line for adaptive scaling\n            perturbation = adaptive_scaling * np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)  # Modified line\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < best_value:\n                best_value = value\n                best_x = x_new\n                step_size *= 1.2  # Increase step size if improving\n                perturbation_strength *= 0.8  # Decrease perturbation as solution improves\n            else:\n                step_size *= 0.5  # Reduce step size if not improving\n                perturbation_strength *= 1.1  # Increase perturbation to escape local optima\n\n            x = x_new\n\n        return best_x", "configspace": "", "generation": 4, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06473 with standard deviation 0.04333.", "error": "", "parent_ids": ["eed5deb7-0a67-4d38-b66b-e6cc733c927f"], "operator": null, "metadata": {"aucs": [0.03409106300245157, 0.02691394721088025, 0.045384727908284295, 0.033501473098868084, 0.0276874599709086, 0.038513870881392465, 0.13020826433717336, 0.1253590127552563, 0.1209097112306261]}}
{"id": "f9384133-96e9-4cb0-894e-aa0db66ad709", "fitness": 0.07274245230312539, "name": "ImprovedAdaptiveGradientSearch", "description": "Enhanced Adaptive Gradient Search (EAGS) boosts convergence by dynamically adjusting step size and perturbation strength based on current gradient magnitude.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n        best_x = x\n        best_value = func(x)\n        self.evaluations += 1\n\n        step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n        \n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n            \n            # Introduce stochastic perturbation to improve exploration\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < best_value:\n                best_value = value\n                best_x = x_new\n                step_size *= 1.2 + 0.05 * np.linalg.norm(grad)  # Increase step size dynamically\n                perturbation_strength *= 0.8  # Decrease perturbation as solution improves\n            else:\n                step_size *= 0.5  # Reduce step size if not improving\n                perturbation_strength *= 1.1  # Increase perturbation to escape local optima\n\n            x = x_new\n\n        return best_x", "configspace": "", "generation": 5, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07274 with standard deviation 0.04084.", "error": "", "parent_ids": ["eed5deb7-0a67-4d38-b66b-e6cc733c927f"], "operator": null, "metadata": {"aucs": [0.02309058943726916, 0.1312207506112436, 0.04318534727586354, 0.04375897057667022, 0.03814939890792168, 0.036129926381708755, 0.11401815901727919, 0.11246028620536397, 0.11266864231480833]}}
{"id": "23296522-8fcd-46e5-a86a-5565dddad083", "fitness": 0.07179388991619809, "name": "ImprovedAdaptiveGradientSearch", "description": "Improved Adaptive Gradient Search (IAGS) enhances exploration by incorporating stochastic perturbations and dynamic learning rates for better convergence under a limited evaluation budget, with an adaptive epsilon for gradient estimation.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n        best_x = x\n        best_value = func(x)\n        self.evaluations += 1\n\n        step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n        \n        while self.evaluations < self.budget:\n            # Change: Adapt epsilon based on the current step size\n            grad = self.estimate_gradient(func, x, epsilon=step_size.min() * 1e-2)\n            if self.evaluations >= self.budget:\n                break\n            \n            # Introduce stochastic perturbation to improve exploration\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < best_value:\n                best_value = value\n                best_x = x_new\n                step_size *= 1.2  # Increase step size if improving\n                perturbation_strength *= 0.8  # Decrease perturbation as solution improves\n            else:\n                step_size *= 0.5  # Reduce step size if not improving\n                perturbation_strength *= 1.1  # Increase perturbation to escape local optima\n\n            x = x_new\n\n        return best_x", "configspace": "", "generation": 6, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07179 with standard deviation 0.04361.", "error": "", "parent_ids": ["eed5deb7-0a67-4d38-b66b-e6cc733c927f"], "operator": null, "metadata": {"aucs": [0.039456633983582456, 0.03392460850642298, 0.13999671413451797, 0.037955624399299226, 0.029193513081295364, 0.02750986958969992, 0.10765451599860543, 0.11374934143925663, 0.11670418811310279]}}
{"id": "943106dd-9124-47ba-8550-8406815bd049", "fitness": 0.07436545056591584, "name": "ImprovedAdaptiveGradientSearch", "description": "Optimizes exploration by integrating adaptive perturbation scaling based on recent improvement metrics for better convergence.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n        best_x = x\n        best_value = func(x)\n        self.evaluations += 1\n\n        step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n        \n        improvement_counter = 0  # Track improvements for adaptive adjustments\n        \n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n            \n            # Introduce stochastic perturbation to improve exploration\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < best_value:\n                best_value = value\n                best_x = x_new\n                step_size *= 1.2  # Increase step size if improving\n                perturbation_strength *= 0.8  # Decrease perturbation as solution improves\n                improvement_counter += 1  # Increment improvement counter\n            else:\n                step_size *= 0.5  # Reduce step size if not improving\n                perturbation_strength *= 1.1  # Increase perturbation to escape local optima\n\n            # Adjust perturbation_strength based on the rate of improvement\n            if improvement_counter >= 5:\n                perturbation_strength *= 0.9  # Further decrease perturbation after consistent improvement\n                improvement_counter = 0  # Reset counter\n\n            x = x_new\n\n        return best_x", "configspace": "", "generation": 7, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07437 with standard deviation 0.04285.", "error": "", "parent_ids": ["eed5deb7-0a67-4d38-b66b-e6cc733c927f"], "operator": null, "metadata": {"aucs": [0.039560385005287246, 0.033196035521429024, 0.1400318432558474, 0.0358003318974911, 0.04231797343565291, 0.03255048855762299, 0.11877165648865651, 0.11264628156468026, 0.11441405936657512]}}
{"id": "1a2eaf26-6cce-4b15-9d11-07bdb1251d17", "fitness": 0.057518607810035115, "name": "EnhancedStochasticAdaptiveGradientSearch", "description": "Enhanced Stochastic Adaptive Gradient Search (ESAGS) incorporates momentum for smoother convergence and adaptive perturbation scaling for improved exploration and exploitation balance under limited evaluations.", "code": "import numpy as np\n\nclass EnhancedStochasticAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n        best_x = np.copy(x)\n        best_value = func(x)\n        self.evaluations += 1\n\n        step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n        momentum = np.zeros(self.dim)\n        beta = 0.9  # Momentum factor\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n            \n            # Momentum update\n            momentum = beta * momentum + grad\n            \n            # Stochastic perturbation to enhance exploration\n            perturbation = np.random.normal(0, perturbation_strength, size=self.dim)\n            x_new = x - step_size * momentum + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < best_value:\n                best_value = value\n                best_x = np.copy(x_new)\n                step_size *= 1.2  # Increase step size if improving\n                perturbation_strength *= 0.8  # Decrease perturbation strength as solution improves\n            else:\n                step_size *= 0.5  # Reduce step size if not improving\n                perturbation_strength *= 1.1  # Increase perturbation to escape local optima\n\n            x = x_new\n\n        return best_x", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedStochasticAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05752 with standard deviation 0.04277.", "error": "", "parent_ids": ["eed5deb7-0a67-4d38-b66b-e6cc733c927f"], "operator": null, "metadata": {"aucs": [0.007581682407436174, 0.037712187160521826, 0.026180500184895594, 0.03435166984161764, 0.029814885396946877, 0.0314724001716834, 0.12226990855821052, 0.11174947539146152, 0.11653476117754247]}}
{"id": "4d3b8106-b064-492d-a5a4-48de82116e77", "fitness": 0.06766390858645152, "name": "EnhancedDynamicPerturbationSearch", "description": "Enhanced Dynamic Perturbation Search (EDPS) introduces adaptive perturbation decoupled from step size adjustments and integrates a memory-based exploration to balance exploitation and exploration for efficient convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicPerturbationSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n        best_x = x\n        best_value = func(x)\n        self.evaluations += 1\n\n        step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_memory = np.zeros(self.dim)\n        \n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n            \n            # Adaptive perturbation decoupled from step size\n            perturbation_memory = 0.9 * perturbation_memory + np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation_memory\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < best_value:\n                best_value = value\n                best_x = x_new\n                step_size *= 1.1  # Slightly increase step size if improving\n                perturbation_strength *= 0.9  # Slightly decrease perturbation strength\n            else:\n                step_size *= 0.9  # Slightly reduce step size if not improving\n                perturbation_strength *= 1.1  # Increase perturbation to escape local optima\n\n            x = x_new\n\n        return best_x", "configspace": "", "generation": 9, "feedback": "The algorithm EnhancedDynamicPerturbationSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06766 with standard deviation 0.03816.", "error": "", "parent_ids": ["eed5deb7-0a67-4d38-b66b-e6cc733c927f"], "operator": null, "metadata": {"aucs": [0.025539155881966757, 0.0588533857197443, 0.06202174709100361, 0.03892125379793643, 0.0391558278416031, 0.028457297290956474, 0.1151179943607149, 0.10995974730205726, 0.13094876799208088]}}
{"id": "23f0db21-9e15-43aa-883e-f39da0edffae", "fitness": 0.0630490615374856, "name": "EnhancedStochasticGradientBasedExploration", "description": "Enhanced Stochastic Gradient-Based Exploration (ESGBE) improves exploration and convergence by integrating adaptive momentum and dynamic perturbation strategies under budget constraints.", "code": "import numpy as np\n\nclass EnhancedStochasticGradientBasedExploration:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            if self.evaluations >= self.budget:\n                break\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n        best_x = x\n        best_value = func(x)\n        self.evaluations += 1\n\n        step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n        \n        momentum = np.zeros(self.dim)\n        beta = 0.9  # Momentum coefficient\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            # Update momentum\n            momentum = beta * momentum + (1 - beta) * grad\n            # Introduce stochastic perturbation with adaptive adjustment\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * momentum + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < best_value:\n                best_value = value\n                best_x = x_new\n                step_size *= 1.1  # Slightly increase step size if improving\n                perturbation_strength *= 0.9  # Reduce perturbation as solution improves\n            else:\n                step_size *= 0.7  # Reduce step size if not improving\n                perturbation_strength *= 1.2  # Increase perturbation to escape local optima\n\n            x = x_new\n\n        return best_x", "configspace": "", "generation": 10, "feedback": "The algorithm EnhancedStochasticGradientBasedExploration got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06305 with standard deviation 0.04720.", "error": "", "parent_ids": ["eed5deb7-0a67-4d38-b66b-e6cc733c927f"], "operator": null, "metadata": {"aucs": [0.007476347228593805, 0.008667982104265337, 0.09856637135874202, 0.037953453094937584, 0.02462784655503214, 0.0318869878388367, 0.127968158345508, 0.11851191720123, 0.11178249011022479]}}
{"id": "8206a3e9-c351-4f77-9471-6a052d6ec6b7", "fitness": 0.08563056147181251, "name": "ImprovedAdaptiveGradientSearch", "description": "Enhanced IAGS by initializing with multiple random starts to avoid poor local optima and improve convergence stability.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 11, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08563 with standard deviation 0.04513.", "error": "", "parent_ids": ["eed5deb7-0a67-4d38-b66b-e6cc733c927f"], "operator": null, "metadata": {"aucs": [0.05314818001493948, 0.1306105869517894, 0.1436665924077153, 0.03199413588763056, 0.034935247539933156, 0.027236867289749722, 0.1116255207637813, 0.12765853158647367, 0.10979939080430001]}}
{"id": "2dc7ffe3-d478-4221-a7e8-7c99995a888a", "fitness": 0.08730103654728516, "name": "ImprovedAdaptiveGradientSearch", "description": "Fine-tuned adaptive parameters for improved balance between exploration and exploitation.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 12, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08730 with standard deviation 0.04803.", "error": "", "parent_ids": ["8206a3e9-c351-4f77-9471-6a052d6ec6b7"], "operator": null, "metadata": {"aucs": [0.05341084946300023, 0.1331085279006997, 0.146778447845746, 0.03199413588763056, 0.030472432981769604, 0.023977634361470157, 0.11649418335023376, 0.1284719276569123, 0.12100118947810412]}}
{"id": "d3fc804f-dd12-4656-baa7-4d7128e53a49", "fitness": 0.08285091540017198, "name": "ImprovedAdaptiveGradientSearch", "description": "Enhanced adaptive gradient perturbation strategy for improved local search precision.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.normal(0, perturbation_strength, size=self.dim)  # Changed from uniform to normal distribution\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 13, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08285 with standard deviation 0.04567.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.0508889846218985, 0.12967005056459624, 0.14090895894907318, 0.03248936371670075, 0.02988390502702254, 0.02006616390129179, 0.11263078024179318, 0.11643972620699328, 0.11268030537217832]}}
{"id": "bcd7d891-9587-4558-a262-279e9b855aac", "fitness": 0.06055245125012016, "name": "MomentumAdaptiveGradientSearch", "description": "Utilize adaptive momentum and stochastic perturbations for enhanced convergence and stability in high-dimensional spaces.", "code": "import numpy as np\n\nclass MomentumAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        momentum = np.zeros(self.dim)\n        beta_momentum = 0.9  # Momentum decay rate\n\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])  # Initial step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n\n                # Update momentum\n                momentum = beta_momentum * momentum - step_size * grad\n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n\n                x_new = x + momentum + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.1\n                    perturbation_strength *= 0.9\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 14, "feedback": "The algorithm MomentumAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06055 with standard deviation 0.03446.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.022181892184213625, 0.06465413848673096, 0.05942709046992933, 0.03199413588763056, 0.04064153202717524, 0.015791435277202726, 0.0898686639673878, 0.11996816713403746, 0.10044500581677374]}}
{"id": "06b193be-58d2-45c5-a007-fdd9067bcda9", "fitness": 0.08730103654728516, "name": "ImprovedAdaptiveGradientSearch", "description": "Enhanced exploration by adjusting random start count based on dimension.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = max(5, self.dim // 2)  # Adjusting number of random starts based on dimension\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 15, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08730 with standard deviation 0.04803.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.05341084946300023, 0.1331085279006997, 0.146778447845746, 0.03199413588763056, 0.030472432981769604, 0.023977634361470157, 0.11649418335023376, 0.1284719276569123, 0.12100118947810412]}}
{"id": "c5793256-c5ce-48cf-8471-576f2ff3792d", "fitness": 0.0835305897292774, "name": "ImprovedAdaptiveGradientSearch", "description": "Refined strategy adjusts perturbation scaling for enhanced convergence stability.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.75  # Slight change to perturbation scaling\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 16, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08353 with standard deviation 0.04555.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.05342957846358731, 0.1359446797994992, 0.14741381798661102, 0.03199413588763056, 0.027937758988685335, 0.02709265795836313, 0.11072739138667143, 0.11023856026229206, 0.1069967268301566]}}
{"id": "864f71a3-7578-40cf-a211-4911ac89740d", "fitness": 0.08730103654728516, "name": "ImprovedAdaptiveGradientSearch", "description": "Introduced dynamic adjustment of number of starts based on early convergence.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = min(5, max(1, self.budget//self.dim//10))  # Dynamic adjustment of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 17, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08730 with standard deviation 0.04803.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.05341084946300023, 0.1331085279006997, 0.146778447845746, 0.03199413588763056, 0.030472432981769604, 0.023977634361470157, 0.11649418335023376, 0.1284719276569123, 0.12100118947810412]}}
{"id": "09be605d-bfcf-4531-a783-9fd6e2d6cb4c", "fitness": 0.060144202373212274, "name": "MomentumAdaptiveGradientSearch", "description": "Incorporating an adaptive learning rate with momentum to enhance convergence speed and stability in gradient-based optimization.", "code": "import numpy as np\n\nclass MomentumAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            velocity = np.zeros_like(x)\n            beta = 0.9  # Momentum factor\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n\n                velocity = beta * velocity - step_size * grad\n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x + velocity + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 18, "feedback": "The algorithm MomentumAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06014 with standard deviation 0.03734.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.022770998873579695, 0.043474120822962314, 0.07937721753851645, 0.035811061930938415, 0.027510858750335432, 0.014785342279607039, 0.09199398566918027, 0.12361282554396813, 0.10196140994982272]}}
{"id": "f59aa3f1-8ffe-41d4-9fa9-dcb3b7aac9d1", "fitness": 0.08526190123025587, "name": "ImprovedAdaptiveGradientSearch", "description": "Introducing adaptive perturbation control to enhance search efficiency and convergence.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.05  # Less aggressive increase\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 19, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08526 with standard deviation 0.04412.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.05346942336369154, 0.1332761551102576, 0.14694341491409835, 0.03921832790527369, 0.029159050845432355, 0.030267857355892103, 0.11106748051143867, 0.11420136566698824, 0.10975403539923034]}}
{"id": "34233ca9-93a3-4b11-acf3-55dbcc660082", "fitness": 0.0832012150136975, "name": "ImprovedAdaptiveGradientSearch", "description": "Enhanced gradient estimation by employing a more accurate central difference method.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step_forward = np.copy(x)\n            x_step_backward = np.copy(x)\n            x_step_forward[i] += epsilon\n            x_step_backward[i] -= epsilon\n            grad[i] = (func(x_step_forward) - func(x_step_backward)) / (2 * epsilon)\n            self.evaluations += 2  # Incremented twice due to two function evaluations\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 20, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08320 with standard deviation 0.04532.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.05341085430469972, 0.13310842091433406, 0.14677845220036045, 0.03199413684118113, 0.028640963511920692, 0.025680215514910842, 0.1106566628642972, 0.11386183629034008, 0.10467939268123327]}}
{"id": "05dddc0e-a276-4134-afa7-cfb94925be2f", "fitness": 0.08730103654728516, "name": "EnhancedGradientSearch", "description": "Enhanced gradient search integrating dynamic step adjustments and elite strategy for robust exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient_estimation = (func(x_step) - func(x)) / epsilon\n            grad[i] = gradient_estimation\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        elite_fraction = 0.2  # Fraction of best solutions to consider as elites\n        elite_solutions = []\n\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n                # Record elite solutions\n                elite_solutions.append((current_value, np.copy(x)))\n                elite_solutions.sort(key=lambda sol: sol[0])\n                elite_solutions = elite_solutions[:int(elite_fraction * len(elite_solutions)) + 1]\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        # Explore from elite solutions\n        for _, elite_x in elite_solutions:\n            if self.evaluations >= self.budget:\n                break\n\n            x = elite_x\n            current_value = func(x)\n            self.evaluations += 1\n\n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 21, "feedback": "The algorithm EnhancedGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08730 with standard deviation 0.04803.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.05341084946300023, 0.1331085279006997, 0.146778447845746, 0.03199413588763056, 0.030472432981769604, 0.023977634361470157, 0.11649418335023376, 0.1284719276569123, 0.12100118947810412]}}
{"id": "58355d62-1e2e-4183-a526-b8a1cda8f0b5", "fitness": 0.07989725358965456, "name": "EnhancedAdaptiveGradientSearch", "description": "Adaptive Gradient Search with Annealed Perturbation and Enhanced Step Adjustment for effective exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])  # Initial step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            decay_rate = 0.99  # Decay rate for annealing perturbation\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.1  # Less aggressive increase\n                    perturbation_strength *= decay_rate  # Annealing strategy\n                else:\n                    step_size *= 0.6  # More aggressive reduction\n                    perturbation_strength /= decay_rate\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 22, "feedback": "The algorithm EnhancedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07990 with standard deviation 0.03910.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.047041933792217216, 0.1168476170390289, 0.12616540496625206, 0.03767630840937175, 0.02423345979765512, 0.03906581101986717, 0.10732234839510302, 0.1134934171682832, 0.10722898171911266]}}
{"id": "a7ca67e1-3f9e-484f-8394-01dd2da42a2e", "fitness": 0.08497699471316819, "name": "ImprovedAdaptiveGradientSearch", "description": "Enhanced perturbation strategy for better exploration-exploitation balance.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])  # Enhanced perturbation strength\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 23, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08498 with standard deviation 0.04587.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.05223260446138378, 0.13759000971982116, 0.14314613623955597, 0.03565902428409484, 0.022757130549713955, 0.03255815579584309, 0.10247143425014305, 0.12192020403545445, 0.11645825308250346]}}
{"id": "7cd6ab14-e2d9-425e-b671-6725087aecb0", "fitness": 0.06138892464015899, "name": "AdaptiveGradientWithMomentum", "description": "Adaptive Gradient with Momentum and Restart: Enhancing gradient-based search using momentum for faster convergence and adaptive multi-start for robustness.", "code": "import numpy as np\n\nclass AdaptiveGradientWithMomentum:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 7  # Increased number of random starts for better robustness\n        momentum_factor = 0.9  # Momentum factor for smoother updates\n        \n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n            \n            step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])  # Improved step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            velocity = np.zeros_like(x)  # Initialize momentum\n            \n            restart_threshold = 50  # Restart if no improvement after these many evaluations\n            evaluations_since_best = 0\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n\n                velocity = momentum_factor * velocity - step_size * grad\n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x + velocity + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n                evaluations_since_best += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.1\n                    perturbation_strength *= 0.9\n                    evaluations_since_best = 0\n                else:\n                    step_size *= 0.7\n                    perturbation_strength *= 1.05\n                \n                if evaluations_since_best > restart_threshold:\n                    break\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 24, "feedback": "The algorithm AdaptiveGradientWithMomentum got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06139 with standard deviation 0.03808.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.021853180878602663, 0.07676891783059359, 0.05602843502336452, 0.03199413588763056, 0.02749991953195441, 0.014826642411320923, 0.08900405197595695, 0.12159045826091297, 0.11293457996109435]}}
{"id": "e1668e71-d9ca-4e31-a30d-0c2d466cc58f", "fitness": 0.08730103654728516, "name": "ImprovedAdaptiveGradientSearch", "description": "Improved initialization strategy to enhance exploration efficiency.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            # Change: Use Sobol sequence for better initial exploration\n            sobol = np.random.uniform(0, 1, size=self.dim)\n            x = bounds[:, 0] + sobol * (bounds[:, 1] - bounds[:, 0])\n            \n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 25, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08730 with standard deviation 0.04803.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.05341084946300023, 0.1331085279006997, 0.146778447845746, 0.03199413588763056, 0.030472432981769604, 0.023977634361470157, 0.11649418335023376, 0.1284719276569123, 0.12100118947810412]}}
{"id": "244c4ffd-9382-44d2-96b6-0ae923d64200", "fitness": 0.06788543823312128, "name": "DynamicPerturbationGradientSearch", "description": "Dynamic Perturbation Gradient Search (DPGS) with adaptive exploration parameters to enhance convergence and solution quality.", "code": "import numpy as np\n\nclass DynamicPerturbationGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = max(3, self.budget // (self.dim * 10))  # Dynamic number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])  # Initial step size\n            perturbation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.1\n                    perturbation_strength *= 0.9\n                else:\n                    step_size *= 0.7\n                    perturbation_strength *= 1.2\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 26, "feedback": "The algorithm DynamicPerturbationGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06789 with standard deviation 0.03955.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.023760549178846846, 0.07532814353889461, 0.07608230824298201, 0.033250582473597246, 0.02320641445189142, 0.029503714995479546, 0.10663074206321821, 0.1273759991653507, 0.11583048998783096]}}
{"id": "02c6beab-7d79-477f-a8c7-e5831267afd0", "fitness": 0.06414900252895907, "name": "EnhancedAdaptiveGradientSearch", "description": "Enhanced exploration-exploitation trade-off using dynamic step size adaptation and multi-scale perturbations.", "code": "import numpy as np\n\nclass EnhancedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 7  # Increased number of random starts for better coverage\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])  # Dynamic initial step size\n            perturbation_strength = 0.02 * (bounds[:, 1] - bounds[:, 0])\n            decay_factor = 0.99  # Step size decay for refined exploration\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.normal(0, perturbation_strength, size=self.dim)  # Normal distribution for varied scale\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.1\n                    perturbation_strength *= 0.95\n                else:\n                    step_size *= decay_factor\n                    perturbation_strength *= 1.05\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 27, "feedback": "The algorithm EnhancedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06415 with standard deviation 0.03746.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.037070059591835736, 0.08278157984182277, 0.08899531438046226, 0.03199413588763056, 0.017196276405675226, 0.013364648128098344, 0.0797219562975735, 0.11922032539737659, 0.1069967268301566]}}
{"id": "ff5cbe50-91ed-4737-b806-27e45b42e5a3", "fitness": 0.07778079905850994, "name": "StochasticGradientPerturbation", "description": "Enhanced Stochastic Gradient-Perturbation strategy with adaptive learning rates and diversified restarts for robust exploration-exploitation balance.", "code": "import numpy as np\n\nclass StochasticGradientPerturbation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        perturbations = np.random.normal(0, epsilon, size=x.shape)  # Using Gaussian noise for more diverse gradient estimation\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += perturbations[i]\n            grad[i] = (func(x_step) - func(x)) / perturbations[i]\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 10  # Increased number of random restarts for better coverage\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])  # Further adjusted step size for finer sensitivity\n            perturbation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.1  # More cautious increase in step size\n                    perturbation_strength *= 0.9  # More cautious decrease in perturbation\n                else:\n                    step_size *= 0.6  # Reduced decrease in step size for more adaptive learning\n                    perturbation_strength *= 1.05  # Reduced increase to maintain exploration potential\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 28, "feedback": "The algorithm StochasticGradientPerturbation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07778 with standard deviation 0.04179.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.051531476789381414, 0.08393988393859708, 0.1255744629837191, 0.032083892423342975, 0.02837602984646148, 0.022143194745074535, 0.11585171340692013, 0.12387236291610393, 0.11665417447698889]}}
{"id": "86b2c26d-e547-43d4-9062-5baa158d7b49", "fitness": 0.07876945641717178, "name": "ImprovedAdaptiveGradientSearch", "description": "Enhanced exploration by implementing dynamic starting points and adaptive perturbation strategies.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        dynamic_starts = max(5, self.budget // (self.dim * 20))  # Adjusted starting points\n        for _ in range(dynamic_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n            perturbation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])  # Increased initial perturbation\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.9  # Adjusted decay factor\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 29, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07877 with standard deviation 0.04136.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.026442089392154644, 0.11352181485271362, 0.11733868756081722, 0.04808226882767086, 0.01962431505318185, 0.03972893631624019, 0.10674054126271437, 0.12192020403545445, 0.11552625045359888]}}
{"id": "0cc701af-e1d4-406c-8e0c-3a09f1fb9468", "fitness": 0.05377502139514359, "name": "EnhancedAdaptiveGradientSearch", "description": "Enhanced Adaptive Gradient Search with dynamic exploitation-exploration adjustment and memory-based perturbation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.memory = []\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def dynamic_adjustment(self, success_rate):\n        base_factor = 0.1\n        return base_factor * (1 + success_rate)\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n\n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n            success_count = 0\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n\n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    success_count += 1\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n                success_rate = success_count / (len(self.memory) + 1)\n                step_size *= self.dynamic_adjustment(success_rate)\n\n                self.memory.append((x, current_value))\n                if len(self.memory) > 10:  # Keep limited memory size\n                    self.memory.pop(0)\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 30, "feedback": "The algorithm EnhancedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05378 with standard deviation 0.04220.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.007282717650568427, 0.00772644400068101, 0.0274351187045605, 0.0369283463123965, 0.03330606796094182, 0.037394815733297615, 0.12133393170313811, 0.10617283284518475, 0.10639491764552356]}}
{"id": "7ab8f851-0c76-40b3-af50-eb5291c7e1a8", "fitness": 0.08730103654728516, "name": "ImprovedAdaptiveGradientSearch", "description": "Slightly increase the initial number of random starts to potentially explore more regions of the search space.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 6  # Increased number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 31, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08730 with standard deviation 0.04803.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.05341084946300023, 0.1331085279006997, 0.146778447845746, 0.03199413588763056, 0.030472432981769604, 0.023977634361470157, 0.11649418335023376, 0.1284719276569123, 0.12100118947810412]}}
{"id": "433d2d13-b437-4312-8a9a-30f17642328a", "fitness": 0.05761564767791684, "name": "EnhancedMomentumGradientSearch", "description": "Dynamic exploration via a learning rate schedule and adaptive momentum for enhanced gradient-based optimization.", "code": "import numpy as np\n\nclass EnhancedMomentumGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])  # Initial step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            momentum = np.zeros(self.dim)\n            learning_rate_decay = 0.95\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n\n                momentum = 0.9 * momentum - step_size * grad\n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x + momentum + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.1\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= learning_rate_decay\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedMomentumGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05762 with standard deviation 0.03599.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.019409441397711724, 0.06510044966915896, 0.05568170223442359, 0.03199413588763056, 0.027753665595359966, 0.013364648128098344, 0.08184271492582029, 0.12159045826091297, 0.10180361300213514]}}
{"id": "4f68a407-f0b0-4ddc-bbc0-c4775b6652b3", "fitness": 0.07408381350444715, "name": "ImprovedAdaptiveMomentumGradientSearch", "description": "Introduce adaptive momentum to the gradient search for better convergence speed and stability.", "code": "import numpy as np\n\nclass ImprovedAdaptiveMomentumGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            momentum = np.zeros_like(x)\n            momentum_coefficient = 0.9  # Initial momentum coefficient\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                momentum = momentum_coefficient * momentum + grad\n                x_new = x - step_size * momentum + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 33, "feedback": "The algorithm ImprovedAdaptiveMomentumGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07408 with standard deviation 0.03773.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.052850478273502866, 0.10037521940978511, 0.10279587260925305, 0.03199413588763056, 0.028560596095693835, 0.020531310330398678, 0.09795442469234261, 0.10970278290970259, 0.12198950133171504]}}
{"id": "30bea8d8-0a89-43ec-b881-a734f37376e1", "fitness": 0.08494466477988206, "name": "ImprovedAdaptiveGradientSearch", "description": "Enhance exploration by adjusting the perturbation strength dynamically based on evaluation progress.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= max(0.5, 0.5 * (1 - self.evaluations / self.budget))\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 34, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08494 with standard deviation 0.04240.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.053872002721834455, 0.13699066387952752, 0.1469125073138836, 0.03199413588763056, 0.033911003656862726, 0.04203217724365804, 0.10041212824476842, 0.11023856026229206, 0.10813880380848118]}}
{"id": "8152331a-5cc8-409c-a5b3-818b8827e78b", "fitness": 0.08730103654728516, "name": "ImprovedAdaptiveGradientSearch", "description": "Slightly improve exploration by increasing the number of random starts.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 6  # Increased number of random starts from 5 to 6\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 35, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08730 with standard deviation 0.04803.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.05341084946300023, 0.1331085279006997, 0.146778447845746, 0.03199413588763056, 0.030472432981769604, 0.023977634361470157, 0.11649418335023376, 0.1284719276569123, 0.12100118947810412]}}
{"id": "d08ee83a-ab39-4119-92eb-015f1aaa7920", "fitness": 0.08730103654728516, "name": "ImprovedAdaptiveGradientSearch", "description": "Slightly increased number of random starts to enhance exploration in the search space.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 6  # Number of random starts increased by 1\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 36, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08730 with standard deviation 0.04803.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.05341084946300023, 0.1331085279006997, 0.146778447845746, 0.03199413588763056, 0.030472432981769604, 0.023977634361470157, 0.11649418335023376, 0.1284719276569123, 0.12100118947810412]}}
{"id": "af550be5-5ba2-429a-9582-e29365524baf", "fitness": 0.06524884496386756, "name": "EnhancedMomentumGradientSearch", "description": "Integrating adaptive perturbation and momentum-enhanced gradients for robust convergence in diverse landscapes.", "code": "import numpy as np\n\nclass EnhancedMomentumGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.momentum = np.zeros(dim)\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 10  # Increased number of random starts for better coverage\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])  # Adjusted initial step size for finer control\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            gamma = 0.9  # Momentum factor\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n\n                self.momentum = gamma * self.momentum + grad\n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * self.momentum + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.1\n                    perturbation_strength *= 0.9\n                else:\n                    step_size *= 0.7\n                    perturbation_strength *= 1.2\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 37, "feedback": "The algorithm EnhancedMomentumGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06525 with standard deviation 0.03628.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.03234576446796256, 0.06828536347536407, 0.0639173090999805, 0.03261976441881753, 0.02771944908822699, 0.02552816647201195, 0.11301697900224916, 0.10625804889835067, 0.11754875975184453]}}
{"id": "ba37d876-1b1c-4613-aa7c-821db85ecab1", "fitness": 0.08465007423085909, "name": "HybridGradientSearch", "description": "Hybrid Gradient Search with Dynamic Perturbation - Combines adaptive gradient descent with dynamically adjusted perturbation to enhance search efficiency.", "code": "import numpy as np\n\nclass HybridGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-6):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            if self.evaluations >= self.budget:\n                break\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 10  # Increased number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            perturbation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                adaptive_perturbation = perturbation_strength * np.exp(-0.5 * np.linalg.norm(grad))\n                perturbation = np.random.uniform(-adaptive_perturbation, adaptive_perturbation, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.1\n                    perturbation_strength *= 0.9\n                else:\n                    step_size *= 0.7\n                    perturbation_strength *= 1.05\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 38, "feedback": "The algorithm HybridGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08465 with standard deviation 0.04438.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.04937167073128823, 0.12891739235331245, 0.13457202733306028, 0.03846461371591481, 0.02619757014151658, 0.029712951776275975, 0.1243964075309979, 0.11885313185927204, 0.11136490263609355]}}
{"id": "65b27938-fd2b-44e8-8e80-76d20004a48c", "fitness": 0.08285091540017198, "name": "ImprovedAdaptiveGradientSearch", "description": "Enhanced perturbation model for better convergence control.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.standard_normal(self.dim) * perturbation_strength  # Change made here\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 39, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08285 with standard deviation 0.04567.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.0508889846218985, 0.12967005056459624, 0.14090895894907318, 0.03248936371670075, 0.02988390502702254, 0.02006616390129179, 0.11263078024179318, 0.11643972620699328, 0.11268030537217832]}}
{"id": "db01a080-e415-4aab-8afa-f6a8d622da4c", "fitness": 0.08503706270421513, "name": "ImprovedAdaptiveGradientSearch", "description": "Optimizes perturbation adaptation to enhance convergence efficiency and precision.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.7  # Modified perturbation adjustment factor\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 40, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08504 with standard deviation 0.04464.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.05343707087488714, 0.1362084142998491, 0.14784678386016725, 0.03199413588763056, 0.03245047851113436, 0.03180485448606629, 0.10847861763725541, 0.11025623581600563, 0.11285697296494046]}}
{"id": "d78abf46-2827-47b4-b8eb-93f42b032be6", "fitness": 0.08730103654728516, "name": "ImprovedAdaptiveGradientSearch", "description": "Enhance the algorithm by dynamically adjusting the number of random starts based on remaining budget.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = max(1, self.budget // (10 * self.dim))  # Dynamically adjust number of starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 41, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08730 with standard deviation 0.04803.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.05341084946300023, 0.1331085279006997, 0.146778447845746, 0.03199413588763056, 0.030472432981769604, 0.023977634361470157, 0.11649418335023376, 0.1284719276569123, 0.12100118947810412]}}
{"id": "30a86d02-3d4e-4e3d-9334-60f203f83be3", "fitness": 0.08730103654728516, "name": "ImprovedAdaptiveGradientSearch", "description": "Introduce dynamic multi-start strategy to enhance exploration of the search space.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = max(1, int(5 * (self.budget - self.evaluations) / self.budget))  # Dynamic number of starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 42, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08730 with standard deviation 0.04803.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.05341084946300023, 0.1331085279006997, 0.146778447845746, 0.03199413588763056, 0.030472432981769604, 0.023977634361470157, 0.11649418335023376, 0.1284719276569123, 0.12100118947810412]}}
{"id": "06f80bbf-c176-43a8-bc03-72f4bd87f565", "fitness": 0.060144202373212274, "name": "ImprovedAdaptiveGradientSearch", "description": "Incorporate momentum to improve the gradient-based search for efficient convergence.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            momentum = np.zeros_like(x)  # Add momentum for improved convergence\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                momentum = 0.9 * momentum - step_size * grad  # Update momentum\n                x_new = x + momentum + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 43, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06014 with standard deviation 0.03734.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.022770998873579695, 0.043474120822962314, 0.07937721753851645, 0.035811061930938415, 0.027510858750335432, 0.014785342279607039, 0.09199398566918027, 0.12361282554396813, 0.10196140994982272]}}
{"id": "9d267d62-466f-4de7-9de6-bf03786973e6", "fitness": 0.08565560404396885, "name": "ImprovedAdaptiveGradientSearch", "description": "Enhanced adaptive gradient scaling for improved optimization precision.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.1  # Change from 1.2 to 1.1 for finer step adaptation\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 44, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08566 with standard deviation 0.04630.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.05618530180783965, 0.14074932992939115, 0.14285790682042554, 0.03199413588763056, 0.03182993759527808, 0.024045769150579654, 0.1094591274957939, 0.11298475027402644, 0.12079417743475473]}}
{"id": "9f2b3e5c-0905-46cb-aefd-be5b5b31bcef", "fitness": 0.08730103654728516, "name": "ImprovedAdaptiveGradientSearch", "description": "Introduced dynamic adjustment of the number of random starts based on budget usage to enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = max(1, self.budget // (10 * self.dim))  # Dynamic adjustment of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 45, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08730 with standard deviation 0.04803.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.05341084946300023, 0.1331085279006997, 0.146778447845746, 0.03199413588763056, 0.030472432981769604, 0.023977634361470157, 0.11649418335023376, 0.1284719276569123, 0.12100118947810412]}}
{"id": "7c2929c8-e1a0-4142-a0a5-ced3a94cdd95", "fitness": 0.05884153794665987, "name": "EnhancedStochasticOscillationSearch", "description": "Enhanced stochastic oscillation method for dynamic exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedStochasticOscillationSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 8  # Increased number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.normal(0, perturbation_strength, size=self.dim)\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.1\n                    perturbation_strength *= 0.9\n                else:\n                    step_size *= 0.7\n                    perturbation_strength *= 1.2\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 46, "feedback": "The algorithm EnhancedStochasticOscillationSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05884 with standard deviation 0.04145.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.007847092217847185, 0.03586711550492405, 0.05774552582239867, 0.03199413588763056, 0.025502879030943504, 0.027837481047168322, 0.10307216531343977, 0.11145543054412343, 0.1282520161514633]}}
{"id": "ef02db6b-58ab-4754-ad8c-fa05aca320b7", "fitness": 0.0807030259830338, "name": "EnhancedAdaptiveGradientSearch", "description": "Enhanced stochastic perturbation and dynamic learning rate adaptation to improve convergence speed and robustness.", "code": "import numpy as np\n\nclass EnhancedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 10  # Increased number of random starts for better coverage\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.1 * np.mean(bounds[:, 1] - bounds[:, 0])  # Dynamic step size\n            perturbation_strength = 0.05 * np.mean(bounds[:, 1] - bounds[:, 0])\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.normal(0, perturbation_strength, size=self.dim)  # Gaussian perturbation\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.1  # Slightly increased adaptation for learning rate\n                    perturbation_strength *= 0.9\n                else:\n                    step_size *= 0.7  # Less aggressive reduction\n                    perturbation_strength *= 1.05\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 47, "feedback": "The algorithm EnhancedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08070 with standard deviation 0.04327.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.045108828100810294, 0.12117370636288416, 0.12678870099694517, 0.03542059935674202, 0.02584556389408632, 0.02530904173238513, 0.10969879493522183, 0.11793271210107503, 0.11904928636715428]}}
{"id": "42ad9439-6fb8-471d-8b0f-8a6ae9caa08f", "fitness": 0.060144202373212274, "name": "ImprovedAdaptiveGradientSearch", "description": "Introduce momentum to the step size for smoother convergence and better performance.", "code": "import numpy as np\n\nclass ImprovedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n        \n        num_starts = 5  # Number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n            \n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            momentum = np.zeros_like(x)  # Initialize momentum\n            \n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n                \n                perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n                momentum = 0.9 * momentum - step_size * grad  # Update momentum\n                x_new = x + momentum + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size *= 1.2\n                    perturbation_strength *= 0.8\n                else:\n                    step_size *= 0.5\n                    perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 48, "feedback": "The algorithm ImprovedAdaptiveGradientSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06014 with standard deviation 0.03734.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.022770998873579695, 0.043474120822962314, 0.07937721753851645, 0.035811061930938415, 0.027510858750335432, 0.014785342279607039, 0.09199398566918027, 0.12361282554396813, 0.10196140994982272]}}
{"id": "2d7f0e0e-0aa1-405c-8af6-60cb2b525f0b", "fitness": -Infinity, "name": "EnhancedAdaptiveGradientSearch", "description": "Adaptive Gradient Perturbation with Dynamic Restart Strategy to Enhance Convergence in Black Box Optimization.", "code": "import numpy as np\n\nclass EnhancedAdaptiveGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        num_starts = max(5, int(self.budget / (10 * self.dim)))  # Dynamic number of random starts\n        for _ in range(num_starts):\n            if self.evaluations >= self.budget:\n                break\n\n            x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n            current_value = func(x)\n            self.evaluations += 1\n\n            step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])  # Fine-tuned step size\n            perturbation_strength = 0.02 * (bounds[:, 1] - bounds[:, 0])  # Reduced perturbation strength\n\n            while self.evaluations < self.budget:\n                grad = self.estimate_gradient(func, x)\n                if self.evaluations >= self.budget:\n                    break\n\n                perturbation = np.random.randn(self.dim) * perturbation_strength  # Gaussian perturbation\n                x_new = x - step_size * grad + perturbation\n                x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n                value = func(x_new)\n                self.evaluations += 1\n\n                if value < current_value:\n                    current_value = value\n                    x = x_new\n                    step_size = min(step_size * 1.1, bounds[:, 1] - bounds[:, 0])\n                    perturbation_strength = max(perturbation_strength * 0.9, 1e-9)\n                else:\n                    step_size *= 0.9\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 49, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {}}
{"id": "007a0e8a-5202-486f-aa5c-1f51938c042f", "fitness": 0.09255221398838484, "name": "HybridAdaptiveGradientDE", "description": "Hybrid Adaptive Gradient and Differential Evolution for enhanced global exploration and local refinement.", "code": "import numpy as np\n\nclass HybridAdaptiveGradientDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F=0.8, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        # Use Differential Evolution for global exploration\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        # Refine using gradient descent\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size *= 1.2\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 50, "feedback": "The algorithm HybridAdaptiveGradientDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09255 with standard deviation 0.04077.", "error": "", "parent_ids": ["2dc7ffe3-d478-4221-a7e8-7c99995a888a"], "operator": null, "metadata": {"aucs": [0.11358419497002137, 0.04807797844804351, 0.062973803020404, 0.05640872059063229, 0.06258877918341921, 0.05594438632358445, 0.14684634770638794, 0.13953430285765345, 0.14701141279531738]}}
{"id": "d5833a93-2fad-4898-af9a-dad3cc350318", "fitness": 0.08164703513524064, "name": "EnhancedHybridGradientDE", "description": "Enhanced Hybrid Gradient DE with Adaptive Mutation and Dynamic Perturbation to balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridGradientDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            if self.evaluations >= self.budget:\n                break\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F=0.8, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = F * (1 - (self.evaluations / self.budget))\n                mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        # Use Differential Evolution for global exploration\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        # Refine using gradient descent\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size *= 1.2\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.7\n                perturbation_strength *= 1.2\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 51, "feedback": "The algorithm EnhancedHybridGradientDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08165 with standard deviation 0.03667.", "error": "", "parent_ids": ["007a0e8a-5202-486f-aa5c-1f51938c042f"], "operator": null, "metadata": {"aucs": [0.060081247243293556, 0.04520080523280112, 0.07453515730286575, 0.05320265531469792, 0.051911416647844044, 0.052876502953822846, 0.12877876837487634, 0.13636095689026595, 0.13187580625669826]}}
{"id": "4e891ef3-8a2b-45bd-ac18-758394592940", "fitness": 0.09371837821073284, "name": "HybridAdaptiveGradientDE", "description": "Enhanced Hybrid Adaptive Gradient and Differential Evolution with dynamic adaptation of crossover rate for improved convergence.", "code": "import numpy as np\n\nclass HybridAdaptiveGradientDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F=0.8, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget))  # Dynamic CR\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic  # Use dynamic CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        # Use Differential Evolution for global exploration\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        # Refine using gradient descent\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size *= 1.2\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 52, "feedback": "The algorithm HybridAdaptiveGradientDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09372 with standard deviation 0.03575.", "error": "", "parent_ids": ["007a0e8a-5202-486f-aa5c-1f51938c042f"], "operator": null, "metadata": {"aucs": [0.08368919408916675, 0.07541396656054966, 0.09445594260848567, 0.05136605320243115, 0.054379125310253906, 0.06148638251834049, 0.1397125709073117, 0.13979811982592305, 0.14316404887413314]}}
{"id": "8792300d-563b-4857-a57b-300bc4b9d671", "fitness": 0.09371837821073284, "name": "HybridAdaptiveGradientDE", "description": "Enhanced Hybrid Adaptive Gradient and Differential Evolution with dynamic perturbation for improved fine-tuning.", "code": "import numpy as np\n\nclass HybridAdaptiveGradientDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F=0.8, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget))  # Dynamic CR\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic  # Use dynamic CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        # Use Differential Evolution for global exploration\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        # Refine using gradient descent\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + 0.1 * perturbation  # Dynamic perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size *= 1.2\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 53, "feedback": "The algorithm HybridAdaptiveGradientDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09372 with standard deviation 0.03575.", "error": "", "parent_ids": ["4e891ef3-8a2b-45bd-ac18-758394592940"], "operator": null, "metadata": {"aucs": [0.08368919408916675, 0.07541396656054966, 0.09445594260848567, 0.05136605320243115, 0.054379125310253906, 0.06148638251834049, 0.1397125709073117, 0.13979811982592305, 0.14316404887413314]}}
{"id": "8e08fc47-c0e8-4b92-9127-781d1fcd6897", "fitness": 0.09371837821073284, "name": "HybridAdaptiveGradientDE", "description": "Improved convergence by introducing adaptive scaling for the perturbation strength.", "code": "import numpy as np\n\nclass HybridAdaptiveGradientDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F=0.8, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget))  # Dynamic CR\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic  # Use dynamic CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        # Use Differential Evolution for global exploration\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        # Refine using gradient descent\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation = np.random.uniform(-perturbation_strength * (1 + self.evaluations / self.budget), perturbation_strength * (1 + self.evaluations / self.budget), size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size *= 1.2\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 54, "feedback": "The algorithm HybridAdaptiveGradientDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09372 with standard deviation 0.03575.", "error": "", "parent_ids": ["4e891ef3-8a2b-45bd-ac18-758394592940"], "operator": null, "metadata": {"aucs": [0.08368919408916675, 0.07541396656054966, 0.09445594260848567, 0.05136605320243115, 0.054379125310253906, 0.06148638251834049, 0.1397125709073117, 0.13979811982592305, 0.14316404887413314]}}
{"id": "0b156bef-74c0-4896-b423-c46ea37d1ef4", "fitness": 0.10426490131750905, "name": "EnhancedHybridAdaptiveDE", "description": "Incorporate adaptive learning rate and adaptive mutation factor to enhance convergence in Hybrid Adaptive Gradient and Differential Evolution.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget))\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)  # Increase mutation factor if improvement\n            F = max(0.1, F * 0.9)  # Decrease mutation factor if no improvement\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)  # Adaptive step size\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 55, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10426 with standard deviation 0.03791.", "error": "", "parent_ids": ["4e891ef3-8a2b-45bd-ac18-758394592940"], "operator": null, "metadata": {"aucs": [0.0626027335879783, 0.09069762665244763, 0.14806023666676016, 0.06328833503292686, 0.06464525244366182, 0.07444861761138744, 0.1427855690661166, 0.14578791778450795, 0.14606782301179477]}}
{"id": "e8920f13-b495-4f13-88af-7e6f1872dcad", "fitness": 0.10426490131750905, "name": "EnhancedHybridAdaptiveDE", "description": "Introduce stochastic ranking to balance exploration and exploitation in Enhanced Hybrid Adaptive Gradient and Differential Evolution.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget))\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        \n        return best\n\n    def stochastic_ranking(self, func, pop, fitness, pf=0.45):\n        sorted_indices = np.argsort(fitness)\n        for i in range(len(pop) - 1):\n            for j in range(len(pop) - 1 - i):\n                u = np.random.uniform(0, 1)\n                if u < pf:\n                    if fitness[sorted_indices[j]] > fitness[sorted_indices[j + 1]]:\n                        sorted_indices[j], sorted_indices[j + 1] = sorted_indices[j + 1], sorted_indices[j]\n                else:\n                    distance_j = np.linalg.norm(pop[sorted_indices[j]])\n                    distance_j1 = np.linalg.norm(pop[sorted_indices[j + 1]])\n                    if distance_j > distance_j1:\n                        sorted_indices[j], sorted_indices[j + 1] = sorted_indices[j + 1], sorted_indices[j]\n        return pop[sorted_indices], fitness[sorted_indices]\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 56, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10426 with standard deviation 0.03791.", "error": "", "parent_ids": ["0b156bef-74c0-4896-b423-c46ea37d1ef4"], "operator": null, "metadata": {"aucs": [0.0626027335879783, 0.09069762665244763, 0.14806023666676016, 0.06328833503292686, 0.06464525244366182, 0.07444861761138744, 0.1427855690661166, 0.14578791778450795, 0.14606782301179477]}}
{"id": "2a9e2cdf-94ce-44ea-98ba-860993daa4bd", "fitness": 0.09823142764995943, "name": "EnhancedHybridAdaptiveDE", "description": "Integrate a self-adaptive elite archive mechanism with gradient-informed mutation to improve exploration-exploitation balance in EnhancedHybridAdaptiveDE.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.elite_archive = []\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget))\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)  # Increase mutation factor if improvement\n                        self.elite_archive.append(best)  # Add to elite archive\n            F = max(0.1, F * 0.9)  # Decrease mutation factor if no improvement\n            self.elite_archive = sorted(self.elite_archive, key=lambda x: func(x))[:5]  # Maintain top 5 elites\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)  # Adaptive step size\n                perturbation_strength *= 0.8\n                if value < best_value:\n                    best_value = value\n                    best_x = x\n                    self.elite_archive.append(best_x)  # Update elite archive with new best\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if self.elite_archive:\n                elite_mutation = np.mean(np.array(self.elite_archive), axis=0)\n                elite_mutation = np.clip(elite_mutation, bounds[:, 0], bounds[:, 1])\n                elite_value = func(elite_mutation)\n                self.evaluations += 1\n                if elite_value < best_value:\n                    best_value = elite_value\n                    best_x = elite_mutation\n\n        return best_x", "configspace": "", "generation": 57, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09823 with standard deviation 0.03797.", "error": "", "parent_ids": ["0b156bef-74c0-4896-b423-c46ea37d1ef4"], "operator": null, "metadata": {"aucs": [0.05317905676841206, 0.07935630883785993, 0.12783291184015733, 0.05961465539531052, 0.06278643432866449, 0.0704506675822858, 0.14106702974597307, 0.14549519875462114, 0.1443005855963505]}}
{"id": "bfcfca4e-a60b-4cc9-9e8c-a0aa9d53502d", "fitness": -Infinity, "name": "EnhancedHybridAdaptiveDE", "description": "Integrate adaptive learning with a dynamic population size and opposition-based learning to enhance exploration and exploitation balance in Differential Evolution.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size_init=10, F_init=0.5, CR=0.9):\n        pop_size = pop_size_init\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget))\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)  # Increase mutation factor if improvement\n                else:\n                    F = max(0.1, F * 0.9)  # Decrease mutation factor if no improvement\n\n            # Dynamic population size; increase or decrease based on budget usage\n            if self.evaluations < self.budget / 2:\n                pop_size = min(pop_size_init * 2, self.budget - self.evaluations)\n            else:\n                pop_size = max(pop_size_init // 2, 1)\n\n            # Opposition-based learning\n            opp_pop = bounds[:, 0] + bounds[:, 1] - pop\n            opp_fitness = np.array([func(ind) for ind in opp_pop])\n            self.evaluations += pop_size\n\n            combined_pop = np.vstack((pop, opp_pop))\n            combined_fitness = np.hstack((fitness, opp_fitness))\n            best_indices = np.argsort(combined_fitness)[:pop_size]\n            pop = combined_pop[best_indices]\n            fitness = combined_fitness[best_indices]\n\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)  # Adaptive step size\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 58, "feedback": "An exception occurred: IndexError('index 9 is out of bounds for axis 0 with size 5').", "error": "IndexError('index 9 is out of bounds for axis 0 with size 5')", "parent_ids": ["0b156bef-74c0-4896-b423-c46ea37d1ef4"], "operator": null, "metadata": {}}
{"id": "4eb2a077-97f8-48cf-8875-ac492e31a4c8", "fitness": 0.16127029513259142, "name": "EnhancedHybridAdaptiveDE", "description": "Enhance convergence by adjusting perturbation strength based on gradient magnitude.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget))\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)  # Increase mutation factor if improvement\n            F = max(0.1, F * 0.9)  # Decrease mutation factor if no improvement\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)  # Adjust perturbation based on gradient magnitude\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)  # Adaptive step size\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 59, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.16127 with standard deviation 0.15061.", "error": "", "parent_ids": ["0b156bef-74c0-4896-b423-c46ea37d1ef4"], "operator": null, "metadata": {"aucs": [0.5756512779237197, 0.09069762665244763, 0.14806023666676016, 0.06328833503292686, 0.06464525244366182, 0.07444861761138744, 0.1427855690661166, 0.14578791778450795, 0.14606782301179477]}}
{"id": "446adf41-9140-40ee-ba65-2048525ad8e5", "fitness": 0.10426490131750905, "name": "EnhancedHybridAdaptiveDE", "description": "Improve exploitation with dynamic local search and exploration with chaotic maps to adaptively refine the search space.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def chaotic_map(self, size):\n        x = np.random.rand(size)\n        for _ in range(10):\n            x = 4 * x * (1 - x)\n        return x\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget))\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = self.chaotic_map(self.dim) * perturbation_strength * 2 - perturbation_strength\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 60, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10426 with standard deviation 0.03791.", "error": "", "parent_ids": ["4eb2a077-97f8-48cf-8875-ac492e31a4c8"], "operator": null, "metadata": {"aucs": [0.0626027335879783, 0.09069762665244763, 0.14806023666676016, 0.06328833503292686, 0.06464525244366182, 0.07444861761138744, 0.1427855690661166, 0.14578791778450795, 0.14606782301179477]}}
{"id": "ad4688b9-1538-4234-b4e5-48be9be1b806", "fitness": 0.11237789875535625, "name": "EnhancedAdaptiveDE", "description": "Enhance convergence by adaptively scaling both mutation factors and crossover rates based on feedback from past iterations.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR_init=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n        CR = CR_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n                        CR = min(1.0, CR + 0.05)  # Increase CR if improvement\n            F = max(0.1, F * 0.9)  # Decrease mutation factor if no improvement\n            CR = max(0.1, CR * 0.95)  # Decrease CR slowly if no improvement\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11238 with standard deviation 0.03934.", "error": "", "parent_ids": ["4eb2a077-97f8-48cf-8875-ac492e31a4c8"], "operator": null, "metadata": {"aucs": [0.13845599163341982, 0.17112771477767585, 0.08381254639836688, 0.06417464986657329, 0.06804372938648873, 0.0645592538718982, 0.14597964902555716, 0.13823829801703702, 0.13700925582118928]}}
{"id": "08cdf008-debc-4128-aa8c-00f9ceeab0e3", "fitness": 0.10836167291394329, "name": "EnhancedHybridAdaptiveDE", "description": "Improve convergence by dynamically adjusting the crossover rate based on population diversity.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget))\n                diversity = np.mean(np.linalg.norm(pop - pop.mean(axis=0), axis=1))  # Calculate population diversity\n                CR_dynamic += 0.1 * (diversity / np.linalg.norm(bounds[:, 1] - bounds[:, 0]))  # Adjust CR based on diversity\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)  # Increase mutation factor if improvement\n            F = max(0.1, F * 0.9)  # Decrease mutation factor if no improvement\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)  # Adjust perturbation based on gradient magnitude\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)  # Adaptive step size\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 62, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10836 with standard deviation 0.03280.", "error": "", "parent_ids": ["4eb2a077-97f8-48cf-8875-ac492e31a4c8"], "operator": null, "metadata": {"aucs": [0.07221346031381104, 0.10199689769936426, 0.1430390743258162, 0.060115329013889385, 0.0750572561486944, 0.09530347649177917, 0.14386679176518957, 0.13837109573455586, 0.1452916747323897]}}
{"id": "5c430e49-1205-432c-8b50-91fa3b35ecab", "fitness": 0.08268825459816266, "name": "EnhancedHybridAdaptiveDE", "description": "Enhance convergence by employing dynamic F adjustment based on population diversity to escape local optima.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget))\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                if np.std(fitness) < 1e-5:\n                    F = min(1.0, F + 0.1)  # Increase F when diversity is low\n                else:\n                    F = max(0.1, F * 0.9)  # Decrease F as usual\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 63, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08269 with standard deviation 0.05363.", "error": "", "parent_ids": ["4eb2a077-97f8-48cf-8875-ac492e31a4c8"], "operator": null, "metadata": {"aucs": [0.01787990814367335, 0.011062257284979782, 0.015737013618393525, 0.07088553157231137, 0.09390556386432292, 0.10147965480731513, 0.1445711683931944, 0.1407430780579726, 0.14793011564130087]}}
{"id": "d854e41e-5387-41b1-b838-5d1fa20561d0", "fitness": 0.10426490131750905, "name": "EnhancedStochasticAdaptiveDE", "description": "Integrate adaptive learning rates and stochastic perturbation to enhance convergence in differential evolution.", "code": "import numpy as np\n\nclass EnhancedStochasticAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget))\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)  # Increase mutation factor if improvement\n            F = max(0.1, F * 0.9)  # Decrease mutation factor if no improvement\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)  # Adjust perturbation based on gradient magnitude\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            learning_rate = 1 / (1 + np.sqrt(self.evaluations))  # Adaptive learning rate\n            x_new = x - learning_rate * step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)  # Adaptive step size\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 64, "feedback": "The algorithm EnhancedStochasticAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10426 with standard deviation 0.03791.", "error": "", "parent_ids": ["4eb2a077-97f8-48cf-8875-ac492e31a4c8"], "operator": null, "metadata": {"aucs": [0.0626027335879783, 0.09069762665244763, 0.14806023666676016, 0.06328833503292686, 0.06464525244366182, 0.07444861761138744, 0.1427855690661166, 0.14578791778450795, 0.14606782301179477]}}
{"id": "99ff2d43-eaaf-468d-9fa2-825f85746321", "fitness": 0.10401120055247016, "name": "EnhancedHybridAdaptiveDE", "description": "Enhance convergence by dynamically adjusting both mutation factor and crossover rate based on population diversity.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                diversity = np.std(pop, axis=0).mean() / (bounds[:, 1] - bounds[:, 0]).mean()\n                CR_dynamic = 0.9 * (1 - ((self.evaluations / self.budget) * diversity))\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 65, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10401 with standard deviation 0.03828.", "error": "", "parent_ids": ["4eb2a077-97f8-48cf-8875-ac492e31a4c8"], "operator": null, "metadata": {"aucs": [0.04376116595848745, 0.1260092639903988, 0.06691111642729786, 0.07123582553244434, 0.07863369116613028, 0.10442934664947967, 0.14246738405677117, 0.14818034869311236, 0.15447266249810954]}}
{"id": "64af91fc-54fe-4daf-9d7e-4a66f0cd291d", "fitness": 0.078584717863972, "name": "RefinedAdaptiveDE", "description": "Refine convergence by integrating dynamic learning rate adjustment and adaptive population control into the differential evolution framework.", "code": "import numpy as np\n\nclass RefinedAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, initial_pop_size=10, F_init=0.5, CR=0.9):\n        pop_size = initial_pop_size\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget))\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)  # Increase mutation factor if improvement\n            F = max(0.1, F * 0.9)  # Decrease mutation factor if no improvement\n\n            # Adaptive population control\n            if self.evaluations < self.budget / 2:\n                additional_pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (2, self.dim))\n                additional_fitness = np.array([func(ind) for ind in additional_pop])\n                self.evaluations += 2\n                pop = np.vstack((pop, additional_pop))\n                fitness = np.append(fitness, additional_fitness)\n                pop_size += 2\n            elif self.evaluations > self.budget * 0.75:\n                worst_idx = np.argmax(fitness)\n                pop = np.delete(pop, worst_idx, axis=0)\n                fitness = np.delete(fitness, worst_idx)\n                pop_size -= 1\n\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)  # Adjust perturbation based on gradient magnitude\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)  # Adaptive step size\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 66, "feedback": "The algorithm RefinedAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07858 with standard deviation 0.05011.", "error": "", "parent_ids": ["4eb2a077-97f8-48cf-8875-ac492e31a4c8"], "operator": null, "metadata": {"aucs": [0.022169956543451663, 0.022485956890583747, 0.06136968165405998, 0.05589159441816005, 0.053791956329986124, 0.05049226567130116, 0.14883893445374696, 0.14596660007509832, 0.14625551473936005]}}
{"id": "179ad073-798f-4a95-8ad1-72bae0fa58c7", "fitness": -Infinity, "name": "EnhancedHybridAdaptiveDE", "description": "Introduce adaptive population size and exploit elite solutions to enhance convergence in hybrid DE.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size_init=10, F_init=0.5, CR=0.9):\n        pop_size = pop_size_init\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            if self.evaluations / self.budget > 0.5:  # Reduce population size as budget depletes\n                pop_size = max(4, int(pop_size * 0.9))\n                pop = pop[:pop_size]\n                fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget))\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            elite_threshold = int(0.1 * pop_size)  # Select top 10% as elite\n            elite = pop[np.argsort(fitness)][:elite_threshold]\n            F = max(0.1, F * 0.9)\n            best = elite[0]  # Assign best as the best from elite\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 67, "feedback": "An exception occurred: IndexError('index 0 is out of bounds for axis 0 with size 0').", "error": "IndexError('index 0 is out of bounds for axis 0 with size 0')", "parent_ids": ["4eb2a077-97f8-48cf-8875-ac492e31a4c8"], "operator": null, "metadata": {}}
{"id": "7028a1ff-bc92-4512-96e4-fd0c59178be3", "fitness": 0.161562088605424, "name": "EnhancedHybridAdaptiveDE", "description": "Adaptively adjust the crossover rate in differential evolution to enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR more dynamically\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 68, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.16156 with standard deviation 0.13889.", "error": "", "parent_ids": ["4eb2a077-97f8-48cf-8875-ac492e31a4c8"], "operator": null, "metadata": {"aucs": [0.5475622746298856, 0.12265130160763771, 0.12920040034881708, 0.09063492773380721, 0.07622279826573175, 0.07102498919149136, 0.13846124799164594, 0.13959249781052663, 0.13870835986927288]}}
{"id": "8fe977f6-4b81-416f-97e3-199e21a90ff3", "fitness": 0.1008149084286434, "name": "AdvancedHybridAdaptiveDE", "description": "Integrate adaptive mutation strategies with dynamic crossover and learning rate to enhance convergence speed and diversity in the search space.", "code": "import numpy as np\n\nclass AdvancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def adaptive_mutation(self, pop, best, F_init=0.5):\n        F = np.clip(F_init * (1 + 0.1 * np.random.randn()), 0.1, 1.0)\n        a, b, c = pop[np.random.choice(len(pop), 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                mutant = self.adaptive_mutation(pop, best)\n                mutant = np.clip(mutant, bounds[:, 0], bounds[:, 1])\n\n                # Dynamic crossover rate\n                CR_dynamic = 0.8 * (1 - (self.evaluations / self.budget)) ** 0.5\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.3, 1.0)\n                perturbation_strength *= 0.85\n            else:\n                step_size *= 0.7\n                perturbation_strength *= 1.15\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 69, "feedback": "The algorithm AdvancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10081 with standard deviation 0.05339.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.04235559698954816, 0.081515585317393, 0.2031322148746939, 0.05713781029710341, 0.05259934006141065, 0.0506887847230546, 0.14203602080841105, 0.13409388922403342, 0.14377493356214233]}}
{"id": "83a9b9da-a97f-405d-aca5-9e67b372a284", "fitness": 0.08317636155179055, "name": "EnhancedHybridAdaptiveDE", "description": "Introduce adaptive mutation scaling based on fitness variance in the population to enhance exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            fitness_std = np.std(fitness)  # Change 1\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR more dynamically\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + fitness_std * 0.01)  # Change 2\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 70, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08318 with standard deviation 0.05093.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.019514307052790558, 0.024340591535573664, 0.025721404374487666, 0.09381614862749854, 0.0656003954825144, 0.08015204688300948, 0.1442792025712537, 0.14186223134844578, 0.15330092609054113]}}
{"id": "a609c4a0-c6c5-421b-bf49-0780a1e82919", "fitness": 0.0885107380903527, "name": "EnhancedHybridAdaptiveDE", "description": "Incorporate adaptive mutation and self-adaptive step size control in Differential Evolution for enhanced convergence in black-box optimization tasks.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                \n                # Adaptive mutation factor F\n                F_dynamic = 0.4 + 0.3 * np.random.rand() * (fitness[best_idx] / (fitness[i] + 1e-8))\n                mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 71, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08851 with standard deviation 0.03899.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.06066481424725767, 0.04288673711473656, 0.10498247492357482, 0.05692446506320725, 0.0629302697906472, 0.052027099821591705, 0.13792153015684794, 0.13765294512837867, 0.14060630656693252]}}
{"id": "6b0386cb-9939-4e36-9c31-8b4f31cb58dd", "fitness": 0.10987146511922379, "name": "EnhancedHybridAdaptiveDE", "description": "Adjust initial step size for gradient descent to improve early search efficiency.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.25 * (bounds[:, 1] - bounds[:, 0])  # Adjust initial step size\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 72, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10987 with standard deviation 0.02756.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.08234666325408346, 0.12265130160763771, 0.12920040034881708, 0.09063492773380721, 0.07622279826573175, 0.07102498919149136, 0.13846124799164594, 0.13959249781052663, 0.13870835986927288]}}
{"id": "1eb61eeb-25d3-4d62-bb1b-1d4e84789ed4", "fitness": 0.10333170358555453, "name": "EnhancedHybridAdaptiveDE", "description": "Introduce a dynamic strategy for scaling factor F to improve balance between exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR more dynamically\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * (0.9 + 0.1 * np.sin(self.evaluations / self.budget * np.pi)))  # Dynamic F adjustment\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 73, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10333 with standard deviation 0.03549.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.047733067354727376, 0.11309969844481693, 0.11390434343404221, 0.08926542588872921, 0.07390283667211583, 0.05962736295292559, 0.1495723279537896, 0.14464566625578346, 0.1382346033130606]}}
{"id": "b3dbfa69-4745-4a0b-ae3b-8e4d26293fa1", "fitness": 0.09390525665268916, "name": "EnhancedHybridAdaptiveDE", "description": "Integrate a dynamic mutation factor and adaptive local search strategy in differential evolution to enhance global exploration and local exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_min=0.4, F_max=0.9, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5\n                F_dynamic = F_min + (F_max - F_min) * (1 - (self.evaluations / self.budget)) ** 0.5\n                mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09391 with standard deviation 0.03743.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.11595089406416026, 0.08348351759527062, 0.07395543091427448, 0.05628060063058893, 0.04948196492994272, 0.04856000643602365, 0.13602124640367697, 0.14337593933518167, 0.1380377095650831]}}
{"id": "3cfc52ad-b107-4ad1-be4f-7406ab8ca456", "fitness": 0.08445568585222757, "name": "EnhancedHybridAdaptiveDE", "description": "Modify mutation strategy in differential evolution to improve convergence in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR more dynamically\n                mutant = np.clip(a + F * (b - c) + (best - pop[i]), bounds[:, 0], bounds[:, 1]) # Added best-guided mutation\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 75, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08446 with standard deviation 0.04316.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.07914992022530776, 0.07469186078620116, 0.024145803759164863, 0.05773157461635148, 0.0505461644625842, 0.04874674778167898, 0.14086996666380802, 0.14634083083735283, 0.13787830353759878]}}
{"id": "afae3e6c-2549-41ac-95a8-2f4264e0e646", "fitness": 0.06983868066330058, "name": "EnhancedHybridAdaptiveDE", "description": "Incorporate dynamic adjustment of the mutation factor F based on diversity to improve exploration.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR more dynamically\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * (0.9 + 0.1 * np.std(fitness)))  # Diversify F based on std deviation\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 76, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06984 with standard deviation 0.05115.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.012851740246944021, 0.02374978991941623, 0.015670510656202064, 0.053506911240065125, 0.05348105662751246, 0.05215105467882142, 0.13861671822508903, 0.13575886339024945, 0.14276148098540542]}}
{"id": "65f2b882-46e1-4183-beb2-28317a4989d8", "fitness": 0.10987146511922379, "name": "EnhancedHybridAdaptiveDE", "description": "Introduce a dynamic scaling factor to the gradient estimation to improve the convergence speed and precision of the EnhancedHybridAdaptiveDE algorithm.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        scaling_factor = 1 + (self.evaluations / self.budget)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon * scaling_factor\n            grad[i] = (func(x_step) - func(x)) / (epsilon * scaling_factor)\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR more dynamically\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 77, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10987 with standard deviation 0.02756.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.08234666325408346, 0.12265130160763771, 0.12920040034881708, 0.09063492773380721, 0.07622279826573175, 0.07102498919149136, 0.13846124799164594, 0.13959249781052663, 0.13870835986927288]}}
{"id": "6139a0a9-7183-4be2-93dd-d17e08bd1dd5", "fitness": 0.10987146511922379, "name": "EnhancedHybridAdaptiveDE", "description": "Tune the mutation factor F more dynamically based on the best solution's improvement.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR more dynamically\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9) if f >= fitness[best_idx] else min(1.0, F + 0.05)  # Modified line\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10987 with standard deviation 0.02756.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.08234666325408346, 0.12265130160763771, 0.12920040034881708, 0.09063492773380721, 0.07622279826573175, 0.07102498919149136, 0.13846124799164594, 0.13959249781052663, 0.13870835986927288]}}
{"id": "59385397-f267-4585-a504-7d05ef5026ea", "fitness": 0.10677965932901716, "name": "EnhancedHybridAdaptiveDE_Levy", "description": "Introduce Levy flights in the mutation step of differential evolution to enhance global exploration and avoid local optima.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE_Levy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            if self.evaluations >= self.budget:\n                break\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n        return grad\n\n    def levy_flight(self, scale=0.01):\n        u = np.random.normal(0, 1, self.dim) * scale\n        v = np.random.normal(0, 1, self.dim)\n        step = u / np.abs(v) ** (1 / 3)\n        return step\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5\n                mutant = np.clip(a + F * (b - c) + self.levy_flight(), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation = self.levy_flight()\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 79, "feedback": "The algorithm EnhancedHybridAdaptiveDE_Levy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10678 with standard deviation 0.04235.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.04967639546949498, 0.12740529684077684, 0.15863949598079485, 0.07806964399301208, 0.056796662822275246, 0.05889739695304752, 0.14107039194031235, 0.1425479552634692, 0.14791369469797133]}}
{"id": "f7ae6480-d573-47bb-b01a-6ef2e0542c4b", "fitness": -Infinity, "name": "EnhancedLevyAdaptiveDE", "description": "Incorporate Levy flight for global exploration in differential evolution to enhance both exploration and convergence.", "code": "import numpy as np\nimport scipy.stats as stats\n\nclass EnhancedLevyAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def levy_flight(self, size, alpha=1.5):\n        sigma = (stats.gamma(1 + alpha) * np.sin(np.pi * alpha / 2) / (stats.gamma((1 + alpha) / 2) * alpha * 2**((alpha - 1) / 2)))**(1/alpha)\n        u = np.random.normal(0, sigma, size=size)\n        v = np.random.normal(0, 1, size=size)\n        step = u / np.abs(v)**(1/alpha)\n        return step\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += self.levy_flight(self.dim) * (trial - best)  # Incorporate Levy flight\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 80, "feedback": "An exception occurred: TypeError(\"unsupported operand type(s) for *: 'rv_continuous_frozen' and 'float'\").", "error": "TypeError(\"unsupported operand type(s) for *: 'rv_continuous_frozen' and 'float'\")", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {}}
{"id": "0142a28a-37aa-4471-926a-57f778243b86", "fitness": 0.09672988125291142, "name": "EnhancedHybridAdaptiveDE", "description": "Improve convergence by adjusting the mutation factor dynamically based on evaluations.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR more dynamically\n                mutant = np.clip(a + (0.4 + 0.6 * self.evaluations/self.budget) * (b - c), bounds[:, 0], bounds[:, 1])  # Dynamic F\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09673 with standard deviation 0.04024.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.07127839789043511, 0.08411343566924867, 0.10214054176430676, 0.05405844481571609, 0.04901788868693757, 0.06240146899568022, 0.14017986927938153, 0.14728684452856822, 0.16009203964592855]}}
{"id": "a4fc889c-afe7-4e3a-902f-7ed4818623f8", "fitness": 0.10681246725432608, "name": "EnhancedHybridAdaptiveDE", "description": "Introduce adaptive mutation scaling factor in differential evolution to enhance convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR more dynamically\n                # Introduce adaptive mutation scaling factor\n                F_adaptive = F * (1 + 0.1 * np.random.rand())  \n                mutant = np.clip(a + F_adaptive * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 82, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10681 with standard deviation 0.05624.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.07954666485888218, 0.032868605628093794, 0.2187264663512869, 0.06477670587248852, 0.06463229272020077, 0.06564984388507578, 0.1528943217314067, 0.1385002546980857, 0.14371704954341447]}}
{"id": "43d57bdf-00d2-46a3-93a0-9efd0dc8db09", "fitness": 0.09898001300705404, "name": "EnhancedHybridAdaptiveDE", "description": "Slightly increase the initial population size for a better exploration phase.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=11, F_init=0.5, CR=0.9):  # Changed pop_size from 10 to 11\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR more dynamically\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09898 with standard deviation 0.03608.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.07706108823819102, 0.06858498558670423, 0.13018217147681976, 0.0709030487753941, 0.06027028468662621, 0.059593522546675604, 0.13630516790931024, 0.13848410196948668, 0.14943574587427855]}}
{"id": "bf5c0e44-2e4d-44fc-921d-01012d905280", "fitness": 0.10987146511922379, "name": "EnhancedHybridAdaptiveDE", "description": "Introduce stochastic gradient estimation to improve exploitation in EnhancedHybridAdaptiveDE.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR more dynamically\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            grad += np.random.normal(0, epsilon)\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 84, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10987 with standard deviation 0.02756.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.08234666325408346, 0.12265130160763771, 0.12920040034881708, 0.09063492773380721, 0.07622279826573175, 0.07102498919149136, 0.13846124799164594, 0.13959249781052663, 0.13870835986927288]}}
{"id": "d20289e8-b099-4891-ac98-f4bed4ab9f9d", "fitness": 0.09920997865126749, "name": "EnhancedLearningDE", "description": "Integrate learning-based adaptation of mutation strategies in differential evolution for enhanced convergence.", "code": "import numpy as np\n\nclass EnhancedLearningDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            if self.evaluations >= self.budget:\n                break\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n        return grad\n\n    def update_mutation_strategy(self, success_rate, F, F_min=0.1, F_max=1.0):\n        if success_rate > 0.2:\n            F = min(F_max, F * 1.1)\n        else:\n            F = max(F_min, F * 0.9)\n        return F\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            successful_mutations = 0\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = CR * (1 - (self.evaluations / self.budget)) ** 0.5\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    successful_mutations += 1\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n            success_rate = successful_mutations / pop_size\n            F = self.update_mutation_strategy(success_rate, F)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedLearningDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09921 with standard deviation 0.03233.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.058670500388046554, 0.10017577736355343, 0.09432207805566761, 0.07435085024995569, 0.08355880039077468, 0.05883139176999996, 0.14171258322671765, 0.14248440306622834, 0.13878342335046345]}}
{"id": "a926f769-94fa-4da2-b34a-8efda386057c", "fitness": 0.09935490131903925, "name": "EnhancedHybridAdaptiveDE", "description": "Enhance mutation strategy by introducing an adaptive scaling factor to improve exploration.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR more dynamically\n                F_adaptive = F * (1 + 0.2 * np.sin(np.pi * self.evaluations / self.budget))  # Introduce adaptive scaling\n                mutant = np.clip(a + F_adaptive * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 86, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09935 with standard deviation 0.03978.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.052158067114896456, 0.1294893283105848, 0.1043860971028221, 0.059000705094452544, 0.061376928478004134, 0.055730895204526676, 0.13565439045492056, 0.15127757164597877, 0.1451201284651672]}}
{"id": "cd1ce265-8a32-423a-9f02-6442f7bfe00b", "fitness": 0.07099324281557688, "name": "EnhancedHybridAdaptiveDE", "description": "Improve the convergence by dynamically adjusting the initial population size based on dimensionality and budget.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=None, F_init=0.5, CR=0.9):  # change to dynamically set pop_size\n        if pop_size is None:\n            pop_size = max(4, self.dim * 5)  # Adjust initial population size based on dimensionality\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR more dynamically\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07099 with standard deviation 0.05349.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.02012553757292268, 0.015926847198162952, 0.016402030944473234, 0.053409967320408036, 0.048757175288825505, 0.052514313934952095, 0.13962246385035304, 0.14918008076798495, 0.1430007684621094]}}
{"id": "4e1afe65-aad7-485b-a97b-1bbde295a3fc", "fitness": 0.10906247005825626, "name": "EnhancedHybridAdaptiveDE", "description": "Introduce dynamic adjustment of F factor in differential evolution for improved adaptability.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR more dynamically\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1 * (1 - (self.evaluations / self.budget)))  # Dynamic F adjustment\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 88, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10906 with standard deviation 0.03006.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.08037539919505188, 0.09469588180938371, 0.1255109457087934, 0.100495186562373, 0.07618938870716685, 0.0680067535140304, 0.1420550028683757, 0.1491035196214079, 0.14513015253772354]}}
{"id": "5f1e139c-9a89-4004-ac1d-ce0b2ecda28a", "fitness": 0.10872828748359559, "name": "EnhancedHybridAdaptiveDE", "description": "Refine the update mechanism by adjusting mutation strategy based on fitness ranking.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR more dynamically\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n            # Adjust mutation strategy based on ranking\n            ranked_indices = np.argsort(fitness)\n            for rank, idx in enumerate(ranked_indices):\n                pop[idx] = np.clip(pop[idx] + 0.1 * (rank/len(ranked_indices)) * (best - pop[idx]), bounds[:, 0], bounds[:, 1])\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 89, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10873 with standard deviation 0.03856.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.0606162021047969, 0.10397280919602658, 0.09606277276568742, 0.08606555401080862, 0.07050291551864962, 0.0796663614877614, 0.15924693317015604, 0.16038158351443454, 0.1620394555840392]}}
{"id": "945eaf34-9d05-4de3-8692-904be2105c30", "fitness": 0.08657331007953292, "name": "EnhancedHybridAdaptiveDE", "description": "Integrate an elite archive and adaptive mutation strategy into the differential evolution framework for enhanced convergence.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        archive = [best]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                if len(archive) > 1:\n                    a = archive[np.random.randint(0, len(archive))]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n                        archive.append(best)\n                        if len(archive) > pop_size:\n                            archive.pop(0)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 90, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08657 with standard deviation 0.04096.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.050993050782699334, 0.03312803387402097, 0.0602740947868593, 0.06939008638437694, 0.05839728279993239, 0.08150349009533775, 0.14389132275272776, 0.1391646190134891, 0.1424178102263528]}}
{"id": "056d54e9-91a4-4cfc-adca-bcb0122813ad", "fitness": 0.08700355265036572, "name": "EnhancedHybridAdaptiveDE", "description": "Introduce adaptive population sizing and local search intensification to balance exploration and exploitation dynamically.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, initial_pop_size=10, F_init=0.5, CR=0.9):\n        pop_size = initial_pop_size\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n\n            # Adaptive population size adjustment\n            pop_size = max(5, int(initial_pop_size * (1 - self.evaluations / self.budget)))\n\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n            else:\n                step_size *= 0.5\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 91, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08700 with standard deviation 0.03851.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.04804204683261715, 0.0431838159165453, 0.054694319240872646, 0.087154230963428, 0.05998343536295858, 0.07432680248095314, 0.1394179975249784, 0.13898448146304676, 0.1372448440678915]}}
{"id": "b2211422-cbce-42b7-b15f-845eba28773d", "fitness": 0.08647066612156751, "name": "EnhancedDynamicDE", "description": "Use a dynamic mutation factor and adaptive learning strategy in differential evolution for better convergence in black box optimization.", "code": "import numpy as np\n\nclass EnhancedDynamicDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            if self.evaluations >= self.budget:\n                break\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5\n                F_dynamic = 0.5 + 0.5 * (1 - self.evaluations / self.budget) ** 0.5\n                mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedDynamicDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08647 with standard deviation 0.04030.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.08151514425006678, 0.058833361734573764, 0.061436066773701836, 0.051065744633159715, 0.04933614084079607, 0.05029726039047733, 0.1462844847233935, 0.13776368868042344, 0.14170410306751513]}}
{"id": "dafe990a-cbb2-4973-8af3-51b07d0a68f1", "fitness": -Infinity, "name": "EnhancedHybridLvyDE", "description": "Introduce a Lvy flight mechanism to enhance exploration and prevent premature convergence in Differential Evolution.", "code": "import numpy as np\n\nclass EnhancedHybridLvyDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def levy_flight(self, size, alpha=1.5):\n        sigma = (np.gamma(1 + alpha) * np.sin(np.pi * alpha / 2) /\n                 (np.gamma((1 + alpha) / 2) * alpha * 2**((alpha - 1) / 2)))**(1 / alpha)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1 / alpha)\n        return step\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i]) + self.levy_flight(self.dim)\n                trial = np.clip(trial, bounds[:, 0], bounds[:, 1])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 93, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {}}
{"id": "a6101196-5e4c-4761-881e-bb4037ea5f02", "fitness": 0.11036576662551731, "name": "EnhancedHybridAdaptiveDE", "description": "Improved gradient estimation step size for better local search performance.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR more dynamically\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.12 * (bounds[:, 1] - bounds[:, 0])  # Adjusted step size for gradient descent\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 94, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11037 with standard deviation 0.02710.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.08679537681072524, 0.12265130160763771, 0.12920040034881708, 0.09063492773380721, 0.07622279826573175, 0.07102498919149136, 0.13846124799164594, 0.13959249781052663, 0.13870835986927288]}}
{"id": "cdae51a7-4e33-43ae-8ceb-6c53a1fe8fb8", "fitness": 0.10607302270706677, "name": "EnhancedHybridAdaptiveDE", "description": "Fine-tune mutation strategy in differential evolution for better convergence.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR more dynamically\n                mutant = np.clip(a + F * (b - c) + 0.1 * (best - pop[i]), bounds[:, 0], bounds[:, 1])  # Include best vector\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 95, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10607 with standard deviation 0.04194.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.08245697081645964, 0.0891118911581038, 0.16917167413422285, 0.060466535582471836, 0.05537042823162086, 0.06348861117827453, 0.14172521976214092, 0.14884868362545256, 0.1440171898748539]}}
{"id": "ee9b10de-8702-4865-b34b-4f7a40a36d17", "fitness": 0.10987146511922379, "name": "EnhancedHybridAdaptiveDE", "description": "Slightly refined the step size adjustment to enhance convergence in the adaptive differential evolution.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR more dynamically\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.1, 1.0)  # Change: Adjust step size multiplier to 1.1\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 96, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10987 with standard deviation 0.02756.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.08234666325408346, 0.12265130160763771, 0.12920040034881708, 0.09063492773380721, 0.07622279826573175, 0.07102498919149136, 0.13846124799164594, 0.13959249781052663, 0.13870835986927288]}}
{"id": "ff9d2c6e-576b-49e4-a288-a8fd7de88ff2", "fitness": 0.10987146511922379, "name": "AdvancedAdaptiveDE", "description": "Incorporate adaptive mutation scaling and local search exploitation to improve convergence and robustness in differential evolution.", "code": "import numpy as np\n\nclass AdvancedAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5  # Adjust CR dynamically\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def local_search(self, func, x, bounds):\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n        current_value = func(x)\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n        return x, current_value\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        x, current_value = self.local_search(func, x, bounds)\n\n        if current_value < best_value:\n            best_value = current_value\n            best_x = x\n\n        return best_x", "configspace": "", "generation": 97, "feedback": "The algorithm AdvancedAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10987 with standard deviation 0.02756.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.08234666325408346, 0.12265130160763771, 0.12920040034881708, 0.09063492773380721, 0.07622279826573175, 0.07102498919149136, 0.13846124799164594, 0.13959249781052663, 0.13870835986927288]}}
{"id": "8cc33185-aa9a-47c8-a902-08d3cdf3045c", "fitness": 0.0730842822225454, "name": "SwarmEnhancedHybridDE", "description": "Introduce swarm intelligence by combining differential evolution with a particle swarm optimization-inspired velocity update for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass SwarmEnhancedHybridDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9, inertia_weight=0.7, cognitive=1.5, social=1.5):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        velocities = np.zeros((pop_size, self.dim))\n        personal_best_positions = pop.copy()\n        personal_best_fitness = fitness.copy()\n        best_idx = np.argmin(fitness)\n        global_best_position = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Differential evolution mutation and crossover\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n\n                # Swarm intelligence component (velocity and position update)\n                velocities[i] = (inertia_weight * velocities[i] +\n                                 cognitive * np.random.rand(self.dim) * (personal_best_positions[i] - pop[i]) +\n                                 social * np.random.rand(self.dim) * (global_best_position - pop[i]))\n                trial += velocities[i]\n                trial = np.clip(trial, bounds[:, 0], bounds[:, 1])\n\n                f = func(trial)\n                self.evaluations += 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    personal_best_positions[i] = trial\n                    personal_best_fitness[i] = f\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        global_best_position = trial\n                        F = min(1.0, F + 0.1)\n\n            F = max(0.1, F * 0.9)\n        return global_best_position\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 98, "feedback": "The algorithm SwarmEnhancedHybridDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07308 with standard deviation 0.05150.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.009597117710474334, 0.032146909797065026, 0.01236173948900543, 0.05894119852845636, 0.05768513873617398, 0.06330231344121051, 0.14204186558132126, 0.14266854604263024, 0.13901371067657142]}}
{"id": "304802b0-8e8a-46d4-9506-f410a4ae7cd7", "fitness": 0.10349458451595622, "name": "EnhancedHybridAdaptiveDE", "description": "Incorporate dynamic scaling of mutation factor with adaptive learning to further balance exploration and exploitation in differential evolution.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def estimate_gradient(self, func, x, epsilon=1e-8):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        return grad\n\n    def differential_evolution(self, func, bounds, pop_size=10, F_init=0.5, CR=0.9):\n        pop = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += pop_size\n\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        F = F_init\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                CR_dynamic = 0.9 * (1 - (self.evaluations / self.budget)) ** 0.5\n                F_dynamic = F * (1.0 - (fitness[i] - fitness[best_idx]) / (abs(fitness[best_idx]) + 1e-10))\n                mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                        F = min(1.0, F + 0.1)\n            F = max(0.1, F * 0.9)\n        return best\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        best_x = None\n        best_value = float('inf')\n\n        x = self.differential_evolution(func, bounds)\n        current_value = func(x)\n\n        step_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n        perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while self.evaluations < self.budget:\n            grad = self.estimate_gradient(func, x)\n            if self.evaluations >= self.budget:\n                break\n\n            perturbation_strength = 0.05 * np.linalg.norm(grad)\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, size=self.dim)\n            x_new = x - step_size * grad + perturbation\n            x_new = np.clip(x_new, bounds[:, 0], bounds[:, 1])\n\n            value = func(x_new)\n            self.evaluations += 1\n\n            if value < current_value:\n                current_value = value\n                x = x_new\n                step_size = min(step_size * 1.2, 1.0)\n                perturbation_strength *= 0.8\n            else:\n                step_size *= 0.5\n                perturbation_strength *= 1.1\n\n            if current_value < best_value:\n                best_value = current_value\n                best_x = x\n\n        return best_x", "configspace": "", "generation": 99, "feedback": "The algorithm EnhancedHybridAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10349 with standard deviation 0.03874.", "error": "", "parent_ids": ["7028a1ff-bc92-4512-96e4-fd0c59178be3"], "operator": null, "metadata": {"aucs": [0.03371945345270011, 0.1168590855267978, 0.12220812532164482, 0.07570053176701486, 0.08211252283077775, 0.06561143283039106, 0.1567769380419799, 0.1341612967477892, 0.14430187412451057]}}
