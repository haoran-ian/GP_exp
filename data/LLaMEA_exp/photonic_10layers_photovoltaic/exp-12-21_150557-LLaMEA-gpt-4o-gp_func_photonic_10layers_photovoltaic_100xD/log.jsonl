{"id": "78ead3cf-d1d1-4139-b389-bd5a67ede117", "fitness": 0.05491996581132678, "name": "DAPSO", "description": "Dynamic Adaptive Particle Swarm Optimization (DAPSO) with Budget-Aware Search Space Compression to optimize black box functions efficiently.", "code": "import numpy as np\n\nclass DAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1 = 2.0  # cognitive component\n        self.c2 = 2.0  # social component\n        self.w = 0.7   # inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = self.c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = self.c2 * r2 * (self.global_best_position - particle_positions[i])\n                particle_velocities[i] = self.w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 0, "feedback": "The algorithm DAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05492 with standard deviation 0.00727.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.056760646129027315, 0.0682479173696402, 0.050808259694138935, 0.052415729202881955, 0.06285228284251954, 0.0469248235112929, 0.05051942891083028, 0.0605315511736928, 0.0452190534679171]}}
{"id": "5b16bb07-22ba-493f-b62b-31bc29125698", "fitness": 0.05687884319248601, "name": "EDAPSO", "description": "Enhanced Dynamic Adaptive Particle Swarm Optimization (EDAPSO) with Adaptive Velocity Control and Dynamic Subgroup Formation to improve convergence on black box optimization.", "code": "import numpy as np\n\nclass EDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = self.population_size // 2  # form dynamic subgroups\n\n        while evaluations < self.budget:\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 1, "feedback": "The algorithm EDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05688 with standard deviation 0.00579.", "error": "", "parent_ids": ["78ead3cf-d1d1-4139-b389-bd5a67ede117"], "operator": null, "metadata": {"aucs": [0.05817932024562, 0.0682479173696402, 0.05567259764573729, 0.05371437196958251, 0.06285228284251954, 0.05140353423425881, 0.051769042375811924, 0.0605315511736928, 0.04953897087551107]}}
{"id": "d19f8375-6184-42a5-ae69-a5321a4d9c78", "fitness": 0.05119525731679623, "name": "ML_DAPSO", "description": "Incorporating Multi-Leader Dynamic Adaptive Particle Swarm Optimization (ML-DAPSO) with Multi-Leader Strategy and Adaptive Local Search for enhanced exploration and exploitation in black box optimization.", "code": "import numpy as np\n\nclass ML_DAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.leader_count = max(2, self.population_size // 10)  # multiple leaders\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_positions = [None] * self.leader_count\n        self.global_best_values = [float('inf')] * self.leader_count\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = self.population_size // 2  # form dynamic subgroups\n\n        while evaluations < self.budget:\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                # Update multiple leaders\n                for j in range(self.leader_count):\n                    if current_value < self.global_best_values[j]:\n                        self.global_best_values[j] = current_value\n                        self.global_best_positions[j] = particle_positions[i]\n                        break\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                leader_index = np.random.choice(self.leader_count)\n                social_velocity = c2 * r2 * (self.global_best_positions[leader_index] - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive local search for better exploitation\n                if np.random.rand() < 0.1:  # 10% chance to exploit locally\n                    local_search_step = 0.1 * (ub - lb) * (1 - progress)\n                    particle_positions[i] += np.random.uniform(-local_search_step, local_search_step, self.dim)\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (particle_positions[i] - lb),\n                                                ub - search_range_reduction * (ub - particle_positions[i]))\n\n        best_idx = np.argmin(self.global_best_values)\n        return self.global_best_positions[best_idx], self.global_best_values[best_idx]", "configspace": "", "generation": 2, "feedback": "The algorithm ML_DAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05120 with standard deviation 0.00337.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.05602915902849659, 0.051210595103012846, 0.052195467207320045, 0.05174512056578362, 0.0472968614028666, 0.05030729058911554, 0.049873726769472326, 0.04557853650190946]}}
{"id": "365c172a-6acd-4558-84a0-257a09cb5fe8", "fitness": 0.05685070504630244, "name": "EDAPSO", "description": "Enhanced EDAPSO with adaptive subgroup size to improve convergence on black box optimization.", "code": "import numpy as np\n\nclass EDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        # Change here: Adaptive subgroup size based on progress\n        while evaluations < self.budget:\n            progress = evaluations / self.budget\n            subgroup_size = int(self.population_size * (0.5 + 0.5 * (1 - progress)))  # Adaptive subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 3, "feedback": "The algorithm EDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05685 with standard deviation 0.00573.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05728325757687813, 0.0682479173696402, 0.05648448099833059, 0.05289358867623495, 0.06285228284251954, 0.05213968326446716, 0.050979038548717326, 0.0605315511736928, 0.05024454496624131]}}
{"id": "4f325ea2-f75a-4db4-a46f-7dbd36342e31", "fitness": 0.055795357022568645, "name": "EDAPSO", "description": "Improved EDAPSO with enhanced subgroup dynamics by introducing a dynamic subgroup size to better explore the search space.", "code": "import numpy as np\n\nclass EDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(1, int(self.population_size * (1 - evaluations / self.budget)))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 4, "feedback": "The algorithm EDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05580 with standard deviation 0.00644.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.0682479173696402, 0.053851364058936646, 0.052195467207320045, 0.06285228284251954, 0.04972817991121492, 0.05030729058911554, 0.0605315511736928, 0.04792360136748908]}}
{"id": "8d4c60c4-16be-46b9-992b-a8a902103dfa", "fitness": 0.05566529289248602, "name": "EDAPSO", "description": "Enhanced Dynamic Adaptive Particle Swarm Optimization with Adaptive Velocity Control, Dynamic Subgroup Formation, and Adaptive Diversity Control to improve exploration and convergence.", "code": "import numpy as np\n\nclass EDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = self.population_size // 2  # form dynamic subgroups\n\n        while evaluations < self.budget:\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive diversity preservation\n                if evaluations < self.budget / 2:\n                    diversity_factor = 1.0 - (2.0 * evaluations / self.budget)\n                    particle_positions[i] = particle_positions[i] + np.random.uniform(-diversity_factor, diversity_factor, self.dim)\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 5, "feedback": "The algorithm EDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05567 with standard deviation 0.00655.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.0682479173696402, 0.053429996731162, 0.052195467207320045, 0.06285228284251954, 0.04934569997778038, 0.05030729058911554, 0.0605315511736928, 0.04755687145795462]}}
{"id": "daac26f4-a7b7-4747-92c9-8399d248c71d", "fitness": 0.055391219599551095, "name": "EDAPSO", "description": "Improved EDAPSO by introducing a time-varying nonlinear inertia weight for better balance between exploration and exploitation.", "code": "import numpy as np\n\nclass EDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = self.population_size // 2  # form dynamic subgroups\n\n        while evaluations < self.budget:\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress**2) + self.w_end * progress**2  # Nonlinear inertia weight\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 6, "feedback": "The algorithm EDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05539 with standard deviation 0.00678.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.056535178935343655, 0.0682479173696402, 0.05253672732878989, 0.05220888550791958, 0.06285228284251954, 0.048523830901838094, 0.05032021610126247, 0.0605315511736928, 0.0467643862349536]}}
{"id": "1896c493-66b9-4d3b-bdbb-0d7da238ffe4", "fitness": 0.05687884319248601, "name": "DD_PSO", "description": "Dynamic Diversity-Based Particle Swarm Optimization (DD-PSO) with Adaptive Velocity Control and Subgroup-Driven Exploration for Enhanced Convergence in Black Box Optimization.", "code": "import numpy as np\n\nclass DD_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n        self.diversity_threshold = 0.1\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = self.population_size // 2\n\n        while evaluations < self.budget:\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive Diversity Control: Inject random variations if diversity is too low\n                if np.std(particle_positions, axis=0).mean() < self.diversity_threshold:\n                    particle_positions[i] += np.random.uniform(-0.1, 0.1, self.dim)\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 7, "feedback": "The algorithm DD_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05688 with standard deviation 0.00579.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05817932024562, 0.0682479173696402, 0.05567259764573729, 0.05371437196958251, 0.06285228284251954, 0.05140353423425881, 0.051769042375811924, 0.0605315511736928, 0.04953897087551107]}}
{"id": "789ebc92-56b8-4705-8a10-7640efef0fb6", "fitness": 0.05457585748363176, "name": "EDAPSO", "description": "EDAPSO with Enhanced Subgroup Dynamics for improved optimization convergence and robustness by refining subgroup size calculation.", "code": "import numpy as np\n\nclass EDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = max(2, int(self.population_size * 0.3))  # improved dynamic subgroup size calculation\n\n        while evaluations < self.budget:\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 8, "feedback": "The algorithm EDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05458 with standard deviation 0.00312.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05997270550428879, 0.058704651212361014, 0.0559465248736748, 0.055349705986337816, 0.054199677390252665, 0.05165300842348097, 0.0533401687018602, 0.05223775575970635, 0.049778519500723206]}}
{"id": "5697bd6b-ef28-4cfb-8579-198664420e39", "fitness": 0.0558989050828189, "name": "EDAPSO", "description": "Introduced an adaptive mutation strategy on particle positions to enhance exploration and improve convergence in EDAPSO.", "code": "import numpy as np\n\nclass EDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = self.population_size // 2  # form dynamic subgroups\n\n        while evaluations < self.budget:\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Added adaptive mutation strategy\n                mutation_strength = 0.1 * (1 - progress)\n                particle_positions[i] += np.random.normal(0, mutation_strength, self.dim)\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 9, "feedback": "The algorithm EDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05590 with standard deviation 0.00636.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.0682479173696402, 0.05418691923237362, 0.052195467207320045, 0.06285228284251954, 0.05003265104175436, 0.05030729058911554, 0.0605315511736928, 0.048215507605764985]}}
{"id": "3750c569-385d-4da0-ac6f-8ed26da90b47", "fitness": 0.05527452494000806, "name": "EDAPSO", "description": "Improved EDAPSO with adaptive particle size adjustment to enhance convergence efficiency.", "code": "import numpy as np\n\nclass EDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = self.population_size // 2  # form dynamic subgroups\n\n        while evaluations < self.budget:\n            # Adaptive particle size adjustment\n            self.population_size = int(min(100, max(10, self.budget // (10 + evaluations // 100)))) \n            \n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 10, "feedback": "The algorithm EDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05527 with standard deviation 0.00689.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.0682479173696402, 0.052195947111207674, 0.052195467207320045, 0.06285228284251954, 0.04818800031691084, 0.05030729058911554, 0.0605315511736928, 0.046431709166476876]}}
{"id": "032c7569-11ea-4d71-b33a-9ab2f0878e83", "fitness": 0.05193238681276843, "name": "EDAPSO", "description": "Improved Enhanced Dynamic Adaptive Particle Swarm Optimization (EDAPSO) by incorporating a multi-phase velocity update to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = self.population_size // 2  # form dynamic subgroups\n\n        while evaluations < self.budget:\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2, r3 = np.random.rand(), np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                additional_phase = 0.5 * r3 * (personal_best_positions[np.random.randint(self.population_size)] - particle_positions[i])  # New phase\n\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity + additional_phase\n                particle_positions[i] += particle_velocities[i]\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 11, "feedback": "The algorithm EDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05193 with standard deviation 0.00485.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05776315664751197, 0.059158801185764, 0.049227252349293926, 0.05333399318846177, 0.054613660278084786, 0.045457229521868814, 0.051403250295323444, 0.052635426609715985, 0.04379871123889123]}}
{"id": "9e4cfb41-12af-413f-a90a-c30f2a1f4c1d", "fitness": 0.05554704853071051, "name": "ImprovedEDAPSO", "description": "Improved EDAPSO with Adaptive Local Search and Dynamic Mutation to enhance exploration and exploitation balance for better black box optimization performance.", "code": "import numpy as np\n\nclass ImprovedEDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n        self.mutation_rate = 0.1  # initial mutation rate for exploration\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = self.population_size // 2  # form dynamic subgroups\n\n        while evaluations < self.budget:\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive local search and mutation\n                if np.random.rand() < self.mutation_rate:\n                    mutation_vector = np.random.normal(0, 1, self.dim)\n                    particle_positions[i] += mutation_vector * (ub - lb) * (1 - progress)\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n            # Dynamic adjustment of mutation rate\n            self.mutation_rate = max(0.01, self.mutation_rate * (1 - progress))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 12, "feedback": "The algorithm ImprovedEDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05555 with standard deviation 0.00665.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.0682479173696402, 0.053050987351086376, 0.052195467207320045, 0.06285228284251954, 0.048996887626724606, 0.05030729058911554, 0.0605315511736928, 0.047220493933106455]}}
{"id": "2c89c41c-7d6b-4173-ae48-44c44673f8fb", "fitness": 0.056424430952010024, "name": "AdaptivePSO", "description": "Adaptive Particle Swarm Optimization with probabilistic regrouping and progressive velocity dampening to enhance convergence efficiency.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5\n        self.c2_start, self.c2_end = 0.5, 2.5\n        self.w_start, self.w_end = 0.9, 0.2\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_indices = np.random.choice(self.population_size, self.population_size // 2, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Velocity dampening\n                if progress > 0.5:\n                    particle_velocities[i] *= 0.9\n\n                # Probabilistic regrouping\n                if np.random.rand() < 0.1:\n                    subgroup_indices = np.random.choice(self.population_size, self.population_size // 2, replace=False)\n                    \n                # Compression\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 13, "feedback": "The algorithm AdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05642 with standard deviation 0.00613.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.057999341721561515, 0.0682479173696402, 0.054393614305245586, 0.05354726354380246, 0.06285228284251954, 0.05023080918220835, 0.05160732272064206, 0.0605315511736928, 0.04840977570877769]}}
{"id": "9c983b1d-f664-4f6c-9685-b61b0ee6b76f", "fitness": 0.05479638292580025, "name": "QIPSO", "description": "Quantum-Inspired Particle Swarm Optimization (QIPSO) with adaptive quantum potential wells and stochastic tunneling to enhance solution exploration and convergence.", "code": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1, self.c2 = 1.5, 1.5  # fixed cognitive and social components\n        self.w = 0.5  # inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = self.c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = self.c2 * r2 * (self.global_best_position - particle_positions[i])\n                particle_velocities[i] = self.w * particle_velocities[i] + cognitive_velocity + social_velocity\n                \n                # Quantum potential update\n                quantum_potential = np.random.normal(0, 1, self.dim)\n                particle_positions[i] += particle_velocities[i] + quantum_potential * np.sign(self.global_best_position - particle_positions[i])\n\n                # Clipping positions within bounds\n                particle_positions[i] = np.clip(particle_positions[i], lb, ub)\n\n                # Stochastic tunneling\n                if np.random.rand() < 0.1:\n                    tunneling_direction = np.random.normal(0, 1, self.dim)\n                    particle_positions[i] += tunneling_direction * np.random.uniform(0, 1)\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 14, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05480 with standard deviation 0.00428.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.06337540323942514, 0.05692295020628346, 0.055054808973293956, 0.05844251142404777, 0.05256573424852884, 0.05083852737489847, 0.05630767227227018, 0.05066432324183956, 0.04899551535161484]}}
{"id": "bd3e4ec2-b50d-4a3c-8681-4efdd1f597be", "fitness": 0.051586806119864326, "name": "EDAPSO", "description": "Enhanced Dynamic Adaptive Particle Swarm Optimization (EDAPSO) with Adaptive Velocity Control, Dynamic Subgroup Formation, and Dynamic Population Size to improve convergence on black box optimization.", "code": "import numpy as np\n\nclass EDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = self.population_size // 2  # form dynamic subgroups\n\n        while evaluations < self.budget:\n            # Dynamically adjust population size\n            self.population_size = max(10, int(self.population_size * 0.99))  # dynamic population reduction\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 15, "feedback": "The algorithm EDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05159 with standard deviation 0.00309.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.05614128876188951, 0.05236434832552517, 0.052195467207320045, 0.05184796276699777, 0.04834614156969097, 0.05030729058911554, 0.049972764975874995, 0.04658543219917588]}}
{"id": "f182702e-99b7-4d5c-95ee-189c5564f17d", "fitness": 0.055394099811673946, "name": "IEDAPSO", "description": "Improved Enhanced Dynamic Adaptive Particle Swarm Optimization (IEDAPSO) adds a local search mechanism and adaptive subgroup reformation to further enhance convergence efficiency in black box optimization.", "code": "import numpy as np\n\nclass IEDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = self.population_size // 2\n\n        while evaluations < self.budget:\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n                \n                # Local Search Enhancement\n                if evaluations < self.budget and np.random.rand() < 0.1:  # 10% chance for local search\n                    local_best_value = self.global_best_value\n                    local_best_position = self.global_best_position\n                    for _ in range(5):  # explore 5 nearby positions\n                        candidate_position = particle_positions[i] + np.random.uniform(-0.05, 0.05, self.dim)\n                        candidate_position = np.clip(candidate_position, lb, ub)\n                        candidate_value = func(candidate_position)\n                        evaluations += 1\n                        if candidate_value < local_best_value:\n                            local_best_value = candidate_value\n                            local_best_position = candidate_position\n                    if local_best_value < self.global_best_value:\n                        self.global_best_value = local_best_value\n                        self.global_best_position = local_best_position\n\n            # Adaptive subgroup reformation\n            if evaluations < self.budget and evaluations % (self.budget // 10) == 0:\n                subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 16, "feedback": "The algorithm IEDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05539 with standard deviation 0.00322.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.0602541344660521, 0.06051980604423601, 0.052195467207320045, 0.055606959270205314, 0.055800575179599354, 0.05030729058911554, 0.05358756459826386, 0.053754542267084204]}}
{"id": "95598926-8691-4978-aa97-1ffd69571c9f", "fitness": 0.05408130755695879, "name": "AMS_PSO", "description": "Adaptive Multi-Swarm Particle Swarm Optimization (AMS-PSO) integrating dynamic subgrouping and search space compression to enhance convergence and maintain diversity in black box optimization.", "code": "import numpy as np\n\nclass AMS_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n        self.num_swarms = 3  # Number of sub-swarms for diversity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        swarm_size = self.population_size // self.num_swarms\n\n        while evaluations < self.budget:\n            for swarm in range(self.num_swarms):\n                # Define sub-swarm boundaries\n                start_idx = swarm * swarm_size\n                end_idx = start_idx + swarm_size\n                \n                # Local subgroup best position initialization\n                subgroup_best_position, subgroup_best_value = None, float('inf')\n\n                for i in range(start_idx, min(end_idx, self.population_size)):\n                    if evaluations >= self.budget:\n                        break\n\n                    current_value = func(particle_positions[i])\n                    evaluations += 1\n\n                    if current_value < personal_best_values[i]:\n                        personal_best_values[i] = current_value\n                        personal_best_positions[i] = particle_positions[i]\n\n                    if current_value < self.global_best_value:\n                        self.global_best_value = current_value\n                        self.global_best_position = particle_positions[i]\n\n                    if current_value < subgroup_best_value:\n                        subgroup_best_value = current_value\n                        subgroup_best_position = particle_positions[i]\n\n                progress = evaluations / self.budget\n                c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n                c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n                w = self.w_start * (1 - progress) + self.w_end * progress\n\n                for i in range(start_idx, min(end_idx, self.population_size)):\n                    if evaluations >= self.budget:\n                        break\n\n                    r1, r2 = np.random.rand(), np.random.rand()\n                    cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                    social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i])\n                    particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                    particle_positions[i] += particle_velocities[i]\n\n                    # Compression: Adaptive search space reduction\n                    search_range_reduction = (self.budget - evaluations) / self.budget\n                    particle_positions[i] = np.clip(\n                        particle_positions[i],\n                        lb + search_range_reduction * (self.global_best_position - lb),\n                        ub - search_range_reduction * (ub - self.global_best_position)\n                    )\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 17, "feedback": "The algorithm AMS_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05408 with standard deviation 0.00279.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.05773533348210924, 0.05876429786000259, 0.052195467207320045, 0.053309226257718945, 0.05424380937068518, 0.05030729058911554, 0.05137968298603213, 0.052276101576456346]}}
{"id": "74ff0ec0-4717-4a18-a71e-7d9bd04c531f", "fitness": 0.05553289419117962, "name": "EDAPSO", "description": "Enhanced Dynamic Adaptive Particle Swarm Optimization (EDAPSO) with Adaptive Velocity Control, Dynamic Subgroup Formation, and Adaptive Mutation for Improved Exploration and Exploitation Balance in Black Box Optimization.", "code": "import numpy as np\n\nclass EDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.mutation_rate = 0.1  # adaptive mutation rate\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = self.population_size // 2  # form dynamic subgroups\n\n        while evaluations < self.budget:\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n            adaptive_mutation = self.mutation_rate * np.exp(-progress)\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Apply adaptive mutation\n                mutation_vector = np.random.normal(0, adaptive_mutation, self.dim)\n                particle_positions[i] += mutation_vector\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 18, "feedback": "The algorithm EDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05553 with standard deviation 0.00669.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.056722564792211494, 0.0682479173696402, 0.05280886652682015, 0.05237995722684463, 0.06285228284251954, 0.04876923863639959, 0.05048464967936395, 0.0605315511736928, 0.04699901947312424]}}
{"id": "8b9e5c96-4a15-4d95-a9ea-005443327a5c", "fitness": 0.0517221913881898, "name": "RefinedEDAPSO", "description": "Refined Enhanced Dynamic Adaptive Particle Swarm Optimization (Refined-EDAPSO) with Adaptive Subgroup Learning and Progressive Search Space Compression to improve convergence on black box optimization.", "code": "import numpy as np\n\nclass RefinedEDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = max(2, self.population_size // 4)  # smaller subgroup for focused search\n\n        while evaluations < self.budget:\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Progressive search space compression\n                compression_factor = 0.5 + 0.5 * progress\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + compression_factor * (self.global_best_position - lb),\n                                                ub - compression_factor * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 19, "feedback": "The algorithm RefinedEDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05172 with standard deviation 0.00303.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.05627181085925759, 0.05265473562088707, 0.052195467207320045, 0.0519681762376234, 0.0486288010228062, 0.05030729058911554, 0.05008873026571825, 0.04686415200779104]}}
{"id": "eeb2549a-d587-4ba6-9726-363ea327a04c", "fitness": 0.051690332853054896, "name": "EDAPSO", "description": "Enhanced Dynamic Adaptive Particle Swarm Optimization (EDAPSO) with Adaptive Velocity Control, Dynamic Subgroup Formation, and Modified Search Space Compression to improve convergence on black box optimization.", "code": "import numpy as np\n\nclass EDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = self.population_size // 2  # form dynamic subgroups\n\n        while evaluations < self.budget:\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n                \n                # Modified Search Space Compression Strategy\n                search_range_reduction = (self.budget - evaluations)**0.5 / self.budget**0.5\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 20, "feedback": "The algorithm EDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05169 with standard deviation 0.00392.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.05812138599137351, 0.0507181273173829, 0.052195467207320045, 0.05366047172051458, 0.04683846651364354, 0.05030729058911554, 0.05171682916312015, 0.04513439849183476]}}
{"id": "598f0c1f-3b2a-4a5d-928a-19e1debc102f", "fitness": 0.05563164145250991, "name": "EDAPSO", "description": "Enhanced Dynamic Adaptive Particle Swarm Optimization (EDAPSO) with Dynamic Velocity Bootstrapping for improved exploitation in black box optimization.", "code": "import numpy as np\n\nclass EDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = self.population_size // 2  # form dynamic subgroups\n\n        while evaluations < self.budget:\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Dynamic Velocity Bootstrapping\n                if np.random.rand() < 0.05:\n                    particle_velocities[i] = np.random.uniform(-0.1, 0.1, self.dim)\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 21, "feedback": "The algorithm EDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05563 with standard deviation 0.00658.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.0682479173696402, 0.05333172624495286, 0.052195467207320045, 0.06285228284251954, 0.04924385070935311, 0.05030729058911554, 0.0605315511736928, 0.04745412825280604]}}
{"id": "9f0331fd-f236-4794-ab04-2a8d902fa841", "fitness": 0.053268016888589426, "name": "AMPSO", "description": "Adaptive Multi-Swarm Particle Swarm Optimization (AMPSO) with dynamic inter-swarm communication and adaptive learning components for enhanced convergence on black box optimization problems.", "code": "import numpy as np\n\nclass AMPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # adaptive cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # adaptive social component\n        self.w_start, self.w_end = 0.9, 0.4    # adaptive inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        num_swarms = 3  # Divide population into multiple swarms\n        swarm_size = self.population_size // num_swarms\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for swarm_id in range(num_swarms):\n                swarm_indices = range(swarm_id * swarm_size, (swarm_id + 1) * swarm_size)\n                swarm_best_position, swarm_best_value = None, float('inf')\n\n                for i in swarm_indices:\n                    if evaluations >= self.budget:\n                        break\n\n                    current_value = func(particle_positions[i])\n                    evaluations += 1\n\n                    if current_value < personal_best_values[i]:\n                        personal_best_values[i] = current_value\n                        personal_best_positions[i] = particle_positions[i]\n\n                    if current_value < self.global_best_value:\n                        self.global_best_value = current_value\n                        self.global_best_position = particle_positions[i]\n\n                    if current_value < swarm_best_value:\n                        swarm_best_value = current_value\n                        swarm_best_position = particle_positions[i]\n\n                progress = evaluations / self.budget\n                c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n                c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n                w = self.w_start * (1 - progress) + self.w_end * progress\n\n                for i in swarm_indices:\n                    if evaluations >= self.budget:\n                        break\n\n                    r1, r2 = np.random.rand(), np.random.rand()\n                    cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                    social_velocity = c2 * r2 * (swarm_best_position - particle_positions[i])\n                    particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                    particle_positions[i] += particle_velocities[i]\n\n                    # Inter-swarm Communication: occasionally share global best information\n                    if np.random.rand() < 0.1:\n                        particle_positions[i] += np.random.rand() * (self.global_best_position - particle_positions[i])\n\n                    # Compression: Reduce the search space dynamically\n                    search_range_reduction = (self.budget - evaluations) / self.budget\n                    particle_positions[i] = np.clip(particle_positions[i],\n                                                    lb + search_range_reduction * (self.global_best_position - lb),\n                                                    ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 22, "feedback": "The algorithm AMPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05327 with standard deviation 0.00271.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.056062007381350365, 0.05781659388261484, 0.052195467207320045, 0.05177532709439603, 0.05338242103612589, 0.05030729058911554, 0.0499028472001426, 0.051449638923050434]}}
{"id": "7d57fd7b-029a-4d8d-8f2e-e0e4cd7c7358", "fitness": 0.05279634006710384, "name": "EDAPSO", "description": "Enhanced EDAPSO with a decoupled compression strategy to improve convergence precision.", "code": "import numpy as np\n\nclass EDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = self.population_size // 2  # form dynamic subgroups\n\n        while evaluations < self.budget:\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (personal_best_positions[i] - lb),\n                                                ub - search_range_reduction * (ub - personal_best_positions[i]))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 23, "feedback": "The algorithm EDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05280 with standard deviation 0.00330.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05654402228834654, 0.058832641001799635, 0.0535177641657385, 0.05221700779211724, 0.054315163082820694, 0.04942772184053934, 0.05032804244634914, 0.05234822367926639, 0.047636474306957055]}}
{"id": "ac1b8877-f539-4866-b108-4a0d1f3b99f8", "fitness": 0.055795357022568645, "name": "EDAPSO", "description": "Enhancing EDAPSO with a dynamic subgroup size based on function evaluations to improve convergence.", "code": "import numpy as np\n\nclass EDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, self.population_size * (1 - evaluations / self.budget))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, int(subgroup_size), replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 24, "feedback": "The algorithm EDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05580 with standard deviation 0.00644.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.0682479173696402, 0.053851364058936646, 0.052195467207320045, 0.06285228284251954, 0.04972817991121492, 0.05030729058911554, 0.0605315511736928, 0.04792360136748908]}}
{"id": "3e8056f4-54df-4777-915f-4bc5f7e026ed", "fitness": 0.05540471500259917, "name": "EDAPSORefined", "description": "Enhanced Dynamic Adaptive Particle Swarm Optimization (EDAPSO) with Adaptive Velocity Control, Dynamic Subgroup Formation, and Nonlinear Inertia Weight Decay for improved convergence on black box optimization.", "code": "import numpy as np\n\nclass EDAPSORefined:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = self.population_size // 2  # form dynamic subgroups\n\n        while evaluations < self.budget:\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            # Nonlinear decay of inertia weight\n            w = self.w_start * (1 - progress)**2 + self.w_end * progress**2\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 25, "feedback": "The algorithm EDAPSORefined got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05540 with standard deviation 0.00680.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05666598154653646, 0.0682479173696402, 0.05245445034770346, 0.05232889212822567, 0.06285228284251954, 0.04844222615000371, 0.050435798700917744, 0.0605315511736928, 0.046683334764152984]}}
{"id": "a6a0e86a-0f91-42b5-aa35-9b1deb703810", "fitness": 0.05657063517033781, "name": "HPESO", "description": "Hybrid Particle-Evolutionary Swarm Optimization (HPESO) introducing evolutionary mutation and elitist selection to enhance exploration and exploitation balance in black box optimization.", "code": "import numpy as np\n\nclass HPESO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.mutation_rate = 0.1  # rate for evolutionary mutation\n        self.elitism_rate = 0.1   # elitism rate for selection\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            num_elites = int(self.population_size * self.elitism_rate)\n            elites_indices = np.argpartition(personal_best_values, num_elites)[:num_elites]\n            elites_positions = personal_best_positions[elites_indices]\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                if i not in elites_indices:\n                    r1, r2 = np.random.rand(), np.random.rand()\n                    cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                    social_velocity = c2 * r2 * (self.global_best_position - particle_positions[i])\n                    particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n\n                    # Apply mutation\n                    if np.random.rand() < self.mutation_rate:\n                        mutation = np.random.normal(0, 0.1 * (ub - lb), self.dim)\n                        particle_positions[i] += mutation\n\n                    particle_positions[i] += particle_velocities[i]\n\n                    # Compression: Reduce the search space dynamically\n                    search_range_reduction = (self.budget - evaluations) / self.budget\n                    particle_positions[i] = np.clip(particle_positions[i],\n                                                    lb + search_range_reduction * (self.global_best_position - lb),\n                                                    ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 26, "feedback": "The algorithm HPESO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05657 with standard deviation 0.00686.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.05873455770843505, 0.06931097369906902, 0.053087928056891, 0.05422284600889138, 0.06380518143115266, 0.04902704015518844, 0.05225840363251211, 0.061440757882233354, 0.04724802795866723]}}
{"id": "01022098-fdb7-44d0-9978-d199589bf48d", "fitness": 0.04492523738164199, "name": "GIAPSO", "description": "Gradient-Inspired Adaptive Particle Swarm Optimization (GIAPSO) enhances convergence by incorporating gradient information for velocity updates alongside dynamic subgroup formation and adaptive parameters.", "code": "import numpy as np\n\nclass GIAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n        self.alpha = 0.01  # learning rate for gradient-inspired updates\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        subgroup_size = self.population_size // 2  # form dynamic subgroups\n\n        while evaluations < self.budget:\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            gradients = np.zeros((self.population_size, self.dim))\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Calculate the gradient approximately\n                epsilon = 1e-8  # small increment for numerical gradient computation\n                for d in range(self.dim):\n                    perturbed_position = particle_positions[i].copy()\n                    perturbed_position[d] += epsilon\n                    gradient_estimate = (func(perturbed_position) - func(particle_positions[i])) / epsilon\n                    gradients[i][d] = gradient_estimate\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                gradient_velocity = -self.alpha * gradients[i]  # gradient-inspired velocity update\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity + gradient_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 27, "feedback": "The algorithm GIAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04493 with standard deviation 0.00283.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.04669909157418051, 0.0505776118534339, 0.04653762926097749, 0.04310112339909988, 0.04665864320876356, 0.04294170143907561, 0.04151466619947464, 0.04494023403337577, 0.04135643546639656]}}
{"id": "878d48b2-7188-4353-874e-fbe7916963f4", "fitness": 0.05701823642795142, "name": "EDAPSO", "description": "Enhanced Dynamic Adaptive Particle Swarm Optimization (EDAPSO) with Adaptive Velocity Control, Dynamic Subgroup Formation, and Improved Exploration-Exploitation Balance through Adaptive Subgroup Sizes.", "code": "import numpy as np\n\nclass EDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 28, "feedback": "The algorithm EDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05702 with standard deviation 0.00563.", "error": "", "parent_ids": ["5b16bb07-22ba-493f-b62b-31bc29125698"], "operator": null, "metadata": {"aucs": [0.057399005043049844, 0.0682479173696402, 0.056908496717162915, 0.05300098142249865, 0.06285228284251954, 0.052525730349496924, 0.05108294084988185, 0.0605315511736928, 0.050615222083620015]}}
{"id": "4b828a22-4ce9-4659-a9ca-f123d27f1341", "fitness": 0.05703345311058436, "name": "EnhancedDAPSO", "description": "Dynamic Adaptive Particle Swarm Optimization with Enhanced Exploration via Chaos Theory and Adaptive Mutation for Improved Global Search Performance.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        # Add chaotic behavior to mutation\n        beta = 0.5  # mutation intensity\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 29, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05703 with standard deviation 0.00648.", "error": "", "parent_ids": ["878d48b2-7188-4353-874e-fbe7916963f4"], "operator": null, "metadata": {"aucs": [0.061422129807829906, 0.0682479173696402, 0.05294885256481818, 0.05667018575021643, 0.06285228284251954, 0.04889743127499524, 0.05460833881998728, 0.0605315511736928, 0.04712238839155969]}}
{"id": "7e06454e-aa57-486d-bca1-0aaf4a3cf679", "fitness": -Infinity, "name": "EnhancedDAPSO", "description": "Enhanced DAPSO with adaptive subgroup selection based on fitness diversity for improved convergence.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        # Add chaotic behavior to mutation\n        beta = 0.5  # mutation intensity\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Change: Adaptive subgroup size based on fitness diversity\n            fitness_diversity = np.std(personal_best_values)\n            subgroup_size = max(2, int(fitness_diversity / np.max(personal_best_values) * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 30, "feedback": "An exception occurred: ValueError('cannot convert float NaN to integer').", "error": "ValueError('cannot convert float NaN to integer')", "parent_ids": ["4b828a22-4ce9-4659-a9ca-f123d27f1341"], "operator": null, "metadata": {}}
{"id": "088e9041-90b3-40c0-8e02-75ce70bc261d", "fitness": 0.025305334844481706, "name": "EnhancedDAPSO", "description": "Enhanced Dynamic Adaptive Particle Swarm Optimization (EDAPSO) with Progressive Multi-Population Strategy and Adaptive Search Space Compression for Improved Global and Local Search.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n        self.num_subpopulations = 3  # introducing multi-population\n        self.positions = [np.random.uniform(0, 1, (self.population_size, self.dim)) for _ in range(self.num_subpopulations)]\n        self.velocities = [np.random.uniform(-1, 1, (self.population_size, self.dim)) for _ in range(self.num_subpopulations)]\n        self.personal_best_positions = [p.copy() for p in self.positions]\n        self.personal_best_values = [np.full(self.population_size, float('inf')) for _ in range(self.num_subpopulations)]\n\n    def chaotically_mutate(self, position, lb, ub, progress):\n        # Add chaotic behavior to mutation\n        beta = 0.5  # mutation intensity\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for idx in range(self.num_subpopulations):\n                subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n                subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n                subgroup_best_position, subgroup_best_value = None, float('inf')\n\n                for i in range(self.population_size):\n                    if evaluations >= self.budget:\n                        break\n\n                    current_value = func(self.positions[idx][i])\n                    evaluations += 1\n\n                    if current_value < self.personal_best_values[idx][i]:\n                        self.personal_best_values[idx][i] = current_value\n                        self.personal_best_positions[idx][i] = self.positions[idx][i]\n\n                    if current_value < self.global_best_value:\n                        self.global_best_value = current_value\n                        self.global_best_position = self.positions[idx][i]\n\n                    if i in subgroup_indices and current_value < subgroup_best_value:\n                        subgroup_best_value = current_value\n                        subgroup_best_position = self.positions[idx][i]\n\n                progress = evaluations / self.budget\n                c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n                c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n                w = self.w_start * (1 - progress) + self.w_end * progress\n\n                for i in range(self.population_size):\n                    if evaluations >= self.budget:\n                        break\n\n                    r1, r2 = np.random.rand(), np.random.rand()\n                    cognitive_velocity = c1 * r1 * (self.personal_best_positions[idx][i] - self.positions[idx][i])\n                    social_velocity = c2 * r2 * (subgroup_best_position - self.positions[idx][i] if i in subgroup_indices else self.global_best_position - self.positions[idx][i])\n                    self.velocities[idx][i] = w * self.velocities[idx][i] + cognitive_velocity + social_velocity\n                    self.positions[idx][i] += self.velocities[idx][i]\n\n                    # Adaptive mutation based on chaos theory\n                    self.positions[idx][i] = self.chaotically_mutate(self.positions[idx][i], lb, ub, progress)\n                    \n                    # Progressive search space compression\n                    search_range_reduction = (self.budget - evaluations) / self.budget\n                    self.positions[idx][i] = np.clip(self.positions[idx][i],\n                                                     lb + search_range_reduction * (self.global_best_position - lb),\n                                                     ub - search_range_reduction * (ub - self.global_best_position))\n        \n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 31, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02531 with standard deviation 0.00175.", "error": "", "parent_ids": ["4b828a22-4ce9-4659-a9ca-f123d27f1341"], "operator": null, "metadata": {"aucs": [0.027558891629743787, 0.028311232257129948, 0.026594034538935052, 0.02484501422505081, 0.025587115127427507, 0.023995219294857928, 0.023640937919740757, 0.024377679191603163, 0.022837889415846413]}}
{"id": "0b58708d-914e-485a-b2c5-00e700ff532a", "fitness": 0.05232045746843797, "name": "EnhancedDAPSO_Quantum", "description": "Enhanced Dynamic Adaptive Particle Swarm Optimization with Quantum-Inspired Mechanisms for Improved Exploration and Exploitation Balance.", "code": "import numpy as np\n\nclass EnhancedDAPSO_Quantum:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.5\n        chaos_factor = 4 * progress * (1 - progress)\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def quantum_position_update(self, position, best_position, lb, ub):\n        dist = np.linalg.norm(position - best_position)\n        new_position = position + np.random.uniform(-dist, dist, self.dim) * (best_position - position)\n        return np.clip(new_position, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n                particle_positions[i] = self.quantum_position_update(particle_positions[i], self.global_best_position, lb, ub)\n\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedDAPSO_Quantum got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05232 with standard deviation 0.00421.", "error": "", "parent_ids": ["4b828a22-4ce9-4659-a9ca-f123d27f1341"], "operator": null, "metadata": {"aucs": [0.060035427788868834, 0.05602915902849659, 0.05133962259338942, 0.05539956049163319, 0.05174512056578362, 0.047401912467829654, 0.05338521319799361, 0.049873726769472326, 0.04567437431247445]}}
{"id": "819471f8-bc89-48b8-9c5b-129e5336e4e9", "fitness": -Infinity, "name": "EnhancedDAPSO", "description": "Enhanced Dynamic Adaptive Particle Swarm Optimization with Quantum-inspired Initialization and Levy Flight-based Exploration for Robust Global Search.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n\n    def levy_flight(self, position, lb, ub):\n        # Introduce Levy Flight for exploration\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                 (np.math.gamma((1 + beta) / 2) * beta * \n                  2**((beta - 1) / 2)))**(1 / beta)\n        u = np.random.normal(0, sigma, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / np.abs(v)**(1 / beta)\n        new_position = position + 0.01 * step * (position - self.global_best_position)\n        return np.clip(new_position, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Quantum-inspired initialization\n        particle_positions = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)**2\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Levy Flight exploration\n                particle_positions[i] = self.levy_flight(particle_positions[i], lb, ub)\n\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 33, "feedback": "An exception occurred: AttributeError(\"'EnhancedDAPSO' object has no attribute 'chaotically_mutate'\").", "error": "AttributeError(\"'EnhancedDAPSO' object has no attribute 'chaotically_mutate'\")", "parent_ids": ["4b828a22-4ce9-4659-a9ca-f123d27f1341"], "operator": null, "metadata": {}}
{"id": "b3db6074-9c9c-4065-bc80-db907233e43e", "fitness": 0.05607453827011445, "name": "MPDEPSO", "description": "Multi-Phase Dynamic Equilibrium Particle Swarm Optimization (MPDEPSO) incorporating adaptive particle grouping and exploration-exploitation balance for enhanced optimization performance.", "code": "import numpy as np\n\nclass MPDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def adaptive_mutate(self, position, lb, ub, progress):\n        # Adaptive mutation for exploration-exploitation balance\n        beta = 0.5  # mutation intensity\n        variation_factor = np.sin(progress * np.pi)  # sinusoidal variation\n        mutation = beta * variation_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            dynamic_group_size = max(2, int((np.cos((evaluations / self.budget) * np.pi / 2) + 1) / 2 * self.population_size))  # dynamic group size\n            group_indices = np.random.choice(self.population_size, dynamic_group_size, replace=False)\n            group_best_position, group_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in group_indices and current_value < group_best_value:\n                    group_best_value = current_value\n                    group_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (group_best_position - particle_positions[i] if i in group_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation for improved exploration-exploitation\n                particle_positions[i] = self.adaptive_mutate(particle_positions[i], lb, ub, progress)\n\n                # Dynamic compression to focus search space\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 34, "feedback": "The algorithm MPDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05607 with standard deviation 0.00642.", "error": "", "parent_ids": ["4b828a22-4ce9-4659-a9ca-f123d27f1341"], "operator": null, "metadata": {"aucs": [0.05788260148927038, 0.0682479173696402, 0.05338720433301325, 0.053443830011719595, 0.06285228284251954, 0.04930255711741982, 0.051509135993884736, 0.0605315511736928, 0.04751376409986974]}}
{"id": "ab25869c-2c18-442e-b8c6-e55208d66c48", "fitness": 0.058188274045843306, "name": "EnhancedDAPSO", "description": "Introduce a slight increase in the chaotic mutation intensity to enhance exploration capability early on.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        # Add chaotic behavior to mutation\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 35, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05819 with standard deviation 0.00571.", "error": "", "parent_ids": ["4b828a22-4ce9-4659-a9ca-f123d27f1341"], "operator": null, "metadata": {"aucs": [0.06278108663886162, 0.0682479173696402, 0.055336988670926957, 0.05786012776471461, 0.06285228284251954, 0.05110143356485397, 0.05573275262874977, 0.0605315511736928, 0.04925032575863031]}}
{"id": "15fdd2a7-4c1b-4b68-8079-3521855fc4be", "fitness": 0.05651778240011941, "name": "EnhancedDAPSO", "description": "Increase chaos influence during the early stages to boost exploration.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        # Add chaotic behavior to mutation\n        beta = 0.65  # mutation intensity, increased from 0.6 to boost early exploration\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 36, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05652 with standard deviation 0.00626.", "error": "", "parent_ids": ["ab25869c-2c18-442e-b8c6-e55208d66c48"], "operator": null, "metadata": {"aucs": [0.05900447049395763, 0.0682479173696402, 0.053692304954041314, 0.05446374490458328, 0.06285228284251954, 0.04958845443811266, 0.052487797578846584, 0.0605315511736928, 0.0477915178456807]}}
{"id": "7fc53f63-0c42-4432-99f6-f2d019ede9a7", "fitness": 0.05777347130522113, "name": "EnhancedDAPSO", "description": "Introduce dynamic mutation intensity for enhanced flexibility in exploration.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        # Add chaotic behavior to mutation\n        beta_start, beta_end = 0.6, 0.4\n        beta = beta_start * (1 - progress) + beta_end * progress  # dynamic mutation intensity\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 37, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05777 with standard deviation 0.00538.", "error": "", "parent_ids": ["ab25869c-2c18-442e-b8c6-e55208d66c48"], "operator": null, "metadata": {"aucs": [0.06019035504328629, 0.0682479173696402, 0.056557824784684185, 0.05554180853037516, 0.06285228284251954, 0.05220736089882305, 0.0535222527104221, 0.0605315511736928, 0.05030988839354689]}}
{"id": "41fcfc40-5562-4304-be2e-0b418cf1c842", "fitness": 0.05672915490165967, "name": "RefinedEnhancedDAPSO", "description": "Leverage dynamic adjustment of exploration and exploitation with rank-based selection to enhance convergence and robustness in black box optimization.", "code": "import numpy as np\n\nclass RefinedEnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n\n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def rank_based_selection(self, values):\n        indices = np.argsort(values)\n        ranks = np.empty_like(indices)\n        ranks[indices] = np.arange(len(values))\n        probabilities = (len(values) - ranks) / np.sum(len(values) - ranks)\n        return np.random.choice(len(values), p=probabilities)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n            # Update the best particles more frequently by rank-based selection\n            best_particle_index = self.rank_based_selection(personal_best_values)\n            self.global_best_position = personal_best_positions[best_particle_index]\n            self.global_best_value = personal_best_values[best_particle_index]\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 38, "feedback": "The algorithm RefinedEnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05673 with standard deviation 0.00580.", "error": "", "parent_ids": ["ab25869c-2c18-442e-b8c6-e55208d66c48"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.0682479173696402, 0.05684504301129112, 0.052195467207320045, 0.06285228284251954, 0.052482628597738556, 0.05030729058911554, 0.0605315511736928, 0.05057965464043013]}}
{"id": "e102817a-2491-4b00-ad58-8edc069334a7", "fitness": -Infinity, "name": "EnhancedDAPSO", "description": "Introduce a mid-phase dynamic subgroup size increase to enhance particle diversity.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        # Add chaotic behavior to mutation\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            mid_phase = (evaluations / self.budget) > 0.25 and (evaluations / self.budget) < 0.75\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size) + (mid_phase * 10))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 39, "feedback": "An exception occurred: ValueError(\"Cannot take a larger sample than population when 'replace=False'\").", "error": "ValueError(\"Cannot take a larger sample than population when 'replace=False'\")", "parent_ids": ["ab25869c-2c18-442e-b8c6-e55208d66c48"], "operator": null, "metadata": {}}
{"id": "8a06483e-f23f-4061-bb7f-01fc35ee5b65", "fitness": 0.058188274045843306, "name": "EnhancedDAPSO", "description": "Introduce time-varying control parameters and adaptive subgrouping to enhance convergence precision and speed.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6\n        chaos_factor = 4 * progress * (1 - progress)\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int(((np.sin((evaluations / self.budget) * np.pi) + 1) / 2) * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05819 with standard deviation 0.00571.", "error": "", "parent_ids": ["ab25869c-2c18-442e-b8c6-e55208d66c48"], "operator": null, "metadata": {"aucs": [0.06278108663886162, 0.0682479173696402, 0.055336988670926957, 0.05786012776471461, 0.06285228284251954, 0.05110143356485397, 0.05573275262874977, 0.0605315511736928, 0.04925032575863031]}}
{"id": "ec570545-6b37-4577-855b-4ecf99350a22", "fitness": 0.056452303706011876, "name": "EnhancedDAPSO", "description": "Adjust the dynamic subgroup size calculation to enhance convergence speed.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        # Add chaotic behavior to mutation\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.cos((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb),\n                                                ub - search_range_reduction * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 41, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05645 with standard deviation 0.00596.", "error": "", "parent_ids": ["ab25869c-2c18-442e-b8c6-e55208d66c48"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.0682479173696402, 0.055953795927810246, 0.052195467207320045, 0.06285228284251954, 0.05166698210550813, 0.05030729058911554, 0.0605315511736928, 0.049794887455311354]}}
{"id": "ae3dc198-d644-4bc9-bdfd-cdadea2c6be8", "fitness": 0.05885965723872856, "name": "EnhancedDAPSO", "description": "Introduce a dynamic resistance factor to improve convergence stability and reliability.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05886 with standard deviation 0.00319.", "error": "", "parent_ids": ["ab25869c-2c18-442e-b8c6-e55208d66c48"], "operator": null, "metadata": {"aucs": [0.06424177924986141, 0.061087349175004024, 0.06319505423973082, 0.059216130155817503, 0.05634465721826898, 0.05823195195532416, 0.05704461561347152, 0.05428778650452715, 0.056087591036551476]}}
{"id": "1945e5e7-d85b-4b28-9f62-9477f2c700fc", "fitness": 0.05406568495383449, "name": "EnhancedDAPSO", "description": "Introduce an enhanced dynamic resistance factor to further improve convergence stability by slightly increasing its influence.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.03  # Introduce enhanced dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 43, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05407 with standard deviation 0.00276.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.05824813466261136, 0.05820300054916938, 0.052195467207320045, 0.05377696157611822, 0.053729572763653866, 0.05030729058911554, 0.051829105478811144, 0.051781073074521844]}}
{"id": "bf66b64e-28fc-4eef-81db-4b885e027cea", "fitness": 0.05735763265173767, "name": "EnhancedDAPSO", "description": "Introduce a dynamic resistance factor to improve convergence stability and reliability, with enhanced subgroup selection.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi * 2) + 1) / 2 * self.population_size))  # enhanced dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00389.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.06424177924986141, 0.05738232876336058, 0.0620033163420185, 0.059216130155817503, 0.05298500670110129, 0.057183426156959816, 0.05704461561347152, 0.05106727817553336, 0.05509481270751504]}}
{"id": "ef4074ad-ca7d-4e5b-9209-2c6956cb4639", "fitness": 0.05855295374261604, "name": "EnhancedDAPSO", "description": "Integrate adaptive learning rates and a chaotic local search to enhance global exploration and local exploitation balance.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotic_local_search(self, position, lb, ub, progress):\n        epsilon = 1e-5\n        gamma = 0.7  # Focus more on exploitation in later stages\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map for chaos\n        mutation = gamma * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Enhanced local search with chaotic behavior\n                particle_positions[i] = self.chaotic_local_search(particle_positions[i], lb, ub, progress)\n\n                # Adaptive search range compression with resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 45, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05855 with standard deviation 0.00370.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.0657291659761633, 0.06195627235386569, 0.05982407880257257, 0.0605455507185616, 0.05715857192921514, 0.05519258944794592, 0.05831160323808182, 0.055078075037553464, 0.053180676179584885]}}
{"id": "99d3bc92-fedf-49d9-8595-34192ea17953", "fitness": 0.057505107400063746, "name": "AdaptiveSubgroupDAPSO", "description": "Integrate adaptive resistance and a dynamic subgroup topology for enhanced convergence stability and efficiency.", "code": "import numpy as np\n\nclass AdaptiveSubgroupDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Adaptive Resistance and Compression: Dynamic resistance factor\n                resistance = 0.02 * (1 - progress)  # Resistance decreases over time to allow finer search\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 46, "feedback": "The algorithm AdaptiveSubgroupDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05751 with standard deviation 0.00453.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.06416576245989047, 0.05602915902849659, 0.06393348030118318, 0.059143042442655, 0.05174512056578362, 0.05892365416761214, 0.05697295520673662, 0.049873726769472326, 0.05675906565874378]}}
{"id": "c812eead-f678-455b-a1df-c2fe436d833e", "fitness": 0.058516334403420536, "name": "EnhancedDAPSO", "description": "Introduce adaptive acceleration to improve particle velocity updates for faster convergence.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity * (1 + 0.001 * evaluations)  # Adaptive acceleration\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 47, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05852 with standard deviation 0.00392.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.0653792771242413, 0.05866422203533306, 0.06337034698637778, 0.06023453813648527, 0.05416020863740967, 0.058387864159543, 0.058015866299924834, 0.05219880806736721, 0.056235878184102694]}}
{"id": "374eedf9-5a5e-406e-a149-73f7986cb7b8", "fitness": 0.05612062849344257, "name": "EnhancedDAPSO", "description": "Implement a dynamic convergence acceleration mechanism by progressively intensifying the cognitive and social factors based on convergence rate.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.0, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.0   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n        \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n                    last_improvement = evaluations\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            time_since_last_improvement = (evaluations - last_improvement) / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress + time_since_last_improvement\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress + time_since_last_improvement\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                resistance = 0.02\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 48, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05612 with standard deviation 0.00417.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.05940739331465861, 0.05602915902849659, 0.06419985628957425, 0.054830170283433066, 0.05174512056578362, 0.05916731548682519, 0.05283945211050178, 0.049873726769472326, 0.0569934625922377]}}
{"id": "ce507164-18f4-49a0-a70a-d3a4cb2428e8", "fitness": 0.05717194370006293, "name": "AdaptiveDAPSO", "description": "Introduce a self-adaptive inertia weight and resistance factor based on convergence speed to balance exploration and exploitation dynamically.", "code": "import numpy as np\n\nclass AdaptiveDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        last_best_value = float('inf')\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            convergence_speed = (last_best_value - self.global_best_value) / last_best_value if last_best_value != float('inf') else 0\n            last_best_value = self.global_best_value\n\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = (self.w_start * (1 - progress) + self.w_end * progress) * (1 + convergence_speed)\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02 * (1 - convergence_speed)  # Adaptive resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 49, "feedback": "The algorithm AdaptiveDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05717 with standard deviation 0.00436.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.064733263513305, 0.060659697067224516, 0.05778950158950069, 0.060967325962442365, 0.05738150591373048, 0.05204898526875379, 0.05703939907619526, 0.05376433166498029, 0.05016348324443398]}}
{"id": "8d5f4304-3b98-4c6d-b9f8-f4df1df77ecd", "fitness": 0.05465854352085442, "name": "EnhancedDAPSO", "description": "Enhance the inertia weight decay function by using a cosine annealing schedule for smoother convergence.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_end + (self.w_start - self.w_end) * (1 + np.cos(np.pi * progress)) / 2  # changed line\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 50, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05466 with standard deviation 0.00324.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.057984322354510454, 0.05611072597745648, 0.060823531102438766, 0.05353603971994125, 0.05182011197620784, 0.056082015760453574, 0.05159750850448197, 0.04994601565698498, 0.05402662063521446]}}
{"id": "2d9054a9-8e81-481e-855d-31678c923eec", "fitness": 0.05839969981391554, "name": "EnhancedDAPSO_V2", "description": "Introduce adaptive feedback-driven resistance based on past performance to enhance convergence reliability and precision.", "code": "import numpy as np\n\nclass EnhancedDAPSO_V2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n        self.previous_best_value = float('inf')\n        self.resistance = 0.02\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6\n        chaos_factor = 4 * progress * (1 - progress)\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def update_resistance(self, improvement):\n        if improvement:\n            self.resistance *= 0.95  # Decrease resistance if improvement is observed\n        else:\n            self.resistance *= 1.05  # Increase resistance if no improvement is observed\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            improvement = self.global_best_value < self.previous_best_value\n            self.update_resistance(improvement)\n            self.previous_best_value = self.global_best_value\n\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + self.resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + self.resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 51, "feedback": "The algorithm EnhancedDAPSO_V2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05840 with standard deviation 0.00450.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.06611079750295978, 0.05732060950359674, 0.06361009581449684, 0.06088864986218767, 0.05293005309845156, 0.05861943460902663, 0.05863938487174791, 0.05101500708820095, 0.05646326597457174]}}
{"id": "f77c5e98-6b7c-4490-9ef5-7acf21e44eb0", "fitness": 0.057200527398191384, "name": "EnhancedDAPSO", "description": "Introduce a dynamic subgroup size based on cosine adjustment for improved diversity and convergence.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.cos((evaluations / self.budget) * np.pi / 2) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 52, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05720 with standard deviation 0.00378.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.06372927954266372, 0.05738232876336058, 0.0620033163420185, 0.05875514134601012, 0.05298500670110129, 0.057183426156959816, 0.05660415684856002, 0.05106727817553336, 0.05509481270751504]}}
{"id": "3169c9e7-80ac-4e0f-aa15-3da03bce626f", "fitness": 0.05885965723872856, "name": "EnhancedDAPSO", "description": "Introduce asynchronous subgroup updates to improve convergence speed and diversity.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n\n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Introduce asynchronous subgroup updates\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 53, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05886 with standard deviation 0.00319.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.06424177924986141, 0.061087349175004024, 0.06319505423973082, 0.059216130155817503, 0.05634465721826898, 0.05823195195532416, 0.05704461561347152, 0.05428778650452715, 0.056087591036551476]}}
{"id": "53a6f1e6-c55f-4b0a-87fb-a6a391925df2", "fitness": 0.05635480342320446, "name": "EnhancedDAPSO", "description": "Apply an enhanced mutation strategy using a chaotic sine map to boost exploration in the search space.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = np.sin(np.pi * progress)  # changed to sine map for mutation\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 54, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05635 with standard deviation 0.00497.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.06629133396339282, 0.058055337439923416, 0.05610071843358333, 0.06104456900805333, 0.05358986863473336, 0.05178227782088218, 0.05878592645901359, 0.051644961463800154, 0.04989823758545797]}}
{"id": "e875cbc2-8ce6-4007-ac14-02b30bf828fb", "fitness": 0.05244077667975032, "name": "HybridChaoticDE", "description": "Introduce a hybrid chaos-enhanced differential evolution strategy to boost exploration and convergence.", "code": "import numpy as np\n\nclass HybridChaoticDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.mutation_factor_start, self.mutation_factor_end = 0.9, 0.5\n        self.crossover_prob_start, self.crossover_prob_end = 0.9, 0.2\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_perturb(self, vector, lb, ub, progress):\n        beta = 0.7 # increased perturbation intensity\n        chaos_factor = 4 * progress * (1 - progress) # logistic map\n        perturbation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(vector + perturbation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(population[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = population[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = population[i]\n\n            progress = evaluations / self.budget\n            mutation_factor = self.mutation_factor_start * (1 - progress) + self.mutation_factor_end * progress\n            crossover_prob = self.crossover_prob_start * (1 - progress) + self.crossover_prob_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                trial_vector = population[a] + mutation_factor * (population[b] - population[c])\n                trial_vector = self.chaotically_perturb(trial_vector, lb, ub, progress)\n                \n                crossover_mask = np.random.rand(self.dim) < crossover_prob\n                offspring = np.where(crossover_mask, trial_vector, population[i])\n                offspring_value = func(offspring)\n                evaluations += 1\n\n                if offspring_value < personal_best_values[i]:\n                    personal_best_values[i] = offspring_value\n                    personal_best_positions[i] = offspring\n\n                if offspring_value < self.global_best_value:\n                    self.global_best_value = offspring_value\n                    self.global_best_position = offspring\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 55, "feedback": "The algorithm HybridChaoticDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05244 with standard deviation 0.00365.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.059060639831420514, 0.056362839435385115, 0.052335990275632405, 0.05452151905408098, 0.05205101315506977, 0.04833720844363176, 0.05254587132085553, 0.05016824582852264, 0.04658366277315418]}}
{"id": "07ac7628-c079-438b-b4ea-63b142b8cdd2", "fitness": 0.057775072129486106, "name": "EnhancedDAPSO", "description": "Enhance convergence through adaptive learning rates and dynamic resistance factors for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n\n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n\n    def adapt_learning_rates(self, progress):\n        decay_factor = 0.01\n        learning_rate = 0.1 * np.exp(-decay_factor * progress)\n        return learning_rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            learning_rate = self.adapt_learning_rates(progress)\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + learning_rate * (cognitive_velocity + social_velocity)\n                particle_positions[i] += particle_velocities[i]\n\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                resistance = 0.02\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 56, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05778 with standard deviation 0.00466.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.06520192888209886, 0.06363329095568981, 0.0561775197349863, 0.060064057537635995, 0.05866549227338813, 0.05187332710868753, 0.05784873792435774, 0.05651723908119788, 0.049994055667332726]}}
{"id": "da5baefc-26f9-439b-92f3-58ad0059d255", "fitness": 0.05590901706005216, "name": "EnhancedDAPSO", "description": "Introduce a dynamic resistance factor with a Gaussian mutation to improve convergence stability and reliability.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def gaussian_mutate(self, position, lb, ub, progress):\n        sigma = 0.1  # standard deviation for the Gaussian mutation\n        mutation = np.random.normal(0, sigma * (1 - progress), self.dim)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on Gaussian distribution\n                particle_positions[i] = self.gaussian_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 57, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05591 with standard deviation 0.00444.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.06489599393364609, 0.05764427883958301, 0.056418247874890914, 0.059798048326852316, 0.05322719730436376, 0.05209292125488785, 0.05759813974971306, 0.051301271999919074, 0.05020505425661337]}}
{"id": "a80e41e3-53b4-41dc-84bf-fc7a61f68ece", "fitness": 0.051731675679622766, "name": "EnhancedDAPSO", "description": "Incorporate a dynamic neighborhood influence and self-adaptive velocity scaling to balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            neighborhood_size = max(5, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                local_neighborhood_indices = (np.arange(i, i + neighborhood_size) % self.population_size).astype(int)\n                neighborhood_best_value = float('inf')\n                neighborhood_best_position = None\n                for idx in local_neighborhood_indices:\n                    if personal_best_values[idx] < neighborhood_best_value:\n                        neighborhood_best_value = personal_best_values[idx]\n                        neighborhood_best_position = personal_best_positions[idx]\n                \n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (neighborhood_best_position - particle_positions[i])\n                velocity_scaler = 1 + 0.5 * np.sin(2 * np.pi * progress)\n                particle_velocities[i] = w * particle_velocities[i] + velocity_scaler * (cognitive_velocity + social_velocity)\n                particle_positions[i] += particle_velocities[i]\n\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 58, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05173 with standard deviation 0.00317.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.05698255106898242, 0.056165838777135324, 0.05233606427231141, 0.05261852797218136, 0.051870723545651476, 0.04832930758494891, 0.050714440687602225, 0.04999478022678616, 0.04657284698100561]}}
{"id": "55b7e3ff-e0b2-4615-8fd0-b07e5041fa31", "fitness": 0.055654499171937016, "name": "EntropyDAPSO", "description": "Incorporate an entropy-based dynamic mutation rate to enhance exploration and adaptivity in convergence.", "code": "import numpy as np\n\nclass EntropyDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def entropy_based_mutate(self, position, lb, ub, progress, evaluations):\n        beta = 0.6  # base mutation intensity\n        # Calculate entropy-like measure to adjust mutation intensity\n        entropy = -progress * np.log(progress + 1e-10) - (1 - progress) * np.log(1 - progress + 1e-10)\n        mutation_intensity = beta * (1 + entropy)\n        mutation = mutation_intensity * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on an entropy-like measure\n                particle_positions[i] = self.entropy_based_mutate(particle_positions[i], lb, ub, progress, evaluations)\n\n                # Compression with added resistance\n                resistance = 0.02\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 59, "feedback": "The algorithm EntropyDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05565 with standard deviation 0.00610.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.06652703367913704, 0.05942817851485782, 0.05223570854628923, 0.06126387407722944, 0.05485994470496003, 0.0482303811899053, 0.058998228465194424, 0.05287229176906694, 0.0464748516007929]}}
{"id": "8ce52672-4f1c-44e2-b604-80748be268c3", "fitness": 0.057616505365964014, "name": "EnhancedDAPSO", "description": "Introduce a multi-phase exploration-exploitation strategy with adaptive chaotic perturbation to enhance convergence and solution quality.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n        self.phase_switch = 0.5  # Switch from exploration to exploitation\n\n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        exploration_phase = True\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            if progress > self.phase_switch:\n                exploration_phase = False\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Apply chaotic mutation during exploration phase\n                if exploration_phase:\n                    particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 60, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05762 with standard deviation 0.00411.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.06424177924986141, 0.05705812379160091, 0.06319505423973082, 0.059216130155817503, 0.05268965902306866, 0.05823195195532416, 0.05704461561347152, 0.050783643228249686, 0.056087591036551476]}}
{"id": "e6962654-f35f-43c2-995b-bafc9b14ce15", "fitness": 0.05750520049356674, "name": "EnhancedDAPSO", "description": "Introduce a multi-phase exploration-exploitation strategy with adaptive subgroup dynamics and inertia for enhanced convergence.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((0.5 * evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # more adaptive subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Multi-phase compression to enhance exploitation\n                resistance = 0.02\n                phase_factor = 0.5 + 0.5 * np.cos(np.pi * progress)  # smooth transition between exploration and exploitation\n                search_range_reduction = phase_factor * (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05751 with standard deviation 0.00300.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.06093905970996949, 0.06042034836470733, 0.06272511005543568, 0.05623109478680488, 0.05575998621681244, 0.05783320169051165, 0.054187033186269606, 0.053735157567387004, 0.055715812864202574]}}
{"id": "ca2472a2-c5ee-491a-a278-9d0461e1d3b0", "fitness": 0.05736463450788606, "name": "AdaptiveChaosPSO", "description": "Implement a self-adaptive chaos-enhanced PSO with dynamic inertia and subgroup selection for improved convergence stability and reliability.", "code": "import numpy as np\n\nclass AdaptiveChaosPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Dynamic resistance factor improves convergence by adapting search range\n                resistance = 0.01 + 0.01 * (1 - progress)  # Refined resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 62, "feedback": "The algorithm AdaptiveChaosPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00344.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.06363787397023213, 0.05856854454389715, 0.06143430962671359, 0.058670184819988536, 0.054071125587734525, 0.05666642072802597, 0.056521929801320314, 0.0521124992676022, 0.054598822225460086]}}
{"id": "cd08c58c-5625-446e-ad97-af7bca70b49c", "fitness": 0.05729439528852387, "name": "EnhancedDAPSO", "description": "Introduce a probability factor to dynamic resistance for enhanced adaptability.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02 * np.random.rand()  # Introduce probability factor to dynamic resistance\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 63, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05729 with standard deviation 0.00338.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.05912751436612107, 0.06377501220789161, 0.06053663836601231, 0.05458146943689224, 0.05880780874621716, 0.055804760271991216, 0.052603062241761034, 0.05665899981831202, 0.05375429214151617]}}
{"id": "bd8f7054-6f09-4631-9835-d0e03acf1111", "fitness": 0.057722670208678904, "name": "EnhancedDAPSO", "description": "Fine-tune the dynamic subgroup size calculation to improve exploration and convergence balance.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int(((np.sin((evaluations / self.budget) * np.pi) + 1) / 2)**0.5 * self.population_size))  # Adjusted dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 64, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05772 with standard deviation 0.00401.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.06424177924986141, 0.05739230792622918, 0.06318058321413944, 0.059216130155817503, 0.05299255814116155, 0.058247998241774845, 0.05704461561347152, 0.051073924445277474, 0.0561141348903772]}}
{"id": "7806c9ea-458b-47f6-888d-7a224649ff46", "fitness": 0.057505107400063746, "name": "EnhancedDAPSO", "description": "Introduce an adaptive resistance factor proportionate to the current progress to improve convergence.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 4 * progress * (1 - progress)  # logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02 * (1 - progress)  # Introduce adaptive resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 65, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05751 with standard deviation 0.00453.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.06416576245989047, 0.05602915902849659, 0.06393348030118318, 0.059143042442655, 0.05174512056578362, 0.05892365416761214, 0.05697295520673662, 0.049873726769472326, 0.05675906565874378]}}
{"id": "5ad954a6-9380-46a2-806c-3752daab8c34", "fitness": 0.05894669731543764, "name": "EnhancedDAPSO", "description": "Introduce a dynamic chaos factor to enhance diversification and convergence speed.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)  # updated logistic map with a dynamic base\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 66, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05895 with standard deviation 0.00401.", "error": "", "parent_ids": ["ae3dc198-d644-4bc9-bdfd-cdadea2c6be8"], "operator": null, "metadata": {"aucs": [0.06539101833298588, 0.06458753861095912, 0.05881473167526163, 0.06024530256838867, 0.059537556256707314, 0.0542693931294389, 0.05802624309739057, 0.057355822441682136, 0.05229266972612456]}}
{"id": "691219e3-148e-41a7-8dc4-dbc1a1f241ba", "fitness": 0.056559332393119326, "name": "EnhancedDAPSO", "description": "Introduce a fitness-based adaptive control mechanism to dynamically balance exploration and exploitation phases to improve convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  \n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def adaptive_control(self, evaluations, personal_best_values):\n        fitness_range = np.max(personal_best_values) - np.min(personal_best_values)\n        fitness_ratio = (self.budget - evaluations) / self.budget\n        return fitness_ratio * (0.5 + (fitness_range / (np.mean(personal_best_values) + 1e-9)))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            adapt_factor = self.adaptive_control(evaluations, personal_best_values)\n            c1 *= adapt_factor\n            c2 *= adapt_factor\n            w *= adapt_factor\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                resistance = 0.02\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05656 with standard deviation 0.00553.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.06661030117416122, 0.05860206221530362, 0.05685692783321994, 0.06256439801937996, 0.05895062446089594, 0.047638722503849595, 0.05187523250540804, 0.05252021733311529, 0.05341550549274032]}}
{"id": "585524e0-ad1f-4331-bc70-f20e3df62756", "fitness": 0.054642943753807134, "name": "EnhancedDAPSO_v2", "description": "Introduce adaptive inertia and chaotic subgroup dynamics to boost convergence efficiency and exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedDAPSO_v2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.0, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.0  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.2    # more dynamic inertia weight for better adaptation\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.7  # slightly more intense mutation to improve exploration\n        chaos_factor = 0.5 + 4.0 * progress * (1 - progress)  # more aggressive logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  \n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Enhanced compression strategy with adaptive resistance\n                resistance = 0.05  # increased resistance factor for more stable convergence\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 68, "feedback": "The algorithm EnhancedDAPSO_v2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05464 with standard deviation 0.00338.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.060459455741095125, 0.0590889908233998, 0.055295655514543296, 0.05579076456703147, 0.0545509542512681, 0.05105714791634641, 0.05376282386382902, 0.05257555635468658, 0.04920514475206439]}}
{"id": "0e566e56-2db2-4fbf-b4fd-a604516c0b43", "fitness": 0.05143443218477701, "name": "EnhancedDAPSOPlus", "description": "Introduce an adaptive learning rate and dynamic diversity promotion to improve convergence and exploration balance.", "code": "import numpy as np\n\nclass EnhancedDAPSOPlus:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n        self.adaptive_learning_rate = 0.1  # initial adaptive learning rate\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def promote_diversity(self, positions, lb, ub):\n        max_dist = np.linalg.norm(ub - lb)\n        for i in range(len(positions)):\n            if np.random.rand() < 0.1:\n                random_direction = np.random.uniform(-1, 1, self.dim)\n                random_distance = np.random.uniform(0, max_dist) * self.adaptive_learning_rate\n                positions[i] = np.clip(positions[i] + random_direction * random_distance, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            self.adaptive_learning_rate = max(0.05, (self.global_best_value - np.mean(personal_best_values)) / (np.std(personal_best_values) + 1e-9))\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n            self.promote_diversity(particle_positions, lb, ub)\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 69, "feedback": "The algorithm EnhancedDAPSOPlus got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05143 with standard deviation 0.00317.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.05602915902849659, 0.05197530042617582, 0.052195467207320045, 0.05174512056578362, 0.04800292048463317, 0.05030729058911554, 0.049873726769472326, 0.0462603459088069]}}
{"id": "125b3775-b37a-46d0-a2e4-a1da18d06346", "fitness": 0.05647114274182737, "name": "EnhancedDAPSO", "description": "Introduce a self-adaptive chaos factor and a dynamic subgroup interaction model to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5\n        self.c2_start, self.c2_end = 0.5, 2.5\n        self.w_start, self.w_end = 0.9, 0.4\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6\n        chaos_factor = 0.5 + 3.5 * np.sin(np.pi * progress) * (1 - progress)\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                if i in subgroup_indices:\n                    social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i])\n                else:\n                    social_velocity = c2 * r2 * (self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                resistance = 0.02\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 70, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05647 with standard deviation 0.00380.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.0635618170039628, 0.056781374081413816, 0.06042169152499033, 0.05860295070842236, 0.05243537116540964, 0.05573692854593282, 0.05645814455973175, 0.05053857963930386, 0.05370342744727896]}}
{"id": "007e554c-ae89-4c89-a30b-cff0b3434ffd", "fitness": 0.05626371107981862, "name": "EnhancedDAPSO", "description": "Introduce a dynamic Lvy flight mutation to enhance exploration and fine-tuning capabilities.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def levy_flight(self, position, lb, ub, progress):\n        alpha = 0.01  # Lvy flight intensity factor\n        beta = 1.5    # Lvy flight distribution parameter\n        sigma_u = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                   (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        sigma_v = 1\n\n        u = np.random.normal(0, sigma_u, self.dim)\n        v = np.random.normal(0, sigma_v, self.dim)\n        step = u / np.abs(v) ** (1 / beta)\n\n        mutation = alpha * step * (progress * np.random.rand(self.dim))\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Lvy flight mutation to enhance exploration\n                particle_positions[i] = self.levy_flight(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically\n                resistance = 0.02\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 71, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05626 with standard deviation 0.00290.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.06054420859309939, 0.06070028566290275, 0.05885084942148189, 0.05585705691648357, 0.056018065025390196, 0.054289507580252416, 0.053822274283006344, 0.0539842079465368, 0.05230694428921423]}}
{"id": "25674420-fcc2-44b8-be16-a74c3f29e5e1", "fitness": 0.057397376281045034, "name": "RefinedDAPSO", "description": "Utilize adaptive tunneling with multiple chaotic maps to enhance exploration and convergence fidelity across diverse landscapes.", "code": "import numpy as np\n\nclass RefinedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5\n        self.c2_start, self.c2_end = 0.5, 2.5\n        self.w_start, self.w_end = 0.9, 0.4\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotic_map(self, progress):\n        # Implementing a composite chaotic system\n        logistic = 4 * progress * (1 - progress)\n        sinusoidal = np.sin(np.pi * progress)\n        return 0.5 * logistic + 0.5 * sinusoidal\n\n    def adaptive_tunneling(self, position, lb, ub, progress):\n        beta = 0.6\n        chaos_factor = self.chaotic_map(progress)\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive tunneling using composite chaotic maps\n                particle_positions[i] = self.adaptive_tunneling(particle_positions[i], lb, ub, progress)\n\n                # Dynamic search space compression\n                resistance = 0.02\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 72, "feedback": "The algorithm RefinedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05740 with standard deviation 0.00454.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.06642824089665222, 0.057351745865373616, 0.06000436712024704, 0.06116974982915757, 0.05295774416900745, 0.05536604544246826, 0.058906287779562416, 0.05104133302316649, 0.05335087240377023]}}
{"id": "78464335-0aaa-41b3-86af-d86ba9f47f13", "fitness": 0.057728087823040584, "name": "EnhancedDAPSO", "description": "Enhance global search efficiency with an improved subgroup selection strategy.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)  # updated logistic map with a dynamic base\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int(((np.cos((evaluations / self.budget) * np.pi) + 1) / 2) * self.population_size))  # refined subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 73, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05773 with standard deviation 0.00421.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.06548214483231751, 0.06197798518407749, 0.0573697090814379, 0.06032749617344835, 0.0571814394043455, 0.052964268682624494, 0.058104848845179524, 0.05510123425522084, 0.051043663948713625]}}
{"id": "f91496a1-5ef9-4060-b143-03352978c931", "fitness": 0.05244330063951906, "name": "EnhancedDAPSO", "description": "Integrate an adaptive memory and dynamic neighborhood strategy into EnhancedDAPSO to improve exploitation and convergence precision.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5\n        self.c2_start, self.c2_end = 0.5, 2.5\n        self.w_start, self.w_end = 0.9, 0.4\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n        self.memory = []\n\n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n                # Adaptive Memory\n                if len(self.memory) < self.population_size:\n                    self.memory.append(particle_positions[i].copy())\n                else:\n                    replace_index = np.random.randint(0, len(self.memory))\n                    self.memory[replace_index] = particle_positions[i].copy()\n\n                # Dynamic Neighborhood\n                if i in subgroup_indices:\n                    neighborhood_positions = [self.memory[np.random.randint(0, len(self.memory))] for _ in range(3)]\n                    neighborhood_best = min(neighborhood_positions, key=lambda pos: func(pos))\n                    if func(neighborhood_best) < func(particle_positions[i]):\n                        particle_positions[i] = neighborhood_best\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05244 with standard deviation 0.00274.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.056738116365275704, 0.05449620726981608, 0.052195467207320045, 0.05239539539979987, 0.050329739636175375, 0.05030729058911554, 0.050499963914583335, 0.04850696669039656]}}
{"id": "5f0fbe29-1409-4e1a-b605-16c21eb49628", "fitness": 0.05637592755469409, "name": "EnhancedDAPSO", "description": "Enhance the convergence by tuning dynamic factors and subgroup exploration intensity.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.7, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.7   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)  # updated logistic map with a dynamic base\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 75, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05638 with standard deviation 0.00335.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.061972554937717694, 0.05738105151456119, 0.061105815498887606, 0.05716553063939256, 0.052986050878428226, 0.05634288682406785, 0.05508169238040572, 0.051069155845477776, 0.054278609473308204]}}
{"id": "7366a003-63da-4133-b3b4-194ac12906f7", "fitness": 0.05597022233598002, "name": "EnhancedDAPSO", "description": "Introduce dynamic subgroup resistance for adaptive search pressure and improved convergence.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)  # updated logistic map with a dynamic base\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02 * (subgroup_best_value if i in subgroup_indices else 1)  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 76, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05597 with standard deviation 0.00428.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.06323092884426385, 0.061792497065562335, 0.054076374122153825, 0.056748198182764664, 0.0573222619406788, 0.051041503252145404, 0.05508390177542222, 0.05532373020121972, 0.0491126056396094]}}
{"id": "42c50784-1bb4-4cf5-b43a-c7083aa2ec01", "fitness": 0.0574528466999579, "name": "RefinedDAPSO", "description": "Integrate adaptive chaos dynamics and a hybridized particle interaction model to enhance search efficiency and solution precision in dynamic environments.", "code": "import numpy as np\n\nclass RefinedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.7  # mutation intensity, increased for enhanced exploration\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)  # updated logistic map with dynamic base\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a refined resistance\n                resistance = 0.05  # Increased dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 77, "feedback": "The algorithm RefinedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05745 with standard deviation 0.00523.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.05912210621648761, 0.0678667303479672, 0.056949393846036145, 0.05457379524836359, 0.06250996626771488, 0.052580098641395234, 0.05259461326577686, 0.06020469153040453, 0.05067422493547502]}}
{"id": "787a23be-8c5b-459c-b960-2b90d69de65a", "fitness": 0.05811569146202137, "name": "EnhancedDAPSO", "description": "Introduce adaptive particle grouping and feedback-driven parameter tuning to enhance convergence and exploration balance.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n        self.feedback_factor = 0.1  # adaptive feedback factor\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            if evaluations > self.budget / 2:  # Adjust parameters based on feedback\n                self.c1_end += self.feedback_factor * (1 - self.global_best_value / sum(personal_best_values))\n                self.c2_end -= self.feedback_factor * (1 - self.global_best_value / sum(personal_best_values))\n            \n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                resistance = 0.02\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05812 with standard deviation 0.00488.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.06539101833298588, 0.06458753861095912, 0.05625261710246643, 0.06024530256838867, 0.059537556256707314, 0.0517982969248546, 0.05802624309739057, 0.05738262202166644, 0.04982002824277332]}}
{"id": "70f69224-f2ef-49fb-b8cd-9dc60682d231", "fitness": 0.05413298088956527, "name": "EnhancedDAPSO_Refined", "description": "Introduce a reactive update mechanism that dynamically adjusts chaos and inertia factors based on real-time performance metrics to enhance convergence precision and speed.", "code": "import numpy as np\n\nclass EnhancedDAPSO_Refined:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n        self.previous_best_value = float('inf')\n        self.stagnation_counter = 0\n\n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n\n    def adaptive_parameters(self, evaluations):\n        progress = evaluations / self.budget\n        c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n        c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n        w = self.w_start * (1 - progress) + self.w_end * progress\n        return c1, c2, w\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            c1, c2, w = self.adaptive_parameters(evaluations)\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, evaluations/self.budget)\n\n                # Dynamic resistance based on stagnation detection\n                resistance = 0.02 + 0.01 * (self.stagnation_counter / self.population_size)\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n            self.previous_best_value = self.global_best_value\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 79, "feedback": "The algorithm EnhancedDAPSO_Refined got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05413 with standard deviation 0.00316.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.05861230774049875, 0.0593866959215158, 0.055202307472196344, 0.054107702352614306, 0.054817894615632246, 0.050971151411792204, 0.052146344120492905, 0.05283023088399441, 0.049122193487350474]}}
{"id": "faa07338-fbfd-41ef-82c5-bb99d08bf8a5", "fitness": 0.05847610187062577, "name": "EnhancedDAPSO", "description": "Utilize an adaptive chaos factor for more effective exploration and exploitation balance in the search space.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 0.5 + 3.8 * progress * (1 - progress)  # updated logistic map with a dynamic base\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 80, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05848 with standard deviation 0.00411.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.0665553350754744, 0.061726913415352525, 0.05896729230904241, 0.0612964715225236, 0.05695375773941724, 0.05442349536694013, 0.059032113209117054, 0.05488298393997948, 0.052446554257785105]}}
{"id": "0d332f45-b067-474e-97d8-4033707bd93a", "fitness": 0.05679451710866674, "name": "EnhancedDAPSO", "description": "Introduce multi-scale chaotic dynamics for enhanced exploration and adaptive subgrouping for intensified exploitation.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def multi_scale_chaotic_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factors = [0.5 + 3.5 * progress * (1 - progress), 4 * progress * (1 - progress)]  # logistic and quadratic maps\n        mutation = sum(beta * cf * (np.random.rand(self.dim) - 0.5) for cf in chaos_factors)\n        return np.clip(position + mutation / len(chaos_factors), lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int(((np.sin((evaluations / self.budget) * np.pi) + 1) / 2) ** 2 * self.population_size))  # adaptive subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.multi_scale_chaotic_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05679 with standard deviation 0.00309.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.05880780295122323, 0.06210569421010392, 0.06088745736801637, 0.054288351385934575, 0.057285779825067995, 0.05615516490254513, 0.05232083188057168, 0.055196772121224646, 0.05410279933331308]}}
{"id": "7bf989c1-e4ee-492b-ac53-2efbd8be2de9", "fitness": 0.05465891702881685, "name": "EnhancedDAPSO", "description": "Refine dynamic chaos factors, introduce adaptive resistance, and reset mechanism for maintaining diversity and improving convergence.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5\n        self.c2_start, self.c2_end = 0.5, 2.5\n        self.w_start, self.w_end = 0.9, 0.4\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n        self.reset_threshold = 0.1  # Reset mechanism threshold to maintain diversity\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.7  # Mutation intensity adjusted for exploration\n        chaos_factor = 0.5 + 4 * progress * (1 - progress)  # Enhanced logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Adaptive resistance and search space dynamic reduction\n                resistance = 0.03  # Increased dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n                \n                # Reset mechanism to maintain diversity\n                if evaluations / self.budget > self.reset_threshold:\n                    if np.random.rand() < 0.05:  # 5% chance to reset a particle\n                        particle_positions[i] = self.chaotically_mutate(np.random.uniform(lb, ub, self.dim), lb, ub, progress)\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 82, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05466 with standard deviation 0.00336.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.06134683229414728, 0.056717833916257065, 0.05684616093123629, 0.056596792466391044, 0.052377429037461454, 0.052467544442678093, 0.05453598989013819, 0.05048291292931206, 0.05055875735173021]}}
{"id": "c8cb83ae-6d04-4e94-96d8-585b165e12b8", "fitness": 0.05647900394930595, "name": "EnhancedDAPSO", "description": "Leverage a hybrid adaptive chaotic mutation with an enhanced subgroup strategy for robust exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n\n    def hybrid_chaotic_mutation(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        sine_wave_adjustment = np.sin(2 * np.pi * progress) * (ub - lb) * 0.01\n        return np.clip(position + mutation + sine_wave_adjustment, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n        while evaluations < self.budget:\n            dynamic_subgroup_size = int(np.ceil(self.population_size * (1 - evaluations / self.budget * 0.5)))\n            subgroup_indices = np.random.choice(self.population_size, dynamic_subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Hybrid adaptive mutation\n                particle_positions[i] = self.hybrid_chaotic_mutation(particle_positions[i], lb, ub, progress)\n\n                # Dynamic search range compression\n                resistance = 0.02\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05648 with standard deviation 0.00311.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.06112228334125769, 0.06143792348152888, 0.05820270736706146, 0.05639300934471703, 0.05668946871846403, 0.05372343527867329, 0.05434058282403642, 0.05462876918367787, 0.0517728560043369]}}
{"id": "2b181627-a03d-48d2-9273-6f17ba51ead6", "fitness": 0.0558584934075326, "name": "EnhancedDAPSO", "description": "Enhance convergence by adjusting mutation intensity based on function evaluations.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6 + 0.4 * progress  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)  # updated logistic map with a dynamic base\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 84, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05586 with standard deviation 0.00360.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.05906265180845016, 0.06285888155301589, 0.05684005431014327, 0.05452321193606491, 0.05798102446541509, 0.05247459747907113, 0.05254744186516214, 0.055867999691157855, 0.050570577559312935]}}
{"id": "75c425b6-0b3f-4412-b03f-6d6c779f0969", "fitness": 0.05371826650648597, "name": "AdaptiveChaosDAPSO", "description": "Introduce adaptive particle grouping and self-tuning chaos dynamics for improved convergence and solution quality.", "code": "import numpy as np\n\nclass AdaptiveChaosDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5\n        self.c2_start, self.c2_end = 0.5, 2.5\n        self.w_start, self.w_end = 0.9, 0.4\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.5 + 0.2 * progress  # adapt mutation intensity over time\n        chaos_factor = 4.0 * progress * (1 - progress)  # refined logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.cos((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                resistance = 0.01  # fine-tuned dynamic resistance\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 85, "feedback": "The algorithm AdaptiveChaosDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05372 with standard deviation 0.00277.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.05851639500337136, 0.05685201245634097, 0.052195467207320045, 0.05402626349986184, 0.05244699154296628, 0.05030729058911554, 0.05207046141083549, 0.05052895816537317]}}
{"id": "612e590b-5eff-4701-aae1-3653d48886bd", "fitness": 0.05410119800268143, "name": "AdaptiveSwarmOptimizer", "description": "Introduce adaptive swarm intelligence with chaos-driven mutation and resistance to enhance exploration and exploitation balance in dynamic environments.", "code": "import numpy as np\n\nclass AdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n        self.mutation_intensity = 0.7           # increased mutation intensity for better diversity\n      \n    def chaotically_mutate(self, position, lb, ub, progress):\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)\n        mutation = self.mutation_intensity * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n        \n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                resistance = 0.03  # slightly adjusted dynamic resistance factor for convergence\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 86, "feedback": "The algorithm AdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05410 with standard deviation 0.00267.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.0575686619950897, 0.05805563152748405, 0.057460316307754944, 0.05315628560678931, 0.053596381552333594, 0.05305433055075259, 0.051232382057032844, 0.051653636517705825, 0.05113315590918999]}}
{"id": "14fef709-3104-41c3-9343-950bbbb0c4d0", "fitness": 0.05653331668105829, "name": "EnhancedDAPSO", "description": "Introduce a dynamic resistance factor to compression for better convergence control.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)  # updated logistic map with a dynamic base\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.03  # Increased dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05653 with standard deviation 0.00434.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.06171131522970752, 0.06385131292999002, 0.05546298374320657, 0.05689318530672194, 0.058837303278762354, 0.05121099878474855, 0.05480709928223626, 0.056672191041822306, 0.04935346053232914]}}
{"id": "14dd28ef-1f73-4eb4-9e53-41113301831a", "fitness": 0.057784608682568434, "name": "EnhancedDAPSO", "description": "Implement a dual-layer adaptive learning mechanism to dynamically balance exploration and exploitation in EnhancedDAPSO.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)  # dynamic base logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def dual_layer_adaptive_learning(self, progress):\n        # Dual-layer adaptive learning rates for exploration-exploitation balance\n        alpha_exploitation = 0.9 - 0.5 * progress  # decrease over time\n        alpha_exploration = 0.1 + 0.5 * progress  # increase over time\n        return alpha_exploitation, alpha_exploration\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n            alpha_exploitation, alpha_exploration = self.dual_layer_adaptive_learning(progress)\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = (w * particle_velocities[i] \n                                          + alpha_exploitation * cognitive_velocity \n                                          + alpha_exploration * social_velocity)\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically\n                resistance = 0.02\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 88, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05778 with standard deviation 0.00377.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.06483591298204827, 0.05838034986406937, 0.061789037813190006, 0.0597531200265814, 0.05389929366834012, 0.05698915015496819, 0.05755867767802303, 0.05194733985945921, 0.05490859609643628]}}
{"id": "702e3130-4c2b-441e-8363-b752c52476f5", "fitness": 0.05557649073064191, "name": "EnhancedDAPSO", "description": "Introduce a dynamic chaos factor to enhance diversification, convergence speed, and adaptive mutation balance.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)  # updated logistic map with a dynamic base\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = (w + 0.01) * particle_velocities[i] + cognitive_velocity + social_velocity  # Adjusted inertia weight\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 89, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05558 with standard deviation 0.00309.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.05809114612361399, 0.058325407405713015, 0.06144159051406928, 0.05363299559742174, 0.05384658310674684, 0.05666721905148109, 0.05169047954225425, 0.05189566459583517, 0.054597330638641806]}}
{"id": "a4e91e2b-2211-4162-9c3f-760239bbf1c1", "fitness": 0.05631978002355052, "name": "EnhancedDAPSO", "description": "Incorporate a stochastic Levy flight operator to improve exploration and convergence diversity in chaotic particle dynamics.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)  # dynamic base logistic map\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def levy_flight(self, position, lb, ub):\n        levy_exponent = 1.5\n        levy_multiplier = 0.01\n        u = np.random.normal(0, 1, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = levy_multiplier * (u / np.abs(v) ** (1 / levy_exponent))\n        return np.clip(position + step, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Incorporate Levy flight for exploration enhancement\n                particle_positions[i] = self.levy_flight(particle_positions[i], lb, ub)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 90, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05632 with standard deviation 0.00491.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.06405432443016612, 0.06218020805883129, 0.054066496065153125, 0.05902671718073471, 0.05736543118114068, 0.04993068572180559, 0.05685561662527394, 0.05527784368853972, 0.04812069726030954]}}
{"id": "326ffc75-6ccc-4f4c-9d1e-ceb7afe238ba", "fitness": 0.053082767344356824, "name": "EnhancedDAPSO", "description": "Enhance convergence by introducing a variable convergence pressure mechanism and an adaptive mutation strategy.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Variable convergence pressure\n                pressure = np.exp(-5 * progress)  # Exponential decrease\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + pressure * (self.global_best_position - lb),\n                                                ub - pressure * (ub - self.global_best_position))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 91, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05308 with standard deviation 0.00299.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.05727779975684277, 0.05809040906738794, 0.05444316686269801, 0.05288918474760351, 0.05363485979597382, 0.05028108435817824, 0.05097503171804085, 0.05169326009127806, 0.048460109701208176]}}
{"id": "e27bed6a-0aca-4cec-abbc-6234fef7dd8e", "fitness": 0.057728087823040584, "name": "EnhancedDAPSO", "description": "Introduce a dynamic chaos factor to enhance diversification and convergence speed with improved subgroup selection.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)  # updated logistic map with a dynamic base\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.cos((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05773 with standard deviation 0.00421.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.06548214483231751, 0.06197798518407749, 0.0573697090814379, 0.06032749617344835, 0.0571814394043455, 0.052964268682624494, 0.058104848845179524, 0.05510123425522084, 0.051043663948713625]}}
{"id": "e7934daa-32a7-4f1a-95eb-ad0738cedf86", "fitness": 0.053810008645785716, "name": "RefinedDAPSO", "description": "Enhance the dynamic adaptation by incorporating feedback from subgroup performance and introduce a self-adaptive mechanism for chaos intensity.", "code": "import numpy as np\n\nclass RefinedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n        self.chaos_intensity = 0.5  # initial intensity for chaos\n\n    def chaotically_mutate(self, position, lb, ub, progress, subgroup_performance):\n        beta = 0.6\n        chaos_factor = self.chaos_intensity * (1 + np.tanh(subgroup_performance))  # adapt chaos intensity based on subgroup performance\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            subgroup_performance = (self.global_best_value - subgroup_best_value) / (self.global_best_value + 1e-10)\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress, subgroup_performance)\n\n                # Compression: Reduce the search space with dynamic resistance\n                resistance = 0.02\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 93, "feedback": "The algorithm RefinedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05381 with standard deviation 0.00281.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.05837474956281996, 0.05635490448004399, 0.05733375106347005, 0.052855878127515354, 0.052695712919111126, 0.054456096003345555, 0.05030729058911554, 0.05048106196042568, 0.0514306331062242]}}
{"id": "5f29b94f-afc8-453a-85e6-6a7f550b733f", "fitness": 0.05410119800268143, "name": "EnhancedDAPSO", "description": "Incorporate adaptive inertia, dynamic subgroup rebalancing, and enhanced chaotic mutation intensity to optimize convergence and exploration balance.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5  # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5  # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4    # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.7  # increased mutation intensity for better exploration\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)  # logistic map with dynamic base\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Dynamic subgroup size with minor adjustments for balancing exploration and exploitation\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) * 0.5 + 0.5) * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Enhanced chaotic mutation for adaptive exploration\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Dynamic search space compression with enhanced resistance\n                resistance = 0.03  # Increased resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 94, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05410 with standard deviation 0.00267.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.0575686619950897, 0.05805563152748405, 0.057460316307754944, 0.05315628560678931, 0.053596381552333594, 0.05305433055075259, 0.051232382057032844, 0.051653636517705825, 0.05113315590918999]}}
{"id": "52e8fca3-deb6-431a-964c-28f690f33052", "fitness": 0.05595318096425069, "name": "EnhancedDAPSO", "description": "Integrate an adaptive elite strategy for dynamic learning to improve convergence precision and stability.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)  \n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            elite_count = max(1, int(0.05 * self.population_size))  # 5% top particles\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            elite_indices = np.argsort(personal_best_values)[:elite_count]\n            elite_positions = particle_positions[elite_indices]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                \n                # Introduce elite influence\n                elite_influence = np.mean(elite_positions, axis=0) - particle_positions[i]\n\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity + 0.1 * elite_influence\n                particle_positions[i] += particle_velocities[i]\n\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                resistance = 0.02\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 95, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05595 with standard deviation 0.00432.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.06380969796269687, 0.06015854242257357, 0.05512605226907075, 0.05882264104322954, 0.055520539754979725, 0.05090693997696094, 0.05666677200029013, 0.05350482955996405, 0.04906261368849063]}}
{"id": "a11ca40e-90db-4dc4-a9db-d527bb28fd59", "fitness": 0.05894669731543764, "name": "EnhancedDAPSO", "description": "Enhance subgroup evaluation by dynamic subgroup selection for balanced exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)  # updated logistic map with a dynamic base\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 96, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05895 with standard deviation 0.00401.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.06539101833298588, 0.06458753861095912, 0.05881473167526163, 0.06024530256838867, 0.059537556256707314, 0.0542693931294389, 0.05802624309739057, 0.057355822441682136, 0.05229266972612456]}}
{"id": "a534973d-9a18-43f9-88cb-7829bba8d902", "fitness": 0.05894669731543764, "name": "RefinedDAPSO", "description": "Introduce adaptive subgroup synergy and dynamic chaos injection to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass RefinedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)  # updated logistic map with a dynamic base\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 97, "feedback": "The algorithm RefinedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05895 with standard deviation 0.00401.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.06539101833298588, 0.06458753861095912, 0.05881473167526163, 0.06024530256838867, 0.059537556256707314, 0.0542693931294389, 0.05802624309739057, 0.057355822441682136, 0.05229266972612456]}}
{"id": "f5d4b248-a4ac-40c6-8676-ab345815a31b", "fitness": 0.05560073558552761, "name": "EnhancedDAPSO", "description": "Introduce a dynamic chaos factor to enhance diversification and convergence speed with fine-tuned resistance for search space compression.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.6  # mutation intensity, increased from 0.5 to enhance exploration\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)  # updated logistic map with a dynamic base\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.01  # Fine-tuned dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 98, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05560 with standard deviation 0.00338.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.05652055868318906, 0.06161126181479126, 0.05982788331398492, 0.052195467207320045, 0.05684116721722032, 0.05517517701310093, 0.05030729058911554, 0.05477205580423872, 0.053155758626787675]}}
{"id": "e1d952f4-9375-41db-a4ec-0bba05364a75", "fitness": 0.057880815327635324, "name": "EnhancedDAPSO", "description": "Enhance dynamic exploration by tuning the mutation intensity factor for better convergence.", "code": "import numpy as np\n\nclass EnhancedDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget // 10)\n        self.c1_start, self.c1_end = 2.5, 0.5   # dynamic cognitive component\n        self.c2_start, self.c2_end = 0.5, 2.5   # dynamic social component\n        self.w_start, self.w_end = 0.9, 0.4     # dynamic inertia weight\n        self.global_best_position = None\n        self.global_best_value = float('inf')\n    \n    def chaotically_mutate(self, position, lb, ub, progress):\n        beta = 0.65  # mutation intensity, adjusted from 0.6 to further enhance exploration\n        chaos_factor = 0.5 + 3.5 * progress * (1 - progress)  # updated logistic map with a dynamic base\n        mutation = beta * chaos_factor * (np.random.rand(self.dim) - 0.5)\n        return np.clip(position + mutation, lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particle_positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        particle_velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.population_size, self.dim))\n        personal_best_positions = particle_positions.copy()\n        personal_best_values = np.full(self.population_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            subgroup_size = max(2, int((np.sin((evaluations / self.budget) * np.pi) + 1) / 2 * self.population_size))  # dynamic subgroup size\n            subgroup_indices = np.random.choice(self.population_size, subgroup_size, replace=False)\n            subgroup_best_position, subgroup_best_value = None, float('inf')\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(particle_positions[i])\n                evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_values[i] = current_value\n                    personal_best_positions[i] = particle_positions[i]\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = particle_positions[i]\n\n                if i in subgroup_indices and current_value < subgroup_best_value:\n                    subgroup_best_value = current_value\n                    subgroup_best_position = particle_positions[i]\n\n            progress = evaluations / self.budget\n            c1 = self.c1_start * (1 - progress) + self.c1_end * progress\n            c2 = self.c2_start * (1 - progress) + self.c2_end * progress\n            w = self.w_start * (1 - progress) + self.w_end * progress\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_velocity = c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n                social_velocity = c2 * r2 * (subgroup_best_position - particle_positions[i] if i in subgroup_indices else self.global_best_position - particle_positions[i])\n                particle_velocities[i] = w * particle_velocities[i] + cognitive_velocity + social_velocity\n                particle_positions[i] += particle_velocities[i]\n\n                # Adaptive mutation based on chaos theory\n                particle_positions[i] = self.chaotically_mutate(particle_positions[i], lb, ub, progress)\n\n                # Compression: Reduce the search space dynamically with a small tweak of added resistance\n                resistance = 0.02  # Introduce dynamic resistance factor\n                search_range_reduction = (self.budget - evaluations) / self.budget\n                particle_positions[i] = np.clip(particle_positions[i],\n                                                lb + search_range_reduction * (self.global_best_position - lb + resistance),\n                                                ub - search_range_reduction * (ub - self.global_best_position + resistance))\n\n        return self.global_best_position, self.global_best_value", "configspace": "", "generation": 99, "feedback": "The algorithm EnhancedDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05788 with standard deviation 0.00432.", "error": "", "parent_ids": ["5ad954a6-9380-46a2-806c-3752daab8c34"], "operator": null, "metadata": {"aucs": [0.06564197900354618, 0.06246561238594206, 0.05722215960065924, 0.06046588844831735, 0.05762457416775535, 0.052830397911637794, 0.058235028337101036, 0.055526382155760445, 0.05091531593799847]}}
