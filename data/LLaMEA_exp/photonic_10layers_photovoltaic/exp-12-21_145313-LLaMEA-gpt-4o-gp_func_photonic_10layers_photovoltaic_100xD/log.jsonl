{"id": "41204f89-d691-45ff-8fc0-01dd598a6cdb", "fitness": 0.007744591951551823, "name": "APSO_DL", "description": "Adaptive Particle Swarm Optimization with Dynamic Learning (APSO-DL) - an enhanced particle swarm algorithm that adapts inertia weight and learning rates dynamically based on performance, optimizing black-box functions efficiently within bounded search spaces.", "code": "import numpy as np\n\nclass APSO_DL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.position = np.random.rand(self.num_particles, dim)\n        self.velocity = np.random.rand(self.num_particles, dim) * 0.1\n        self.best_personal_position = np.copy(self.position)\n        self.best_personal_value = np.full(self.num_particles, np.inf)\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                if self.evaluations < self.budget:\n                    # Evaluate current position\n                    value = func(self.position[i])\n                    self.evaluations += 1\n\n                    # Update personal and global bests\n                    if value < self.best_personal_value[i]:\n                        self.best_personal_value[i] = value\n                        self.best_personal_position[i] = self.position[i]\n                    if value < self.best_global_value:\n                        self.best_global_value = value\n                        self.best_global_position = self.position[i]\n\n            # Dynamic adjustment of inertia weight and learning rates\n            self.inertia_weight = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n            self.cognitive_coeff = 1.5 + np.random.rand() * 1.5\n            self.social_coeff = 1.5 + np.random.rand() * 1.5\n\n            # Update velocity and position\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.cognitive_coeff * r1 * (self.best_personal_position - self.position) +\n                             self.social_coeff * r2 * (self.best_global_position - self.position))\n            self.position += self.velocity\n\n            # Ensure particles are within bounds\n            self.position = np.clip(self.position, func.bounds.lb, func.bounds.ub)\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 0, "feedback": "The algorithm APSO_DL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.00774 with standard deviation 0.00107.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.009184736676305483, 0.009184736676305483, 0.009184736676305483, 0.007422984058760962, 0.007422984058760962, 0.007422984058760962, 0.006626055119589025, 0.006626055119589025, 0.006626055119589025]}}
{"id": "eddda1cd-2cc7-4aab-9216-20642938d4d7", "fitness": 0.03689490644733537, "name": "APSO_DL", "description": "Enhanced APSO-DL with adaptive particle restart to escape local optima efficiently.", "code": "import numpy as np\n\nclass APSO_DL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.position = np.random.rand(self.num_particles, dim)\n        self.velocity = np.random.rand(self.num_particles, dim) * 0.1\n        self.best_personal_position = np.copy(self.position)\n        self.best_personal_value = np.full(self.num_particles, np.inf)\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                if self.evaluations < self.budget:\n                    # Evaluate current position\n                    value = func(self.position[i])\n                    self.evaluations += 1\n\n                    # Update personal and global bests\n                    if value < self.best_personal_value[i]:\n                        self.best_personal_value[i] = value\n                        self.best_personal_position[i] = self.position[i]\n                    if value < self.best_global_value:\n                        self.best_global_value = value\n                        self.best_global_position = self.position[i]\n\n            # Dynamic adjustment of inertia weight and learning rates\n            self.inertia_weight = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n            self.cognitive_coeff = 1.5 + np.random.rand() * 1.5\n            self.social_coeff = 1.5 + np.random.rand() * 1.5\n\n            # Update velocity and position\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.cognitive_coeff * r1 * (self.best_personal_position - self.position) +\n                             self.social_coeff * r2 * (self.best_global_position - self.position))\n            self.position += self.velocity\n\n            # Ensure particles are within bounds\n            self.position = np.clip(self.position, func.bounds.lb, func.bounds.ub)\n            \n            # Adaptive restart strategy\n            if self.evaluations % (self.budget // 10) == 0:  # Restart 10% of the time\n                worst_indices = np.argsort(self.best_personal_value)[-2:]  # Identify a couple of worst particles\n                self.position[worst_indices] = np.random.rand(2, self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n                self.velocity[worst_indices] = np.random.rand(2, self.dim) * 0.1\n                self.best_personal_value[worst_indices] = np.inf\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 1, "feedback": "The algorithm APSO_DL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03689 with standard deviation 0.00207.", "error": "", "parent_ids": ["41204f89-d691-45ff-8fc0-01dd598a6cdb"], "operator": null, "metadata": {"aucs": [0.040276213963593044, 0.03873107837430656, 0.0397190683883748, 0.03692005115932029, 0.03550549827133165, 0.036419056071393285, 0.0354408254887012, 0.03407985179289008, 0.034962514516107435]}}
{"id": "8ab4e104-d9ad-4583-874b-faf74bc10901", "fitness": 0.036611616224095686, "name": "AHPSO_RR", "description": "Adaptive Hierarchical Particle Swarm Optimization with Randomized Restarts (AHPSO-RR) incorporates a multi-layered search and randomized particle restarts to enhance global exploration and convergence speed.", "code": "import numpy as np\n\nclass AHPSO_RR:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.position = np.random.rand(self.num_particles, dim)\n        self.velocity = np.random.rand(self.num_particles, dim) * 0.1\n        self.best_personal_position = np.copy(self.position)\n        self.best_personal_value = np.full(self.num_particles, np.inf)\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.adaptive_layer = 2  # New hierarchical layer for adaptive exploration\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                if self.evaluations < self.budget:\n                    value = func(self.position[i])\n                    self.evaluations += 1\n\n                    if value < self.best_personal_value[i]:\n                        self.best_personal_value[i] = value\n                        self.best_personal_position[i] = self.position[i]\n                    if value < self.best_global_value:\n                        self.best_global_value = value\n                        self.best_global_position = self.position[i]\n\n            self.inertia_weight = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n            self.cognitive_coeff = 1.5 + np.random.rand() * 1.5\n            self.social_coeff = 1.5 + np.random.rand() * 1.5\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.cognitive_coeff * r1 * (self.best_personal_position - self.position) +\n                             self.social_coeff * r2 * (self.best_global_position - self.position))\n            self.position += self.velocity\n            self.position = np.clip(self.position, func.bounds.lb, func.bounds.ub)\n\n            if self.evaluations % (self.budget // (10 * self.adaptive_layer)) == 0:\n                restart_count = max(1, self.num_particles // (5 * self.adaptive_layer))  # Dynamic restart amount\n                worst_indices = np.argsort(self.best_personal_value)[-restart_count:]\n                self.position[worst_indices] = np.random.rand(restart_count, self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n                self.velocity[worst_indices] = np.random.rand(restart_count, self.dim) * 0.1\n                self.best_personal_value[worst_indices] = np.inf\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 2, "feedback": "The algorithm AHPSO_RR got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03661 with standard deviation 0.00496.", "error": "", "parent_ids": ["eddda1cd-2cc7-4aab-9216-20642938d4d7"], "operator": null, "metadata": {"aucs": [0.0451685103988384, 0.03934925056002869, 0.033345358081364296, 0.0414375201688284, 0.03608841906180704, 0.030470694888563465, 0.03980400570820597, 0.03464796838925932, 0.0291928187599656]}}
{"id": "dbaecc4c-ff1c-4327-94ef-c911cd8c04e4", "fitness": 0.0362721557785692, "name": "APSO_DL_Enhanced", "description": "APSO-DL with swarm diversity preservation and dynamic neighborhood topology to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass APSO_DL_Enhanced:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.position = np.random.rand(self.num_particles, dim) * 2.0 - 1.0\n        self.velocity = np.random.rand(self.num_particles, dim) * 0.1\n        self.best_personal_position = np.copy(self.position)\n        self.best_personal_value = np.full(self.num_particles, np.inf)\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                if self.evaluations < self.budget:\n                    # Evaluate current position\n                    value = func(self.position[i])\n                    self.evaluations += 1\n\n                    # Update personal and global bests\n                    if value < self.best_personal_value[i]:\n                        self.best_personal_value[i] = value\n                        self.best_personal_position[i] = self.position[i]\n                    if value < self.best_global_value:\n                        self.best_global_value = value\n                        self.best_global_position = self.position[i]\n\n            # Adjust parameters dynamically\n            self.inertia_weight = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n            self.cognitive_coeff = 1.5 + np.random.rand() * 1.5\n            self.social_coeff = 1.5 + np.random.rand() * 1.5\n\n            # Preserve diversity by randomly selecting a neighborhood best for each particle\n            neighborhood = np.random.choice(self.num_particles, self.num_particles, replace=True)\n            \n            # Update velocity and position\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            local_best_positions = self.best_personal_position[neighborhood]\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.cognitive_coeff * r1 * (self.best_personal_position - self.position) +\n                             self.social_coeff * r2 * (local_best_positions - self.position))\n            self.position += self.velocity\n\n            # Ensure particles are within bounds\n            self.position = np.clip(self.position, func.bounds.lb, func.bounds.ub)\n            \n            # Adaptive restart strategy\n            if self.evaluations % (self.budget // 10) == 0:  # Restart 10% of the time\n                worst_indices = np.argsort(self.best_personal_value)[-2:]  # Identify a couple of worst particles\n                self.position[worst_indices] = np.random.rand(2, self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n                self.velocity[worst_indices] = np.random.rand(2, self.dim) * 0.1\n                self.best_personal_value[worst_indices] = np.inf\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 3, "feedback": "The algorithm APSO_DL_Enhanced got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03627 with standard deviation 0.00283.", "error": "", "parent_ids": ["eddda1cd-2cc7-4aab-9216-20642938d4d7"], "operator": null, "metadata": {"aucs": [0.03812095907583657, 0.03674453272945699, 0.04186482362983668, 0.03493941929435673, 0.03365158393935541, 0.0384164607334625, 0.03353211177347659, 0.03228159185511981, 0.03689791897622152]}}
{"id": "cf8a0bb0-2847-46f5-8792-df7912a4b9c8", "fitness": 0.03454959679229888, "name": "APSO_DL", "description": "APSO_DL with adaptive velocity reset for better exploration in stagnation scenarios. ", "code": "import numpy as np\n\nclass APSO_DL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.position = np.random.rand(self.num_particles, dim)\n        self.velocity = np.random.rand(self.num_particles, dim) * 0.1\n        self.best_personal_position = np.copy(self.position)\n        self.best_personal_value = np.full(self.num_particles, np.inf)\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                if self.evaluations < self.budget:\n                    # Evaluate current position\n                    value = func(self.position[i])\n                    self.evaluations += 1\n\n                    # Update personal and global bests\n                    if value < self.best_personal_value[i]:\n                        self.best_personal_value[i] = value\n                        self.best_personal_position[i] = self.position[i]\n                    if value < self.best_global_value:\n                        self.best_global_value = value\n                        self.best_global_position = self.position[i]\n\n            # Dynamic adjustment of inertia weight and learning rates\n            self.inertia_weight = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n            self.cognitive_coeff = 1.5 + np.random.rand() * 1.5\n            self.social_coeff = 1.5 + np.random.rand() * 1.5\n\n            # Update velocity and position\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.cognitive_coeff * r1 * (self.best_personal_position - self.position) +\n                             self.social_coeff * r2 * (self.best_global_position - self.position))\n            self.position += self.velocity\n\n            # Ensure particles are within bounds\n            self.position = np.clip(self.position, func.bounds.lb, func.bounds.ub)\n            \n            # Adaptive restart strategy\n            if self.evaluations % (self.budget // 10) == 0:  # Restart 10% of the time\n                worst_indices = np.argsort(self.best_personal_value)[-2:]  # Identify a couple of worst particles\n                self.position[worst_indices] = np.random.rand(2, self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n                self.velocity[worst_indices] = 0.0  # Reset velocity to zero for better exploration\n                self.best_personal_value[worst_indices] = np.inf\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 4, "feedback": "The algorithm APSO_DL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03455 with standard deviation 0.00274.", "error": "", "parent_ids": ["eddda1cd-2cc7-4aab-9216-20642938d4d7"], "operator": null, "metadata": {"aucs": [0.034876096686360936, 0.036513333686454263, 0.03992028677014847, 0.031876110083719045, 0.033437762353381695, 0.03658572737973198, 0.0305464022272528, 0.03207499563550131, 0.0351156563081394]}}
{"id": "8a7fac9d-fd14-465a-81b5-c830d055dd62", "fitness": 0.03696096856231121, "name": "APSO_DL", "description": "Enhanced APSO-DL with improved adaptive restart mechanism for better exploration.", "code": "import numpy as np\n\nclass APSO_DL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.position = np.random.rand(self.num_particles, dim)\n        self.velocity = np.random.rand(self.num_particles, dim) * 0.1\n        self.best_personal_position = np.copy(self.position)\n        self.best_personal_value = np.full(self.num_particles, np.inf)\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                if self.evaluations < self.budget:\n                    # Evaluate current position\n                    value = func(self.position[i])\n                    self.evaluations += 1\n\n                    # Update personal and global bests\n                    if value < self.best_personal_value[i]:\n                        self.best_personal_value[i] = value\n                        self.best_personal_position[i] = self.position[i]\n                    if value < self.best_global_value:\n                        self.best_global_value = value\n                        self.best_global_position = self.position[i]\n\n            # Dynamic adjustment of inertia weight and learning rates\n            self.inertia_weight = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n            self.cognitive_coeff = 1.5 + np.random.rand() * 1.5\n            self.social_coeff = 1.5 + np.random.rand() * 1.5\n\n            # Update velocity and position\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.cognitive_coeff * r1 * (self.best_personal_position - self.position) +\n                             self.social_coeff * r2 * (self.best_global_position - self.position))\n            self.position += self.velocity\n\n            # Ensure particles are within bounds\n            self.position = np.clip(self.position, func.bounds.lb, func.bounds.ub)\n            \n            # Adaptive restart strategy\n            if self.evaluations % (self.budget // 10) == 0:  # Restart 10% of the time\n                worst_indices = np.argsort(self.best_personal_value)[-3:]  # Identify a few worst particles\n                self.position[worst_indices] = np.random.rand(3, self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n                self.velocity[worst_indices] = np.random.rand(3, self.dim) * 0.1\n                self.best_personal_value[worst_indices] = np.inf\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 5, "feedback": "The algorithm APSO_DL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03696 with standard deviation 0.00283.", "error": "", "parent_ids": ["eddda1cd-2cc7-4aab-9216-20642938d4d7"], "operator": null, "metadata": {"aucs": [0.03666534280779288, 0.04068247133834024, 0.04155951959493809, 0.03357812052722753, 0.037331235311752864, 0.03813794706712614, 0.03221064686325359, 0.03585287808522608, 0.03663055546514349]}}
{"id": "813e680c-e0e5-4e75-9c44-2b4612111841", "fitness": -Infinity, "name": "APSO_DL_Enhanced", "description": "Adaptive Particle Swarm Optimization with a dynamic learning strategy and self-adaptive velocity clamping for enhanced exploration and convergence.", "code": "import numpy as np\n\nclass APSO_DL_Enhanced:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.position = np.random.rand(self.num_particles, dim)\n        self.velocity = np.random.rand(self.num_particles, dim) * 0.1\n        self.best_personal_position = np.copy(self.position)\n        self.best_personal_value = np.full(self.num_particles, np.inf)\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.max_velocity = 0.2 * (func.bounds.ub - func.bounds.lb)\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                if self.evaluations < self.budget:\n                    # Evaluate current position\n                    value = func(self.position[i])\n                    self.evaluations += 1\n\n                    # Update personal and global bests\n                    if value < self.best_personal_value[i]:\n                        self.best_personal_value[i] = value\n                        self.best_personal_position[i] = self.position[i]\n                    if value < self.best_global_value:\n                        self.best_global_value = value\n                        self.best_global_position = self.position[i]\n\n            # Dynamic adjustment of inertia weight and learning rates\n            self.inertia_weight = 0.5 + 0.4 * np.random.rand()\n            self.cognitive_coeff = 1.5 + np.random.rand() * 1.5\n            self.social_coeff = 1.5 + np.random.rand() * 1.5\n\n            # Update velocity and position\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.cognitive_coeff * r1 * (self.best_personal_position - self.position) +\n                             self.social_coeff * r2 * (self.best_global_position - self.position))\n            \n            # Self-adaptive velocity clamping\n            velocity_norm = np.linalg.norm(self.velocity, axis=1)\n            clamped_velocity = np.clip(velocity_norm, None, self.max_velocity)\n            self.velocity *= clamped_velocity[:, np.newaxis] / velocity_norm[:, np.newaxis]\n\n            self.position += self.velocity\n\n            # Ensure particles are within bounds\n            self.position = np.clip(self.position, func.bounds.lb, func.bounds.ub)\n            \n            # Adaptive restart strategy\n            if self.evaluations % (self.budget // 10) == 0:  # Restart 10% of the time\n                worst_indices = np.argsort(self.best_personal_value)[-3:]  # Identify a few worst particles\n                self.position[worst_indices] = np.random.rand(3, self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n                self.velocity[worst_indices] = np.random.rand(3, self.dim) * 0.1\n                self.best_personal_value[worst_indices] = np.inf\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 6, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_ids": ["8a7fac9d-fd14-465a-81b5-c830d055dd62"], "operator": null, "metadata": {}}
{"id": "393e5e69-1444-4616-9349-fb82f8c96c6f", "fitness": 0.03536127941695747, "name": "APSO_Tunnel", "description": "Integrating a novel stochastic tunneling mechanism with APSO-DL for enhanced escape from local optima and improved convergence.", "code": "import numpy as np\n\nclass APSO_Tunnel:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.position = np.random.rand(self.num_particles, dim)\n        self.velocity = np.random.rand(self.num_particles, dim) * 0.1\n        self.best_personal_position = np.copy(self.position)\n        self.best_personal_value = np.full(self.num_particles, np.inf)\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.tunneling_rate = 0.05  # Rate at which tunneling occurs\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                if self.evaluations < self.budget:\n                    # Evaluate current position\n                    value = func(self.position[i])\n                    self.evaluations += 1\n\n                    # Update personal and global bests\n                    if value < self.best_personal_value[i]:\n                        self.best_personal_value[i] = value\n                        self.best_personal_position[i] = self.position[i]\n                    if value < self.best_global_value:\n                        self.best_global_value = value\n                        self.best_global_position = self.position[i]\n\n            # Dynamic adjustment of inertia weight and learning rates\n            self.inertia_weight = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n            self.cognitive_coeff = 1.5 + np.random.rand() * 1.5\n            self.social_coeff = 1.5 + np.random.rand() * 1.5\n\n            # Update velocity and position\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.cognitive_coeff * r1 * (self.best_personal_position - self.position) +\n                             self.social_coeff * r2 * (self.best_global_position - self.position))\n            self.position += self.velocity\n\n            # Ensure particles are within bounds\n            self.position = np.clip(self.position, func.bounds.lb, func.bounds.ub)\n            \n            # Adaptive restart strategy\n            if self.evaluations % (self.budget // 10) == 0:\n                worst_indices = np.argsort(self.best_personal_value)[-3:]\n                self.position[worst_indices] = np.random.rand(3, self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n                self.velocity[worst_indices] = np.random.rand(3, self.dim) * 0.1\n                self.best_personal_value[worst_indices] = np.inf\n\n            # Stochastic tunneling mechanism\n            if np.random.rand() < self.tunneling_rate:\n                tunnel_index = np.random.choice(self.num_particles)\n                new_position = np.random.rand(self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n                new_value = func(new_position)\n                self.evaluations += 1\n                if new_value < self.best_personal_value[tunnel_index]:\n                    self.best_personal_value[tunnel_index] = new_value\n                    self.best_personal_position[tunnel_index] = new_position\n                if new_value < self.best_global_value:\n                    self.best_global_value = new_value\n                    self.best_global_position = new_position\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 7, "feedback": "The algorithm APSO_Tunnel got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03536 with standard deviation 0.00413.", "error": "", "parent_ids": ["8a7fac9d-fd14-465a-81b5-c830d055dd62"], "operator": null, "metadata": {"aucs": [0.042675440728007, 0.03321526466639235, 0.03795807996989553, 0.039165356435124754, 0.0303360802303817, 0.03480866953335249, 0.03762155748541296, 0.029056770022579226, 0.03341429568147125]}}
{"id": "09338048-6fad-4728-8b47-e9fc50cb8adb", "fitness": 0.035845711942652875, "name": "APSO_DL", "description": "Improved APSO-DL by enhancing particle diversity through occasional velocity resetting.", "code": "import numpy as np\n\nclass APSO_DL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.position = np.random.rand(self.num_particles, dim)\n        self.velocity = np.random.rand(self.num_particles, dim) * 0.1\n        self.best_personal_position = np.copy(self.position)\n        self.best_personal_value = np.full(self.num_particles, np.inf)\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                if self.evaluations < self.budget:\n                    # Evaluate current position\n                    value = func(self.position[i])\n                    self.evaluations += 1\n\n                    # Update personal and global bests\n                    if value < self.best_personal_value[i]:\n                        self.best_personal_value[i] = value\n                        self.best_personal_position[i] = self.position[i]\n                    if value < self.best_global_value:\n                        self.best_global_value = value\n                        self.best_global_position = self.position[i]\n\n            # Dynamic adjustment of inertia weight and learning rates\n            self.inertia_weight = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n            self.cognitive_coeff = 1.5 + np.random.rand() * 1.5\n            self.social_coeff = 1.5 + np.random.rand() * 1.5\n\n            # Update velocity and position\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.cognitive_coeff * r1 * (self.best_personal_position - self.position) +\n                             self.social_coeff * r2 * (self.best_global_position - self.position))\n            self.position += self.velocity\n\n            # Ensure particles are within bounds\n            self.position = np.clip(self.position, func.bounds.lb, func.bounds.ub)\n            \n            # Adaptive restart strategy\n            if self.evaluations % (self.budget // 10) == 0:  # Restart 10% of the time\n                worst_indices = np.argsort(self.best_personal_value)[-3:]  # Identify a few worst particles\n                self.position[worst_indices] = np.random.rand(3, self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n                self.velocity[worst_indices] = np.random.rand(3, self.dim) * 0.1\n                self.best_personal_value[worst_indices] = np.inf\n            if np.random.rand() < 0.05:  # Occasionally reset velocity to improve diversity\n                self.velocity = np.random.rand(self.num_particles, self.dim) * 0.1\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 8, "feedback": "The algorithm APSO_DL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03585 with standard deviation 0.00420.", "error": "", "parent_ids": ["8a7fac9d-fd14-465a-81b5-c830d055dd62"], "operator": null, "metadata": {"aucs": [0.042675440728007, 0.03321526466639235, 0.03950587550444473, 0.039165356435124754, 0.0303360802303817, 0.03623911915311129, 0.03762155748541296, 0.029056770022579226, 0.034795943258421835]}}
{"id": "5d1ffc9c-070c-45bf-b1db-9eedcc10acfd", "fitness": 0.007744591951551823, "name": "Enhanced_APSO_DL", "description": "Enhanced APSO-DL with adaptive inertia and neighborhood-based restart for improved convergence speed.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.position = np.random.rand(self.num_particles, dim)\n        self.velocity = np.random.rand(self.num_particles, dim) * 0.1\n        self.best_personal_position = np.copy(self.position)\n        self.best_personal_value = np.full(self.num_particles, np.inf)\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                if self.evaluations < self.budget:\n                    # Evaluate current position\n                    value = func(self.position[i])\n                    self.evaluations += 1\n\n                    # Update personal and global bests\n                    if value < self.best_personal_value[i]:\n                        self.best_personal_value[i] = value\n                        self.best_personal_position[i] = self.position[i]\n                    if value < self.best_global_value:\n                        self.best_global_value = value\n                        self.best_global_position = self.position[i]\n\n            # Adaptive inertia weight decreases over time for better exploration to exploitation transition\n            self.inertia_weight = 0.5 + 0.4 * (1 - self.evaluations / self.budget)\n            self.cognitive_coeff = 1.5 + np.random.rand() * 1.0\n            self.social_coeff = 2.0 + np.random.rand() * 1.0\n\n            # Update velocity and position\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.cognitive_coeff * r1 * (self.best_personal_position - self.position) +\n                             self.social_coeff * r2 * (self.best_global_position - self.position))\n            self.position += self.velocity\n\n            # Ensure particles are within bounds\n            self.position = np.clip(self.position, func.bounds.lb, func.bounds.ub)\n            \n            # Neighborhood-based restart strategy\n            if self.evaluations % (self.budget // 10) == 0:  # Restart 10% of the time\n                worst_indices = np.argsort(self.best_personal_value)[-3:]  # Identify a few worst particles\n                for idx in worst_indices:\n                    neighbors = np.random.choice(self.num_particles, size=3, replace=False)\n                    centroid = np.mean(self.position[neighbors], axis=0)\n                    self.position[idx] = centroid\n                    self.velocity[idx] = np.random.rand(self.dim) * 0.1\n                    self.best_personal_value[idx] = np.inf\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 9, "feedback": "The algorithm Enhanced_APSO_DL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.00774 with standard deviation 0.00107.", "error": "", "parent_ids": ["8a7fac9d-fd14-465a-81b5-c830d055dd62"], "operator": null, "metadata": {"aucs": [0.009184736676305483, 0.009184736676305483, 0.009184736676305483, 0.007422984058760962, 0.007422984058760962, 0.007422984058760962, 0.006626055119589025, 0.006626055119589025, 0.006626055119589025]}}
{"id": "67c5994c-b972-422e-bd87-145bf80ff56e", "fitness": -Infinity, "name": "Enhanced_APSO_DL", "description": "Enhanced APSO-DL with adaptive diversity control and velocity clamping for improved convergence and exploration-exploitation balance.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.position = np.random.rand(self.num_particles, dim)\n        self.velocity = np.random.rand(self.num_particles, dim) * 0.1\n        self.best_personal_position = np.copy(self.position)\n        self.best_personal_value = np.full(self.num_particles, np.inf)\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        max_velocity = (func.bounds.ub - func.bounds.lb) * 0.1  # Added max velocity constraint\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                if self.evaluations < self.budget:\n                    value = func(self.position[i])\n                    self.evaluations += 1\n\n                    if value < self.best_personal_value[i]:\n                        self.best_personal_value[i] = value\n                        self.best_personal_position[i] = self.position[i]\n                    if value < self.best_global_value:\n                        self.best_global_value = value\n                        self.best_global_position = self.position[i]\n\n            self.inertia_weight = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n            self.cognitive_coeff = 1.5 + np.random.rand() * 1.5\n            self.social_coeff = 1.5 + np.random.rand() * 1.5\n\n            diversity = np.mean(np.std(self.position, axis=0))  # Calculate current diversity\n            if diversity < 0.1 * (func.bounds.ub - func.bounds.lb):\n                self.inertia_weight *= 0.9  # Reduce inertia weight if diversity is low\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.cognitive_coeff * r1 * (self.best_personal_position - self.position) +\n                             self.social_coeff * r2 * (self.best_global_position - self.position))\n            \n            # Clamp velocity\n            self.velocity = np.clip(self.velocity, -max_velocity, max_velocity)\n            self.position += self.velocity\n            self.position = np.clip(self.position, func.bounds.lb, func.bounds.ub)\n            \n            if self.evaluations % (self.budget // 10) == 0:\n                worst_indices = np.argsort(self.best_personal_value)[-3:]\n                self.position[worst_indices] = np.random.rand(3, self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n                self.velocity[worst_indices] = np.random.rand(3, self.dim) * 0.1\n                self.best_personal_value[worst_indices] = np.inf\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 10, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_ids": ["8a7fac9d-fd14-465a-81b5-c830d055dd62"], "operator": null, "metadata": {}}
{"id": "2444244f-9c00-480e-be52-655614b32dde", "fitness": 0.03696096856231121, "name": "APSO_DL", "description": "Slight modification of adaptive restart strategy to dynamically adjust restart frequency for improved exploration.", "code": "import numpy as np\n\nclass APSO_DL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.position = np.random.rand(self.num_particles, dim)\n        self.velocity = np.random.rand(self.num_particles, dim) * 0.1\n        self.best_personal_position = np.copy(self.position)\n        self.best_personal_value = np.full(self.num_particles, np.inf)\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                if self.evaluations < self.budget:\n                    # Evaluate current position\n                    value = func(self.position[i])\n                    self.evaluations += 1\n\n                    # Update personal and global bests\n                    if value < self.best_personal_value[i]:\n                        self.best_personal_value[i] = value\n                        self.best_personal_position[i] = self.position[i]\n                    if value < self.best_global_value:\n                        self.best_global_value = value\n                        self.best_global_position = self.position[i]\n\n            # Dynamic adjustment of inertia weight and learning rates\n            self.inertia_weight = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n            self.cognitive_coeff = 1.5 + np.random.rand() * 1.5\n            self.social_coeff = 1.5 + np.random.rand() * 1.5\n\n            # Update velocity and position\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.cognitive_coeff * r1 * (self.best_personal_position - self.position) +\n                             self.social_coeff * r2 * (self.best_global_position - self.position))\n            self.position += self.velocity\n\n            # Ensure particles are within bounds\n            self.position = np.clip(self.position, func.bounds.lb, func.bounds.ub)\n            \n            # Adaptive restart strategy\n            if self.evaluations % (self.budget // 20) == 0:  # Restart at a dynamically adjusted frequency\n                worst_indices = np.argsort(self.best_personal_value)[-3:]  # Identify a few worst particles\n                self.position[worst_indices] = np.random.rand(3, self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n                self.velocity[worst_indices] = np.random.rand(3, self.dim) * 0.1\n                self.best_personal_value[worst_indices] = np.inf\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 11, "feedback": "The algorithm APSO_DL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03696 with standard deviation 0.00283.", "error": "", "parent_ids": ["8a7fac9d-fd14-465a-81b5-c830d055dd62"], "operator": null, "metadata": {"aucs": [0.03666534280779288, 0.04068247133834024, 0.04155951959493809, 0.03357812052722753, 0.037331235311752864, 0.03813794706712614, 0.03221064686325359, 0.03585287808522608, 0.03663055546514349]}}
{"id": "173b192d-3cdb-4ce3-b8c1-f3f3ca229ffc", "fitness": 0.007744591951551823, "name": "APSO_DL", "description": "Enhanced APSO-DL with improved inertia weight adaptation and more frequent adaptive restarts.", "code": "import numpy as np\n\nclass APSO_DL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.position = np.random.rand(self.num_particles, dim)\n        self.velocity = np.random.rand(self.num_particles, dim) * 0.1\n        self.best_personal_position = np.copy(self.position)\n        self.best_personal_value = np.full(self.num_particles, np.inf)\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                if self.evaluations < self.budget:\n                    value = func(self.position[i])\n                    self.evaluations += 1\n\n                    if value < self.best_personal_value[i]:\n                        self.best_personal_value[i] = value\n                        self.best_personal_position[i] = self.position[i]\n                    if value < self.best_global_value:\n                        self.best_global_value = value\n                        self.best_global_position = self.position[i]\n\n            self.inertia_weight = 0.8 - 0.5 * (self.evaluations / self.budget)  # Modified line\n            self.cognitive_coeff = 1.5 + np.random.rand() * 1.5\n            self.social_coeff = 1.5 + np.random.rand() * 1.5\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.cognitive_coeff * r1 * (self.best_personal_position - self.position) +\n                             self.social_coeff * r2 * (self.best_global_position - self.position))\n            self.position += self.velocity\n\n            self.position = np.clip(self.position, func.bounds.lb, func.bounds.ub)\n            \n            if self.evaluations % (self.budget // 15) == 0:  # Changed restart frequency\n                worst_indices = np.argsort(self.best_personal_value)[-3:]\n                self.position[worst_indices] = np.random.rand(3, self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n                self.velocity[worst_indices] = np.random.rand(3, self.dim) * 0.1\n                self.best_personal_value[worst_indices] = np.inf\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 12, "feedback": "The algorithm APSO_DL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.00774 with standard deviation 0.00107.", "error": "", "parent_ids": ["8a7fac9d-fd14-465a-81b5-c830d055dd62"], "operator": null, "metadata": {"aucs": [0.009184736676305483, 0.009184736676305483, 0.009184736676305483, 0.007422984058760962, 0.007422984058760962, 0.007422984058760962, 0.006626055119589025, 0.006626055119589025, 0.006626055119589025]}}
{"id": "76e151d3-4c8c-4c49-a394-3342b117d0ea", "fitness": 0.03624850751225539, "name": "AQI_APSO_DL", "description": "Adaptive Quantum-Inspired APSO-DL with diversity-preserving mechanisms for enhanced global exploration and convergence speed.", "code": "import numpy as np\n\nclass AQI_APSO_DL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.quantum_coeff = 0.1\n        self.position = np.random.rand(self.num_particles, dim)\n        self.velocity = np.random.rand(self.num_particles, dim) * 0.1\n        self.best_personal_position = np.copy(self.position)\n        self.best_personal_value = np.full(self.num_particles, np.inf)\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                if self.evaluations < self.budget:\n                    value = func(self.position[i])\n                    self.evaluations += 1\n\n                    if value < self.best_personal_value[i]:\n                        self.best_personal_value[i] = value\n                        self.best_personal_position[i] = self.position[i]\n                    if value < self.best_global_value:\n                        self.best_global_value = value\n                        self.best_global_position = self.position[i]\n\n            self.inertia_weight = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n            self.cognitive_coeff = 1.5 + np.random.rand() * 1.5\n            self.social_coeff = 1.5 + np.random.rand() * 1.5\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.cognitive_coeff * r1 * (self.best_personal_position - self.position) +\n                             self.social_coeff * r2 * (self.best_global_position - self.position))\n                             \n            quantum_fluctuation = self.quantum_coeff * (np.random.rand(self.num_particles, self.dim) - 0.5)\n            self.position += self.velocity + quantum_fluctuation\n\n            self.position = np.clip(self.position, func.bounds.lb, func.bounds.ub)\n            \n            if self.evaluations % (self.budget // 10) == 0:\n                worst_indices = np.argsort(self.best_personal_value)[-3:]\n                self.position[worst_indices] = np.random.rand(3, self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n                self.velocity[worst_indices] = np.random.rand(3, self.dim) * 0.1\n                self.best_personal_value[worst_indices] = np.inf\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 13, "feedback": "The algorithm AQI_APSO_DL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03625 with standard deviation 0.00414.", "error": "", "parent_ids": ["8a7fac9d-fd14-465a-81b5-c830d055dd62"], "operator": null, "metadata": {"aucs": [0.04323002353262695, 0.03958467738605653, 0.033849625741415146, 0.03968851652896466, 0.03629731025912286, 0.03094902309822678, 0.03813128355964701, 0.034846169806961536, 0.02965993769727704]}}
{"id": "7957ac45-6921-4234-9ad9-45f64a20731f", "fitness": 0.040990119225745456, "name": "Enhanced_APSO_DL", "description": "Enhanced APSO-DL with a novel hybrid exploration strategy integrating particle swarming with adaptive differential mutation for improved convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.mutation_prob = 0.1\n        self.position = np.random.rand(self.num_particles, dim)\n        self.velocity = np.random.rand(self.num_particles, dim) * 0.1\n        self.best_personal_position = np.copy(self.position)\n        self.best_personal_value = np.full(self.num_particles, np.inf)\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                if self.evaluations < self.budget:\n                    # Evaluate current position\n                    value = func(self.position[i])\n                    self.evaluations += 1\n\n                    # Update personal and global bests\n                    if value < self.best_personal_value[i]:\n                        self.best_personal_value[i] = value\n                        self.best_personal_position[i] = self.position[i]\n                    if value < self.best_global_value:\n                        self.best_global_value = value\n                        self.best_global_position = self.position[i]\n\n            # Dynamic adjustment of inertia weight and learning rates\n            self.inertia_weight = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n            self.cognitive_coeff = 1.5 + np.random.rand() * 1.5\n            self.social_coeff = 1.5 + np.random.rand() * 1.5\n\n            # Update velocity and position using hybrid exploration strategy\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.cognitive_coeff * r1 * (self.best_personal_position - self.position) +\n                             self.social_coeff * r2 * (self.best_global_position - self.position))\n\n            # Apply differential mutation to some particles\n            for i in range(self.num_particles):\n                if np.random.rand() < self.mutation_prob:\n                    indices = np.random.choice(self.num_particles, 3, replace=False)\n                    F = np.random.rand()\n                    mutant = self.position[indices[0]] + F * (self.position[indices[1]] - self.position[indices[2]])\n                    self.position[i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n            self.position += self.velocity\n\n            # Ensure particles are within bounds\n            self.position = np.clip(self.position, func.bounds.lb, func.bounds.ub)\n            \n            # Adaptive restart strategy\n            if self.evaluations % (self.budget // 10) == 0:  # Restart 10% of the time\n                worst_indices = np.argsort(self.best_personal_value)[-3:]  # Identify a few worst particles\n                self.position[worst_indices] = np.random.rand(3, self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n                self.velocity[worst_indices] = np.random.rand(3, self.dim) * 0.1\n                self.best_personal_value[worst_indices] = np.inf\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 14, "feedback": "The algorithm Enhanced_APSO_DL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04099 with standard deviation 0.00454.", "error": "", "parent_ids": ["8a7fac9d-fd14-465a-81b5-c830d055dd62"], "operator": null, "metadata": {"aucs": [0.04338610634812634, 0.038845795570397246, 0.04923776071494568, 0.03993868552431057, 0.03569198633540138, 0.04539281513158877, 0.03841764376291412, 0.03429449706097576, 0.043705782583049246]}}
{"id": "eb63e093-f132-436f-be99-3b9df0fc5c79", "fitness": 0.04475101804126236, "name": "Enhanced_APSO_DL", "description": "Enhanced APSO-DL refined by adopting a dynamic mutation probability strategy for more adaptive exploration.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.mutation_prob = 0.1\n        self.position = np.random.rand(self.num_particles, dim)\n        self.velocity = np.random.rand(self.num_particles, dim) * 0.1\n        self.best_personal_position = np.copy(self.position)\n        self.best_personal_value = np.full(self.num_particles, np.inf)\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                if self.evaluations < self.budget:\n                    # Evaluate current position\n                    value = func(self.position[i])\n                    self.evaluations += 1\n\n                    # Update personal and global bests\n                    if value < self.best_personal_value[i]:\n                        self.best_personal_value[i] = value\n                        self.best_personal_position[i] = self.position[i]\n                    if value < self.best_global_value:\n                        self.best_global_value = value\n                        self.best_global_position = self.position[i]\n\n            # Dynamic adjustment of inertia weight and learning rates\n            self.inertia_weight = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n            self.cognitive_coeff = 1.5 + np.random.rand() * 1.5\n            self.social_coeff = 1.5 + np.random.rand() * 1.5\n            self.mutation_prob = 0.05 + 0.45 * (1 - self.evaluations / self.budget)  # Dynamically adjust mutation probability\n\n            # Update velocity and position using hybrid exploration strategy\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.cognitive_coeff * r1 * (self.best_personal_position - self.position) +\n                             self.social_coeff * r2 * (self.best_global_position - self.position))\n\n            # Apply differential mutation to some particles\n            for i in range(self.num_particles):\n                if np.random.rand() < self.mutation_prob:\n                    indices = np.random.choice(self.num_particles, 3, replace=False)\n                    F = np.random.rand()\n                    mutant = self.position[indices[0]] + F * (self.position[indices[1]] - self.position[indices[2]])\n                    self.position[i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n            self.position += self.velocity\n\n            # Ensure particles are within bounds\n            self.position = np.clip(self.position, func.bounds.lb, func.bounds.ub)\n            \n            # Adaptive restart strategy\n            if self.evaluations % (self.budget // 10) == 0:  # Restart 10% of the time\n                worst_indices = np.argsort(self.best_personal_value)[-3:]  # Identify a few worst particles\n                self.position[worst_indices] = np.random.rand(3, self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n                self.velocity[worst_indices] = np.random.rand(3, self.dim) * 0.1\n                self.best_personal_value[worst_indices] = np.inf\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 15, "feedback": "The algorithm Enhanced_APSO_DL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04475 with standard deviation 0.00322.", "error": "", "parent_ids": ["7957ac45-6921-4234-9ad9-45f64a20731f"], "operator": null, "metadata": {"aucs": [0.047408470625243004, 0.04499950146771803, 0.050934440964093164, 0.04372875693022238, 0.0414412715418464, 0.046995515344364525, 0.04210905719310476, 0.0398740181221533, 0.04526813018261566]}}
{"id": "167579a8-95f1-4d5c-96cb-845ae3cc79df", "fitness": 0.04312517670308222, "name": "Enhanced_APSO_DL_Cluster", "description": "Enhanced APSO-DL with adaptive particle clustering and smart restart to improve convergence and exploration balance.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass Enhanced_APSO_DL_Cluster:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.mutation_prob = 0.1\n        self.position = np.random.rand(self.num_particles, dim)\n        self.velocity = np.random.rand(self.num_particles, dim) * 0.1\n        self.best_personal_position = np.copy(self.position)\n        self.best_personal_value = np.full(self.num_particles, np.inf)\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                if self.evaluations < self.budget:\n                    # Evaluate current position\n                    value = func(self.position[i])\n                    self.evaluations += 1\n\n                    # Update personal and global bests\n                    if value < self.best_personal_value[i]:\n                        self.best_personal_value[i] = value\n                        self.best_personal_position[i] = self.position[i]\n                    if value < self.best_global_value:\n                        self.best_global_value = value\n                        self.best_global_position = self.position[i]\n\n            # Dynamic adjustment of inertia weight and learning rates\n            self.inertia_weight = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n            self.cognitive_coeff = 1.5 + np.random.rand() * 1.5\n            self.social_coeff = 1.5 + np.random.rand() * 1.5\n            self.mutation_prob = 0.05 + 0.45 * (1 - self.evaluations / self.budget)  # Dynamically adjust mutation probability\n\n            # Cluster particles and use centroids to guide exploration\n            clusters = min(5, self.num_particles // 3)\n            kmeans = KMeans(n_clusters=clusters, random_state=0).fit(self.position)\n            centroids = kmeans.cluster_centers_\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.cognitive_coeff * r1 * (self.best_personal_position - self.position) +\n                             self.social_coeff * r2 * (self.best_global_position - self.position))\n            \n            # Apply differential mutation to cluster centroids\n            for i in range(clusters):\n                indices = np.where(kmeans.labels_ == i)[0]\n                if indices.size > 0:\n                    F = np.random.rand()\n                    centroid = centroids[i]\n                    for j in indices:\n                        if np.random.rand() < self.mutation_prob:\n                            self.position[j] = np.clip(centroid + F * (self.position[j] - centroid), func.bounds.lb, func.bounds.ub)\n\n            self.position += self.velocity\n            self.position = np.clip(self.position, func.bounds.lb, func.bounds.ub)\n\n            # Smart restart strategy based on clustering\n            if self.evaluations % (self.budget // 10) == 0:\n                for i in range(clusters):\n                    indices = np.where(kmeans.labels_ == i)[0]\n                    if indices.size > 0:\n                        worst_index = indices[np.argmax(self.best_personal_value[indices])]\n                        self.position[worst_index] = np.random.rand(self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n                        self.velocity[worst_index] = np.random.rand(self.dim) * 0.1\n                        self.best_personal_value[worst_index] = np.inf\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 16, "feedback": "The algorithm Enhanced_APSO_DL_Cluster got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04313 with standard deviation 0.00227.", "error": "", "parent_ids": ["eb63e093-f132-436f-be99-3b9df0fc5c79"], "operator": null, "metadata": {"aucs": [0.04678368459161031, 0.04514686492869202, 0.04625314853053519, 0.04312708432357715, 0.04159409264282865, 0.042634402367428215, 0.04151784438923711, 0.040028518595991325, 0.04104094995784002]}}
{"id": "41a5cb32-a5a0-4c32-8d08-8f76587e3206", "fitness": 0.039573235995012085, "name": "Enhanced_APSO_DL", "description": "Introduce a dynamic restart mechanism to improve exploration during stagnant phases.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.mutation_prob = 0.1\n        self.position = np.random.rand(self.num_particles, dim)\n        self.velocity = np.random.rand(self.num_particles, dim) * 0.1\n        self.best_personal_position = np.copy(self.position)\n        self.best_personal_value = np.full(self.num_particles, np.inf)\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                if self.evaluations < self.budget:\n                    # Evaluate current position\n                    value = func(self.position[i])\n                    self.evaluations += 1\n\n                    # Update personal and global bests\n                    if value < self.best_personal_value[i]:\n                        self.best_personal_value[i] = value\n                        self.best_personal_position[i] = self.position[i]\n                    if value < self.best_global_value:\n                        self.best_global_value = value\n                        self.best_global_position = self.position[i]\n\n            # Dynamic adjustment of inertia weight and learning rates\n            self.inertia_weight = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n            self.cognitive_coeff = 1.5 + np.random.rand() * 1.5\n            self.social_coeff = 1.5 + np.random.rand() * 1.5\n            self.mutation_prob = 0.05 + 0.45 * (1 - self.evaluations / self.budget)  # Dynamically adjust mutation probability\n\n            # Update velocity and position using hybrid exploration strategy\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.cognitive_coeff * r1 * (self.best_personal_position - self.position) +\n                             self.social_coeff * r2 * (self.best_global_position - self.position))\n\n            # Apply differential mutation to some particles\n            for i in range(self.num_particles):\n                if np.random.rand() < self.mutation_prob:\n                    indices = np.random.choice(self.num_particles, 3, replace=False)\n                    F = np.random.rand()\n                    mutant = self.position[indices[0]] + F * (self.position[indices[1]] - self.position[indices[2]])\n                    self.position[i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n            self.position += self.velocity\n\n            # Ensure particles are within bounds\n            self.position = np.clip(self.position, func.bounds.lb, func.bounds.ub)\n            \n            # Adaptive restart strategy\n            if self.evaluations % (self.budget // 8) == 0:  # Restart 12.5% of the time (changed from 10%)\n                worst_indices = np.argsort(self.best_personal_value)[-3:]  # Identify a few worst particles\n                self.position[worst_indices] = np.random.rand(3, self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n                self.velocity[worst_indices] = np.random.rand(3, self.dim) * 0.1\n                self.best_personal_value[worst_indices] = np.inf\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 17, "feedback": "The algorithm Enhanced_APSO_DL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03957 with standard deviation 0.00871.", "error": "", "parent_ids": ["eb63e093-f132-436f-be99-3b9df0fc5c79"], "operator": null, "metadata": {"aucs": [0.04599092471222155, 0.030055489025999527, 0.050934440964093164, 0.04241240410066882, 0.027421975185649594, 0.046995515344364525, 0.040834883822930546, 0.02624536061656535, 0.04526813018261566]}}
{"id": "33e73a8a-8690-4dee-9f08-9dfe4e407d6c", "fitness": 0.046210606492660386, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm strategy for diversified exploration and dynamic learning rates.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand()\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 18, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04621 with standard deviation 0.00378.", "error": "", "parent_ids": ["eb63e093-f132-436f-be99-3b9df0fc5c79"], "operator": null, "metadata": {"aucs": [0.045639423894263675, 0.053374618797264684, 0.0490950205166939, 0.04202696784393434, 0.04917863337155737, 0.04524286565960722, 0.04043730747923879, 0.04734749761240187, 0.04355312325898164]}}
{"id": "be5dbe4c-d337-410f-89dd-d2a01218bb18", "fitness": 0.04161427840029784, "name": "Enhanced_APSO_DL_HybridReallocation", "description": "Enhanced Adaptive Multi-Swarm Hybrid Algorithm with Dynamic Leadership and Resource Reallocation for Balancing Exploration and Exploitation.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_HybridReallocation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 3\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1,\n            'resource_factor': 1.0\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n            self.dynamic_leadership()\n            self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand()\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def dynamic_leadership(self):\n        # Adjust resource factor based on global performance\n        improvement = (self.best_global_value + 1e-10) / (np.mean([swarm['best_personal_value'].min() for swarm in self.swarms]) + 1e-10)\n        for swarm in self.swarms:\n            swarm['resource_factor'] = improvement\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (3 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, int(self.num_particles * self.swarms[i]['resource_factor']), replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 19, "feedback": "The algorithm Enhanced_APSO_DL_HybridReallocation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04161 with standard deviation 0.00348.", "error": "", "parent_ids": ["33e73a8a-8690-4dee-9f08-9dfe4e407d6c"], "operator": null, "metadata": {"aucs": [0.04093764271430089, 0.04809314949514942, 0.04454691072844974, 0.03762917381175657, 0.04420760103644639, 0.04100052423397116, 0.03616700602134715, 0.04250801934608539, 0.03943847821517388]}}
{"id": "bfbf5ec0-1950-4bc2-bc3e-47fb501d6990", "fitness": 0.04413052814362333, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhance swarm communication by improving velocity update and migration strategy.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 2.0 + np.random.rand() * 1.5  # Slightly increased social influence\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position with improved strategy\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = 0.8 + 0.2 * np.random.rand()  # Adjusted scaling factor for mutation\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically with variation\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)  # Adjusted migration rate\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 20, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04413 with standard deviation 0.00234.", "error": "", "parent_ids": ["33e73a8a-8690-4dee-9f08-9dfe4e407d6c"], "operator": null, "metadata": {"aucs": [0.046352825909436235, 0.048146507905385216, 0.04693170793764312, 0.04270946157736544, 0.04436205300514462, 0.04324372162936296, 0.041106088394949, 0.04270044049724464, 0.04162194643607875]}}
{"id": "80a923ab-40d3-4d3b-b5f8-90213d70fcbb", "fitness": 0.04365132021691271, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_EliteChaos", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm strategy using Elite Particle Learning and Chaos Theory for improved convergence and exploration.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_EliteChaos:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.elite_particles = []\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n            self.migrate_swarms()\n            self.elite_learning(func)\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n                    self.elite_particles.append(swarm['position'][i])\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand()\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]\n\n    def elite_learning(self, func):\n        # Use elite particles to guide the search\n        sorted_elites = sorted(self.elite_particles, key=lambda x: func(x))\n        if len(sorted_elites) > self.num_particles:\n            self.elite_particles = sorted_elites[:self.num_particles]\n        for elite in self.elite_particles:\n            chaos_value = np.random.rand() * (np.sin(self.evaluations) - elite)\n            new_position = elite + chaos_value\n            new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n            new_value = func(new_position)\n            self.evaluations += 1\n            if new_value < self.best_global_value:\n                self.best_global_value = new_value\n                self.best_global_position = new_position", "configspace": "", "generation": 21, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_EliteChaos got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04365 with standard deviation 0.00228.", "error": "", "parent_ids": ["33e73a8a-8690-4dee-9f08-9dfe4e407d6c"], "operator": null, "metadata": {"aucs": [0.046355959886380305, 0.047474200311942716, 0.04608571710392928, 0.042685019715909434, 0.04374759583263588, 0.04246353878491671, 0.04107129290451561, 0.0421096425288201, 0.04086891488316435]}}
{"id": "2ca8e68d-feed-4477-b262-4994a44e8b11", "fitness": 0.046026078322526906, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Introduced a dynamic swarm merging strategy and refined mutation probability adjustment to enhance convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        # Adjusted mutation probability formula for better adaptation\n        swarm['mutation_prob'] = 0.1 + 0.4 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand()\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Improved swarm merging strategy for better information exchange\n        if self.evaluations % (self.budget // (10 * self.num_swarms)) == 0:\n            all_positions = np.concatenate([swarm['position'] for swarm in self.swarms])\n            np.random.shuffle(all_positions)\n            split_positions = np.array_split(all_positions, self.num_swarms)\n            for i, swarm in enumerate(self.swarms):\n                swarm['position'] = split_positions[i]", "configspace": "", "generation": 22, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04603 with standard deviation 0.00384.", "error": "", "parent_ids": ["33e73a8a-8690-4dee-9f08-9dfe4e407d6c"], "operator": null, "metadata": {"aucs": [0.04686174268702148, 0.04689225188358981, 0.053763179156990915, 0.04316060583124037, 0.0432094919084256, 0.04953411700527388, 0.041534048216673725, 0.04158971253551014, 0.04768955567801625]}}
{"id": "15d2ea69-4256-42d7-8847-e4ef373c14bf", "fitness": -Infinity, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_SelfAdaptiveMutation", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm strategy and Self-adaptive Mutation for improved diversity and convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_SelfAdaptiveMutation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1,\n            'mutation_factor': np.random.rand(self.num_particles),\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply self-adaptive differential mutation\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = swarm['mutation_factor'][i]\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                # Self-adaptation of mutation factor\n                if value < swarm['best_personal_value'][i]:\n                    swarm['mutation_factor'][i] = np.clip(F + np.random.normal(0, 0.1), 0, 1)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 23, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'value' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'value' referenced before assignment\")", "parent_ids": ["33e73a8a-8690-4dee-9f08-9dfe4e407d6c"], "operator": null, "metadata": {}}
{"id": "5d5717c7-814b-4bac-84a1-ea2e29854480", "fitness": 0.014946988232543392, "name": "Dynamic_Hierarchical_PSO", "description": "Introduce a Dynamic Hierarchical Particle Swarm Optimization (DHPSO) with multi-layered learning and adaptive exploration for enhanced convergence.", "code": "import numpy as np\n\nclass Dynamic_Hierarchical_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 3  # Increased the number of swarms for better exploration\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of parameters\n        swarm['inertia_weight'] = 0.5 + 0.4 * np.exp(-3 * (self.evaluations / self.budget))\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1 + 2 * np.cos(np.pi * (self.evaluations / self.budget))\n        swarm['mutation_prob'] = 0.1 * (1 - np.cos(np.pi * (self.evaluations / self.budget)))\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = 0.5 * np.random.rand()\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (3 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 24, "feedback": "The algorithm Dynamic_Hierarchical_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.01495 with standard deviation 0.01027.", "error": "", "parent_ids": ["33e73a8a-8690-4dee-9f08-9dfe4e407d6c"], "operator": null, "metadata": {"aucs": [0.0316001013358681, 0.009184736676305483, 0.009184736676305483, 0.02884204967598214, 0.007422984058760962, 0.007422984058760962, 0.02761319137172935, 0.006626055119589025, 0.006626055119589025]}}
{"id": "20a4df5a-c016-4e4f-81ca-dca6f6380b1b", "fitness": 0.04467772483537442, "name": "Enhanced_APSO_DL_InformedMultiSwarm", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm and Informed Swarm Migration using historical best positions to guide exploration.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_InformedMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1,\n            'historical_best_position': None,\n            'historical_best_value': np.inf\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.informed_migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n                # Update historical best\n                if value < swarm['historical_best_value']:\n                    swarm['historical_best_value'] = value\n                    swarm['historical_best_position'] = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand()\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def informed_migrate_swarms(self):\n        # Migrate particles between swarms based on historical best positions\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                current_swarm_best = self.swarms[i]['historical_best_position']\n                next_swarm_best = self.swarms[i+1]['historical_best_position']\n                if current_swarm_best is not None and next_swarm_best is not None:\n                    self.swarms[i]['position'][idx] = next_swarm_best\n                    self.swarms[i+1]['position'][idx] = current_swarm_best", "configspace": "", "generation": 25, "feedback": "The algorithm Enhanced_APSO_DL_InformedMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04468 with standard deviation 0.00459.", "error": "", "parent_ids": ["33e73a8a-8690-4dee-9f08-9dfe4e407d6c"], "operator": null, "metadata": {"aucs": [0.04313353412796661, 0.053374618797264684, 0.046709887848134235, 0.039706526300116174, 0.04917863337155737, 0.04303550557704383, 0.038193825984746965, 0.04734749761240187, 0.041419493899138016]}}
{"id": "f0a5d78d-dd6c-4c1e-b9f2-a74f04c81f5c", "fitness": 0.04374572791239528, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Refined", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm strategy, leveraging adaptive learning rates, differential evolution, and a new particle exchange method for increased convergence speed and stability.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Refined:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.exchange_particles()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        inertia_weight_decay = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['inertia_weight'] = inertia_weight_decay\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand()\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def exchange_particles(self):\n        if self.evaluations % (self.budget // (3 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 26, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Refined got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04375 with standard deviation 0.00499.", "error": "", "parent_ids": ["33e73a8a-8690-4dee-9f08-9dfe4e407d6c"], "operator": null, "metadata": {"aucs": [0.04246730965912837, 0.053374618797264684, 0.04440263372969788, 0.039087810755018104, 0.04917863337155737, 0.040901217272557244, 0.037594923658741064, 0.04734749761240187, 0.03935690635519096]}}
{"id": "9b5553a4-8076-431f-830c-d230c8a0a318", "fitness": 0.040831833202120235, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm and Dynamic Neighborhood Strategy for improved convergence and exploitation.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1,\n            'neighborhood_best_position': np.random.rand(self.num_particles, self.dim)\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n                # Update neighborhood best\n                neighbors = self.get_neighbors(i)\n                best_neighbor = min(neighbors, key=lambda x: swarm['best_personal_value'][x])\n                swarm['neighborhood_best_position'][i] = swarm['best_personal_position'][best_neighbor]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2, r3 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']) +\n                             0.5 * r3 * (swarm['neighborhood_best_position'] - swarm['position']))\n\n        # Apply differential mutation\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand()\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def get_neighbors(self, index):\n        # Simple dynamic neighborhood strategy\n        return [(index - 1) % self.num_particles, index, (index + 1) % self.num_particles]\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 27, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04083 with standard deviation 0.00394.", "error": "", "parent_ids": ["33e73a8a-8690-4dee-9f08-9dfe4e407d6c"], "operator": null, "metadata": {"aucs": [0.03933221864359138, 0.04368120034991285, 0.04795961628367673, 0.036159801263514146, 0.040188674501534405, 0.04420552384640419, 0.034753949110026716, 0.03864965763833117, 0.04255585718209054]}}
{"id": "23a720cd-2efd-4149-95de-aa74ef0380c4", "fitness": 0.04944821869371972, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm and adaptive mutation scaling for improved exploration.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 28, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04945 with standard deviation 0.00273.", "error": "", "parent_ids": ["33e73a8a-8690-4dee-9f08-9dfe4e407d6c"], "operator": null, "metadata": {"aucs": [0.054411888219242366, 0.05154156855801395, 0.052534751549889736, 0.05009335708588136, 0.04749616587650651, 0.04840891742215503, 0.04821402383986639, 0.045726761301008056, 0.04660653439091411]}}
{"id": "cafad7aa-3897-42c4-a1ab-aeced3849be4", "fitness": 0.041583956431112665, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_V2", "description": "Enhanced APSO-DL with Dynamic Multi-Swarm Communication and Adaptive Mutation for improved convergence and exploration-exploitation balance.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_V2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = min(3, budget // (self.num_particles * 5))\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n            self.dynamic_migration()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def dynamic_migration(self):\n        if self.evaluations % (self.budget // (3 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 29, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_V2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04158 with standard deviation 0.00350.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04808282444998313, 0.04090184101157501, 0.044463776948770795, 0.044251320331222144, 0.03758037809427117, 0.04092468114239245, 0.04257184746800591, 0.0361133180015335, 0.039365620432259885]}}
{"id": "d927e2a9-311a-4d83-b80e-b468a6c90363", "fitness": 0.04517267919198764, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced APSO-DL with dynamic swarm count, adaptive learning rates, and improved mutation diversity for better convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2 + (budget > 1000)  # Dynamic number of swarms\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.0 + np.random.rand()  # Reduced cognitive influence\n        swarm['social_coeff'] = 2.0 + np.random.rand()  # Increased social influence\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = 0.5 + np.random.rand()  # Increased mutation diversity\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 30, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04517 with standard deviation 0.00285.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.0506555524530925, 0.0462706531634679, 0.04781464582336026, 0.04670061658040525, 0.04263044944846006, 0.04406652360081298, 0.04496765647661494, 0.041028477141546116, 0.04241953804012877]}}
{"id": "818286eb-e884-4478-b323-6f7471c1631b", "fitness": 0.04517513495942492, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Introduced adaptive inertia weight decay and enhanced swarm migration for improved convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.2 + 0.7 * (1 - self.evaluations / self.budget)  # Modified line\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (4 * self.num_swarms)) == 0:  # Modified line\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 31, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04518 with standard deviation 0.00280.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04677317613624565, 0.05068319555561307, 0.04731591865658347, 0.04309756484582783, 0.046700026565356545, 0.0436009965266263, 0.04148082270232767, 0.044956569841838045, 0.041967943804405716]}}
{"id": "5ddd0ff8-d381-4189-b5b2-327d96140fb3", "fitness": 0.03919072024123259, "name": "Improved_Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Improved Enhanced_APSO_DL_AdaptiveMultiSwarm by introducing multi-level adaptive inertia weights and cognitive coefficients for balanced exploration and exploitation.", "code": "import numpy as np\n\nclass Improved_Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Multi-level adaptive inertia and cognitive coefficients\n        progress_ratio = self.evaluations / self.budget\n        swarm['inertia_weight'] = 0.4 + (0.5 * (1 - progress_ratio**2))\n        swarm['cognitive_coeff'] = 1.5 + (1.5 * (1 - progress_ratio))\n        swarm['social_coeff'] = 1.5 + (1.5 * (progress_ratio))\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - progress_ratio)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - progress_ratio))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 32, "feedback": "The algorithm Improved_Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03919 with standard deviation 0.00364.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04431828937471238, 0.04402095038635789, 0.03739735237832176, 0.04082532236663727, 0.04053312200248116, 0.03434797547037294, 0.039284437204645695, 0.03899501121729432, 0.03299402177026989]}}
{"id": "8bbab26e-9031-4987-a1ba-835a892a32a9", "fitness": 0.045866893420487526, "name": "Enhanced_APSO_DL_DynamicSwarmStochasticAdaptation", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm incorporating dynamic swarm grouping and stochastic parameter adaptation for improved convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_DynamicSwarmStochasticAdaptation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = max(2, budget // 1000)  # Dynamic number of swarms based on budget\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.uniform(0.5, 2.0)\n        swarm['social_coeff'] = 1.5 + np.random.uniform(0.5, 2.0)\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.uniform(0.4, 0.9) * (1 - self.evaluations / self.budget)\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 33, "feedback": "The algorithm Enhanced_APSO_DL_DynamicSwarmStochasticAdaptation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04587 with standard deviation 0.00264.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04733149790373603, 0.04900509325767288, 0.05064249543321209, 0.043611874472593626, 0.04515354286108597, 0.04667661978873627, 0.041976989320349545, 0.04346424985194419, 0.04493967789505715]}}
{"id": "b42c040b-fed7-4124-9fac-78486a326e1f", "fitness": 0.04231140056482271, "name": "RefinedEnhanced_APSO_DL_AdaptiveMultiSwarm", "description": "A refined Enhanced APSO-DL with Adaptive Multi-Swarm integrating Lvy flight for improved exploration and swarm synergy.", "code": "import numpy as np\n\nclass RefinedEnhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        # Integrate Levy flight for exploration\n        if np.random.rand() < 0.3:  # 30% chance to perform Levy flight\n            levy = np.random.standard_cauchy(size=(self.num_particles, self.dim))\n            swarm['position'] += levy * (swarm['position'] - swarm['best_personal_position'])\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 34, "feedback": "The algorithm RefinedEnhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04231 with standard deviation 0.00424.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04030786712528456, 0.0455734746973081, 0.04978661112206817, 0.03707791079419631, 0.041981646792104144, 0.04587149323053641, 0.03564760249014487, 0.04039989748771233, 0.04415610134404946]}}
{"id": "0d3df820-3e03-4024-b608-f2e66cb86c32", "fitness": 0.045361082724902614, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Introduced a mutation operator with Gaussian noise to enhance the exploration capability.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling and Gaussian perturbation\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant + np.random.normal(0, 0.1, self.dim), func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 35, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04536 with standard deviation 0.00284.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.048577519996193974, 0.046225600687305635, 0.050579747037014955, 0.04477392161969307, 0.04258896522143418, 0.04658036280271216, 0.043103843087632265, 0.04098846693790914, 0.044831317134228166]}}
{"id": "9a85fda8-f34b-445d-a0aa-7e5ae5e58c30", "fitness": 0.041825653780149255, "name": "Optimized_ADR_HybridMutation_PSO", "description": "Optimized Adaptive Multi-Swarm PSO with Dynamic Dimensionality Reduction and Hybrid Mutation for Enhanced Exploration and Exploitation", "code": "import numpy as np\n\nclass Optimized_ADR_HybridMutation_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 3\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1,\n            'dimension_mask': np.ones(self.dim, dtype=bool)\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n            self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                masked_position = swarm['position'][i] * swarm['dimension_mask']\n                value = func(masked_position)\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of parameters\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply hybrid mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n        # Dynamic dimensionality reduction\n        if self.evaluations % (self.budget // 10) == 0:\n            swarm['dimension_mask'] = np.random.rand(self.dim) < (0.5 + 0.5 * (self.evaluations / self.budget))\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (4 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 36, "feedback": "The algorithm Optimized_ADR_HybridMutation_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04183 with standard deviation 0.00314.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04808282444998313, 0.04241010263057943, 0.04372690851427219, 0.044251320331222144, 0.03897824402273842, 0.04024069027312083, 0.04257184746800591, 0.03746528807724292, 0.03870365825417832]}}
{"id": "db91e564-810e-44df-846e-aae15c40ce31", "fitness": 0.03565788161665931, "name": "Improved_APSO_DL_AdaptiveMultiSwarm", "description": "Improved APSO-DL with dynamic multi-swarm interaction and adaptive learning strategy for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass Improved_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 8)\n        self.num_swarms = 3\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.adaptive_migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.5 + 0.4 * (1 - (self.evaluations / float(self.budget)) ** 0.5)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - (self.evaluations / float(self.budget)) ** 0.5)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.7 + 0.3 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def adaptive_migrate_swarms(self):\n        # Migrate particles between swarms dynamically based on performance\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                better_swarm = i if np.mean(self.swarms[i]['best_personal_value']) < np.mean(self.swarms[i+1]['best_personal_value']) else i+1\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[better_swarm]['position'][idx], self.swarms[i+1-better_swarm]['position'][idx] = \\\n                    self.swarms[i+1-better_swarm]['position'][idx], self.swarms[better_swarm]['position'][idx]", "configspace": "", "generation": 37, "feedback": "The algorithm Improved_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03566 with standard deviation 0.00729.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04082485468792996, 0.02818299557642734, 0.045723602711833844, 0.03751245576399764, 0.025616054892389495, 0.04207390316876025, 0.03604890622093049, 0.024468842733543772, 0.04046931879412097]}}
{"id": "0dbaa926-654d-4e25-b818-fd00a276c308", "fitness": 0.04244359218057023, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced APSO-DL with Multi-Swarm Communication and Adaptive Chaos-Inspired Perturbation for Superior Exploration.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1,\n            'chaos_perturbation': 0.05\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n        swarm['chaos_perturbation'] = 0.05 + 0.15 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        # Chaos-inspired perturbation for enhanced exploration\n        chaotic_seq = np.sin(np.arange(self.dim) + np.pi * np.random.rand())\n        perturbation = swarm['chaos_perturbation'] * chaotic_seq\n        swarm['position'] += perturbation\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 38, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04244 with standard deviation 0.00464.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.03922859359945219, 0.048678577331225936, 0.048182451541256555, 0.036069157123946094, 0.04485489360238182, 0.04439754098985649, 0.034668536479784406, 0.04317694415814466, 0.04273563479908393]}}
{"id": "4ab03edc-cfb1-493f-a1e7-f7b5c91ab905", "fitness": 0.04620044580600002, "name": "Improved_Adaptive_MultiSwarm_APSO", "description": "Introducing a novel dynamic topology adjustment mechanism alongside adaptive parameters for a more effective search balance between exploration and exploitation.", "code": "import numpy as np\n\nclass Improved_Adaptive_MultiSwarm_APSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if np.random.rand() < 0.1:  # Dynamic topology adjustment\n                    self.adjust_topology(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]\n\n    def adjust_topology(self, swarm):\n        # Randomly shuffle the particles to reconfigure topology\n        np.random.shuffle(swarm['position'])", "configspace": "", "generation": 39, "feedback": "The algorithm Improved_Adaptive_MultiSwarm_APSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04620 with standard deviation 0.00307.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.047851131705406225, 0.047852083973357606, 0.05234780475810963, 0.0440933928285675, 0.044089739259799754, 0.0482421308923624, 0.04244267410584668, 0.042437290855406484, 0.04644776387514393]}}
{"id": "3c737152-66c1-4361-9bff-df60fc0b7896", "fitness": 0.03636571141423084, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm and dynamic multi-scale mutation for improved global exploration and convergence speed.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 3  # Increased number of swarms for better exploration\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                scale_factor = np.random.rand() * (0.5 + (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + scale_factor * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                intermediate = np.random.rand(self.dim) * (swarm['position'][i] - mutant)  # New intermediate step for mutation\n                swarm['position'][i] = np.clip(mutant + intermediate, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 40, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03637 with standard deviation 0.00613.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04150146747646766, 0.0305280676285391, 0.04486674380350053, 0.0381724225436344, 0.02785193854942003, 0.04129279213943193, 0.036700989906931714, 0.026657496704404093, 0.03971948397574809]}}
{"id": "54c24222-d30b-462e-ae49-981543e70b4a", "fitness": 0.041583956431112665, "name": "Improved_APSO_DL_EnhancedMultiSwarm", "description": "Improved APSO-DL with Enhanced Multi-Swarm Cooperation and Adaptive Learning for Robust Exploration and Exploitation.", "code": "import numpy as np\n\nclass Improved_APSO_DL_EnhancedMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 3\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n            self.migrate_swarms()\n\n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]\n\n            # Introduce more interaction between swarms for better convergence\n            for i in range(self.num_swarms):\n                for j in range(i+1, self.num_swarms):\n                    idx = np.random.choice(self.num_particles, self.num_particles // 4, replace=False)\n                    self.swarms[i]['position'][idx], self.swarms[j]['position'][idx] = \\\n                        0.5 * (self.swarms[i]['position'][idx] + self.swarms[j]['position'][idx]), \\\n                        0.5 * (self.swarms[i]['position'][idx] + self.swarms[j]['position'][idx])", "configspace": "", "generation": 41, "feedback": "The algorithm Improved_APSO_DL_EnhancedMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04158 with standard deviation 0.00350.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04808282444998313, 0.04090184101157501, 0.044463776948770795, 0.044251320331222144, 0.03758037809427117, 0.04092468114239245, 0.04257184746800591, 0.0361133180015335, 0.039365620432259885]}}
{"id": "0202c011-7558-4355-a48a-6765e9651158", "fitness": 0.04386970108162966, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Crossover", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm featuring a novel cooperative crossover mechanism for improved convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Crossover:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        # Cooperative crossover between particles\n        if np.random.rand() < 0.3:\n            idx1, idx2 = np.random.choice(self.num_particles, 2, replace=False)\n            crossover_point = np.random.randint(1, self.dim)\n            swarm['position'][idx1][:crossover_point], swarm['position'][idx2][:crossover_point] = \\\n                swarm['position'][idx2][:crossover_point], swarm['position'][idx1][:crossover_point]\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 42, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Crossover got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04387 with standard deviation 0.00463.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04179946288658565, 0.046481536247436894, 0.05234780475810963, 0.038472362698743434, 0.04282238496764157, 0.0482421308923624, 0.03700117347251475, 0.0412126899361287, 0.04644776387514393]}}
{"id": "8c223ca5-0fa6-4830-b77a-9df422991351", "fitness": 0.04601778243793028, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Introduce non-linear dynamic adjustment for inertia weight to enhance balance between exploration and exploitation.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Non-linear dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * np.cos(np.pi * self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 43, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04602 with standard deviation 0.00332.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.049653165819810496, 0.051965377422408476, 0.045849546268652897, 0.045759494594665884, 0.04788361085670112, 0.042242606911590874, 0.04405248419773167, 0.04609934948411376, 0.040654406385697395]}}
{"id": "349746b4-1b3c-455f-92cc-ec2aa0709a29", "fitness": 0.0395106902509772, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhancing the mutation strategy with hybrid crossover to improve diversity and convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                # Hybrid crossover: blend parent and mutant\n                swarm['position'][i] = 0.5 * (swarm['position'][i] + mutant)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 44, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03951 with standard deviation 0.00399.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.03821912855968124, 0.04158716645208149, 0.047001480339078694, 0.03510676343625896, 0.038237841822313934, 0.04329424350667155, 0.03372646482571873, 0.03675844252815763, 0.041664680788832564]}}
{"id": "74d9c683-dcb4-4a26-847c-cc71074bd1b5", "fitness": -Infinity, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Clustering", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm using dynamic mutation strategies and clustering for robust exploration-exploitation balance.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Clustering:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n            self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 0.5 + np.random.rand() * 2.5\n        swarm['social_coeff'] = 0.5 + np.random.rand() * 2.5\n        swarm['mutation_prob'] = 0.1 + 0.4 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]\n\n        all_positions = np.vstack([swarm['position'] for swarm in self.swarms])\n        kmeans = KMeans(n_clusters=self.num_swarms)\n        labels = kmeans.fit_predict(all_positions)\n\n        for i, swarm in enumerate(self.swarms):\n            cluster_indices = np.where(labels == i)[0]\n            if len(cluster_indices) > 0:\n                selected_positions = all_positions[cluster_indices]\n                swarm['position'][:len(selected_positions)] = selected_positions", "configspace": "", "generation": 45, "feedback": "An exception occurred: ValueError('could not broadcast input array from shape (79,10) into shape (40,10)').", "error": "ValueError('could not broadcast input array from shape (79,10) into shape (40,10)')", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {}}
{"id": "1bede30b-cfd0-4ee0-a5f7-1618447b9997", "fitness": 0.0461625316576295, "name": "Enhanced_APSO_DL_EliteAdaptiveMultiSwarm", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm and elite particle strategy for improved convergence and exploration.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_EliteAdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.elite_ratio = 0.1  # Proportion of elite particles\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Elite particle strategy: only mutate top particles\n        num_elites = int(self.num_particles * self.elite_ratio)\n        elite_indices = np.argsort(swarm['best_personal_value'])[:num_elites]\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        # Ensure elite particles retain their positions post-mutation\n        swarm['position'][elite_indices] = np.clip(swarm['best_personal_position'][elite_indices], func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 46, "feedback": "The algorithm Enhanced_APSO_DL_EliteAdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04616 with standard deviation 0.00275.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04710381879109127, 0.050632122006343705, 0.050179394204594385, 0.04340911143612902, 0.046663158389564474, 0.0462448686339314, 0.041784251098034964, 0.04492503220167565, 0.044521028157300635]}}
{"id": "ce340b1b-304c-4963-94f9-ca4b41ae8154", "fitness": 0.04688288799806347, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Introduced adaptive swarm count and dynamic velocity scaling for enhanced adaptability.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = min(5, max(2, budget // 200))  # Adaptive swarm count\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['velocity'] *= (0.9 + 0.1 * np.random.rand())  # Dynamic velocity scaling\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 47, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04688 with standard deviation 0.00278.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04796450081764414, 0.05042729478451036, 0.051777543531678405, 0.04422235799808394, 0.046463947712985454, 0.047769614121512816, 0.04257755803775465, 0.044729255808452884, 0.04601391916994857]}}
{"id": "54429896-05e3-498f-8f29-c4ce7e159b7c", "fitness": -Infinity, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm, introducing adaptive learning rate scaling for improved convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             max(0.5, min(2.0, swarm['cognitive_coeff'] * r1)) * (swarm['best_personal_position'] - swarm['position']) +\n                             max(0.5, min(2.0, swarm['social_coeff'] * r2)) * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 48, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {}}
{"id": "81728fb0-0651-4c9f-aa19-ae8fd77ec372", "fitness": 0.04701344987078296, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Optimized Enhanced APSO-DL with adaptive mutation scaling and swarm communication for improved exploration and convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n\n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]\n            # Added communication between swarms for improved convergence\n            if self.best_global_position is not None:\n                for swarm in self.swarms:\n                    global_info = np.random.rand(self.num_particles, self.dim)\n                    swarm['position'] += global_info * (self.best_global_position - swarm['position'])", "configspace": "", "generation": 49, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04701 with standard deviation 0.00264.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.05172195879076713, 0.04872213967565364, 0.05021127894806687, 0.04765268924017285, 0.04489848433007193, 0.04627274249097679, 0.04587394528146316, 0.043220489385528915, 0.044547320694345305]}}
{"id": "575615b5-9c0f-4586-8559-7f25ae818033", "fitness": 0.046982610123001235, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Introduced a dynamic swarm interaction mechanism based on fitness proximity to enhance exploration.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles based on fitness proximity to enhance exploration\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                if np.random.rand() < 0.5:  # New condition for dynamic interaction\n                    self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                        self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 50, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04698 with standard deviation 0.00344.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04648049339154159, 0.05154156855801395, 0.052534751549889736, 0.0428277890818376, 0.04749616587650651, 0.04840891742215503, 0.041220509535144556, 0.045726761301008056, 0.04660653439091411]}}
{"id": "06b78386-b822-4afe-ab11-1ff6a7ca168f", "fitness": 0.04096032154965439, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced swarm migration and diversified differential mutation strategy to improve convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.6 + 0.4 * (1 - self.evaluations / self.budget))  # Increased adaptive scaling\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                mutant += np.random.normal(0, 0.01, self.dim)  # Added Gaussian noise for diversification\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (4 * self.num_swarms)) == 0:  # More frequent migration\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 51, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04096 with standard deviation 0.00542.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.03633356476837735, 0.0476035824429486, 0.04744078520125594, 0.03334701397349138, 0.043862659568317586, 0.04372513569255798, 0.03201974306421207, 0.04221896535055636, 0.042091443885172275]}}
{"id": "111169a2-e320-4d04-b779-5c9ed6fb3303", "fitness": 0.04624235635466437, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm and modified velocity update for improved convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']) * (1 - self.evaluations / self.budget))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 52, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04624 with standard deviation 0.00300.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.052175576166318405, 0.04853050413670812, 0.04746456370546659, 0.04808550124888755, 0.04472032286281846, 0.04374691456748647, 0.04629748954955526, 0.04304793052412326, 0.04211240443061526]}}
{"id": "ed30ac5a-0513-49f4-8fff-d30c9e975081", "fitness": 0.030757346716440245, "name": "Enhanced_APSO_DL_FeedbackLearning", "description": "Incorporate a feedback-based learning mechanism to dynamically adjust swarm parameters using performance history to enhance convergence speed and solution accuracy.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_FeedbackLearning:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.history = []\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.feedback_adjustment(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def feedback_adjustment(self, swarm):\n        # Use performance history to adjust swarm parameters\n        if len(self.history) >= self.num_particles:\n            improvement = [p - c for p, c in zip(self.history[-self.num_particles:], swarm['best_personal_value'])]\n            improvement_rate = np.mean(improvement)\n            # Adjust inertia weight and learning rates based on improvement rate\n            swarm['inertia_weight'] = max(0.4, 0.9 - 0.5 * improvement_rate)\n            swarm['cognitive_coeff'] = max(1.5, 2.5 - improvement_rate)\n            swarm['social_coeff'] = max(1.5, 2.5 - improvement_rate)\n            swarm['mutation_prob'] = max(0.05, 0.5 - 0.45 * improvement_rate)\n        self.history.append(np.mean(swarm['best_personal_value']))\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 53, "feedback": "The algorithm Enhanced_APSO_DL_FeedbackLearning got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03076 with standard deviation 0.00538.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.038509155812826545, 0.02580802534168347, 0.034847915421501896, 0.03538960525936774, 0.0233765693096063, 0.031942168455237896, 0.034006010714635715, 0.02228722461786803, 0.03064944551523463]}}
{"id": "e84de14b-9e50-42a6-92fd-30d8dc73c1b4", "fitness": 0.04398641665433077, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_LeaderMutation", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm and Leader-Based Mutation for robust exploration and exploitation balance.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_LeaderMutation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                leader = swarm['best_personal_position'][np.argmin(swarm['best_personal_value'])]\n                mutant = self.leader_based_mutation(i, swarm['position'], leader, func)\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def leader_based_mutation(self, idx, positions, leader, func):\n        indices = np.random.choice(self.num_particles, 2, replace=False)\n        F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n        mutant = leader + F * (positions[indices[0]] - positions[indices[1]])\n        return mutant\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 54, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_LeaderMutation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04399 with standard deviation 0.00409.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04187090126924908, 0.04931921037296161, 0.04980699375298303, 0.03853224453537185, 0.045450950702029536, 0.04589956044081278, 0.037056446718470415, 0.04375444440197784, 0.0441869976951208]}}
{"id": "7f590ce5-ef02-4031-b6bb-2456f45f0760", "fitness": -Infinity, "name": "Enhanced_APSO_DL_DynamicHierarchicalMultiSwarm", "description": "Enhanced APSO-DL with Dynamic Hierarchical Multi-Swarm that adapts swarm sizes and utilizes an elite selection mechanism for improved convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_DynamicHierarchicalMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n            self.adapt_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]\n\n    def adapt_swarms(self):\n        # Dynamically adapt swarm size based on performance\n        self.swarms.sort(key=lambda s: np.min(s['best_personal_value']))\n        top_swarm = self.swarms[0]\n        for i in range(1, self.num_swarms):\n            if np.random.rand() < 0.5:\n                # Merge weaker swarm into stronger one\n                indices = np.random.choice(self.num_particles, self.num_particles // 4, replace=False)\n                top_swarm['position'] = np.concatenate(\n                    (top_swarm['position'], self.swarms[i]['position'][indices]), axis=0)\n                top_swarm['velocity'] = np.concatenate(\n                    (top_swarm['velocity'], self.swarms[i]['velocity'][indices]), axis=0)\n                top_swarm['best_personal_position'] = np.concatenate(\n                    (top_swarm['best_personal_position'], self.swarms[i]['best_personal_position'][indices]), axis=0)\n                top_swarm['best_personal_value'] = np.concatenate(\n                    (top_swarm['best_personal_value'], self.swarms[i]['best_personal_value'][indices]), axis=0)\n                # Reduce the size of the weaker swarm\n                self.swarms[i]['position'] = self.swarms[i]['position'][self.num_particles // 4:]\n                self.swarms[i]['velocity'] = self.swarms[i]['velocity'][self.num_particles // 4:]\n                self.swarms[i]['best_personal_position'] = self.swarms[i]['best_personal_position'][self.num_particles // 4:]\n                self.swarms[i]['best_personal_value'] = self.swarms[i]['best_personal_value'][self.num_particles // 4:]\n\n            # Ensure swarm sizes remain the same\n            self.swarms[i]['position'] = self.swarms[i]['position'][:self.num_particles]\n            self.swarms[i]['velocity'] = self.swarms[i]['velocity'][:self.num_particles]\n            self.swarms[i]['best_personal_position'] = self.swarms[i]['best_personal_position'][:self.num_particles]\n            self.swarms[i]['best_personal_value'] = self.swarms[i]['best_personal_value'][:self.num_particles]", "configspace": "", "generation": 55, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (40,10) (50,10) ').", "error": "ValueError('operands could not be broadcast together with shapes (40,10) (50,10) ')", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {}}
{"id": "72fbed97-35c2-4da0-bf28-7e122fbc871a", "fitness": -Infinity, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm, incorporating adaptive swarm merging and improved mutation for better convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_and_merge_swarms(func)\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_and_merge_swarms(self, func):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]\n\n            best_swarms = sorted(self.swarms, key=lambda x: np.min(x['best_personal_value']))[:self.num_swarms // 2]\n            new_swarms = []\n            for swarm in best_swarms:\n                new_swarm = self.init_swarm()\n                new_swarm['position'] = np.copy(swarm['position'])\n                new_swarm['best_personal_position'] = np.copy(swarm['best_personal_position'])\n                new_swarm['best_personal_value'] = np.copy(swarm['best_personal_value'])\n                new_swarms.append(new_swarm)\n            self.swarms = new_swarms", "configspace": "", "generation": 56, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {}}
{"id": "b3db93b0-e741-4846-ba1f-2e980ba806cd", "fitness": 0.0440010358522274, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm using dynamic diversity promotion and improved mutation mechanism for better exploration and exploitation balance.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = 0.8 + 0.2 * np.random.rand()  # Improved mutation scaling\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])  # Enhanced differential mutation\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        distance_to_best = np.linalg.norm(swarm['position'] - self.best_global_position, axis=1)  # Dynamic diversity promotion\n        if np.mean(distance_to_best) < 0.1:\n            swarm['velocity'] += np.random.randn(self.num_particles, self.dim) * 0.1  # Add random noise for diversity\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 57, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04400 with standard deviation 0.00397.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04209294841362132, 0.04947592166144055, 0.04947532245597719, 0.038733020242931016, 0.0455956608048832, 0.04559717732181656, 0.037248534965468094, 0.043894192022234324, 0.043896544781674374]}}
{"id": "9729f592-03da-4e34-a5cb-d178b624232b", "fitness": 0.04420421825934335, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm and hybrid mutation using crossover for improved exploration.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with crossover\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                crossover = np.random.rand(self.dim) < 0.5\n                swarm['position'][i] = np.where(crossover, mutant, swarm['position'][i])\n                swarm['position'][i] = np.clip(swarm['position'][i], func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 58, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04420 with standard deviation 0.00379.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04557354490347931, 0.044346167550864846, 0.05176087211382896, 0.04198641619071064, 0.040840907297293394, 0.04770175539130184, 0.040406436566823634, 0.039295270489613565, 0.045926593830173945]}}
{"id": "48ff7d42-e904-42af-aa7b-4aed45139c3d", "fitness": 0.04944821869371972, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_LeaderSelection", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm and dynamic leader selection for improved convergence speed.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_LeaderSelection:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1,\n            'leader_idx': None\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.update_leader(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def update_leader(self, swarm):\n        # Select a new leader from the swarm based on best personal value\n        best_index = np.argmin(swarm['best_personal_value'])\n        swarm['leader_idx'] = best_index\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 59, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_LeaderSelection got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04945 with standard deviation 0.00273.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.054411888219242366, 0.05154156855801395, 0.052534751549889736, 0.05009335708588136, 0.04749616587650651, 0.04840891742215503, 0.04821402383986639, 0.045726761301008056, 0.04660653439091411]}}
{"id": "222359ce-69eb-4af8-b0b3-cb358bac0a13", "fitness": 0.046871743337387944, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm featuring dynamic swarm migration based on swarm performance and adaptive inertia weight convergence for better optimization.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n            self.dynamic_migration(func)\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def dynamic_migration(self, func):\n        # Migrate particles between swarms based on swarm performance\n        performances = [np.mean(swarm['best_personal_value']) for swarm in self.swarms]\n        best_swarm_idx = np.argmin(performances)\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms):\n                if i != best_swarm_idx:\n                    idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                    self.swarms[i]['position'][idx], self.swarms[best_swarm_idx]['position'][idx] = \\\n                        self.swarms[best_swarm_idx]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 60, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04687 with standard deviation 0.00331.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04659249760030115, 0.05154156855801395, 0.052068350893008764, 0.042931519240300586, 0.04749616587650651, 0.04797788445494722, 0.04132080379863867, 0.045726761301008056, 0.046190138313766615]}}
{"id": "cacb3fbb-d058-478b-bf9c-374e9bb519fa", "fitness": 0.04312254395537148, "name": "HybridEnhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Hybrid Enhanced APSO-DL with Adaptive Multi-Swarm incorporating Lvy Flights for enhanced exploration and convergence.", "code": "import numpy as np\n\nclass HybridEnhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n            # Apply Lvy Flights for enhanced exploration\n            if np.random.rand() < 0.2:  # 20% probability to apply Lvy flight\n                levy_step = self.levy_flight(self.dim)\n                swarm['position'][i] += levy_step\n                swarm['position'][i] = np.clip(swarm['position'][i], func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def levy_flight(self, dim, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma, dim)\n        v = np.random.normal(0, 1, dim)\n        step = u / abs(v) ** (1 / beta)\n        return step\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 61, "feedback": "The algorithm HybridEnhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04312 with standard deviation 0.00424.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04141852014050318, 0.050844959838029946, 0.045967589060298186, 0.03811382713506739, 0.046863512990128586, 0.04235621191931482, 0.03665217968115031, 0.04512009934280248, 0.04076599549104842]}}
{"id": "b06f8a63-fc93-4810-953e-3293ce0148df", "fitness": 0.043428882296945144, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Refined the mutation probability update to enable enhanced exploration by adding a small random factor.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget) + np.random.rand() * 0.01  # Modified line\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 62, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04343 with standard deviation 0.00632.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.03900307190446106, 0.04588378444588481, 0.054372765195424555, 0.0358578238135, 0.042273749323468945, 0.05009328343276964, 0.03446319723671909, 0.04068425838348533, 0.048228006936792855]}}
{"id": "baa20726-508b-4f20-8e17-08c97907ef43", "fitness": -Infinity, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Improved swarm communication by adjusting migration frequency based on fitness diversity.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        fitness_diversity = np.std([s['best_personal_value'] for s in self.swarms])\n        migration_interval = max(self.budget // (5 * self.num_swarms), int(self.budget // (10 * fitness_diversity)))\n        if self.evaluations % migration_interval == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 63, "feedback": "An exception occurred: ValueError('cannot convert float NaN to integer').", "error": "ValueError('cannot convert float NaN to integer')", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {}}
{"id": "fdba8d65-d63a-4cb0-ab7b-6976e3475439", "fitness": 0.017350147179659707, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced multi-swarm strategy with improved inertia and mutation dynamics for better convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.5 + 0.4 * np.cos((np.pi / 2) * (self.evaluations / self.budget))\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.1 * np.exp(-0.03 * self.evaluations)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 64, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.01735 with standard deviation 0.00854.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.030447184777091962, 0.017725363934956384, 0.009184736676305483, 0.02778640886309569, 0.01564792809415516, 0.007422984058760962, 0.026598253441519448, 0.014712409651463254, 0.006626055119589025]}}
{"id": "6a10d91a-2494-4374-85bb-5bbb85aa2004", "fitness": 0.043209969897893025, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm and refined mutation probability for improved exploration.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget) + 0.05\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 65, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04321 with standard deviation 0.00228.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04711748455760423, 0.045604081821570275, 0.04576766593933801, 0.043407425243367714, 0.042019258072310106, 0.042170613458686335, 0.04177673374400559, 0.040440099511396044, 0.04058636673275895]}}
{"id": "9c630350-b11c-4475-a1e5-5c8d80527fd2", "fitness": 0.0472334597771218, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm, featuring improved global best sharing across swarms for better convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically and share global best\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]\n                self.swarms[i+1]['position'] = np.where(self.best_global_value < np.min(self.swarms[i+1]['best_personal_value']),\n                                                        self.best_global_position, self.swarms[i+1]['position'])", "configspace": "", "generation": 66, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04723 with standard deviation 0.00320.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04728133968304504, 0.05154156855801395, 0.052534751549889736, 0.043568628303368606, 0.04749616587650651, 0.04840891742215503, 0.04193647090919517, 0.045726761301008056, 0.04660653439091411]}}
{"id": "39fe83ac-f838-4285-a6ac-097376ad5ee7", "fitness": 0.04492657768268328, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Refinement of adaptive mutation scaling and swarm migration to enhance exploration and convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.35 * (1 - self.evaluations / self.budget)  # Modified line\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.6 + 0.4 * (1 - self.evaluations / self.budget))  # Modified line\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (3 * self.num_swarms)) == 0:  # Modified line\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 67, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04493 with standard deviation 0.00506.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04158934711057738, 0.04931356183891067, 0.05311860101701693, 0.03826857229955305, 0.04543593886151698, 0.04894872181976362, 0.036800358144443046, 0.043735955090461465, 0.04712814296190637]}}
{"id": "0b607103-3766-428e-870c-aa077b202a2e", "fitness": 0.04525220324437224, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm and a more frequent adaptive mutation for improved exploration.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling more frequently\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob'] * 1.5:  # Increased mutation frequency\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 68, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04525 with standard deviation 0.00318.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04513031055266026, 0.05066321393467044, 0.04922652700038033, 0.041569948787744115, 0.04668716252031557, 0.0453686466615163, 0.04000136860984205, 0.044946325366181705, 0.043676325766039414]}}
{"id": "41e585e7-8e69-43e6-8dee-a17c583f83ba", "fitness": 0.04740182855013529, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm, now including elite particle preservation for improved convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n        \n        # Preserve the best particle's position\n        best_particle_index = np.argmin(swarm['best_personal_value'])\n        best_position = swarm['position'][best_particle_index]\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n        \n        # Restore the best particle's position to maintain elite particles\n        swarm['position'][best_particle_index] = best_position\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 69, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04740 with standard deviation 0.00406.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.054411888219242366, 0.05154156855801395, 0.045985309952070286, 0.05009335708588136, 0.04749616587650651, 0.04236963428617291, 0.04821402383986639, 0.045726761301008056, 0.0407777478324558]}}
{"id": "314c5034-cd6b-4544-b276-aada330954ff", "fitness": 0.04610421037242759, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced APSO-DL with Improved Migration and Adaptive Mutation for better convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.1 + 0.4 * (1 - self.evaluations / self.budget)  # Changed\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (10 * self.num_swarms)) == 0:  # Changed\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)  # Changed\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 70, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04610 with standard deviation 0.00237.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04893646462879442, 0.0499099901077642, 0.048885030449937084, 0.04509689462974664, 0.04599443397892833, 0.04505297782874984, 0.043412281862474145, 0.04427847164469634, 0.04337134822075728]}}
{"id": "6f6a1453-5548-464e-8d39-b1083f32c147", "fitness": 0.03806935983212746, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Improve exploration by adjusting the random coefficients dynamically in velocity update.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1 = np.random.rand(self.num_particles, self.dim) * (0.5 + 0.5 * np.random.rand(self.num_particles, self.dim))  # change line\n        r2 = np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 71, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03807 with standard deviation 0.00317.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.04012192058752584, 0.04420467451795096, 0.03786373234281881, 0.036897161949550505, 0.040705015281851886, 0.03478371746286346, 0.03546930954307581, 0.03916184610144302, 0.03341686070206684]}}
{"id": "a88c6d10-61be-4554-9146-7f611969326d", "fitness": -Infinity, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm", "description": "Enhanced APSO-DL with improved swarm migration strategy and adaptive learning coefficients for better convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.8  # Changed line\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.8  # Changed line\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Apply differential mutation with adaptive scaling\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        # Migrate particles between swarms periodically\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]\n            \n            # Enhance migration strategy by reinitializing half of the migrated particles\n            for j in range(self.num_swarms):\n                idx = np.random.choice(self.num_particles, self.num_particles // 4, replace=False)  # Changed line\n                self.swarms[j]['position'][idx] = np.random.rand(len(idx), self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb  # Changed line", "configspace": "", "generation": 72, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {}}
{"id": "0d49b92f-a26e-4c21-bfa6-70784f1709c1", "fitness": 0.047138450661429174, "name": "Enhanced_APSO_DL_MultiSwarm_Chaos", "description": "An advanced Adaptive Multi-Swarm approach with dynamic strategy switching and chaos-induced mutation to enhance diversity and convergence speed.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_MultiSwarm_Chaos:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1,\n            'chaotic_map': np.random.rand()\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        # Apply chaos-based mutation if stagnation is detected\n        if self.evaluations % (self.budget // 5) == 0:\n            swarm['chaotic_map'] = 4 * swarm['chaotic_map'] * (1 - swarm['chaotic_map'])\n            random_indices = np.random.choice(self.num_particles, self.num_particles // 4, replace=False)\n            for idx in random_indices:\n                swarm['position'][idx] = swarm['chaotic_map'] * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 73, "feedback": "The algorithm Enhanced_APSO_DL_MultiSwarm_Chaos got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04714 with standard deviation 0.00328.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.047098436225438634, 0.052845885397018955, 0.05110888529626856, 0.04339855859291053, 0.04868571425308177, 0.04710907063763492, 0.04177179497712624, 0.04686963444999337, 0.0453580761233896]}}
{"id": "73a1440c-11a3-41d9-9d32-600e56392612", "fitness": 0.007744591951551823, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_EliteGuided", "description": "Multi-phase adaptive particle swarm optimization with elite-guided mutation for enhanced convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_EliteGuided:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                self.elite_guided_mutation(swarm, func)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                # Evaluate current position\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Dynamic adjustment of inertia weight and learning rates\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Update velocity and position\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def elite_guided_mutation(self, swarm, func):\n        num_elites = max(1, self.num_particles // 10)\n        elite_indices = np.argsort(swarm['best_personal_value'])[:num_elites]\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                elite_idx = np.random.choice(elite_indices)\n                indices = np.random.choice(self.num_particles, 2, replace=False)\n                F = np.random.rand() * 0.5\n                mutant = (swarm['position'][elite_idx] +\n                          F * (swarm['position'][indices[0]] - swarm['position'][indices[1]]))\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 74, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_EliteGuided got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.00774 with standard deviation 0.00107.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.009184736676305483, 0.009184736676305483, 0.009184736676305483, 0.007422984058760962, 0.007422984058760962, 0.007422984058760962, 0.006626055119589025, 0.006626055119589025, 0.006626055119589025]}}
{"id": "9f929c7f-24b3-4651-96d7-da7bbc2d7f43", "fitness": 0.049689503182825084, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm incorporating Dynamic Subpopulation Restructuring and Context-Aware Mutation for superior exploration-exploitation balance.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 75, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04969 with standard deviation 0.00267.", "error": "", "parent_ids": ["23a720cd-2efd-4149-95de-aa74ef0380c4"], "operator": null, "metadata": {"aucs": [0.05445500701929473, 0.052271792322338584, 0.052534751549889736, 0.0501332248552987, 0.048168074917954495, 0.04840891742215503, 0.04825254475969254, 0.04637468140788781, 0.04660653439091411]}}
{"id": "8fb48860-ab67-44c6-ada5-dcdd535fcaa7", "fitness": 0.045856405360769265, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved", "description": "Enhanced APSO-DL with optimized inertia weight decay and adaptive mutation scaling for better convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.5 + 0.4 * np.exp(-5 * self.evaluations / self.budget)  # Changed line\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.4 * (1 - self.evaluations / self.budget)  # Changed line\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = 0.8 * (0.5 + 0.5 * (1 - self.evaluations / self.budget))  # Changed line\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 76, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04586 with standard deviation 0.00524.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.04253369554079389, 0.0497713487425101, 0.05467834106574054, 0.03914916255595191, 0.04586991818045283, 0.050381909115972956, 0.03765421090485199, 0.0441597025117727, 0.04850935962887648]}}
{"id": "3df1efc8-c018-4590-bb67-e9caaa099ba8", "fitness": 0.043390056711339206, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved", "description": "Introduced adaptive inertia weight scheduling and enhanced swarm migration for improved convergence dynamics.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        inertia_min, inertia_max = 0.4, 0.9\n        swarm['inertia_weight'] = inertia_min + (inertia_max - inertia_min) * (1 - (self.evaluations / self.budget)**2)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]\n                np.random.shuffle(self.swarms[i]['position'])", "configspace": "", "generation": 77, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04339 with standard deviation 0.00377.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.04282828026853347, 0.05068652191924106, 0.04557205884184923, 0.03943308839983184, 0.046704578818252074, 0.041985469951803855, 0.037933296577872366, 0.04496148512775677, 0.04040573049691221]}}
{"id": "02f1a33e-779e-43a9-9efd-cbfaa78db97f", "fitness": 0.049689503182825084, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved", "description": "Minor enhancement to mutation mechanism by introducing adaptive differential weight within the first swarm.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 78, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04969 with standard deviation 0.00267.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.05445500701929473, 0.052271792322338584, 0.052534751549889736, 0.0501332248552987, 0.048168074917954495, 0.04840891742215503, 0.04825254475969254, 0.04637468140788781, 0.04660653439091411]}}
{"id": "9861078b-4765-4c0b-bc93-c475e15de2dd", "fitness": 0.04575958229090534, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved", "description": "Enhanced APSO-DL with refined inertia weight adjustment and improved mutation strategy for better convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.7 + 0.3 * (1 - (self.best_global_value / self.budget))  # Changed line\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = 0.6 + np.random.rand() * 0.4  # Changed line\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 79, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04576 with standard deviation 0.00200.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.04834409988597288, 0.047304821655919205, 0.04935528555724722, 0.044761752423764345, 0.045347809352218604, 0.04549882926097282, 0.04317924032100928, 0.044237599905554315, 0.043806802255489385]}}
{"id": "80a2147e-7182-4906-ad31-8f97187a9881", "fitness": 0.04705993241399334, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved", "description": "Enhanced APSO-DL with refined velocity update rule and diversified swarm migration for improved convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position'] * 0.8))  # Fine-tuned this multiplier\n        \n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]\n                self.swarms[i]['velocity'][idx], self.swarms[i+1]['velocity'][idx] = \\\n                    -self.swarms[i+1]['velocity'][idx], -self.swarms[i]['velocity'][idx]  # Improved velocity migration", "configspace": "", "generation": 80, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04706 with standard deviation 0.00254.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.05133141759738569, 0.0489277091340502, 0.050505968503494314, 0.04731114624937016, 0.04509859361928892, 0.04656163483666298, 0.04555183467969204, 0.04341795344238142, 0.0448331336636143]}}
{"id": "57615e0e-dd83-4d62-8c30-480fa1858946", "fitness": -Infinity, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved_V2", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm and Dynamic Subpopulation Restructuring, featuring Progressive Hierarchical Migration and Adaptive Mutation for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved_V2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 3\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n        self.mutation_adapt_rate = 0.05 + 0.5 * np.exp(-3 * np.arange(budget) / budget)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = self.mutation_adapt_rate[self.evaluations]\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]\n            # Hierarchical migration between the first and the last swarm\n            idx = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)\n            self.swarms[0]['position'][idx], self.swarms[-1]['position'][idx] = \\\n                self.swarms[-1]['position'][idx], self.swarms[0]['position'][idx]", "configspace": "", "generation": 81, "feedback": "An exception occurred: IndexError('index 1000 is out of bounds for axis 0 with size 1000').", "error": "IndexError('index 1000 is out of bounds for axis 0 with size 1000')", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {}}
{"id": "f8c6b762-7748-4722-96b3-8f272ee847ee", "fitness": 0.04777765191536597, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved", "description": "Introduced a dynamic adjustment feature for the mutation probability based on swarm diversity to improve convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        position_variance = np.var(swarm['position'], axis=0).mean()\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - position_variance)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 82, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04778 with standard deviation 0.00535.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.04648049339154159, 0.058186799193528094, 0.04848885230784028, 0.0428277890818376, 0.053552579253474564, 0.04468394657261543, 0.041220509535144556, 0.05154420502352697, 0.04301369287878465]}}
{"id": "004cbdbe-0122-49b5-85a8-d45f656622eb", "fitness": 0.048275858480347704, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved", "description": "Introducing a non-uniform mutation strategy to enhance the convergence velocity and adaptability of the algorithm.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                tau = 1 - (self.evaluations / self.budget)**2  # Non-uniform mutation factor\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]]) * tau\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 83, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04828 with standard deviation 0.00411.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.047062902865569245, 0.05568393618868728, 0.05197116072542485, 0.043367242904955416, 0.05128610922574728, 0.04789014576440509, 0.04174212255999388, 0.049372881154671866, 0.046106224933674445]}}
{"id": "03636f56-74b7-4f57-ae8a-2de036f3bf2b", "fitness": -Infinity, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm using Dynamic Subpopulation Restructuring, Context-Aware Mutation, and Adaptive Learning Rates for improved convergence speed and precision.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n        self.adaptive_learning_rate = np.linspace(0.5, 0.1, self.budget)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        # Adaptive learning rate for position updates\n        learning_rate = self.adaptive_learning_rate[self.evaluations]\n        swarm['position'] += learning_rate * swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 84, "feedback": "An exception occurred: IndexError('index 1000 is out of bounds for axis 0 with size 1000').", "error": "IndexError('index 1000 is out of bounds for axis 0 with size 1000')", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {}}
{"id": "3a6c702b-f76e-40c1-a7f8-6fe81b53c84c", "fitness": 0.049689503182825084, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Refined", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm utilizing Dynamic Subpopulation Restructuring, Context-Aware Mutation, and Cross-Swarm Best Sharing for improved search efficiency and convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Refined:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]\n        # Cross-Swarm Best Sharing\n        for swarm in self.swarms:\n            swarm_best_value = np.min(swarm['best_personal_value'])\n            if swarm_best_value < self.best_global_value:\n                swarm_best_position = swarm['position'][np.argmin(swarm['best_personal_value'])]\n                self.best_global_position = swarm_best_position\n                self.best_global_value = swarm_best_value", "configspace": "", "generation": 85, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Refined got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04969 with standard deviation 0.00267.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.05445500701929473, 0.052271792322338584, 0.052534751549889736, 0.0501332248552987, 0.048168074917954495, 0.04840891742215503, 0.04825254475969254, 0.04637468140788781, 0.04660653439091411]}}
{"id": "ef911c6a-e8e3-4902-93d5-a834406824b8", "fitness": 0.04624753468062323, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Enhanced", "description": "Enhanced APSO-DL with Adaptive Multi-Swarm using Enhanced Dynamic Subpopulation and Randomized Differential Mutation for improved exploration-exploitation synergy.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Enhanced:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 8)\n        self.num_swarms = 3\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (12 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.enhanced_restructure_subpopulations(swarm)\n                self.randomized_migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.8 + np.random.rand() * 1.2\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 4, replace=False)\n                F = 0.5 + np.random.rand() * (1 - self.evaluations / self.budget)\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]]) + \\\n                         np.random.rand() * (swarm['position'][indices[3]] - swarm['position'][i])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def enhanced_restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        for i in range(self.num_particles // 2, self.num_particles):\n            if np.random.rand() < 0.5:\n                idx = sorted_indices[i]\n                swarm['position'][idx] = swarm['position'][sorted_indices[np.random.randint(0, self.num_particles // 2)]]\n                swarm['velocity'][idx] *= 0.3\n\n    def randomized_migrate_swarms(self):\n        if self.evaluations % (self.budget // (6 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 86, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Enhanced got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04625 with standard deviation 0.00557.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.043244960879990635, 0.05637727693016892, 0.04884499107989271, 0.039766939640442645, 0.05178928732026156, 0.04493847151440544, 0.03823409024730151, 0.049803270725391036, 0.04322852378775466]}}
{"id": "354fc93c-4168-447e-b7de-2f0c98534d25", "fitness": 0.04406286629926689, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_DivImproved", "description": "Enhanced APSO-DL with Adaptive Swarming, incorporating Dynamic Subpopulation Restructuring, Context-Aware Mutation, and Diversity-driven Exploration for improved optimization performance.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_DivImproved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n        self.diversity_coeff = 0.1\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        diversity = self.calculate_diversity(swarm['position'])\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget) + self.diversity_coeff * diversity\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def calculate_diversity(self, positions):\n        centroid = np.mean(positions, axis=0)\n        diversity = np.mean(np.linalg.norm(positions - centroid, axis=1))\n        return diversity\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 87, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_DivImproved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04406 with standard deviation 0.00314.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.04595177496405112, 0.04492535115097873, 0.05033604235593503, 0.04233728150655569, 0.04138039636629076, 0.04639823004462951, 0.04074592387826537, 0.039818129073711184, 0.044672667352984585]}}
{"id": "192467c1-dd03-4ea9-8837-5b69cf283374", "fitness": 0.044186387667554006, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved", "description": "Introducing Adaptive Mutation Scaling and Position Reset Mechanism to further enhance convergence and diversity in the swarm.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n        \n        if self.evaluations % (self.budget // 20) == 0:  # New reset mechanism every 5% of evaluations\n            swarm['position'] = np.random.rand(self.num_particles, self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 88, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04419 with standard deviation 0.00241.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.04773720866405751, 0.04585635257832377, 0.048006972340294496, 0.04399058826802427, 0.04224908651188497, 0.04424288586432901, 0.042344360574750994, 0.04066070439162528, 0.042589329814695764]}}
{"id": "30decc9f-db1f-4ba4-a625-d942c3f090f3", "fitness": 0.007744591951551823, "name": "Hybrid_AMDE_MultiSwarm_Adaptive", "description": "Introducing a Hybrid Adaptive Memory-Enhanced Differential Evolution with Multi-Swarm Dynamics and Self-Adjusting Control Parameters for improved convergence and diversity maintenance.", "code": "import numpy as np\n\nclass Hybrid_AMDE_MultiSwarm_Adaptive:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 3\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.memory = []\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'memory': np.random.rand(self.num_particles // 2, self.dim),\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        F = 0.5 + 0.5 * np.random.rand()\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, swarm['position'][i])\n                if func(trial) < func(swarm['position'][i]):\n                    swarm['position'][i] = trial\n\n        swarm['velocity'] += r1 * (swarm['best_personal_position'] - swarm['position']) + r2 * (self.best_global_position - swarm['position'])\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 89, "feedback": "The algorithm Hybrid_AMDE_MultiSwarm_Adaptive got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.00774 with standard deviation 0.00107.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.009184736676305483, 0.009184736676305483, 0.009184736676305483, 0.007422984058760962, 0.007422984058760962, 0.007422984058760962, 0.006626055119589025, 0.006626055119589025, 0.006626055119589025]}}
{"id": "3042bd8b-6551-484a-b1cd-d011582e6ab3", "fitness": 0.041583956431112665, "name": "Enhanced_APSO_DL_CooperativeMultiSwarm_AdaptiveMemory", "description": "Enhanced APSO-DL with Cooperative Multi-Swarm featuring Dynamic Role Assignment and Adaptive Memory Exploitation for improved convergence behavior.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_CooperativeMultiSwarm_AdaptiveMemory:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 3\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n        self.memory = {\n            'global_best_positions': np.zeros((self.num_swarms, self.dim)), \n            'global_best_values': np.full(self.num_swarms, np.inf)\n        }\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for idx, swarm in enumerate(self.swarms):\n                self.update_swarm(swarm, func, idx)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func, swarm_idx):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                \n                if value < self.memory['global_best_values'][swarm_idx]:\n                    self.memory['global_best_values'][swarm_idx] = value\n                    self.memory['global_best_positions'][swarm_idx] = swarm['position'][i]\n\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        # Update dynamic coefficients\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        # Velocity and position update\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            # Exchange best positions among swarms\n            for i in range(self.num_swarms):\n                other_swarm = (i + 1) % self.num_swarms\n                if self.memory['global_best_values'][other_swarm] < self.memory['global_best_values'][i]:\n                    self.memory['global_best_positions'][i], self.memory['global_best_positions'][other_swarm] = \\\n                    self.memory['global_best_positions'][other_swarm], self.memory['global_best_positions'][i]\n                    self.memory['global_best_values'][i], self.memory['global_best_values'][other_swarm] = \\\n                    self.memory['global_best_values'][other_swarm], self.memory['global_best_values'][i]", "configspace": "", "generation": 90, "feedback": "The algorithm Enhanced_APSO_DL_CooperativeMultiSwarm_AdaptiveMemory got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04158 with standard deviation 0.00350.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.04808282444998313, 0.04090184101157501, 0.044463776948770795, 0.044251320331222144, 0.03758037809427117, 0.04092468114239245, 0.04257184746800591, 0.0361133180015335, 0.039365620432259885]}}
{"id": "03ce8a54-46dd-4c73-910d-3a69ff74dd14", "fitness": 0.043232003465794776, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Refined", "description": "Adaptive Multi-Swarm with Differential Evolution and Dynamic Learning Rates for enhanced convergence speed and accuracy.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Refined:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        progress_ratio = self.evaluations / self.budget\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - progress_ratio)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - progress_ratio)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = 0.5 + progress_ratio * (1.0 - 0.5)\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        best_half, worst_half = sorted_indices[:self.num_particles // 2], sorted_indices[self.num_particles // 2:]\n        swarm['position'][worst_half] = swarm['position'][best_half]\n        swarm['velocity'][worst_half] = swarm['velocity'][best_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 91, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Refined got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04323 with standard deviation 0.00225.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.045430354761505254, 0.046593145464712604, 0.04654384191297711, 0.041852891433234785, 0.042925385379438796, 0.042882043359638855, 0.040276984239158375, 0.041312145835694536, 0.04127123880579264]}}
{"id": "f63a23da-3e22-4bdf-9562-e3b60fc8e178", "fitness": 0.04344099364706408, "name": "Advanced_APSO_DL_HybridAdaptive", "description": "Advanced APSO-DL with Hybrid Adaptive Mechanics and Self-Adaptive Learning Strategy for Enhanced Multi-Modal Optimization Performance.", "code": "import numpy as np\n\nclass Advanced_APSO_DL_HybridAdaptive:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 3\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (8 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1,\n            'learning_rate': np.random.rand(self.num_particles) * 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = (swarm['position'][indices[0]] + \n                          F * (swarm['position'][indices[1]] - swarm['position'][indices[2]]) +\n                          swarm['learning_rate'][i] * (self.best_global_position - swarm['position'][i]))\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (4 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 92, "feedback": "The algorithm Advanced_APSO_DL_HybridAdaptive got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04344 with standard deviation 0.00229.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.04567859955477793, 0.04664567189336877, 0.04702097127004279, 0.04202472193391604, 0.042931248297753966, 0.043293098008103326, 0.04041901839247031, 0.041299953263236655, 0.041655660209906964]}}
{"id": "88ac8759-b2ce-4a7e-a586-0529c6538dcd", "fitness": 0.044086522298745066, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Refined", "description": "Improved adaptive inertia and mutation strategies with a dynamic swarm communication mechanism to enhance convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Refined:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - (self.evaluations / self.budget)**2)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - (self.evaluations / self.budget)**2)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 93, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Refined got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04409 with standard deviation 0.00229.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.04627805028477683, 0.04753866386563643, 0.047468586140926194, 0.042640583349923156, 0.04380272205321856, 0.04374337033543252, 0.0410396299068726, 0.042161093498139124, 0.04210600125378017]}}
{"id": "1e3cc20d-abbe-4499-9469-7feea207c03c", "fitness": 0.04365070606439166, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved_V2", "description": "Enhanced APSO-DL with Dynamic Leader Selection and Adaptive Mutation Strategy integrating Variable Swarm Convergence to enhance global-wide exploration and exploitation trade-off.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved_V2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n        self.leader_index = 0\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n                    self.leader_index = i\n\n        leader_position = swarm['best_personal_position'][self.leader_index]\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (leader_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 94, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved_V2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04365 with standard deviation 0.00700.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.04986627436486901, 0.03691589436873377, 0.05322659768223725, 0.04595085549009026, 0.033894531212912415, 0.04902383102085983, 0.04423511273530767, 0.032552488700031024, 0.04719076900448371]}}
{"id": "3d6255e6-b1c7-4a7b-8e94-675f361a9494", "fitness": 0.04323107643034543, "name": "Advanced_APSO_DL_AdaptiveMultiSwarm", "description": "Advanced APSO-DL with Adaptive Multi-Swarm utilizing Dynamic Subpopulation Restructuring, Context-Aware Mutation, and Multi-Objective Synchronization to enhance convergence precision and speed.", "code": "import numpy as np\n\nclass Advanced_APSO_DL_AdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1,\n            'diverse_positions': np.random.rand(self.num_particles, self.dim)\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.multi_objective_synchronization()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n        swarm['diverse_positions'] = np.random.rand(self.num_particles, self.dim)  # Enhance exploration\n\n    def multi_objective_synchronization(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]\n                self.swarms[i]['velocity'][idx], self.swarms[i+1]['velocity'][idx] = \\\n                    self.swarms[i+1]['velocity'][idx], self.swarms[i]['velocity'][idx]", "configspace": "", "generation": 95, "feedback": "The algorithm Advanced_APSO_DL_AdaptiveMultiSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04323 with standard deviation 0.00368.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.04955020995370474, 0.04700954197168594, 0.04202721848263924, 0.04566363309046084, 0.04331174445346553, 0.03867627187567313, 0.043959647197070795, 0.04168599527312711, 0.03719542557528155]}}
{"id": "c808c37c-ef99-4d5d-98c2-facb2856df29", "fitness": 0.04587975201471507, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved", "description": "Enhanced APSO-DL with refined mutation strategy using dynamic mutation scaling and targeted crossover for improved convergence.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = 0.8 * (1 - self.evaluations / self.budget) + 0.3\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                target_crossover = np.where(np.random.rand(self.dim) < 0.5, swarm['position'][i], mutant)\n                swarm['position'][i] = np.clip(target_crossover, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 96, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04588 with standard deviation 0.00471.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.04318092760422132, 0.05364869532591798, 0.05023780333516281, 0.03975954425156725, 0.04940710792254421, 0.0463007106483202, 0.03824887378757236, 0.04755837090339832, 0.04457573435373119]}}
{"id": "2149eead-da35-4f8d-b10b-ec839a67c49c", "fitness": 0.04894000377176999, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved", "description": "Introduced adaptive inertia weight dynamic scaling for enhanced convergence speed and accuracy.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.5 + 0.4 * np.cos(3.14 * self.evaluations / self.budget)  # Change made here\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 97, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04894 with standard deviation 0.00676.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.06183177388893557, 0.0482729138396667, 0.04683720003115832, 0.05683448357459575, 0.044482796691909576, 0.04315936238621987, 0.054681191481831926, 0.04281868099991326, 0.041541631051698946]}}
{"id": "83d6e05f-59d9-463e-9b52-c3a4e7b8f540", "fitness": 0.04370848348309822, "name": "Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved", "description": "Introduced Dynamic Inertia Weight Adaptation and Enhanced Mutation Strategy to further improve exploration-exploitation balance.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                self.migrate_swarms()\n        \n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.5 + 0.4 * np.cos(np.pi * self.evaluations / self.budget)  # Adjusted inertia weight\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = 0.5 + 0.5 * np.sin(np.pi * self.evaluations / self.budget)  # Adjusted mutation factor F\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def migrate_swarms(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                self.swarms[i]['position'][idx], self.swarms[i+1]['position'][idx] = \\\n                    self.swarms[i+1]['position'][idx], self.swarms[i]['position'][idx]", "configspace": "", "generation": 98, "feedback": "The algorithm Enhanced_APSO_DL_AdaptiveMultiSwarm_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04371 with standard deviation 0.00290.", "error": "", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {"aucs": [0.04393955893330381, 0.047565850292747514, 0.04858161448004927, 0.040470574783970203, 0.04382724138593341, 0.044770098456819674, 0.03893980834773192, 0.04218453711428283, 0.04309706755304532]}}
{"id": "33412d2e-eb14-4de9-a197-003ebf4c5f70", "fitness": -Infinity, "name": "Enhanced_APSO_DL_ReinforcedAdaptiveMultiSwarm", "description": "Enhanced APSO-DL with Reinforced Adaptive Multi-Swarm using Dynamic Topology Adjustment and Fitness-driven Migration for improved convergence dynamics.", "code": "import numpy as np\n\nclass Enhanced_APSO_DL_ReinforcedAdaptiveMultiSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // 10)\n        self.num_swarms = 2\n        self.swarms = [self.init_swarm() for _ in range(self.num_swarms)]\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.evaluations = 0\n        self.dynamic_subpop_threshold = budget // (10 * self.num_swarms)\n        self.topology_update_interval = budget // 100\n\n    def init_swarm(self):\n        return {\n            'position': np.random.rand(self.num_particles, self.dim),\n            'velocity': np.random.rand(self.num_particles, self.dim) * 0.1,\n            'best_personal_position': np.random.rand(self.num_particles, self.dim),\n            'best_personal_value': np.full(self.num_particles, np.inf),\n            'inertia_weight': 0.9,\n            'cognitive_coeff': 2.0,\n            'social_coeff': 2.0,\n            'mutation_prob': 0.1\n        }\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for swarm in self.swarms:\n                self.update_swarm(swarm, func)\n                if self.evaluations % self.dynamic_subpop_threshold == 0:\n                    self.restructure_subpopulations(swarm)\n                if self.evaluations % self.topology_update_interval == 0:\n                    self.adjust_topology(swarm)\n                self.fitness_driven_migration()\n\n        return self.best_global_position, self.best_global_value\n\n    def update_swarm(self, swarm, func):\n        for i in range(self.num_particles):\n            if self.evaluations < self.budget:\n                value = func(swarm['position'][i])\n                self.evaluations += 1\n\n                if value < swarm['best_personal_value'][i]:\n                    swarm['best_personal_value'][i] = value\n                    swarm['best_personal_position'][i] = swarm['position'][i]\n                if value < self.best_global_value:\n                    self.best_global_value = value\n                    self.best_global_position = swarm['position'][i]\n\n        swarm['inertia_weight'] = 0.4 + 0.5 * (1 - self.evaluations / self.budget)\n        swarm['cognitive_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['social_coeff'] = 1.5 + np.random.rand() * 1.5\n        swarm['mutation_prob'] = 0.05 + 0.45 * (1 - self.evaluations / self.budget)\n\n        r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n        swarm['velocity'] = (swarm['inertia_weight'] * swarm['velocity'] +\n                             swarm['cognitive_coeff'] * r1 * (swarm['best_personal_position'] - swarm['position']) +\n                             swarm['social_coeff'] * r2 * (self.best_global_position - swarm['position']))\n\n        for i in range(self.num_particles):\n            if np.random.rand() < swarm['mutation_prob']:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                F = np.random.rand() * (0.5 + 0.5 * (1 - self.evaluations / self.budget))\n                mutant = swarm['position'][indices[0]] + F * (swarm['position'][indices[1]] - swarm['position'][indices[2]])\n                swarm['position'][i] = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n        swarm['position'] += swarm['velocity']\n        swarm['position'] = np.clip(swarm['position'], func.bounds.lb, func.bounds.ub)\n\n    def restructure_subpopulations(self, swarm):\n        sorted_indices = np.argsort(swarm['best_personal_value'])\n        half_point = self.num_particles // 2\n        top_half, bottom_half = sorted_indices[:half_point], sorted_indices[half_point:]\n        swarm['position'][bottom_half], swarm['velocity'][bottom_half] = swarm['position'][top_half], swarm['velocity'][top_half] * 0.5\n\n    def adjust_topology(self, swarm):\n        connectivity = np.random.randint(1, self.num_particles // 2)\n        neighbors = [np.random.choice(self.num_particles, connectivity, replace=False)\n                     for _ in range(self.num_particles)]\n        for i in range(self.num_particles):\n            neighbor_positions = swarm['position'][neighbors[i]]\n            neighbor_values = [func(pos) for pos in neighbor_positions]\n            best_neighbor_index = np.argmin(neighbor_values)\n            if neighbor_values[best_neighbor_index] < swarm['best_personal_value'][i]:\n                swarm['best_personal_value'][i] = neighbor_values[best_neighbor_index]\n                swarm['best_personal_position'][i] = neighbor_positions[best_neighbor_index]\n\n    def fitness_driven_migration(self):\n        if self.evaluations % (self.budget // (5 * self.num_swarms)) == 0:\n            for i in range(self.num_swarms - 1):\n                idx = np.random.choice(self.num_particles, self.num_particles // 2, replace=False)\n                fitness_diffs = (self.swarms[i]['best_personal_value'][idx] - \n                                 self.swarms[i+1]['best_personal_value'][idx])\n                migrate_idx = idx[fitness_diffs > 0]\n                self.swarms[i]['position'][migrate_idx], self.swarms[i+1]['position'][migrate_idx] = \\\n                    self.swarms[i+1]['position'][migrate_idx], self.swarms[i]['position'][migrate_idx]", "configspace": "", "generation": 99, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_ids": ["9f929c7f-24b3-4651-96d7-da7bbc2d7f43"], "operator": null, "metadata": {}}
