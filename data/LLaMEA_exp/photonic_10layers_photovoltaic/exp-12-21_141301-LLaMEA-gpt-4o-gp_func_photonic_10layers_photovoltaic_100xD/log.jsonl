{"id": "ecb18221-f9f7-4349-8515-49a87bc03d4a", "fitness": 0.05310288705872246, "name": "HybridOptimizer", "description": "A hybrid metaheuristic algorithm combining differential evolution and simulated annealing to explore and exploit the search space effectively for black box optimization problems.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 20\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.99  # Cooling rate\n\n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n\n        while num_evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 0, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05310 with standard deviation 0.00287.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.054997432660607526, 0.05680125403743963, 0.05810624949326326, 0.05077996165678311, 0.052454424561991675, 0.05362225843517465, 0.04893672739629629, 0.05055725213469209, 0.0516704231522539]}}
{"id": "987f6caa-d6f0-4fee-bdbe-cc0b4759d4ca", "fitness": 0.05161773636695591, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid metaheuristic algorithm that integrates adaptive parameters and a restart mechanism to improve exploration and exploitation in black box optimization problems.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 20\n        F = 0.8  # Initial differential weight\n        CR = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.99  # Cooling rate\n        restart_threshold = 0.2 * self.budget\n\n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        evaluations_since_improvement = 0\n\n        while num_evaluations < self.budget:\n            for i in range(pop_size):\n                # Adapt F and CR over time\n                F = 0.5 + 0.5 * np.random.rand()\n                CR = 0.5 + 0.5 * np.random.rand()\n                \n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    evaluations_since_improvement = 0\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n                        evaluations_since_improvement = 0\n                    else:\n                        evaluations_since_improvement += 1\n\n                if num_evaluations >= self.budget:\n                    break\n\n            # Restart mechanism\n            if evaluations_since_improvement >= restart_threshold:\n                # Reinitialize population except for the best solution\n                for j in range(pop_size):\n                    if j != best_idx:\n                        population[j] = func.bounds.lb + np.random.rand(self.dim) * (func.bounds.ub - func.bounds.lb)\n                fitness = np.array([func(ind) for ind in population])\n                num_evaluations += pop_size\n                evaluations_since_improvement = 0\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 1, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05162 with standard deviation 0.00338.", "error": "", "parent_ids": ["ecb18221-f9f7-4349-8515-49a87bc03d4a"], "operator": null, "metadata": {"aucs": [0.05521797309817633, 0.057867833623741816, 0.05203291839094315, 0.05098963753653407, 0.05343343448868476, 0.04805950278338489, 0.04914162777845599, 0.05150035378920781, 0.046316345813474324]}}
{"id": "a2d020ba-07fa-4fa2-8d60-12ff07cb0254", "fitness": 0.05000533317256227, "name": "HybridOptimizer", "description": "An enhanced hybrid metaheuristic algorithm incorporating a dynamic crossover probability to adaptively balance exploration and exploitation in black box optimization.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 20\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability, now made dynamic\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.99  # Cooling rate\n\n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n\n        while num_evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                CR_dynamic = 0.9 * (1 - (num_evaluations / self.budget))  # Dynamic crossover probability\n                crossover_mask = np.random.rand(self.dim) < CR_dynamic\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 2, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05001 with standard deviation 0.00338.", "error": "", "parent_ids": ["ecb18221-f9f7-4349-8515-49a87bc03d4a"], "operator": null, "metadata": {"aucs": [0.05202544589131364, 0.05678477939925619, 0.051144062072743135, 0.048052888627474366, 0.052436005891637216, 0.047236221659185174, 0.046310079288789274, 0.050538216609284814, 0.04552029911337663]}}
{"id": "fd444374-6871-47c5-8289-1e4b87da1066", "fitness": 0.05151572684024205, "name": "HybridOptimizer", "description": "An enhanced hybrid algorithm combining differential evolution, simulated annealing, and adaptive parameter control for effective exploration and exploitation in black box optimization.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 20\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Adjusted cooling rate for better exploration\n\n        # Adaptive parameters for dynamic control\n        min_F, max_F = 0.5, 1.0\n        min_CR, max_CR = 0.1, 1.0\n\n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n\n        while num_evaluations < self.budget:\n            for i in range(pop_size):\n                # Adaptive differential weight and crossover probability\n                F = min_F + (max_F - min_F) * (1 - num_evaluations / self.budget)\n                CR = min_CR + (max_CR - min_CR) * (best_fitness / (1e-9 + np.min(fitness)))\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            # Decrease temperature\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 3, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05152 with standard deviation 0.00508.", "error": "", "parent_ids": ["ecb18221-f9f7-4349-8515-49a87bc03d4a"], "operator": null, "metadata": {"aucs": [0.05447025407498374, 0.06369243707702854, 0.05312407395218788, 0.04895152455993601, 0.051782238357156674, 0.04894116848256591, 0.047566662209209576, 0.050023616744087396, 0.04508956610502268]}}
{"id": "6a32b436-330d-4b00-8697-6f03afeb9ee2", "fitness": 0.05072185722339843, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid metaheuristic combining differential evolution and simulated annealing with adaptive parameters for improved exploration and exploitation in black box optimization problems.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 20\n        F_start, F_end = 0.5, 0.9  # Adaptive differential weight\n        CR_start, CR_end = 0.8, 0.95  # Adaptive crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n\n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n\n        while num_evaluations < self.budget:\n            for i in range(pop_size):\n                # Adaptive Differential Evolution parameters\n                F = F_start + (F_end - F_start) * (num_evaluations / self.budget)\n                CR = CR_start + (CR_end - CR_start) * (num_evaluations / self.budget)\n                \n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05072 with standard deviation 0.00359.", "error": "", "parent_ids": ["ecb18221-f9f7-4349-8515-49a87bc03d4a"], "operator": null, "metadata": {"aucs": [0.05602609286421878, 0.05602915902849659, 0.05019200694771031, 0.05174221411122615, 0.05174512056578362, 0.04635247360791617, 0.049870911692198816, 0.049873726769472326, 0.04466500942356311]}}
{"id": "8c55a4c2-17a6-45ec-9814-26d11a9b0cb5", "fitness": 0.05534748908545652, "name": "RefinedHybridOptimizer", "description": "A refined hybrid optimizer combining adaptive differential evolution with simulated annealing, incorporating dynamic parameter tuning and elitism for enhanced exploration and exploitation in black box optimization problems.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.1\n\n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n\n        while num_evaluations < self.budget:\n            # Dynamic adjustment of parameters\n            F = F_init * (1 - num_evaluations / self.budget)\n            CR = CR_init * (1 - num_evaluations / self.budget)\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 5, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05535 with standard deviation 0.00336.", "error": "", "parent_ids": ["ecb18221-f9f7-4349-8515-49a87bc03d4a"], "operator": null, "metadata": {"aucs": [0.05644503714421312, 0.05912655873039807, 0.061591068605209354, 0.052113585423294806, 0.05457902787102331, 0.05676803648462947, 0.05022341851994139, 0.05260009246424768, 0.054680576526151436]}}
{"id": "ff703fec-7830-415d-820b-1dfad89b2b4c", "fitness": 0.05460062908727768, "name": "QuantumInspiredHybridOptimizer", "description": "An advanced hybrid optimizer incorporating quantum-inspired adaptive differential evolution with simulated annealing, dynamic parameter tuning, and elitism for superior exploration and exploitation in black box optimization problems.", "code": "import numpy as np\n\nclass QuantumInspiredHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 20\n        F_init = 0.8\n        CR_init = 0.9\n        T0 = 1.0\n        alpha = 0.98\n        elitism_rate = 0.1\n        quantum_prob = 0.1\n\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n\n        while num_evaluations < self.budget:\n            F = F_init * (1 - num_evaluations / self.budget)\n            CR = CR_init * (1 - num_evaluations / self.budget)\n\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                if np.random.rand() < quantum_prob:\n                    quantum_vector = np.random.rand(self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n                    trial = np.where(np.random.rand(self.dim) < CR, quantum_vector, population[i])\n                else:\n                    crossover_mask = np.random.rand(self.dim) < CR\n                    trial = np.where(crossover_mask, mutant, population[i])\n\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            population[:elite_count] = elites\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 6, "feedback": "The algorithm QuantumInspiredHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05460 with standard deviation 0.00444.", "error": "", "parent_ids": ["8c55a4c2-17a6-45ec-9814-26d11a9b0cb5"], "operator": null, "metadata": {"aucs": [0.054126419307909734, 0.06326516687489758, 0.05735715536415609, 0.04999196572621212, 0.0583226765588184, 0.05294830686366969, 0.04818225686558819, 0.056185128073601764, 0.05102658615064559]}}
{"id": "0d5e9f28-2692-4873-8751-ce453e9cc6da", "fitness": 0.05534748908545652, "name": "AdvancedQuantumHybridOptimizer", "description": "An advanced hybrid optimizer enhancing adaptive differential evolution with quantum annealing and population diversity preservation, incorporating dynamic parameter tuning, elitism, and quantum-inspired mechanisms for superior exploration and exploitation in black box optimization problems.", "code": "import numpy as np\n\nclass AdvancedQuantumHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for quantum annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.1\n        diversity_threshold = 0.05  # Threshold for diversity re-injection\n\n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n\n        while num_evaluations < self.budget:\n            # Dynamic adjustment of parameters\n            F = F_init * (1 - num_evaluations / self.budget)\n            CR = CR_init * (1 - num_evaluations / self.budget)\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Quantum Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            # Reintroduce elites and maintain population diversity\n            population[:elite_count] = elites\n            if np.std(fitness) < diversity_threshold:\n                for i in range(elite_count, pop_size):\n                    if i not in elite_indices:\n                        population[i] = func.bounds.lb + np.random.rand(self.dim) * (func.bounds.ub - func.bounds.lb)\n                        fitness[i] = func(population[i])\n                        num_evaluations += 1\n                        if num_evaluations >= self.budget:\n                            break\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 7, "feedback": "The algorithm AdvancedQuantumHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05535 with standard deviation 0.00336.", "error": "", "parent_ids": ["8c55a4c2-17a6-45ec-9814-26d11a9b0cb5"], "operator": null, "metadata": {"aucs": [0.05644503714421312, 0.05912655873039807, 0.061591068605209354, 0.052113585423294806, 0.05457902787102331, 0.05676803648462947, 0.05022341851994139, 0.05260009246424768, 0.054680576526151436]}}
{"id": "6e91b5cd-9c0d-437f-9002-ca088cda1f1d", "fitness": 0.05446307827504006, "name": "RefinedHybridOptimizer", "description": "Enhanced RefinedHybridOptimizer with adaptive elitism rate, balancing exploration and exploitation dynamically.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.1\n\n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n\n        while num_evaluations < self.budget:\n            # Dynamic adjustment of parameters\n            F = F_init * (1 - num_evaluations / self.budget)\n            CR = CR_init * (1 - num_evaluations / self.budget)\n\n            # Adaptive Elitism: dynamically adjust based on progress\n            adaptive_elitism_rate = elitism_rate * (1 - num_evaluations / self.budget)\n            elite_count = int(adaptive_elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 8, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05446 with standard deviation 0.00307.", "error": "", "parent_ids": ["8c55a4c2-17a6-45ec-9814-26d11a9b0cb5"], "operator": null, "metadata": {"aucs": [0.05990664757368325, 0.05602915902849659, 0.05833604817445104, 0.05527876426722356, 0.05174512056578362, 0.05384322206342507, 0.05326778729579407, 0.049873726769472326, 0.051887228737031]}}
{"id": "5ed5a348-d1ef-4900-9eee-eef7373130f7", "fitness": 0.05170147074925694, "name": "AdvancedHybridOptimizer", "description": "An advanced hybrid optimizer integrating adaptive differential evolution with simulated annealing and dynamic neighborhood selection to enhance convergence and solution quality.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.1\n\n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n\n        while num_evaluations < self.budget:\n            # Dynamic adjustment of parameters\n            F = F_init * (1 - num_evaluations / self.budget)\n            CR = CR_init * (1 - num_evaluations / self.budget)\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Dynamic neighborhood selection for diversity\n                neighbors_idx = np.random.choice(pop_size, 2, replace=False)\n                x1, x2 = population[neighbors_idx]\n\n                # Differential Evolution mutation and crossover\n                mutant = population[i] + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 9, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05170 with standard deviation 0.00339.", "error": "", "parent_ids": ["8c55a4c2-17a6-45ec-9814-26d11a9b0cb5"], "operator": null, "metadata": {"aucs": [0.0517543100120722, 0.05676083284046607, 0.05688060110781312, 0.04779764935882824, 0.0524172646943718, 0.05251236110104418, 0.04606163691547149, 0.050521432537801614, 0.05060714817544376]}}
{"id": "e3673f29-2d05-4d37-aa83-f7c3e44b5ab1", "fitness": 0.052058493196073176, "name": "AdaptiveHybridOptimizer", "description": "A novel adaptive hybrid optimizer that combines dynamic differential evolution with self-adaptive simulated annealing, incorporating selective elitism and random immigrant strategies for improved convergence and robustness in black box optimization problems.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.1\n        immigrant_rate = 0.05  # Rate of random immigrants\n        min_temp_factor = 0.1  # Minimum temperature factor\n\n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n\n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n\n        while num_evaluations < self.budget:\n            # Dynamic adjustment of parameters\n            F = F_init * (1 - num_evaluations / self.budget)\n            CR = CR_init * (1 - num_evaluations / self.budget)\n            temperature = max(T0 * alpha**(num_evaluations / pop_size), min_temp_factor * T0)\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing with dynamic temperature\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            # Reintroduce elites and random immigrants to maintain diversity\n            population[:elite_count] = elites\n            num_immigrants = int(immigrant_rate * pop_size)\n            immigrants = np.random.rand(num_immigrants, self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n            population[-num_immigrants:] = immigrants\n\n        return best_solution", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05206 with standard deviation 0.00344.", "error": "", "parent_ids": ["8c55a4c2-17a6-45ec-9814-26d11a9b0cb5"], "operator": null, "metadata": {"aucs": [0.052594620711438944, 0.05856822502100223, 0.0553863067166569, 0.04856243763780521, 0.05407139631177693, 0.05114418766701623, 0.04679573870918896, 0.05211298516879448, 0.04929054082097872]}}
{"id": "e609d06c-d447-4c80-91c3-d53774c26de9", "fitness": -Infinity, "name": "AdvancedHybridOptimizer", "description": "An advanced hybrid optimizer integrating adaptive differential evolution with dynamic niche clustering and simulated annealing, leveraging multi-population strategies for enhanced diversity and convergence in black box optimization problems.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 30\n        F_init = 0.8\n        CR_init = 0.9\n        T0 = 1.0\n        alpha = 0.98\n        elitism_rate = 0.2\n        niche_radius = 0.1\n        num_clusters = 3\n\n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n\n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n\n        def distance(x, y):\n            return np.linalg.norm(x - y)\n\n        while num_evaluations < self.budget:\n            # Dynamic adjustment of parameters\n            F = F_init * (1 - num_evaluations / self.budget)\n            CR = CR_init * (1 - num_evaluations / self.budget)\n\n            # Clustering for niche preservation\n            clusters = [population[i::num_clusters] for i in range(num_clusters)]\n            cluster_fitness = [fitness[i::num_clusters] for i in range(num_clusters)]\n\n            for cluster_idx, (cluster, cluster_fitness) in enumerate(zip(clusters, cluster_fitness)):\n                elite_count = int(elitism_rate * len(cluster))\n                elite_indices = cluster_fitness.argsort()[:elite_count]\n                elites = cluster[elite_indices]\n\n                for i in range(len(cluster)):\n                    if i in elite_indices:\n                        continue\n\n                    idxs = np.random.choice(len(cluster), 3, replace=False)\n                    x0, x1, x2 = cluster[idxs]\n                    mutant = x0 + F * (x1 - x2)\n                    mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                    crossover_mask = np.random.rand(self.dim) < CR\n                    trial = np.where(crossover_mask, mutant, cluster[i])\n\n                    trial_fitness = func(trial)\n                    num_evaluations += 1\n\n                    # Simulated Annealing acceptance\n                    if trial_fitness < cluster_fitness[i]:\n                        cluster[i] = trial\n                        cluster_fitness[i] = trial_fitness\n                        if trial_fitness < best_fitness:\n                            best_solution = trial\n                            best_fitness = trial_fitness\n                    else:\n                        acceptance_prob = np.exp((cluster_fitness[i] - trial_fitness) / temperature)\n                        if np.random.rand() < acceptance_prob:\n                            cluster[i] = trial\n                            cluster_fitness[i] = trial_fitness\n\n                    if num_evaluations >= self.budget:\n                        break\n\n                # Reintroduce elites to maintain diversity within clusters\n                cluster[:elite_count] = elites\n\n            # Update global population and fitness\n            population = np.vstack(clusters)\n            fitness = np.hstack(cluster_fitness)\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 11, "feedback": "An exception occurred: IndexError('index 4 is out of bounds for axis 0 with size 4').", "error": "IndexError('index 4 is out of bounds for axis 0 with size 4')", "parent_ids": ["8c55a4c2-17a6-45ec-9814-26d11a9b0cb5"], "operator": null, "metadata": {}}
{"id": "5a21f00d-c383-4a1e-b0f4-d13e21fbea6f", "fitness": 0.05217233217843288, "name": "RefinedHybridOptimizer", "description": "Improved hybrid optimizer by introducing a dynamic mutation strategy for enhanced exploration and exploitation balance in black box optimization problems.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.1\n\n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n\n        while num_evaluations < self.budget:\n            # Dynamic adjustment of parameters\n            F = F_init * (1 - num_evaluations / self.budget)\n            CR = CR_init * (1 - num_evaluations / self.budget)\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + np.random.uniform(0.5, 1.0) * (x1 - x2)  # Dynamic mutation factor\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 12, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05217 with standard deviation 0.00325.", "error": "", "parent_ids": ["8c55a4c2-17a6-45ec-9814-26d11a9b0cb5"], "operator": null, "metadata": {"aucs": [0.05725524432697038, 0.057028067118196746, 0.05261363254447704, 0.05286355499190376, 0.052661754026144925, 0.04859311599417715, 0.050948382488558264, 0.05075663741806602, 0.046830600697401636]}}
{"id": "cacd4306-40f5-459a-ae03-522d884508b1", "fitness": 0.056029072856395884, "name": "RefinedHybridOptimizer", "description": "A refined hybrid optimizer now leveraging adaptive learning rates and enhanced elitism strategies while incorporating noise reduction to improve convergence in black box optimization problems. ", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.15  # Increased elitism rate\n\n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n\n        while num_evaluations < self.budget:\n            # Dynamic adjustment of parameters\n            F = F_init * (1 - (num_evaluations / self.budget)**1.5)  # Adaptive learning rate\n            CR = CR_init * (1 - num_evaluations / self.budget)\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Add noise reduction\n                trial = 0.95 * trial + 0.05 * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 13, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05603 with standard deviation 0.00417.", "error": "", "parent_ids": ["8c55a4c2-17a6-45ec-9814-26d11a9b0cb5"], "operator": null, "metadata": {"aucs": [0.05521554876542756, 0.06092805344631169, 0.06316788876642065, 0.05099299289357795, 0.05622453502399993, 0.058263606773347854, 0.04914708513555821, 0.05418209591913714, 0.05613984898378199]}}
{"id": "753096c3-5003-461a-b839-af5fb423425f", "fitness": 0.0567534386396521, "name": "ImprovedHybridOptimizer", "description": "ImprovedHybridOptimizer: An enhanced hybrid optimizer integrating adaptive memory-based learning to refine parameter tuning and leverage dynamic population diversity to achieve superior convergence in black box optimization problems.", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.15  # Elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n\n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        while num_evaluations < self.budget:\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / self.budget)**1.5)\n\n            CR = CR_init * (1 - num_evaluations / self.budget)\n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Add noise reduction\n                trial = 0.95 * trial + 0.05 * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 14, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05675 with standard deviation 0.00288.", "error": "", "parent_ids": ["cacd4306-40f5-459a-ae03-522d884508b1"], "operator": null, "metadata": {"aucs": [0.059680262616628066, 0.061262890819489546, 0.06071709368720413, 0.055070717584266515, 0.05653071097848805, 0.056009140261284784, 0.05306729606726668, 0.054476586141198435, 0.05396624960104268]}}
{"id": "59ec21d9-3942-4782-90e4-dee50b2c630a", "fitness": 0.05739470351885333, "name": "ImprovedHybridOptimizer", "description": "ImprovedHybridOptimizerV2: A refined hybrid optimizer enhancing the adaptive memory mechanism with a cyclical learning rate to achieve improved convergence in black box optimization problems.", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.15  # Elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n\n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        while num_evaluations < self.budget:\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                # Change applied: Implemented a cyclical learning rate for F\n                F = F_init * (1 - (num_evaluations / (self.budget * 1.5)))\n\n            CR = CR_init * (1 - num_evaluations / self.budget)\n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Add noise reduction\n                trial = 0.95 * trial + 0.05 * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 15, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05739 with standard deviation 0.00334.", "error": "", "parent_ids": ["753096c3-5003-461a-b839-af5fb423425f"], "operator": null, "metadata": {"aucs": [0.058679683693064755, 0.0619382880287046, 0.06310564254234696, 0.05416806034094068, 0.05713631126372842, 0.05819568655289842, 0.0522038842933209, 0.055054463555958444, 0.056070311398716766]}}
{"id": "672b6cbf-a0d0-4618-8f6a-1f201cc56fdc", "fitness": 0.056029618517506465, "name": "EnhancedHybridOptimizerV3", "description": "EnhancedHybridOptimizerV3: This optimizer introduces an adaptive mutation strategy based on population diversity and a global restart mechanism to escape local optima, enhancing convergence in black box optimization problems.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.15  # Elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        restart_threshold = 0.001  # Threshold for diversity-based restart\n\n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        while num_evaluations < self.budget:\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                # Adaptive mutation based on population diversity\n                diversity = np.std(population, axis=0).mean()\n                F = F_init * (1 + 0.5 * diversity)\n\n            CR = CR_init * (1 - num_evaluations / self.budget)\n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Add noise reduction\n                trial = 0.95 * trial + 0.05 * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            # Check population diversity for global restart\n            if np.std(population) < restart_threshold:\n                population = np.random.rand(pop_size, self.dim)\n                for i in range(pop_size):\n                    population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n                fitness = np.array([func(ind) for ind in population])\n                num_evaluations += pop_size\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 16, "feedback": "The algorithm EnhancedHybridOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05603 with standard deviation 0.00300.", "error": "", "parent_ids": ["59ec21d9-3942-4782-90e4-dee50b2c630a"], "operator": null, "metadata": {"aucs": [0.05842907978047229, 0.0594819885257587, 0.061396651662144475, 0.05392971371607691, 0.054905889412598885, 0.05664868586379357, 0.051971004061584725, 0.05291520269907457, 0.054588350936054075]}}
{"id": "b5cbeabd-53bb-4c66-8d22-a05f68a047fb", "fitness": 0.05161173190854619, "name": "EnhancedAdaptiveHybridOptimizer", "description": "EnhancedAdaptiveHybridOptimizer: A refined hybrid optimizer with enhanced memory utilization and dynamic elitism to improve adaptability and convergence speed in black box optimization problems.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate improved for faster exploration\n        elitism_rate = 0.15  # Elitism rate\n        memory_size = 15  # Increased memory size for better adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n\n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        while num_evaluations < self.budget:\n            # Adaptive learning rate using memory with enhanced selection\n            if np.random.rand() < 0.4:\n                F = memory[memory_idx]\n            else:\n                # Implemented a smoother cyclical learning rate for F\n                F = F_init * (0.5 + 0.5 * np.sin((np.pi * num_evaluations) / (self.budget)))\n\n            CR = CR_init * (1 - num_evaluations / self.budget)\n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Dynamic Elitism: Adjusting elitism rate based on convergence\n            dynamic_elitism_rate = elitism_rate * (1 - num_evaluations / self.budget)\n            elite_count = max(1, int(dynamic_elitism_rate * pop_size))\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Add noise reduction with dynamic adjustment\n                trial = 0.95 * trial + 0.05 * (1 - num_evaluations / self.budget) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            # Reintroduce elites to enhance search diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05161 with standard deviation 0.00278.", "error": "", "parent_ids": ["59ec21d9-3942-4782-90e4-dee50b2c630a"], "operator": null, "metadata": {"aucs": [0.054484968127289535, 0.056722085021017166, 0.05387665149865151, 0.05032452863626924, 0.0523819562489386, 0.04976260716456282, 0.04850400201029703, 0.050487521312934214, 0.04796126715695559]}}
{"id": "389ce09d-1f74-4341-a49a-fc7dd9437aba", "fitness": 0.051847516440190894, "name": "EnhancedAdaptiveEvolutionaryOptimizer", "description": "EnhancedAdaptiveEvolutionaryOptimizer: A hybrid optimizer that integrates adaptive differential evolution with elite-guided dynamic perturbations to achieve superior convergence in black box optimization problems.", "code": "import numpy as np\n\nclass EnhancedAdaptiveEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.15  # Elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n\n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        while num_evaluations < self.budget:\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                # Implementing a sinusoidal adaptation for F\n                F = F_init * (0.5 * (1 + np.sin(2 * np.pi * num_evaluations / self.budget)))\n\n            CR = CR_init * (1 - num_evaluations / self.budget)\n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Elite-guided dynamic perturbation\n                dynamic_perturbation = np.random.randn(self.dim) * 0.01\n                trial += dynamic_perturbation\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedAdaptiveEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05185 with standard deviation 0.00322.", "error": "", "parent_ids": ["59ec21d9-3942-4782-90e4-dee50b2c630a"], "operator": null, "metadata": {"aucs": [0.05308653366716398, 0.05813278886895523, 0.05463007154893784, 0.04903213896242198, 0.05367627152937238, 0.05045356018040248, 0.04725551587315502, 0.0517341402424113, 0.04862662708889787]}}
{"id": "7912b790-0d68-4d25-ac74-7eccc1476702", "fitness": 0.058099551008898645, "name": "ImprovedHybridOptimizer", "description": "An advanced hybrid optimizer that incorporates a dynamic crossover strategy and an adaptive population size for enhanced exploration and exploitation in black box optimization problems.", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.15  # Elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.5\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Adaptive population reduction\n        reduction_factor = 0.95\n\n        while num_evaluations < self.budget:\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 1.5)))\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - num_evaluations / self.budget) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Add noise reduction\n                trial = 0.95 * trial + 0.05 * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n            reduction_factor *= 0.99  # Gradually reduce population size\n\n        return best_solution", "configspace": "", "generation": 19, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05810 with standard deviation 0.00299.", "error": "", "parent_ids": ["59ec21d9-3942-4782-90e4-dee50b2c630a"], "operator": null, "metadata": {"aucs": [0.06254105333348559, 0.0626570612626347, 0.060810103014623085, 0.057683607553836724, 0.05778927815587109, 0.0560995543679631, 0.05557928154397229, 0.05568080657057828, 0.05405521327712293]}}
{"id": "fdc27bfc-192c-41db-b7fb-f92ef7280dd6", "fitness": 0.056829546211367314, "name": "ImprovedHybridOptimizer", "description": "A refined hybrid optimizer that uses a stochastic selection mechanism for diversity and an adaptive mutation scaling to improve convergence.", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.15  # Elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.5\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Adaptive population reduction\n        reduction_factor = 0.95\n\n        while num_evaluations < self.budget:\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 1.5)))\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - num_evaluations / self.budget) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Add noise reduction\n                trial = 0.95 * trial + 0.05 * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n            reduction_factor *= 0.99  # Gradually reduce population size\n            \n            # Introduce stochastic selection for diversity\n            for j in range(int(pop_size * 0.1)):  # Change 1: Added stochastic selection\n                random_idx = np.random.randint(0, pop_size)\n                if fitness[random_idx] > fitness[np.random.randint(0, pop_size)]:\n                    population[random_idx] = population[np.random.randint(0, pop_size)]\n                    fitness[random_idx] = func(population[random_idx])  # Change 2: Evaluate new individual\n                    num_evaluations += 1\n\n        return best_solution", "configspace": "", "generation": 20, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05683 with standard deviation 0.00286.", "error": "", "parent_ids": ["7912b790-0d68-4d25-ac74-7eccc1476702"], "operator": null, "metadata": {"aucs": [0.05996842508581368, 0.06137972553050175, 0.06054094921917119, 0.055342567126244635, 0.0566361222285805, 0.05586057481411322, 0.053332048397958354, 0.054577418212317874, 0.05382808528760463]}}
{"id": "04bebcbc-697d-4bca-bd10-6b4a9301cecd", "fitness": 0.05774155922708483, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer using adaptive differential evolution with memory-based learning rates and self-adaptive population dynamics for improved convergence in black box optimization problems.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8\n        CR_init = 0.9\n        T0 = 1.0\n        alpha = 0.97\n        elitism_rate = 0.15\n        memory_size = 10\n        memory = np.full(memory_size, F_init)\n        \n        # New dynamic crossover strategy factor\n        beta = 0.4\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Adaptive population control\n        reduction_factor = 0.95\n\n        while num_evaluations < self.budget:\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 1.5)))\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - num_evaluations / self.budget) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Add noise reduction\n                trial = 0.95 * trial + 0.05 * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n            reduction_factor *= 0.99  # Gradually reduce population size\n\n        return best_solution", "configspace": "", "generation": 21, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05774 with standard deviation 0.00414.", "error": "", "parent_ids": ["7912b790-0d68-4d25-ac74-7eccc1476702"], "operator": null, "metadata": {"aucs": [0.06406297487413593, 0.0637052166428198, 0.05707907404472634, 0.05907188201191349, 0.05875312273942124, 0.05269509809343309, 0.05691327680135405, 0.05660994833577038, 0.050783439500189154]}}
{"id": "b2bfe0c6-cc72-47e4-9862-07cc7e544d08", "fitness": 0.05802268317927189, "name": "ImprovedHybridOptimizer", "description": "An enhanced hybrid optimizer with adaptive selection pressure and reinforced memory for better convergence in black box optimization.", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  \n        CR_init = 0.9\n        T0 = 1.0\n        alpha = 0.98\n        elitism_rate = 0.15\n        memory_size = 10\n        memory = np.full(memory_size, F_init)\n        \n        # New dynamic crossover strategy factor\n        beta = 0.5\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Adaptive population reduction\n        reduction_factor = 0.95\n\n        while num_evaluations < self.budget:\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Reinforced adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx] * 1.05  # Slightly reinforce the learning factor\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 1.5)))\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - num_evaluations / self.budget) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Add noise reduction\n                trial = 0.95 * trial + 0.05 * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n            reduction_factor *= 0.99\n\n        return best_solution", "configspace": "", "generation": 22, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05802 with standard deviation 0.00346.", "error": "", "parent_ids": ["7912b790-0d68-4d25-ac74-7eccc1476702"], "operator": null, "metadata": {"aucs": [0.06358205907363412, 0.0590162363602007, 0.06315020195553167, 0.05863602430038961, 0.0544800076017512, 0.05823330655568326, 0.05649554200942031, 0.05250558308728692, 0.056105187669549195]}}
{"id": "5190fc58-f31c-4625-ab56-7090580bb9b4", "fitness": 0.05821841545466547, "name": "ImprovedHybridOptimizer", "description": "A refined hybrid optimizer enhancing dynamic feedback in crossover and mutation through adaptive strategies based on convergence rate for improved exploration and exploitation.", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.15  # Elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.5\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Adaptive population reduction\n        reduction_factor = 0.95\n\n        while num_evaluations < self.budget:\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                # Changed line\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.1\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - num_evaluations / self.budget) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)  # Changed line\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Add noise reduction\n                trial = 0.95 * trial + 0.05 * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    # Changed line\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                # Changed line\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n            # Changed line\n            reduction_factor *= 0.985  # Gradually reduce population size\n\n        return best_solution", "configspace": "", "generation": 23, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05822 with standard deviation 0.00414.", "error": "", "parent_ids": ["7912b790-0d68-4d25-ac74-7eccc1476702"], "operator": null, "metadata": {"aucs": [0.06217256409616678, 0.06600475246863735, 0.05822368186484461, 0.057336352391356926, 0.06083114549649116, 0.05375233685215286, 0.055241276274134554, 0.0585991013653504, 0.051804528282854534]}}
{"id": "73b6380a-2c2c-408a-b4d9-8251b7d40f5b", "fitness": 0.058095889649992426, "name": "ImprovedHybridOptimizer", "description": "A refined hybrid optimizer integrating dynamic parameter adaptation and chaotic local search to enhance solution diversity and convergence efficiency.", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8\n        CR_init = 0.9\n        T0 = 1.0\n        alpha = 0.98\n        elitism_rate = 0.15\n        memory_size = 10\n        memory = np.full(memory_size, F_init)\n        beta = 0.5\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n\n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        reduction_factor = 0.95\n\n        while num_evaluations < self.budget:\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.1\n\n            CR = CR_init * (1 - num_evaluations / self.budget) * beta\n\n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Add chaotic local search\n                z = np.random.rand(self.dim)\n                chaotic_factor = 1.0 / (1 + np.exp(-10 * (z - 0.5)))\n                trial = trial * chaotic_factor + population[i] * (1 - chaotic_factor)\n\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            population[:elite_count] = elites\n\n            temperature *= alpha\n            reduction_factor *= 0.985\n\n        return best_solution", "configspace": "", "generation": 24, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05810 with standard deviation 0.00410.", "error": "", "parent_ids": ["5190fc58-f31c-4625-ab56-7090580bb9b4"], "operator": null, "metadata": {"aucs": [0.058941643374349595, 0.06631772277533343, 0.0607475119238311, 0.054404466617826164, 0.06109822735423165, 0.05605603104552148, 0.05243002714128464, 0.05884869616481536, 0.05401868045273839]}}
{"id": "1de33547-7c54-49ca-a854-3d00c78a18ce", "fitness": 0.05749372894748143, "name": "ImprovedHybridOptimizer", "description": "An enhanced adaptive hybrid optimizer with refined noise reduction to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.15  # Elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.5\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Adaptive population reduction\n        reduction_factor = 0.95\n\n        while num_evaluations < self.budget:\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.1\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - num_evaluations / self.budget) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Adjusted noise reduction\n                trial = 0.9 * trial + 0.1 * population[i]  # Changed line\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n            reduction_factor *= 0.985  # Gradually reduce population size\n\n        return best_solution", "configspace": "", "generation": 25, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05749 with standard deviation 0.00334.", "error": "", "parent_ids": ["5190fc58-f31c-4625-ab56-7090580bb9b4"], "operator": null, "metadata": {"aucs": [0.05942474657188057, 0.06388624501038842, 0.060718363834407096, 0.05485264857515737, 0.058904350411976725, 0.05603838189855459, 0.0528636514441162, 0.05674992521962663, 0.05400524756122527]}}
{"id": "7c0af739-ceec-42df-bef6-dd3b92c35f3d", "fitness": 0.05673808158833815, "name": "ImprovedHybridOptimizer", "description": "An adaptive optimizer fine-tuning the balance between exploration and exploitation through dynamic mutation control for enhanced black box optimization. ", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.15  # Elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.5\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Adaptive population reduction\n        reduction_factor = 0.95\n\n        while num_evaluations < self.budget:\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                # Changed line\n                F = F_init * (1 - (num_evaluations / (self.budget * 1.5))) + 0.1\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - num_evaluations / self.budget) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)  # Changed line\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Add noise reduction\n                trial = 0.95 * trial + 0.05 * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    # Changed line\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                # Changed line\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n            # Changed line\n            reduction_factor *= 0.985  # Gradually reduce population size\n\n        return best_solution", "configspace": "", "generation": 26, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05674 with standard deviation 0.00291.", "error": "", "parent_ids": ["5190fc58-f31c-4625-ab56-7090580bb9b4"], "operator": null, "metadata": {"aucs": [0.05944075387641956, 0.06141587158787487, 0.060728236855086215, 0.0548634441517889, 0.05667184291953553, 0.05603717323522295, 0.052872497378304906, 0.05461281312156352, 0.05400010116924692]}}
{"id": "62e709ca-c7cd-43a4-b3c4-04af458522f6", "fitness": 0.05764781546763961, "name": "ImprovedHybridOptimizer", "description": "Enhanced hybrid optimizer with improved exploration via adaptive differential weight using elite performance and dynamic memory update.", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.15  # Elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.5\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Adaptive population reduction\n        reduction_factor = 0.95\n\n        while num_evaluations < self.budget:\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                # Changed line: adapt F based on best fitness\n                F = F_init * (1 - best_fitness / (1 + num_evaluations / self.budget))\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - num_evaluations / self.budget) * beta\n            \n            # Changed line: more frequent memory update\n            memory[memory_idx] = F * (1 + np.random.uniform(-0.05, 0.05))\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)  # Changed line\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Add noise reduction\n                trial = 0.95 * trial + 0.05 * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n            reduction_factor *= 0.985  # Gradually reduce population size\n\n        return best_solution", "configspace": "", "generation": 27, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05765 with standard deviation 0.00465.", "error": "", "parent_ids": ["5190fc58-f31c-4625-ab56-7090580bb9b4"], "operator": null, "metadata": {"aucs": [0.06286242168357137, 0.06277346735600675, 0.06533429255626122, 0.058092233727473186, 0.056592779943600346, 0.053516164662643195, 0.0534490740643353, 0.05470109590172245, 0.05150880931314272]}}
{"id": "2cd622da-2ff4-4f3e-a7ad-99048d88f726", "fitness": 0.058536347394573376, "name": "ImprovedHybridOptimizer", "description": "Further refined hybrid optimizer with improved adaptive memory update to enhance the exploration-exploitation balance.", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.15  # Elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.5\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Adaptive population reduction\n        reduction_factor = 0.95\n\n        while num_evaluations < self.budget:\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                # Changed line\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - num_evaluations / self.budget) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)  # Changed line\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Add noise reduction\n                trial = 0.95 * trial + 0.05 * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    # Changed line\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                # Changed line\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n            # Changed line\n            reduction_factor *= 0.985  # Gradually reduce population size\n\n        return best_solution", "configspace": "", "generation": 28, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05854 with standard deviation 0.00352.", "error": "", "parent_ids": ["5190fc58-f31c-4625-ab56-7090580bb9b4"], "operator": null, "metadata": {"aucs": [0.06435764942956768, 0.06357503495771766, 0.0594932770285419, 0.05932921386444756, 0.05862163874907045, 0.05490525592017581, 0.057156149067170925, 0.05647865537493135, 0.05291025215953704]}}
{"id": "fa60b8c9-fc30-4f0b-99bc-b142fc565d6d", "fitness": 0.05841231033928548, "name": "ImprovedHybridOptimizer", "description": "Enhanced adaptive memory and exploration-exploitation balance with targeted mutation and diversified crossover strategies.", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.15  # Elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.5\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Adaptive population reduction\n        reduction_factor = 0.95\n\n        while num_evaluations < self.budget:\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                # Changed line\n                F = F_init * (1 - (num_evaluations / (self.budget * 1.5))) + 0.2\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - num_evaluations / self.budget) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                # Changed line\n                mutant = x0 + F * (x2 - x1)  # Inverted mutation direction\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Changed line\n                crossover_mask = np.random.rand(self.dim) > CR  # Inverted crossover strategy\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Add noise reduction\n                trial = 0.95 * trial + 0.05 * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    # Changed line\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.2))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                # Changed line\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n            # Changed line\n            reduction_factor *= 0.98  # Slightly slower population reduction\n\n        return best_solution", "configspace": "", "generation": 29, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05841 with standard deviation 0.00295.", "error": "", "parent_ids": ["2cd622da-2ff4-4f3e-a7ad-99048d88f726"], "operator": null, "metadata": {"aucs": [0.06248819194866906, 0.06294849679932502, 0.06154922584656097, 0.0576487039126522, 0.05807055253450255, 0.05678255256168463, 0.055551001826384394, 0.05595705895151826, 0.05471500867227219]}}
{"id": "7629896f-0eee-40b2-9992-94d202ba4667", "fitness": 0.05951042631821287, "name": "ImprovedHybridOptimizer", "description": "Enhanced adaptive crossover strategy and memory update to better balance exploration and exploitation.", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.15  # Elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.5\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Adaptive population reduction\n        reduction_factor = 0.95\n\n        while num_evaluations < self.budget:\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Add noise reduction\n                trial = 0.95 * trial + 0.05 * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n            reduction_factor *= 0.985  # Gradually reduce population size\n\n        return best_solution", "configspace": "", "generation": 30, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05951 with standard deviation 0.00449.", "error": "", "parent_ids": ["2cd622da-2ff4-4f3e-a7ad-99048d88f726"], "operator": null, "metadata": {"aucs": [0.06116200598494659, 0.06074730500159364, 0.06865281911887677, 0.056440978136469644, 0.05605848502187416, 0.06322621277179297, 0.05439127697963442, 0.05402211690710912, 0.060892636941618505]}}
{"id": "2b14f59c-a485-494a-937d-89492cd8a785", "fitness": 0.05951042631821287, "name": "EnhancedHybridOptimizer", "description": "EnhancedHybridOptimizer with adaptive crossover and mutation, dynamic population control, and elite reinforcement for improved convergence.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.15  # Elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.5\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Adaptive population reduction\n        reduction_factor = 0.95\n\n        while num_evaluations < self.budget:\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Add noise reduction\n                trial = 0.95 * trial + 0.05 * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n            reduction_factor *= 0.985  # Gradually reduce population size\n\n        return best_solution", "configspace": "", "generation": 31, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05951 with standard deviation 0.00449.", "error": "", "parent_ids": ["7629896f-0eee-40b2-9992-94d202ba4667"], "operator": null, "metadata": {"aucs": [0.06116200598494659, 0.06074730500159364, 0.06865281911887677, 0.056440978136469644, 0.05605848502187416, 0.06322621277179297, 0.05439127697963442, 0.05402211690710912, 0.060892636941618505]}}
{"id": "65826921-6cd0-4f56-8da6-f1905b5c5b05", "fitness": 0.057363467192482656, "name": "EnhancedHybridOptimizer", "description": "Introduce an adaptive mutation strategy and multi-phase exploration to enhance convergence and solution diversity.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_min, F_max = 0.5, 1.0  # Range for adaptive differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Cooling rate\n        elitism_rate = 0.15  # Elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        F_memory = np.full(memory_size, (F_min + F_max) / 2)\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Adaptive population reduction\n        reduction_factor = 0.95\n\n        while num_evaluations < self.budget:\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive differential weight strategy using memory\n            F = np.random.uniform(F_min, F_max) if np.random.rand() < 0.3 else F_memory[memory_idx]\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget))\n            \n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Add noise reduction\n                trial = 0.95 * trial + 0.05 * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Update differential weight memory\n            F_memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n            reduction_factor *= 0.985  # Gradually reduce population size\n\n        return best_solution", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00369.", "error": "", "parent_ids": ["7629896f-0eee-40b2-9992-94d202ba4667"], "operator": null, "metadata": {"aucs": [0.06472765723338492, 0.05893918961656519, 0.05995668867552861, 0.059669295063454975, 0.05441159949569807, 0.05532705809242733, 0.057483929455910965, 0.05244058674069929, 0.05331520035867454]}}
{"id": "06da3de9-2f62-42f6-9966-61262d21e106", "fitness": 0.06169336830064173, "name": "RefinedHybridOptimizer", "description": "Integrate a multi-scale exploration strategy with adaptive elitism and temperature decay to enhance convergence speed and solution diversity.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 33, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06169 with standard deviation 0.00351.", "error": "", "parent_ids": ["7629896f-0eee-40b2-9992-94d202ba4667"], "operator": null, "metadata": {"aucs": [0.0657356386668404, 0.06377138831395401, 0.06814691432326048, 0.060572832374058305, 0.05881132826831281, 0.0627550394834907, 0.05834592635641023, 0.056665025643782285, 0.06043622127566639]}}
{"id": "3435aba9-9807-4cb7-9d3c-d934b62e1111", "fitness": 0.047766991538825146, "name": "SynergisticAdaptiveSearch", "description": "A Synergistic Adaptive Search Algorithm combining inertia-driven exploitation and cooperative exploration to enhance robustness and convergence efficiency.", "code": "import numpy as np\n\nclass SynergisticAdaptiveSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        inertia_weight = 0.7  # Inertia weight for velocity update\n        cognitive_coeff = 1.5  # Cognitive component for personal best\n        social_coeff = 1.5  # Social component for global best\n        F_init = 0.8  # Differential weight\n        CR_init = 0.9  # Crossover probability\n        temperature = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Elitism rate\n        \n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        velocities = np.random.randn(initial_pop_size, self.dim) * (func.bounds.ub - func.bounds.lb) * 0.1\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        \n        best_idx = np.argmin(fitness)\n        global_best = population[best_idx].copy()\n        global_best_fitness = fitness[best_idx]\n\n        while num_evaluations < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(initial_pop_size, self.dim)\n            r2 = np.random.rand(initial_pop_size, self.dim)\n            new_velocities = (inertia_weight * velocities +\n                              cognitive_coeff * r1 * (personal_best - population) +\n                              social_coeff * r2 * (global_best - population))\n            \n            population = population + new_velocities\n            velocities = new_velocities\n\n            # Differential Evolution crossover\n            for i in range(initial_pop_size):\n                idxs = np.random.choice(initial_pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F_init * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR_init\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                    if trial_fitness < global_best_fitness:\n                        global_best = trial\n                        global_best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Elitism\n            elite_count = int(elitism_rate * initial_pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n            population[:elite_count] = elites\n\n            temperature *= alpha  # Cool down the temperature\n\n        return global_best", "configspace": "", "generation": 34, "feedback": "The algorithm SynergisticAdaptiveSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04777 with standard deviation 0.00414.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.0478287171974191, 0.05602915902849659, 0.04895936394902256, 0.0441579113456837, 0.05174512056578362, 0.045209209636772885, 0.042540778553593506, 0.049873726769472326, 0.04355893680318201]}}
{"id": "ada0a6fa-486e-4b7a-8571-765dae0557c1", "fitness": 0.059241705423925595, "name": "RefinedHybridOptimizer", "description": "Enhance temperature decay rate to improve convergence speed and solution exploration.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.97  # Cooling rate (adjusted for improvement)\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 35, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05924 with standard deviation 0.00316.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06234716138581364, 0.06241953095374275, 0.06492847857859474, 0.05748807606586248, 0.057585742506194526, 0.05986077010786006, 0.05538417583001509, 0.05549032885767624, 0.057671084529570815]}}
{"id": "9b5903f7-c8eb-49e1-8c11-f84f1cdab30e", "fitness": 0.05478080961924828, "name": "RefinedHybridOptimizer", "description": "Enhance adaptive learning with quantum-inspired exploration and dynamic elitism for improved convergence and diversity.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.15  # Reduced elitism rate to promote diversity\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population with quantum-inspired initialization\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction with quantum randomness\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions and dynamically adjust\n            elite_count = max(1, int(elitism_rate * pop_size))\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration with added quantum perturbation\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                quantum_shift = np.random.normal(0, 0.1, size=self.dim)\n                trial = scale * trial + (1 - scale) * population[i] + quantum_shift\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 36, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05478 with standard deviation 0.00535.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.05228773468519443, 0.06438464404340494, 0.058671160966530356, 0.048292183399578525, 0.05936765216914952, 0.05413001194364864, 0.04653999878766224, 0.05719839335636134, 0.05215550722170448]}}
{"id": "1d55b95b-fba7-4b5b-a78c-733b67d271df", "fitness": 0.06100444782928786, "name": "EnhancedAdaptiveOptimizer", "description": "Introduce a dynamic population size with adaptive scaling and crossover, combined with a probabilistic restart mechanism to enhance global search capabilities and avoid premature convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Dynamic population size adjustment\n            pop_size = max(4, int(initial_pop_size * (1 - (num_evaluations / self.budget))))\n            \n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            # Probabilistic restart mechanism\n            if np.random.rand() < 0.05:\n                population[np.random.randint(pop_size)] = np.random.uniform(\n                    func.bounds.lb, func.bounds.ub, size=self.dim\n                )\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 37, "feedback": "The algorithm EnhancedAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06100 with standard deviation 0.00347.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.0638305518043526, 0.06763637275894019, 0.06398983817068082, 0.05884493694010828, 0.062275301303311825, 0.05898348991344837, 0.05668951330500083, 0.05997002976132826, 0.05681999650641956]}}
{"id": "437b5f35-20ba-40b4-bc85-ecceece5355f", "fitness": 0.056887447205820646, "name": "RefinedHybridOptimizer", "description": "Integrate multi-scale exploration with adaptive differential weighting, elitism, and dynamic crossover for efficient convergence and diverse solution landscape.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8\n        CR_init = 0.9\n        T0 = 1.0\n        alpha = 0.95\n        elitism_rate = 0.2\n        memory_size = 10\n        memory = np.full(memory_size, F_init)\n        \n        # Adjusted dynamic crossover strategy factor\n        beta = 0.6\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.3]  # Modified exploration scale\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.35:  # Adjusted probability\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 1.8))) + 0.15  # Adjusted decay rate\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / (self.budget * 1.1))) * beta  # Adjusted decay\n\n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 38, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05689 with standard deviation 0.00419.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.05800780511472314, 0.05868866346579826, 0.06539018620798043, 0.05355109420832882, 0.054182700729588085, 0.06027295747796391, 0.05160945800805039, 0.05222049938781392, 0.05806366025213883]}}
{"id": "cef6969d-ca43-4d24-9088-66cd8190ad8b", "fitness": 0.059677400261666395, "name": "RefinedHybridOptimizer", "description": "Introduce a diversity-enhancing strategy by periodically perturbing non-elites to escape local optima while maintaining the overall structure.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            # Periodic perturbation for non-elites to enhance diversity\n            if num_evaluations % 10 == 0:  # Change introduced here\n                non_elites = set(range(pop_size)) - set(elite_indices)\n                for idx in non_elites:\n                    perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                    population[idx] += perturbation\n                    population[idx] = np.clip(population[idx], func.bounds.lb, func.bounds.ub)\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 39, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05968 with standard deviation 0.00336.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.0657356386668404, 0.06377138831395401, 0.0615945246837728, 0.060572832374058305, 0.05881132826831281, 0.05683299694530619, 0.05834592635641023, 0.056665025643782285, 0.05476694110256053]}}
{"id": "ce004066-acfb-424c-a49e-f01115759e5f", "fitness": 0.05899239454672297, "name": "RefinedHybridOptimizer", "description": "Enhance exploration by dynamically adjusting the differential weight based on the population diversity.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (np.std(fitness) / (np.mean(fitness) + 1e-9)))  # Adjust F based on population diversity\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 40, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05899 with standard deviation 0.00411.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06096952463574723, 0.0619598185960647, 0.06700856377911535, 0.05800691756327281, 0.05805024763720146, 0.0584243099925722, 0.0559159474245825, 0.05954577450183729, 0.05105044679011317]}}
{"id": "819592df-a1cb-4293-a1bb-32776269ae22", "fitness": 0.05639698758926201, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Incorporate dynamic parameter adaptation and swarm-inspired local search to enhance solution accuracy and convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            F = memory[memory_idx] if np.random.rand() < 0.3 else F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget))\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Swarm-inspired local search\n                local_scale = 0.1 * np.random.randn(self.dim)\n                local_search = trial + local_scale * (best_solution - trial)\n                local_search = np.clip(local_search, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                local_fitness = func(local_search)\n                num_evaluations += 2\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i] or local_fitness < trial_fitness:\n                    better_trial = trial if trial_fitness < local_fitness else local_search\n                    better_fitness = min(trial_fitness, local_fitness)\n                    population[i] = better_trial\n                    fitness[i] = better_fitness\n                    if better_fitness < best_fitness:\n                        best_solution = better_trial\n                        best_fitness = better_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 41, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05640 with standard deviation 0.00295.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.05939255539043553, 0.06159707950068383, 0.05950874914512172, 0.05479960755305191, 0.056841854305256234, 0.05492296320236756, 0.05280335227023092, 0.054778014807760456, 0.05292871212844996]}}
{"id": "e551843c-3dbf-44df-bf7f-123f1127bbea", "fitness": 0.06041578230122946, "name": "RefinedHybridOptimizer", "description": "Enhance convergence by refining differential weight adaptation and dynamic crossover.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.6  # Adjusted crossover scaling factor\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.85, 0.5, 0.15]  # Adjusted exploration scales\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 1.5))) + 0.2  # Adjusted learning rate evolution\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 42, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06042 with standard deviation 0.00553.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.07102121833970698, 0.0637026014713653, 0.05883210315853571, 0.06531190516491803, 0.058752491819978214, 0.05430654823995906, 0.06286843276629894, 0.05661000488659507, 0.05233673486370782]}}
{"id": "1703bd34-a501-4a7a-8adc-292d70634c08", "fitness": 0.05792965812572226, "name": "AdvancedHybridOptimizer", "description": "Enhance exploration-exploitation balance using dual differential strategies and adaptive F-CR learning to improve convergence and diversity.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 30\n        F1_init = 0.7  # Differential weight for first strategy\n        F2_init = 0.5  # Differential weight for second strategy\n        CR_init = 0.8  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.92  # Cooling rate\n        elitism_rate = 0.25  # Elitism rate\n        memory_size = 15  # Memory size for adaptive learning\n        memory_F = np.full(memory_size, F1_init)  # Memory for differential weights\n        \n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration with additional strategy\n        exploration_scales = [0.7, 0.4, 0.1]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.9 - 0.004 * (num_evaluations / self.budget)\n            pop_size = max(5, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.4:\n                F1 = memory_F[memory_idx]\n                F2 = F2_init * (1 - (num_evaluations / (self.budget * 1.5))) + 0.1\n            else:\n                F1 = F1_init * (1 - (num_evaluations / (self.budget * 1.8))) + 0.2\n                F2 = memory_F[memory_idx]\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - (num_evaluations / self.budget) ** 0.5)\n            \n            memory_F[memory_idx] = (F1 + F2) / 2\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover with two strategies\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant1 = x0 + F1 * (x1 - x2)\n                mutant1 = np.clip(mutant1, func.bounds.lb, func.bounds.ub)\n                \n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant2 = x0 + F2 * (x1 - x2)\n                mutant2 = np.clip(mutant2, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial1 = np.where(crossover_mask, mutant1, population[i])\n                trial2 = np.where(crossover_mask, mutant2, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial1 = scale * trial1 + (1 - scale) * population[i]\n                trial2 = scale * trial2 + (1 - scale) * population[i]\n\n                # Evaluate trial vectors\n                trial1_fitness = func(trial1)\n                trial2_fitness = func(trial2)\n                num_evaluations += 2\n\n                # Simulated Annealing acceptance\n                if trial1_fitness < fitness[i] or trial2_fitness < fitness[i]:\n                    if trial1_fitness < trial2_fitness:\n                        selected_trial, selected_fitness = trial1, trial1_fitness\n                    else:\n                        selected_trial, selected_fitness = trial2, trial2_fitness\n\n                    population[i] = selected_trial\n                    fitness[i] = selected_fitness\n                    if selected_fitness < best_fitness:\n                        best_solution = selected_trial\n                        best_fitness = selected_fitness\n                else:\n                    acceptance_prob1 = np.exp((fitness[i] - trial1_fitness) / (temperature + 0.1))\n                    acceptance_prob2 = np.exp((fitness[i] - trial2_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob1:\n                        population[i] = trial1\n                        fitness[i] = trial1_fitness\n                    elif np.random.rand() < acceptance_prob2:\n                        population[i] = trial2\n                        fitness[i] = trial2_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 43, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05793 with standard deviation 0.00337.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.05923422097439224, 0.06365502235548226, 0.06257010487807413, 0.05466855440873897, 0.05869698890697317, 0.057706933095035384, 0.05268266376654562, 0.05655197963515557, 0.05560045511110301]}}
{"id": "46933552-d4e9-44fe-8d0a-88f5b3ab8207", "fitness": 0.06150809653458601, "name": "EnhancedAdaptiveOptimizer", "description": "Leverage adaptive scaling and dynamic selection pressure with stochastic ranking to enhance exploration and exploitation balance for robust convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 30\n        F_init = 0.7\n        CR_init = 0.8\n        T0 = 1.0\n        alpha = 0.9\n        elitism_rate = 0.3\n        memory_size = 15\n        memory = np.full(memory_size, F_init)\n        \n        # New adaptive scaling and selection pressure\n        gamma = 0.6\n        \n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Exploration scales\n        exploration_scales = [0.9, 0.6, 0.3]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.9 - 0.006 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.4:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 1.5))) + 0.1\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - pow(num_evaluations / self.budget, gamma))\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Stochastic ranking acceptance\n                if trial_fitness < fitness[i] or np.random.rand() < np.exp(-abs(fitness[i] - trial_fitness) / (temperature + 1e-9)):\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06151 with standard deviation 0.00392.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.0640919098053695, 0.06360412417986983, 0.06937565009129143, 0.0590998660416745, 0.05864490707145231, 0.06384462084529374, 0.056940809299394934, 0.0564997786542244, 0.061471202822703463]}}
{"id": "29dcddcf-6655-48aa-aba3-411494c016e2", "fitness": 0.05578343118973142, "name": "EnhancedAdaptiveOptimizer", "description": "Enhance the adaptive elitism strategy with dynamic adjustment of exploration-exploitation balance and memory-driven mutation control for improved convergence and solution robustness.", "code": "import numpy as np\n\nclass EnhancedAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8\n        CR_init = 0.9\n        T0 = 1.0\n        alpha = 0.95\n        elitism_rate = 0.2\n        memory_size = 10\n        memory = np.full(memory_size, F_init)\n        beta = 0.55\n\n        # Enhanced dynamic exploration-exploitation factor\n        gamma = 0.1\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Adjust exploration-exploitation balance dynamically\n                if np.random.rand() < gamma:\n                    trial = mutant\n                else:\n                    crossover_mask = np.random.rand(self.dim) < CR\n                    trial = np.where(crossover_mask, mutant, population[i])\n\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 45, "feedback": "The algorithm EnhancedAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05578 with standard deviation 0.00390.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.05943557387434406, 0.06317943984470265, 0.05591692015288674, 0.054856405018185894, 0.05825979648266277, 0.05163815479862821, 0.052864814607949895, 0.05613065659742522, 0.04976911933079731]}}
{"id": "db450e8f-ac6f-4079-88a1-27f7e46ad626", "fitness": 0.05812770615093163, "name": "RefinedHybridOptimizer", "description": "Enhance the exploration by slightly increasing the dynamic crossover factor for improved diversity and convergence.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.57  # Slightly increased from 0.55 to improve diversity\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 46, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05813 with standard deviation 0.00473.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.05646244052037297, 0.06579981627776454, 0.06383827907022499, 0.052136244385856845, 0.060643532000492884, 0.05887512864755384, 0.05024793402011518, 0.058418568655137504, 0.05672741178086593]}}
{"id": "e185b2c4-d445-498c-b267-82795f2d393a", "fitness": 0.060782480794026116, "name": "RefinedHybridOptimizer", "description": "Enhance exploration and exploitation balance with adaptive learning, hybrid crossover strategies, and elite migration to improve convergence reliability.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 25  # Modified population size for diversity\n        F_init = 0.7  # Adjusted differential weight\n        CR_init = 0.85  # Adjusted crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.9  # Faster cooling rate\n        elitism_rate = 0.15  # Reduced elitism rate for more exploration\n        memory_size = 15  # Increased memory size for adaptive learning\n        memory = np.full(memory_size, F_init)\n        \n        # New hybrid crossover strategy factor\n        gamma = 0.6\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration with additional scale\n        exploration_scales = [0.85, 0.5, 0.15, 0.05]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.9 - 0.006 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.4:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 1.5))) + 0.2\n\n            # Hybrid crossover strategy\n            CR = CR_init * (1 - np.power(num_evaluations / self.budget, gamma))\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration with additional scale\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 47, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06078 with standard deviation 0.00313.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06530000386289514, 0.06385176105189216, 0.06556840552192689, 0.060182613715151545, 0.05886536932531894, 0.06040800392523382, 0.05797376261278697, 0.056709622558023365, 0.058182784573006185]}}
{"id": "6fcebaa0-e993-46b9-97f8-9519ddc81a11", "fitness": 0.053808715493301094, "name": "QuantumCrowdingHybridOptimizer", "description": "Introduce a synergy of quantum-inspired exploration and crowding distance to enhance the balance between diversity and convergence speed.", "code": "import numpy as np\n\nclass QuantumCrowdingHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.7  # Initial differential weight\n        CR_init = 0.85  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.9  # Cooling rate\n        elitism_rate = 0.2  # Elitism rate\n        memory_size = 15  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New quantum exploration factor\n        gamma = 0.1\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.9 - 0.004 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.1\n\n            # Update memory\n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR_init\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Quantum-inspired exploration\n                if np.random.rand() < gamma:\n                    quantum_step = np.random.normal(0, 1, self.dim)\n                    trial += quantum_step\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 48, "feedback": "The algorithm QuantumCrowdingHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05381 with standard deviation 0.00380.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.05416836291559657, 0.0567560014779086, 0.06125075853804751, 0.050028412302014824, 0.052412083693178846, 0.05649607880194529, 0.04821654924201846, 0.050516147527220556, 0.05443404494177917]}}
{"id": "716101b3-d6f1-4749-b266-a9cdf131dbad", "fitness": 0.05761300442045601, "name": "RefinedHybridOptimizer", "description": "Introduce a random search phase to inject diversity occasionally during optimization.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Random search injection\n                if np.random.rand() < 0.05:  # 5% probability to inject random search\n                    trial = func.bounds.lb + np.random.rand(self.dim) * (func.bounds.ub - func.bounds.lb)\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 49, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05761 with standard deviation 0.00289.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06084286074056311, 0.061663977135954506, 0.06189982686296602, 0.05613596239046903, 0.05689828105765837, 0.057114421672657456, 0.05409274722722213, 0.05483043251275044, 0.055038530183863066]}}
{"id": "f4b81a79-6d8a-45ed-9751-333d21e97241", "fitness": 0.052886513332566304, "name": "EnhancedHybridOptimizer", "description": "Integrate adaptive learning rates and dynamic shrinkage with a chaotic local search to enhance convergence accuracy and robustness.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n        chaos_param = 3.9  # Chaotic parameter for logistic map\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction with dynamic shrinkage\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration with chaotic local search\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                chaotic_factor = np.random.rand() * chaos_param * (1 - np.random.rand())\n                trial = scale * trial + (1 - scale) * population[i] + chaotic_factor * (trial - population[i])\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 50, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05289 with standard deviation 0.00339.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.05532948326940168, 0.05962532373432383, 0.05422722063463514, 0.05110046473791541, 0.055042324316794455, 0.05008348939316987, 0.0492517678303076, 0.05304853847007751, 0.048270007606471266]}}
{"id": "bc23e338-d1a9-4d90-ae19-e51c5b609d61", "fitness": 0.057945241435104614, "name": "RefinedHybridOptimizer", "description": "Enhance the adaptive learning by introducing a dynamic mutation scaling factor to improve convergence precision.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2) * (1 - num_evaluations / self.budget)  # Introduced dynamic mutation scaling\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 51, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05795 with standard deviation 0.00379.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.058408350378864826, 0.06220273606109794, 0.06488257292534305, 0.05393005676926743, 0.05737936669628685, 0.059813217766410576, 0.051978985747933026, 0.0552886656416508, 0.057623220929087005]}}
{"id": "aa051822-8bd0-41fc-8a03-c630c7f3a84b", "fitness": 0.05778476229205282, "name": "RefinedHybridOptimizer", "description": "Enhance the adaptive learning by adjusting the differential weight update and increase the memory size for broader learning capacity.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 12  # Increased Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 1.5))) + 0.15  # Adjusted update\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 52, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05778 with standard deviation 0.00330.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06243838920908518, 0.05921278416505782, 0.06332939512633218, 0.057595806363636703, 0.054661759273608834, 0.05839135798268469, 0.05549725709865183, 0.05268118895794349, 0.056254922451474676]}}
{"id": "a2c002e5-c3b8-4b04-b978-310e4ef3219e", "fitness": 0.058432930249289194, "name": "EnhancedHybridOptimizer", "description": "Enhance the RefinedHybridOptimizer by integrating adaptive mutation scaling with learning from historical successes to boost convergence efficiency and robustness.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        beta = 0.55  # Dynamic crossover strategy factor\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover with adaptive scaling\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                adaptive_scale = np.random.uniform(0.5, 1.0)\n                mutant = x0 + F * adaptive_scale * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                        memory[memory_idx] = F  # Learn from success\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 53, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05843 with standard deviation 0.00377.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.05881730795865847, 0.06332242348083206, 0.06494532736954106, 0.0542938014173856, 0.05840218445450773, 0.059857630577037746, 0.05232479553749403, 0.056272044814287714, 0.05766085663385834]}}
{"id": "e1d311f1-1c3e-4a34-be10-49c98fca1f15", "fitness": 0.058442981619859884, "name": "RefinedHybridOptimizer", "description": "Introduce an adaptive exploration scale to improve balance between exploration and exploitation based on progress.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Adaptive Multi-scale exploration\n                scale = exploration_scales[int((num_evaluations / self.budget) * len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 54, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05844 with standard deviation 0.00362.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06564489735449297, 0.06102307306346044, 0.06044679532104025, 0.060500862843525294, 0.05629753553939987, 0.05578550160631923, 0.05828097376206742, 0.0542469728582633, 0.053760222230170185]}}
{"id": "f1c69333-2f1b-422a-afe0-2aa2dd68569e", "fitness": 0.06080422956038561, "name": "EnhancedDynamicScaleOptimizer", "description": "Introduce dynamic scale exploration with a memory-based learning rate and adaptive elitism to enhance diversity and convergence in optimization.", "code": "import numpy as np\n\nclass EnhancedDynamicScaleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.25  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Dynamic scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Dynamic scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 55, "feedback": "The algorithm EnhancedDynamicScaleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06080 with standard deviation 0.00321.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06344351477673504, 0.06530000699220384, 0.06603822257211056, 0.058506378757615884, 0.06016150870266368, 0.06085446720687637, 0.05636978364978451, 0.05794542148806081, 0.05861876189741977]}}
{"id": "382478df-479c-4639-bde8-85358a9dc79f", "fitness": 0.05616870291825035, "name": "RefinedHybridOptimizer", "description": "Introduce a location-based adaptive mutation strategy and enhanced global-local exploration to improve convergence precision.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n\n        # Modify: New dynamic exploration scale\n        gamma = 0.4\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Modify: Improved crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * gamma\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Modify: Location-based mutation strategy\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2 + 0.5 * (best_solution - x0))\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Modify: Enhanced exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Modify: Adjusted acceptance criterion\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 56, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05617 with standard deviation 0.00283.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.05946252249862827, 0.05952347765022448, 0.06076614547687109, 0.054878887468770454, 0.05494769431444857, 0.056068368687405945, 0.05288561634579858, 0.052956930571212424, 0.05402868325089338]}}
{"id": "57384669-363e-440e-bdc7-af9b7bb69bfd", "fitness": 0.06025482342370625, "name": "RefinedHybridOptimizer", "description": "Enhance adaptive learning by updating the memory index condition for improved solution convergence.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size if num_evaluations % 2 == 0 else memory_idx  # Change here\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 57, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06025 with standard deviation 0.00328.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06236615003911683, 0.06549072192168814, 0.06511629983385692, 0.057522213467016914, 0.06036160380918354, 0.06003099517156707, 0.05542350967919529, 0.058147696727105735, 0.057834220164625805]}}
{"id": "6efa22ef-390b-4e18-880e-c95db39058a6", "fitness": 0.058442981619859884, "name": "RefinedHybridOptimizer", "description": "Introduce adaptive exploration scales tied to evaluation count to enhance balance between exploration and exploitation.  ", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n\n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n\n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Adaptive multi-scale exploration\n        base_exploration_scales = [0.8, 0.5, 0.2]  # Renamed for clarity\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n\n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Adaptive multi-scale exploration\n                scale_idx = int((num_evaluations / self.budget) * len(base_exploration_scales))\n                scale = base_exploration_scales[min(scale_idx, len(base_exploration_scales) - 1)]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 58, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05844 with standard deviation 0.00362.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06564489735449297, 0.06102307306346044, 0.06044679532104025, 0.060500862843525294, 0.05629753553939987, 0.05578550160631923, 0.05828097376206742, 0.0542469728582633, 0.053760222230170185]}}
{"id": "a93f6b18-c228-462d-861b-f17a83ea9f41", "fitness": 0.06169336830064173, "name": "RefinedHybridOptimizerV2", "description": "Combine adaptive memory programming and dynamic population size scaling to enhance search efficiency and solution quality.", "code": "import numpy as np\n\nclass RefinedHybridOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n\n        # Dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        # Adaptive memory initialization\n        fitness_memory = np.inf * np.ones(memory_size)\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Enhanced memory-based learning rate\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Update fitness memory for adaptive F\n                fitness_memory[memory_idx] = trial_fitness\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            # Update memory index based on fitness improvements\n            if trial_fitness < best_fitness:\n                memory_idx = np.argmin(fitness_memory)\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 59, "feedback": "The algorithm RefinedHybridOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06169 with standard deviation 0.00351.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.0657356386668404, 0.06377138831395401, 0.06814691432326048, 0.060572832374058305, 0.05881132826831281, 0.0627550394834907, 0.05834592635641023, 0.056665025643782285, 0.06043622127566639]}}
{"id": "a7977bcb-5e19-4ad3-b4f7-bb5510666df8", "fitness": 0.06115155249266072, "name": "RefinedHybridOptimizer", "description": "Introduce dynamic temperature cooling based on iteration progress alongside adaptive population reduction to enhance convergence.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha - 0.01 * (num_evaluations / self.budget)  # Dynamic temperature cooling\n\n        return best_solution", "configspace": "", "generation": 60, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06115 with standard deviation 0.00323.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.0657356386668404, 0.06377138831395401, 0.06637520303303346, 0.060572832374058305, 0.05881132826831281, 0.061166403060124774, 0.05834592635641023, 0.056665025643782285, 0.058920226717430246]}}
{"id": "ba9b7523-6bc1-4c22-979a-08eb1bda9742", "fitness": 0.05902949142650174, "name": "RefinedHybridOptimizer", "description": "Enhance exploration by introducing dynamic exploration scales and adaptive cooling.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.9, 0.6, 0.3]  # Adjusted exploration scales for improved adaptability\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha * (0.9 + 0.1 * (num_evaluations / self.budget))  # Enhanced temperature decay\n\n        return best_solution", "configspace": "", "generation": 61, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05903 with standard deviation 0.00313.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.061899119503395994, 0.06456686383061894, 0.0625561614820459, 0.05710512013923941, 0.05952000905829957, 0.05768144306276024, 0.05502623217404001, 0.0573394662683967, 0.05557100731971887]}}
{"id": "d661e99a-c0eb-41eb-a126-4403e6c2dfbf", "fitness": 0.06119882709934078, "name": "RefinedHybridOptimizer", "description": "Enhance the search diversity by introducing a rotational crossover mechanism to improve solution quality and convergence.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n                \n                # Rotational crossover modification\n                deviation = np.random.uniform(-0.1, 0.1, size=self.dim)\n                trial = np.roll(trial, int(deviation[0] * self.dim))\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 62, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06120 with standard deviation 0.00313.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06477344596283008, 0.06516962521572722, 0.06617202873517603, 0.059660188758337784, 0.06006946216167053, 0.06093780056533127, 0.05745568219019037, 0.05786726087245864, 0.05868394943234512]}}
{"id": "3598a78c-40aa-4041-b47f-179bc870ab71", "fitness": 0.05724848334538736, "name": "EnhancedMemoryDE", "description": "Employ an adaptive memory-enhanced differential evolution with dynamic elitism and temperature decay for improved convergence and diversity.", "code": "import numpy as np\n\nclass EnhancedMemoryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 30\n        F_init = 0.7  # Initial differential weight\n        CR_init = 0.8  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.25  # Dynamic elitism rate\n        memory_size = 15  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n\n        # New dynamic crossover strategy factor\n        beta = 0.6\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.9, 0.6, 0.3]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.9 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(5, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.2:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 1.5))) + 0.2\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions with dynamic rate\n            elite_count = int((elitism_rate * (1 - num_evaluations / self.budget)) * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n                \n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 63, "feedback": "The algorithm EnhancedMemoryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05725 with standard deviation 0.00302.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06136826581792065, 0.06229447697611934, 0.05959033623227872, 0.05660725408871681, 0.057463769332051995, 0.054998259151200934, 0.05454253493459171, 0.05537004388898181, 0.05300140968662426]}}
{"id": "f9679cdb-82ad-4e5d-b557-845a90d60889", "fitness": 0.05621948085534282, "name": "RefinedHybridOptimizer", "description": "Enhance convergence by introducing adaptive mutation scaling for diversity.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Adaptive mutation scaling for diversity\n            exploration_scales = [0.9, 0.6, 0.3]  # Modified line for adaptation\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 64, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05622 with standard deviation 0.00322.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.05837242941696663, 0.06241586351481232, 0.05912403523469889, 0.05389309062236414, 0.05757664327039147, 0.05457567856621126, 0.051941792687467014, 0.055479333471863734, 0.05259646091330994]}}
{"id": "fdc374b9-f037-4ced-b22e-7603c51cbd1a", "fitness": 0.007744591951551823, "name": "RefinedHybridOptimizer", "description": "Enhanced adaptive elitism and mutation strategies for improved solution quality in black-box optimization.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.25  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.ub)\n\n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2, 0.1]  # Added an additional scale\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 65, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.00774 with standard deviation 0.00107.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.009184736676305483, 0.009184736676305483, 0.009184736676305483, 0.007422984058760962, 0.007422984058760962, 0.007422984058760962, 0.006626055119589025, 0.006626055119589025, 0.006626055119589025]}}
{"id": "6669a26d-7113-47e5-8e27-4e090c075d6d", "fitness": 0.058372583566971774, "name": "EnhancedHybridOptimizer", "description": "Enhance solution quality and speed by integrating chaotic maps for diversity, adaptive learning rates, and elitism within a memory-enhanced hybrid optimization framework.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor and chaotic map parameter\n        beta = 0.55\n        z = np.random.rand()\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Chaotic map for diversity\n            z = (4.0 * z * (1 - z))  # Logistic map\n            diversify_factor = 0.1 + 0.1 * z\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + diversify_factor\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 66, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05837 with standard deviation 0.00395.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.05881731434276016, 0.06586801283402977, 0.06224558124258184, 0.0542893441866239, 0.06066216909858391, 0.0574131780657956, 0.05231877817913555, 0.05841975699737034, 0.055319117155864905]}}
{"id": "69020f57-2e3f-4542-825f-ac228b6f47e7", "fitness": 0.05647914850838579, "name": "AdvancedClusteredOptimizer", "description": "Introduce adaptive clustered mutation with stochastic scaling and dynamic elitism to enhance convergence and diversity in complex landscapes.", "code": "import numpy as np\n\nclass AdvancedClusteredOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 30\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.15  # Dynamic elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction with clustered approach\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(5, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Dynamic elitism: adjust based on current diversity\n            elite_count = int(elitism_rate * pop_size * (1 + np.std(fitness)))\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Clustered Differential Evolution mutation with stochastic scaling\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration with stochastic scaling\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 67, "feedback": "The algorithm AdvancedClusteredOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05648 with standard deviation 0.00299.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.060769329192651544, 0.06133785684673032, 0.05864286160089138, 0.05607474290772818, 0.0566045988794972, 0.05413062732169971, 0.05403617926162185, 0.0545497307867705, 0.05216640977788145]}}
{"id": "d5958f25-71e4-4088-9cfd-40a851d1a5b5", "fitness": 0.05766639930175989, "name": "EnhancedChaosOptimizer", "description": "Integrate dynamic adaptive scaling and elitist selection with chaotic perturbations to enhance exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedChaosOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 30\n        F_init = 0.8\n        CR_init = 0.9\n        T0 = 1.0\n        alpha = 0.9  # Faster cooling rate\n        elitism_rate = 0.3\n        memory_size = 15\n        memory = np.full(memory_size, F_init)\n        \n        # Chaotic sequence initialization\n        chaos_seq = np.linspace(0.1, 0.9, self.budget)\n        np.random.shuffle(chaos_seq)\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n\n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration with chaotic perturbations\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            reduction_factor = 0.9 - 0.004 * (num_evaluations / self.budget)\n            pop_size = max(5, int(initial_pop_size * reduction_factor))\n\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / self.budget)) + chaos_seq[num_evaluations % len(chaos_seq)]\n\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget))\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 68, "feedback": "The algorithm EnhancedChaosOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05767 with standard deviation 0.00382.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06326169837222395, 0.06364773936701995, 0.05768912403642157, 0.05834686870473582, 0.05869652478157272, 0.0532571575416394, 0.056218913743431465, 0.05655387027400449, 0.05132569689478961]}}
{"id": "50ba4760-aa0d-4b56-bba8-9b56d29e208f", "fitness": 0.06028722971424137, "name": "EnhancedHybridOptimizer", "description": "Enhance adaptive memory with exponential decay and incorporate stochastic scaling in multi-scale exploration to improve convergence and resilience.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8\n        CR_init = 0.9\n        T0 = 1.0\n        alpha = 0.95\n        elitism_rate = 0.2\n        memory_size = 10\n        memory = np.full(memory_size, F_init)\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration enhanced with stochastic scaling\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using exponential decay\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * np.exp(-num_evaluations / (self.budget * 2.0))\n\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Stochastic multi-scale exploration\n                scale = np.random.choice(exploration_scales) + 0.1 * np.random.randn()\n                trial = np.clip(scale * trial + (1 - scale) * population[i], func.bounds.lb, func.bounds.ub)\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 69, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06029 with standard deviation 0.00370.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06645197149716542, 0.06114045359090525, 0.06548207430279851, 0.06124073211786196, 0.056420359901474315, 0.06034998533947722, 0.05899332970928495, 0.05437106753251253, 0.05813509343669221]}}
{"id": "d43c37ec-adb8-46cc-9fa1-0c17e72336c1", "fitness": 0.05767386731548527, "name": "DynamicFeedbackOptimizer", "description": "Incorporate a dynamic feedback mechanism with adaptive scale exploration and elite preservation to enhance convergence precision while maintaining diversity.", "code": "import numpy as np\n\nclass DynamicFeedbackOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8\n        CR_init = 0.9\n        T0 = 1.0\n        alpha = 0.95\n        elitism_rate = 0.2\n        memory_size = 10\n        memory = np.full(memory_size, F_init)\n\n        adaptive_scale_factor = 0.45\n        feedback_threshold = 0.1\n\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        exploration_scales = [0.8, 0.5, 0.25]\n\n        while num_evaluations < self.budget:\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * adaptive_scale_factor\n            \n            if num_evaluations < self.budget * feedback_threshold:\n                adaptive_scale_factor *= 1.05\n\n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            population[:elite_count] = elites\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 70, "feedback": "The algorithm DynamicFeedbackOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05767 with standard deviation 0.00328.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06288901183689855, 0.06262710268126115, 0.05909161723195733, 0.05800684028163239, 0.057766793776085845, 0.05455290299399673, 0.055892180812162295, 0.055661091725146106, 0.05257726450022704]}}
{"id": "cc80a730-efdf-49a2-97cc-aac9f87da468", "fitness": 0.06169336830064173, "name": "RefinedHybridOptimizer", "description": "Introduce stochastic exploration scale selection for enhanced solution diversity.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration with stochastic selection\n                scale = np.random.choice(exploration_scales)\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 71, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06169 with standard deviation 0.00351.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.0657356386668404, 0.06377138831395401, 0.06814691432326048, 0.060572832374058305, 0.05881132826831281, 0.0627550394834907, 0.05834592635641023, 0.056665025643782285, 0.06043622127566639]}}
{"id": "c5fdba93-2e1f-4183-b84a-47539437be6e", "fitness": 0.057678107929271444, "name": "RefinedHybridOptimizer", "description": "Introduce a small perturbation in elitism rate and exploration scales for improved diversity and convergence.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.22  # Slightly increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.85, 0.55, 0.25]  # Slightly adjusted scales\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 72, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05768 with standard deviation 0.00363.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.060076385432289436, 0.059639973563777815, 0.06494306922045767, 0.05543856054618712, 0.05505505373678987, 0.059834961795921404, 0.05342324676017485, 0.053060519620112045, 0.05763120068773275]}}
{"id": "58c572c2-9252-439e-a6b8-e98cc15acd5e", "fitness": 0.05913256493107498, "name": "EnhancedHybridOptimizer", "description": "Enhance convergence by incorporating adaptive memory strategies with a focus on diversity preservation and multi-phase search mechanisms.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 30  # Larger initial population for diversity\n        F_init = 0.85  # Adjusted differential weight\n        CR_init = 0.9  # Crossover probability\n        T0 = 1.0  # Temperature for simulated annealing\n        alpha = 0.9  # Cooling rate for temperature decay\n        elitism_rate = 0.25  # More elitism to retain top solutions\n        memory_size = 15  # Extended memory size for more adaptive learning\n        memory = np.full(memory_size, F_init)\n        \n        # Introduce adaptive scaling for crossover strategy\n        gamma = 0.60\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Different exploration scales\n        exploration_scales = [0.9, 0.6, 0.3]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.9 - 0.01 * (num_evaluations / self.budget)\n            pop_size = max(5, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.35:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 1.5))) + 0.1\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * gamma\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 73, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05913 with standard deviation 0.00374.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06262749541451795, 0.06638478851622365, 0.06038437674379715, 0.05773224643780017, 0.06115125305937241, 0.05571504346311229, 0.05561415480469667, 0.0588965472049946, 0.053687178735159935]}}
{"id": "fbaf03fa-8808-46bb-9ea6-df659eed0805", "fitness": 0.05879545929471649, "name": "RefinedHybridOptimizer", "description": "Enhance differential mutation by incorporating a memory-based perturbation for improved convergence.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                # Modification: change mutant calculation with memory perturbation\n                mutant = x0 + F * (x1 - x2 + memory[memory_idx])  \n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 74, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05880 with standard deviation 0.00372.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06544821336591689, 0.06332328030705581, 0.05948340451646472, 0.06031946560570478, 0.05840973902800861, 0.054891712265700776, 0.05810587582129545, 0.05628192921590558, 0.052895513526395765]}}
{"id": "a6b695f3-f63b-44c4-a458-be87715f5435", "fitness": 0.05785840395606058, "name": "RefinedHybridOptimizer", "description": "Introduce adaptive mutation scaling based on fitness improvement to balance exploration and exploitation.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                improvement_factor = 1 + (best_fitness - fitness[i]) / (np.abs(best_fitness) + 1e-9)\n                mutant = x0 + F * (x1 - x2) * improvement_factor  # Modified line\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 75, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05786 with standard deviation 0.00221.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.0627132955729719, 0.059476935775261475, 0.05741348638251831, 0.05579302472056524, 0.05784589444255084, 0.0594911433896016, 0.05581630378995972, 0.056074596676910726, 0.05610095485420541]}}
{"id": "e9232b90-7e8c-49a6-9163-ee0c04ae3a54", "fitness": 0.05981088690940158, "name": "RefinedHybridOptimizer", "description": "Enhance mutation strategy by combining exploration scales for more robust search dynamics.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = np.mean(exploration_scales)  # Change: Use average scale for exploration\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 76, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05981 with standard deviation 0.00341.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06555105919672755, 0.06138433215924144, 0.06461581930575, 0.06040523737806014, 0.05664505321159019, 0.05955455419170841, 0.05818534455327673, 0.05458784488379387, 0.0573687373044659]}}
{"id": "a883ec5d-08f2-4098-9583-7776934ad413", "fitness": 0.0591316311614193, "name": "DynamicCooperativeOptimizer", "description": "Introduce dynamic multi-agent cooperation and local acceleration to boost convergence rate and solution accuracy in diverse optimization landscapes.", "code": "import numpy as np\n\nclass DynamicCooperativeOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 25\n        F_init = 0.7  # Differential weight\n        CR_init = 0.8  # Crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.9  # Cooling rate\n        elitism_rate = 0.25  # Elitism rate\n        memory_size = 8  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # Dynamic crossover strategy factor\n        beta = 0.65\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Enhanced multi-scale exploration with local acceleration\n        exploration_scales = [0.9, 0.6, 0.3, 0.1]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.9 - 0.01 * (num_evaluations / self.budget)\n            pop_size = max(5, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.4:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 1.5))) + 0.2\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Enhanced multi-scale exploration with local acceleration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 77, "feedback": "The algorithm DynamicCooperativeOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05913 with standard deviation 0.00303.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06416715245815852, 0.062443806570853466, 0.06275610455376346, 0.0591324512879543, 0.057609784616921345, 0.057860831102202304, 0.05695815816037275, 0.05551416244126539, 0.05574222926128214]}}
{"id": "7b1f7b48-b6e5-40e8-9e6c-e8f379ef7c87", "fitness": 0.057532581324900384, "name": "RefinedHybridOptimizer", "description": "Enhance the probability of exploring less visited regions by adjusting the dynamic crossover strategy and initial population size.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 25  # Increased initial population size\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / (self.budget * 1.5))) * beta  # Adjusted crossover factor\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 78, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05753 with standard deviation 0.00327.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06369839120569754, 0.06088484198870303, 0.05957003481537548, 0.05873541536275717, 0.056187314881182227, 0.05498743118566107, 0.05658854492714105, 0.054147196496051486, 0.05299406106153437]}}
{"id": "6f3667aa-f8f3-4847-90ec-796e02b578d6", "fitness": 0.05467919241656635, "name": "EnhancedHybridOptimizer", "description": "Introduce a dynamic multi-strategy integration combining adaptive memory, gradient-inspired mutation, and random sampling synergy for enhanced performance and exploration.  ", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_pop_size = 30\n        F_init = 0.9  # Initial differential weight\n        CR_init = 0.85  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.93  # Cooling rate\n        elitism_rate = 0.1\n        memory_size = 15  # Expanded memory size\n        memory = np.full(memory_size, F_init)\n        beta = 0.6\n\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        exploration_scales = [0.8, 0.4, 0.1]\n\n        while num_evaluations < self.budget:\n            pop_size = max(4, int(initial_pop_size * (0.95 - 0.005 * (num_evaluations / self.budget))))\n\n            if np.random.rand() < 0.35:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.2\n\n            CR = CR_init * (1 - np.power(num_evaluations / self.budget, beta))\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                gradient_mutant = x0 + F * (x1 - x2)\n                gradient_mutant = np.clip(gradient_mutant, func.bounds.lb, func.bounds.ub)\n\n                idxs_random = np.random.choice(pop_size, 3, replace=False)\n                x_rand_0, x_rand_1, x_rand_2 = population[idxs_random]\n                random_mutant = x_rand_0 + F * (x_rand_1 - x_rand_2)\n                random_mutant = np.clip(random_mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial_gradient = np.where(crossover_mask, gradient_mutant, population[i])\n                trial_random = np.where(crossover_mask, random_mutant, population[i])\n                \n                trial = (trial_gradient + trial_random) / 2.0\n\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            population[:elite_count] = elites\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 79, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05468 with standard deviation 0.00309.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.05669598555654409, 0.0605538374957908, 0.057699718407964395, 0.05235733746023974, 0.055886361836843146, 0.05326499469939794, 0.05046356609971303, 0.05385839261619052, 0.05133253757641354]}}
{"id": "6ee3afe5-d41b-4bac-9c25-826234bdbd7c", "fitness": 0.057009344258029976, "name": "RefinedHybridOptimizer", "description": "Enhance solution diversity by introducing diversity-driven mutation scaling in the RefinedHybridOptimizer.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                # Diversity-driven mutation scaling\n                F_dynamic = F * (1 + 0.5 * np.std(fitness) / (np.mean(fitness) + 1e-9))\n                mutant = x0 + F_dynamic * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 80, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05701 with standard deviation 0.00617.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06822076580364878, 0.06563324739707777, 0.06048816358333198, 0.05409804302312349, 0.053963924110946415, 0.05550870036892852, 0.04893893581139386, 0.055823315507642524, 0.05040900271617643]}}
{"id": "70cf7a1b-e297-4784-ac81-d6b8eb8f96f2", "fitness": 0.06111781874850294, "name": "RefinedHybridOptimizer", "description": "Enhance adaptive memory strategy by introducing a dynamic mutation scale for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F * (1 + np.sin(num_evaluations / self.budget * np.pi))  # Altered line\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 81, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06112 with standard deviation 0.00390.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06423959864334672, 0.06270443284585481, 0.06885497908489124, 0.05921175603308926, 0.05784314237475485, 0.06338918190321352, 0.05703950294792881, 0.05573669282275506, 0.061041082080692144]}}
{"id": "7a59885a-72d4-48df-842c-3d730f55a1df", "fitness": 0.055798603120969265, "name": "RefinedHybridOptimizer", "description": "Enhance memory adaptation and introduce chaotic sequence initialization for improved exploration and convergence.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population with chaotic sequence\n        population = np.random.rand(initial_pop_size, self.dim)\n        population = np.sin(np.pi * population)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[(memory_idx + num_evaluations) % memory_size]  # Enhanced memory adaptation\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 82, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05580 with standard deviation 0.00426.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.0547225380922024, 0.061029129735174825, 0.06282880304909644, 0.05054401404684994, 0.056296654903563614, 0.05795852000455959, 0.048716007488405766, 0.05424349172518339, 0.05584826904368745]}}
{"id": "58a34cc4-bd2f-4a2a-926b-f1a16d91f267", "fitness": 0.05954159502193279, "name": "RefinedHybridOptimizer", "description": "Enhance exploration by adjusting mutation strategy and diversify crossover for better performance.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2 + np.random.rand(self.dim) * (best_solution - x0))  # Adjusted mutation strategy\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < (CR + np.sin(num_evaluations / self.budget))  # Diversified crossover\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= alpha\n\n        return best_solution", "configspace": "", "generation": 83, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05954 with standard deviation 0.00564.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.06060312989025207, 0.07088930011525574, 0.059274435280122884, 0.05591935391188785, 0.06516355751199321, 0.05470550874803126, 0.053885409844712906, 0.0627153448348794, 0.05271831506025981]}}
{"id": "ed71b654-1df6-472d-b019-8df8b4229b7d", "fitness": 0.06261416275837813, "name": "RefinedHybridOptimizer", "description": "Enhance the exploration phase by introducing a logarithmic temperature decay for improved convergence.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= np.log1p(alpha)\n\n        return best_solution", "configspace": "", "generation": 84, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06261 with standard deviation 0.00343.", "error": "", "parent_ids": ["06da3de9-2f62-42f6-9966-61262d21e106"], "operator": null, "metadata": {"aucs": [0.0655695377152481, 0.06620330780142236, 0.06888703330788182, 0.060416439822501, 0.06100723549693998, 0.06341675424316284, 0.058193831096557735, 0.05876637361174941, 0.06106695172993992]}}
{"id": "bb69b72f-f274-4683-8b61-d46657369bf3", "fitness": 0.061712277238540766, "name": "RefinedHybridOptimizer", "description": "Introduce a new temperature update strategy to improve convergence by enhancing the exploration phase.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= 1 / (1 + alpha)  # Change the temperature update strategy\n\n        return best_solution", "configspace": "", "generation": 85, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06171 with standard deviation 0.00352.", "error": "", "parent_ids": ["ed71b654-1df6-472d-b019-8df8b4229b7d"], "operator": null, "metadata": {"aucs": [0.06590529559039693, 0.06374276127228717, 0.0681166169361056, 0.06067430106517424, 0.058778889477570706, 0.0627277451277456, 0.058423355665794485, 0.05663141038332509, 0.060410119628467096]}}
{"id": "378edaa6-ee77-440a-b0ab-f80b178e90e1", "fitness": 0.06233473330228418, "name": "AdaptiveNeighborhoodOptimizer", "description": "Introduce an adaptive neighborhood search with a dynamic feedback mechanism to enhance local exploration and convergence.", "code": "import numpy as np\n\nclass AdaptiveNeighborhoodOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n\n        # New dynamic factors\n        beta = 0.55\n        gamma = 0.1  # Feedback mechanism factor\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory and feedback\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n            F += gamma * (best_fitness - np.mean(fitness))\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance with feedback\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= np.log1p(alpha)\n\n        return best_solution", "configspace": "", "generation": 86, "feedback": "The algorithm AdaptiveNeighborhoodOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06233 with standard deviation 0.00352.", "error": "", "parent_ids": ["ed71b654-1df6-472d-b019-8df8b4229b7d"], "operator": null, "metadata": {"aucs": [0.06718826534481948, 0.0653588654668229, 0.06453839999745636, 0.06565663991320281, 0.06101116623657876, 0.055766404504714484, 0.062394610270072315, 0.060651675914734304, 0.0584465720721562]}}
{"id": "8e8ebb3a-1abc-4d7c-a082-a7c72cfad966", "fitness": 0.062478722249337294, "name": "EnhancedRefinedHybridOptimizer", "description": "Introduce adaptive scaling factors and a hypermutation phase for enhanced diversity and convergence speed.", "code": "import numpy as np\n\nclass EnhancedRefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration with adaptive scaling\n        exploration_scales = np.array([0.8, 0.5, 0.2])\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration with adaptive scaling\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                adjusted_scale = scale * (1.0 - num_evaluations / (self.budget + 1.0))\n                trial = adjusted_scale * trial + (1 - adjusted_scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            # Hypermutation phase to introduce diversity\n            if num_evaluations % (self.budget // 5) == 0:\n                random_idx = np.random.randint(0, pop_size)\n                population[random_idx] = np.random.rand(self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n\n            temperature *= np.log1p(alpha)\n\n        return best_solution", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedRefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00374.", "error": "", "parent_ids": ["ed71b654-1df6-472d-b019-8df8b4229b7d"], "operator": null, "metadata": {"aucs": [0.06514915167700597, 0.06981458006978036, 0.06526104675746047, 0.060037923973846, 0.06426325434793678, 0.06014000983287049, 0.057832045730558446, 0.06188040501817382, 0.05793008283640333]}}
{"id": "44bbdfe8-2164-4681-8cd4-d02b8b9eb143", "fitness": 0.060166392395115724, "name": "EnhancedAdaptiveOptimizer", "description": "Enhance adaptability by introducing an adaptive population size and dynamic scaling factors for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Dynamic exploration scales\n        exploration_scales = [0.9, 0.6, 0.3]\n\n        while num_evaluations < self.budget:\n            # Adaptive population size based on budget utilization\n            reduction_factor = 0.9 - 0.0075 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Dynamic scale selection for exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= np.log1p(alpha)\n\n        return best_solution", "configspace": "", "generation": 88, "feedback": "The algorithm EnhancedAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06017 with standard deviation 0.00359.", "error": "", "parent_ids": ["ed71b654-1df6-472d-b019-8df8b4229b7d"], "operator": null, "metadata": {"aucs": [0.06393388253125631, 0.061813852958195414, 0.06696240857769975, 0.05891933756364265, 0.05703445478579261, 0.061694459089375764, 0.056753361225691745, 0.05496106800542744, 0.05942470681895984]}}
{"id": "16b8d1c0-1671-4014-826d-bf36d0c8949e", "fitness": 0.05821924404410277, "name": "EnhancedChaoticHybridOptimizer", "description": "Enhance exploration and exploitation by integrating a memory-based adaptive F and CR strategy with chaotic maps for dynamic parameter tuning.", "code": "import numpy as np\n\nclass EnhancedChaoticHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory_F = np.full(memory_size, F_init)  # Memory for differential weight\n        memory_CR = np.full(memory_size, CR_init)  # Memory for crossover probability\n        \n        # Chaotic map initialization\n        chaotic_map = np.random.rand(initial_pop_size)\n        \n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Chaotic map update\n            chaotic_map = 4 * chaotic_map * (1 - chaotic_map)\n            \n            # Adaptive learning rate using memory and chaotic maps\n            if np.random.rand() < 0.3:\n                F = memory_F[memory_idx]\n                CR = memory_CR[memory_idx]\n            else:\n                F = F_init * chaotic_map[np.random.randint(len(chaotic_map))]\n                CR = CR_init * chaotic_map[np.random.randint(len(chaotic_map))]\n\n            memory_F[memory_idx] = F\n            memory_CR[memory_idx] = CR\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= np.log1p(alpha)\n\n        return best_solution", "configspace": "", "generation": 89, "feedback": "The algorithm EnhancedChaoticHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05822 with standard deviation 0.00466.", "error": "", "parent_ids": ["ed71b654-1df6-472d-b019-8df8b4229b7d"], "operator": null, "metadata": {"aucs": [0.05731971700670413, 0.06694642007545648, 0.06214514672551308, 0.05292154766910606, 0.0616695716065625, 0.057328993934621875, 0.05100379608196348, 0.05939688311368907, 0.05524112018330829]}}
{"id": "a3a8725e-f4cb-4d01-97c7-0e8b63999695", "fitness": 0.058425862407865264, "name": "EnhancedHybridOptimizer", "description": "Introduce adaptive mutation and crossover controlled by exploration-exploitation dynamics to enhance global convergence and local refinement.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8\n        CR_init = 0.9\n        T0 = 1.0\n        alpha = 0.95\n        elitism_rate = 0.2\n        memory_size = 10\n        memory = np.full(memory_size, F_init)\n        \n        # Dynamic factors\n        beta = 0.55\n        gamma = 0.5  # New factor for controlling mutation adaptively\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + gamma\n\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            population[:elite_count] = elites\n\n            temperature *= np.log1p(alpha)\n\n        return best_solution", "configspace": "", "generation": 90, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05843 with standard deviation 0.00339.", "error": "", "parent_ids": ["ed71b654-1df6-472d-b019-8df8b4229b7d"], "operator": null, "metadata": {"aucs": [0.06320244537523523, 0.06409977824725355, 0.059753982132758066, 0.05828369056916938, 0.059098917576507315, 0.055151868876920584, 0.05615462984028041, 0.056936730749715725, 0.05315071830294715]}}
{"id": "61c591f0-a1a0-4506-9ff4-7b9b2c17631d", "fitness": 0.05851345251330674, "name": "RefinedHybridOptimizer", "description": "Introduce adaptive scaling and dynamic elitism to balance exploration and exploitation for improved optimization.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 30  # Increased initial population size\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.98  # Faster cooling rate\n        memory_size = 15  # Increased memory size for adaptive learning\n        memory = np.full(memory_size, F_init)\n\n        # New adaptive scaling factor\n        adaptive_scaling_factor = 0.6\n        \n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.6, 0.4]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.9 - 0.004 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.4:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - (num_evaluations / self.budget)) * adaptive_scaling_factor\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Dynamic elitism: gradually increase the elitism rate\n            dynamic_elitism_rate = 0.1 + 0.1 * (num_evaluations / self.budget)\n            elite_count = int(dynamic_elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= np.log1p(alpha)\n\n        return best_solution", "configspace": "", "generation": 91, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05851 with standard deviation 0.00327.", "error": "", "parent_ids": ["ed71b654-1df6-472d-b019-8df8b4229b7d"], "operator": null, "metadata": {"aucs": [0.06043244224655919, 0.06426215932930401, 0.06269870558210122, 0.055752396049725705, 0.059248207382865625, 0.057776742241831824, 0.05372058455804518, 0.057080652567457646, 0.055649182661870245]}}
{"id": "e48bcd20-8067-43d2-9316-753ceee01434", "fitness": 0.06215498298879047, "name": "RefinedHybridOptimizer", "description": "Introduce a dynamic elitism rate that decreases linearly with the number of evaluations to balance exploration and exploitation.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Dynamic elitism rate\n            dynamic_elitism_rate = elitism_rate * (1 - num_evaluations / self.budget)\n            elite_count = int(dynamic_elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= np.log1p(alpha)\n\n        return best_solution", "configspace": "", "generation": 92, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06215 with standard deviation 0.00415.", "error": "", "parent_ids": ["ed71b654-1df6-472d-b019-8df8b4229b7d"], "operator": null, "metadata": {"aucs": [0.06520253062280801, 0.06352434672143781, 0.07045535940160397, 0.060084899313127216, 0.058579999699683416, 0.06482250390292543, 0.05787649925880589, 0.05644046517698542, 0.06240824280173707]}}
{"id": "dc9b3c53-f49a-4c77-8d5d-d5f229463bde", "fitness": 0.04895617611549045, "name": "AdvancedHybridOptimizer", "description": "Enhance solution diversification and local search precision by integrating adaptive mutation scaling and neighborhood search strategies.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.9  # Faster cooling rate\n        elitism_rate = 0.25  # Increased elitism rate\n        memory_size = 15  # Extended memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.9, 0.6, 0.3]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.9 - 0.007 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.4:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 1.5))) + 0.1\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Adaptive mutation with neighborhood search\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Local search with multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                local_neighborhood = scale * (trial + 0.1 * np.random.randn(self.dim))\n                local_neighborhood = np.clip(local_neighborhood, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate local neighborhood\n                neighborhood_fitness = func(local_neighborhood)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if neighborhood_fitness < fitness[i]:\n                    population[i] = local_neighborhood\n                    fitness[i] = neighborhood_fitness\n                    if neighborhood_fitness < best_fitness:\n                        best_solution = local_neighborhood\n                        best_fitness = neighborhood_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - neighborhood_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = local_neighborhood\n                        fitness[i] = neighborhood_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= np.log1p(alpha)\n\n        return best_solution", "configspace": "", "generation": 93, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04896 with standard deviation 0.00365.", "error": "", "parent_ids": ["ed71b654-1df6-472d-b019-8df8b4229b7d"], "operator": null, "metadata": {"aucs": [0.05161481168463433, 0.05602915902849659, 0.04895936394902256, 0.04767271207715951, 0.05174512056578362, 0.045209209636772885, 0.045942544524890216, 0.049873726769472326, 0.04355893680318201]}}
{"id": "ac4ca7af-d5d1-4c36-829a-9eba674ce657", "fitness": 0.06143305107064426, "name": "AdvancedHybridOptimizer", "description": "Introduce a dynamic elitism rate and a mutation factor based on population diversity to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8\n        CR_init = 0.9\n        T0 = 1.0\n        alpha = 0.95\n        memory_size = 10\n        memory = np.full(memory_size, F_init)\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n\n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                diversity = np.std(population, axis=0).mean() / (func.bounds.ub - func.bounds.lb).mean()\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15 * diversity\n\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n\n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            elite_count = int((0.2 + 0.2 * np.sin(0.01 * num_evaluations)) * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            population[:elite_count] = elites\n\n            temperature *= np.log1p(alpha)\n\n        return best_solution", "configspace": "", "generation": 94, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06143 with standard deviation 0.00341.", "error": "", "parent_ids": ["ed71b654-1df6-472d-b019-8df8b4229b7d"], "operator": null, "metadata": {"aucs": [0.06527633250305487, 0.0638935067496188, 0.06766811812942874, 0.06015433140639703, 0.05892210058350833, 0.062289564956864774, 0.057944054825720626, 0.056771226342970604, 0.059978224138234504]}}
{"id": "865560cb-f922-4107-b006-1fb8a95e55b5", "fitness": 0.05897030131984614, "name": "RefinedHybridOptimizer", "description": "Introduce a memory-based mutation strategy to enhance diversity and convergence.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 15  # Increase memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.6\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.4, 0.1]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.93 - 0.006 * (num_evaluations / self.budget)\n            pop_size = max(5, int(initial_pop_size * reduction_factor))  # Adjusted pop size limits\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.35:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.12\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n\n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= np.log1p(alpha)\n\n        return best_solution", "configspace": "", "generation": 95, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05897 with standard deviation 0.00348.", "error": "", "parent_ids": ["ed71b654-1df6-472d-b019-8df8b4229b7d"], "operator": null, "metadata": {"aucs": [0.06426910469217795, 0.06447538570115063, 0.06014307376847583, 0.059209664259227135, 0.059407174254067385, 0.0554998702523658, 0.057026167750050094, 0.05721987568175657, 0.05348239551934386]}}
{"id": "45b6c85c-332f-4a15-899e-2c7644ae09fa", "fitness": 0.06208130294497601, "name": "RefinedHybridOptimizer", "description": "Refine exploration dynamics by introducing a multi-scale temperature decay and adaptive recombination strategy.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            # Adjust memory and index\n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            # Multi-scale temperature decay\n            temperature *= np.log1p(alpha * (num_evaluations / self.budget))\n\n        return best_solution", "configspace": "", "generation": 96, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06208 with standard deviation 0.00347.", "error": "", "parent_ids": ["ed71b654-1df6-472d-b019-8df8b4229b7d"], "operator": null, "metadata": {"aucs": [0.06612112623051125, 0.06438987443892819, 0.06838515813809587, 0.06093431133494298, 0.059371963008511774, 0.06297768714589291, 0.05869711308557357, 0.05720236071293372, 0.06065213240939382]}}
{"id": "cd35adad-983a-45f8-95fb-656a8f51feac", "fitness": 0.05859954671042722, "name": "RefinedHybridOptimizer", "description": "Introduce adaptive exploration scale selection based on convergence progress to enhance exploitation phase.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n        \n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale_idx = int((num_evaluations / self.budget) * len(exploration_scales))\n                scale = exploration_scales[max(0, scale_idx - 1)]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= np.log1p(alpha)\n\n        return best_solution", "configspace": "", "generation": 97, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05860 with standard deviation 0.00369.", "error": "", "parent_ids": ["ed71b654-1df6-472d-b019-8df8b4229b7d"], "operator": null, "metadata": {"aucs": [0.06177312259704726, 0.060010177132680975, 0.06585351337310275, 0.056989844179578086, 0.05538979615458406, 0.060660488171479066, 0.05491532722733894, 0.05338100436416793, 0.05842264719386592]}}
{"id": "0aca5ed0-8b11-4364-bfc4-dc28551a2b03", "fitness": 0.060904490904093195, "name": "RefinedCyclicElitismOptimizer", "description": "Introduce a cyclic learning rate schedule and adaptive elitism to balance exploration and exploitation dynamically.", "code": "import numpy as np\n\nclass RefinedCyclicElitismOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        base_elitism_rate = 0.15  # Base elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)\n        \n        # New cyclic learning rate factor\n        learning_cycle = 5\n        cyclic_amp = 0.2\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n\n            # Cyclic learning rate\n            cycle_progress = (num_evaluations // pop_size) % learning_cycle\n            F = F_init * (1 - cyclic_amp * np.cos(2 * np.pi * cycle_progress / learning_cycle))\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget))\n\n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Adaptive elitism rate\n            elitism_rate = base_elitism_rate + 0.05 * np.sin(2 * np.pi * num_evaluations / (self.budget / 2))\n            elite_count = int(elitism_rate * pop_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= np.log1p(alpha)\n\n        return best_solution", "configspace": "", "generation": 98, "feedback": "The algorithm RefinedCyclicElitismOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06090 with standard deviation 0.00453.", "error": "", "parent_ids": ["ed71b654-1df6-472d-b019-8df8b4229b7d"], "operator": null, "metadata": {"aucs": [0.06325792349496728, 0.06180289133170702, 0.0700969081342423, 0.05832195600214207, 0.05701391879391715, 0.06446700562727803, 0.05618670920552926, 0.05493725254705095, 0.06205585300000471]}}
{"id": "b7af29d3-7de6-4fa2-9831-db7fb2822b96", "fitness": 0.06215498298879047, "name": "RefinedHybridOptimizer", "description": "Introduce adaptive elitism rate and dynamic crossover strategy to enhance convergence efficiency.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 20\n        F_init = 0.8  # Initial differential weight\n        CR_init = 0.9  # Initial crossover probability\n        T0 = 1.0  # Initial temperature for simulated annealing\n        alpha = 0.95  # Cooling rate\n        elitism_rate = 0.2  # Increased elitism rate\n        memory_size = 10  # Memory size for adaptive learning\n        memory = np.full(memory_size, F_init)  # Memory for differential weight\n\n        # New dynamic crossover strategy factor\n        beta = 0.55\n\n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(initial_pop_size):\n            population[i] = func.bounds.lb + population[i] * (func.bounds.ub - func.bounds.lb)\n\n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        num_evaluations = initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        temperature = T0\n        memory_idx = 0\n\n        # Multi-scale exploration\n        exploration_scales = [0.8, 0.5, 0.2]\n\n        while num_evaluations < self.budget:\n            # Adaptive population reduction\n            reduction_factor = 0.95 - 0.005 * (num_evaluations / self.budget)\n            pop_size = max(4, int(initial_pop_size * reduction_factor))\n            \n            # Adaptive elitism rate\n            dynamic_elitism_rate = elitism_rate * (1 - num_evaluations / self.budget)\n\n            # Adaptive learning rate using memory\n            if np.random.rand() < 0.3:\n                F = memory[memory_idx]\n            else:\n                F = F_init * (1 - (num_evaluations / (self.budget * 2.0))) + 0.15\n\n            # Dynamic crossover strategy\n            CR = CR_init * (1 - np.sqrt(num_evaluations / self.budget)) * beta\n            \n            memory[memory_idx] = F\n            memory_idx = (memory_idx + 1) % memory_size\n\n            # Elitism: protect the top solutions\n            elite_count = int(dynamic_elitism_rate * pop_size)  # Modified line\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                # Differential Evolution mutation and crossover\n                idxs = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[idxs]\n                mutant = x0 + F * (x1 - x2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Multi-scale exploration\n                scale = exploration_scales[np.random.randint(len(exploration_scales))]\n                trial = scale * trial + (1 - scale) * population[i]\n\n                # Evaluate trial vector\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Simulated Annealing acceptance\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n                else:\n                    acceptance_prob = np.exp((fitness[i] - trial_fitness) / (temperature + 0.1))\n                    if np.random.rand() < acceptance_prob:\n                        population[i] = trial\n                        fitness[i] = trial_fitness\n\n                if num_evaluations >= self.budget - 1:\n                    break\n\n            # Reintroduce elites to maintain diversity\n            population[:elite_count] = elites\n\n            temperature *= np.log1p(alpha)\n\n        return best_solution", "configspace": "", "generation": 99, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06215 with standard deviation 0.00415.", "error": "", "parent_ids": ["ed71b654-1df6-472d-b019-8df8b4229b7d"], "operator": null, "metadata": {"aucs": [0.06520253062280801, 0.06352434672143781, 0.07045535940160397, 0.060084899313127216, 0.058579999699683416, 0.06482250390292543, 0.05787649925880589, 0.05644046517698542, 0.06240824280173707]}}
