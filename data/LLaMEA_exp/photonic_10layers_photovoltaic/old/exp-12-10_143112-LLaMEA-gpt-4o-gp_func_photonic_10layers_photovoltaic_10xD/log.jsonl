{"id": "34a5cf32-bb23-450e-83ac-6799a70d0f57", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "A hybrid metaheuristic combining differential evolution's exploration with local search's exploitation for efficient optimization.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "909a8389-cc83-49e5-ae97-47611d5b0286", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid metaheuristic that integrates adaptive differential evolution with a dynamic local search radius to improve convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.search_radius = 0.1  # Initial local search radius\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Adaptive Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    # Dynamically adjust the search radius\n                    adaptive_radius = self.search_radius * (1 - (evaluations / self.budget))\n                    neighbor = trial + np.random.uniform(-adaptive_radius, adaptive_radius, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 1, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "344e7757-04ce-4081-ad8c-a0b4d87d2657", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid metaheuristic that combines differential evolution with a gradient-based refinement phase for improved local exploitation and convergence.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.gradient_step_size = 0.01\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search with gradient-based refinement\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    # Gradient approximation\n                    grad = np.zeros(self.dim)\n                    for d in range(self.dim):\n                        perturbed = np.copy(trial)\n                        perturbed[d] += self.gradient_step_size\n                        grad[d] = (func(perturbed) - trial_fit) / self.gradient_step_size\n                        evaluations += 1\n                        if evaluations >= self.budget:\n                            break\n\n                    # Gradient descent step\n                    trial = np.clip(trial - self.gradient_step_size * grad, lb, ub)\n                    trial_fit = func(trial)\n                    evaluations += 1\n\n                    if trial_fit < fitness[i]:\n                        population[i] = trial\n                        fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 2, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "06bf76e3-9b06-4558-96ba-c8c162229a32", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "An enhanced hybrid metaheuristic that introduces adaptive mutation scaling for improved convergence.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                \n                # Adaptive mutation scaling\n                adaptive_f = self.f * (1 - (evaluations / self.budget))  # New line 1\n                mutant = np.clip(a + adaptive_f * (b - c), lb, ub)      # New line 2\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 3, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "4248e3d9-b44d-4d8b-9786-c671f05d6806", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "A hybrid metaheuristic that integrates adaptive differential evolution with a multi-directional search for improved exploration and exploitation in optimization.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution with adaptive strategy\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_adaptive = self.f * (1 + np.random.uniform(-0.1, 0.1))\n                mutant = np.clip(a + F_adaptive * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Multi-directional local search for improved exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    for direction in [-1, 1]:\n                        step = direction * np.random.uniform(0.05, 0.15, self.dim) * (ub - lb)\n                        neighbor = trial + step\n                        neighbor = np.clip(neighbor, lb, ub)\n                        neighbor_fit = func(neighbor)\n                        evaluations += 1\n\n                        if neighbor_fit < trial_fit:\n                            trial = neighbor\n                            trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "cadfb851-1a62-4d17-9e87-a590418f14ec", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "A novel hybrid metaheuristic combining enhanced differential evolution with adaptive local search to improve convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.7  # Adjusted differential mutation factor\n        self.cr = 0.85  # Adjusted crossover probability\n        self.local_search_iters = 5\n        self.adaptive_factor = 0.05  # Adaptive adjustment factor\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Adaptive Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    step_size = np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = trial + step_size\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n                    else:\n                        # Adaptively adjust step size\n                        step_size *= self.adaptive_factor\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "15765d84-f9c8-431e-9d40-873e93b63142", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "Enhanced HybridDE with dynamic strategy adaptation and neighborhood-based local search for improved convergence.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Adaptive Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                \n                # Adapt F and CR dynamically\n                f = np.random.uniform(0.5, 1.0)\n                cr = np.random.uniform(0.7, 1.0)\n\n                mutant = np.clip(a + f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Enhanced Local search using neighborhood exploration\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbors = [trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb) for _ in range(3)]\n                    neighbors = [np.clip(neighbor, lb, ub) for neighbor in neighbors]\n                    neighbors_fit = [func(neighbor) for neighbor in neighbors]\n                    evaluations += len(neighbors)\n\n                    best_neighbor_idx = np.argmin(neighbors_fit)\n                    if neighbors_fit[best_neighbor_idx] < trial_fit:\n                        trial = neighbors[best_neighbor_idx]\n                        trial_fit = neighbors_fit[best_neighbor_idx]\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "5f6b71e7-a3a2-4db4-80da-ca841e85c6d1", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced HybridDEOptimizer with adaptive crossover probability for improved convergence.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Initial crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        adapt_cr_step = (self.cr - 0.6) / self.budget  # Adaptive step\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                self.cr = max(0.6, self.cr - adapt_cr_step)  # Adaptive crossover probability\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 7, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "ca500a00-e326-45e3-8ced-050c74c4e5b0", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhancing HybridDEOptimizer by introducing adaptive crossover probability and mutation factor for better convergence.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                # Adaptive crossover probability\n                cross_points = np.random.rand(self.dim) < (self.cr * (evaluations/self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 8, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "504c5f50-3d76-4d9d-89e0-b0db2193dee4", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhance exploitation by adapting local search parameters dynamically based on fitness improvement trends.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    # Dynamic adjustment of local search scaling factor\n                    scale_factor = 0.1 if trial_fit < fitness[i] else 0.05\n                    neighbor = trial + np.random.uniform(-scale_factor, scale_factor, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 9, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "576bfd5f-1a81-4a2a-91f4-499a3b598a11", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "A novel multi-strategy optimizer enhancing differential evolution with adaptive mutation and guided local search for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f_min = 0.5  # Minimum mutation factor\n        self.f_max = 1.0  # Maximum mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.adaptive_rate = 0.05  # Rate for adaptive strategy\n\n    def adaptive_mutation_factor(self, fitness, idx):\n        rank = np.argsort(fitness)\n        scale = (self.f_max - self.f_min) * (rank[idx] / (self.population_size - 1))\n        return self.f_max - scale\n\n    def guided_local_search(self, current_best, trial, lb, ub):\n        direction = current_best - trial\n        step = np.random.uniform(0.01, 0.1, self.dim) * direction\n        guided_neighbor = np.clip(trial + step, lb, ub)\n        return guided_neighbor\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n\n                f_i = self.adaptive_mutation_factor(fitness, i)\n                mutant = np.clip(a + f_i * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                current_best = population[np.argmin(fitness)]\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = self.guided_local_search(current_best, trial, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 10, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "aebe26dd-cd47-4bf9-b214-47931373f717", "fitness": 0.05489238978066418, "name": "ImprovedHybridDEOptimizer", "description": "An improved hybrid metaheuristic combining dynamic differential evolution and adaptive local search to enhance exploration and exploitation balance for efficient optimization.", "code": "import numpy as np\n\nclass ImprovedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.adaptive_cr = 0.1  # Adaptive crossover probability increment\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        best_global = None\n        best_global_fitness = np.inf\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n                    # Adaptively increase crossover probability if improvement is found\n                    self.cr = min(1.0, self.cr + self.adaptive_cr)\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n                # Update best global solution found\n                if trial_fit < best_global_fitness:\n                    best_global = trial\n                    best_global_fitness = trial_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        return best_global", "configspace": "", "generation": 11, "feedback": "The algorithm ImprovedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "a6c057c7-662a-4192-aee1-3c488ea9091e", "fitness": 0.05489238978066418, "name": "AdaptiveHybridOptimizer", "description": "Adaptive Hybrid Metaheuristic using combined differential evolution, dynamic scaling, and simulated annealing for enhanced optimization.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.temp = 1.0  # Initial temperature for simulated annealing\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        best_solution = population[np.argmin(fitness)]\n        best_fitness = np.min(fitness)\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i] or np.exp((fitness[i] - trial_fit) / self.temp) > np.random.rand():\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search with dynamic scaling factor for exploitation\n                dynamic_scale = (ub - lb) * (0.1 + 0.9 * (self.budget - evaluations) / self.budget)\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * dynamic_scale\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update the best solution found\n            current_best_idx = np.argmin(fitness)\n            if fitness[current_best_idx] < best_fitness:\n                best_solution = population[current_best_idx]\n                best_fitness = fitness[current_best_idx]\n\n            # Simulated annealing temperature decrease\n            self.temp *= 0.99\n\n        return best_solution", "configspace": "", "generation": 12, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "e0e3becc-1149-4c4d-a1c9-648a80c615ac", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid optimizer integrating adaptive differential evolution with stochastic local search for improved convergence and precision.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.5  # Adaptive differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 3\n        self.lr = 0.1  # Learning rate for stochastic local search\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                \n                # Adaptive differential mutation\n                f_local = np.random.uniform(0.4, 0.9)\n                mutant = np.clip(a + f_local * (b - c), lb, ub)\n                \n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = trial_fit\n                \n                # Stochastic local search\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n                    \n                    direction = np.random.uniform(-1, 1, self.dim)\n                    neighbor = trial + self.lr * direction * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n                # Update the best solution found in local search\n                new_population[i] = trial\n                new_fitness[i] = trial_fit\n            \n            population = new_population\n            fitness = new_fitness\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 13, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "bce7ce4b-00f8-4fda-a2c2-4530ebcb77a8", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid metaheuristic that integrates adaptive differential evolution with an intensified local search for superior optimization performance.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Initial differential mutation factor\n        self.cr = 0.9  # Initial crossover probability\n        self.local_search_iters = 5\n        self.f_adapt_step = 0.05\n        self.cr_adapt_step = 0.05\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Adaptive differential evolution\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                # Adapt mutation factor and crossover rate\n                if trial_fit < fitness[i]:\n                    self.f = min(1.0, self.f + self.f_adapt_step)\n                    self.cr = max(0.1, self.cr - self.cr_adapt_step)\n                    population[i] = trial\n                    fitness[i] = trial_fit\n                else:\n                    self.f = max(0.1, self.f - self.f_adapt_step)\n                    self.cr = min(1.0, self.cr + self.cr_adapt_step)\n\n                # Intensified Local Search\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    perturbation = np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = trial + perturbation\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 14, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "2164c5c2-d099-470b-98cd-c0b544b321c7", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "A novel enhancement of differential evolution integrates adaptive parameter control and elitist selection for robust black-box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.elitism_rate = 0.1  # Fraction of elitist solutions to preserve\n        self.adaptive_rate = 0.05  # Rate of adaptation for f and cr\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Sort population by fitness for elitism\n            elites_count = int(self.elitism_rate * self.population_size)\n            sorted_indices = np.argsort(fitness)\n            elites = population[sorted_indices[:elites_count]]\n\n            # Differential evolution - mutation and crossover\n            new_population = []\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    new_population.append(trial)\n                else:\n                    new_population.append(population[i])\n\n                # Local search for exploitation within trial\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with elite preservation\n            new_population = np.array(new_population)\n            population = np.vstack((elites, new_population[elites_count:]))\n            fitness = np.array([func(ind) for ind in population])\n\n            # Adaptive parameter control\n            self.f = np.clip(self.f + self.adaptive_rate * (np.random.rand() - 0.5), 0.5, 1.0)\n            self.cr = np.clip(self.cr + self.adaptive_rate * (np.random.rand() - 0.5), 0.1, 1.0)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 15, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "b4444003-60b1-42e2-8fab-63126ec78933", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhance HybridDEOptimizer by adjusting differential mutation and crossover for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.9  # Differential mutation factor\n        self.cr = 0.85  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 16, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "936dd6bb-6f13-4956-91af-54f1be64d405", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced differential evolution by adapting mutation factor and crossover probability dynamically for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                \n                # Adapt mutation factor and crossover probability\n                self.f = 0.5 + 0.3 * np.random.rand()\n                self.cr = 0.6 + 0.4 * np.random.rand()\n                \n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 17, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "5cc4ecf5-db31-45a9-95ae-18a6b47e8357", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhance the hybrid optimizer by incorporating adaptive mutation factor for dynamic exploration-exploitation balance.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                adaptive_f = self.f * (1 - evaluations / self.budget) # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_f * (b - c), lb, ub) # Use adaptive_f instead of self.f\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 18, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "c5b42c63-10a1-4a43-abdf-12a9786475f7", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced HybridDEOptimizer with adaptive mutation factor for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptive mutation factor based on evaluations\n                self.f = 0.5 + (0.5 - 0.5 * (evaluations / self.budget)) \n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 19, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "f1134912-3001-423b-91f6-5e1afb6db5f3", "fitness": 0.05489238978066418, "name": "AdaptiveHybridDEOptimizer", "description": "An enhanced hybrid metaheuristic using adaptive differential evolution with dynamic local search intensity for improved convergence.", "code": "import numpy as np\n\nclass AdaptiveHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f_min = 0.5  # Minimum mutation factor\n        self.f_max = 0.9  # Maximum mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.dynamic_local_search_iters = 3 * (dim // 10)  # Dynamic based on dimensionality\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation, and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n\n                # Adaptive mutation factor\n                f = self.f_min + (self.f_max - self.f_min) * (1 - evaluations / self.budget)\n\n                mutant = np.clip(a + f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Dynamic local search for exploitation\n                local_iters = min(self.dynamic_local_search_iters, self.budget - evaluations)\n                for _ in range(local_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 20, "feedback": "The algorithm AdaptiveHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "f38edf9c-9c8d-4c19-8cea-67d2f88832f7", "fitness": 0.05489238978066418, "name": "EnhancedHybridDE", "description": "EnhancedHybridDE: An enhanced differential evolution algorithm leveraging adaptive parameter control and a dynamic local search mechanism to improve convergence and solution quality.", "code": "import numpy as np\n\nclass EnhancedHybridDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f_base = 0.5  # Base differential mutation factor\n        self.cr_base = 0.7  # Base crossover probability\n        self.local_search_iters = 5\n        self.dynamic_factor = 0.1  # Factor for adaptive parameter control\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Adaptive parameter control\n            f = self.f_base + self.dynamic_factor * np.random.uniform(-0.1, 0.1)\n            cr = self.cr_base + self.dynamic_factor * np.random.uniform(-0.1, 0.1)\n\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Dynamic local search for exploitation\n                local_search_intensity = int(self.local_search_iters * (1 - evaluations / self.budget))\n                for _ in range(local_search_intensity):\n                    if evaluations >= self.budget:\n                        break\n\n                    step_size = np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = trial + step_size\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 21, "feedback": "The algorithm EnhancedHybridDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "3f2f5b3b-3617-458d-9684-d24f3c483c92", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "A novel metaheuristic enhancing differential evolution with adaptive parameter control and a stochastic Nelder-Mead search for improved exploration and exploitation balance.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.initial_f = 0.8\n        self.initial_cr = 0.9\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        f = self.initial_f\n        cr = self.initial_cr\n\n        while evaluations < self.budget:\n            # Adaptive parameter control\n            f = 0.5 + 0.5 * np.random.rand()\n            cr = 0.5 + 0.5 * np.random.rand()\n\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Stochastic Nelder-Mead local search\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n                    neighbor_res = minimize(func, trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb),\n                                            method='Nelder-Mead', bounds=[(lb[j], ub[j]) for j in range(self.dim)])\n                    neighbor = neighbor_res.x\n                    neighbor_fit = neighbor_res.fun\n                    evaluations += neighbor_res.nfev\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 22, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "476321c4-bd60-4f1c-b1f0-3b6efa99c939", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid metaheuristic integrating ensemble differential evolution strategies with adaptive local search to refine exploration and exploitation balance for improved optimization performance.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f_min, self.f_max = 0.5, 0.9  # Dynamic differential mutation factors\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.adaptive_scale = 0.1  # Adaptive local search scaling factor\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                f = np.random.uniform(self.f_min, self.f_max)  # Dynamic f\n                mutant = np.clip(a + f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Adaptive local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    scale = self.adaptive_scale * (ub - lb) * np.random.uniform()\n                    neighbor = trial + np.random.uniform(-scale, scale, self.dim)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 23, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "2a34366d-cd47-42c8-ac7d-f74440a648c4", "fitness": 0.05489238978066418, "name": "AdaptiveHybridDEOptimizer", "description": "An adaptive hybrid metaheuristic combining differential evolution's exploration with dynamic local search intensity based on performance improvement for efficient optimization.", "code": "import numpy as np\n\nclass AdaptiveHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.initial_local_search_iters = 5\n        self.local_search_step_factor = 0.1\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Adaptive local search for exploitation\n                local_search_iters = self.initial_local_search_iters\n                while local_search_iters > 0:\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-self.local_search_step_factor, self.local_search_step_factor, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n                        local_search_iters = max(local_search_iters + 1, self.initial_local_search_iters * 2)  # Increase intensity if improvement found\n                    else:\n                        local_search_iters -= 1  # Decrease intensity if no improvement\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 24, "feedback": "The algorithm AdaptiveHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "49b7cb3e-e8f7-4858-8e18-ffd792860703", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced HybridDEOptimizer by introducing adaptive mutation factor for improved convergence.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                \n                # Adaptive mutation factor\n                adaptive_f = 0.5 + (0.5 * evaluations / self.budget)\n                mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 25, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "359b5b47-7140-4fdb-99d6-2dd65911dc59", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid metaheuristic that integrates adaptive differential evolution with dynamic local search intensity for improved optimization efficiency.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.base_local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        best_fitness = np.min(fitness)\n\n        while evaluations < self.budget:\n            # Adaptive differential evolution\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                # Update strategy based on fitness improvement\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Dynamic local search intensity\n                improvement_ratio = (best_fitness - trial_fit) / max(abs(best_fitness), 1e-10)\n                dynamic_iters = max(1, int(self.base_local_search_iters * improvement_ratio))\n                \n                # Local search for exploitation\n                for _ in range(dynamic_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n                # Update best fitness found\n                if trial_fit < best_fitness:\n                    best_fitness = trial_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 26, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "ea9aac63-f691-4188-a212-d7ea41f422a7", "fitness": 0.05489238978066418, "name": "EnhancedDEOptimizer", "description": "Enhanced Differential Evolution with Adaptive Mutation and Strategic Local Exploitation for Optimized Performance.", "code": "import numpy as np\n\nclass EnhancedDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f_min, self.f_max = 0.5, 1.0  # Adaptive differential mutation factor range\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - adaptive mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = self.f_min + np.random.rand() * (self.f_max - self.f_min)\n                mutant = np.clip(a + F_dynamic * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Strategic local search for exploitation\n                if np.random.rand() < 0.5:  # Probability to perform local search\n                    for _ in range(self.local_search_iters):\n                        if evaluations >= self.budget:\n                            break\n\n                        neighbor_step = np.random.uniform(-0.05, 0.05, self.dim) * (ub - lb)\n                        neighbor = trial + neighbor_step\n                        neighbor = np.clip(neighbor, lb, ub)\n                        neighbor_fit = func(neighbor)\n                        evaluations += 1\n\n                        if neighbor_fit < trial_fit:\n                            trial = neighbor\n                            trial_fit = neighbor_fit\n\n                # Update population with the best found in local search\n                population[i] = trial\n                fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 27, "feedback": "The algorithm EnhancedDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "51a0ac7a-8332-40c8-9f38-f010b8b8b729", "fitness": 0.05489238978066418, "name": "HybridDE_SA_Optimizer", "description": "A hybrid optimizer integrating differential evolution with simulated annealing to enhance exploration and exploitation balance for robust optimization.", "code": "import numpy as np\n\nclass HybridDE_SA_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.initial_temp = 1.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.95  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        temperature = self.initial_temp\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i] or np.exp((fitness[i] - trial_fit) / temperature) > np.random.rand():\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Simulated annealing for exploitation\n                for _ in range(5):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit or np.exp((trial_fit - neighbor_fit) / temperature) > np.random.rand():\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n                population[i] = trial\n                fitness[i] = trial_fit\n\n            temperature *= self.cooling_rate\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 28, "feedback": "The algorithm HybridDE_SA_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "9d6a33c9-3544-4040-bcf8-28a12c74fc70", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "An enhanced hybrid optimizer combining differential evolution with local search, introducing adaptive mutation control to improve convergence.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutation_factor = np.random.uniform(0.5, 1.0)  # Adaptive mutation\n                mutant = np.clip(a + mutation_factor * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 29, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "0f5ff000-bcee-4e60-be1c-7657ace1d4bf", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid metaheuristic combining differential evolution's exploration, adaptive local search, and elitism to improve convergence and solution quality.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.elitism_rate = 0.1  # Proportion of elites retained\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Implement elitism - retain top individuals\n            elite_count = int(self.elitism_rate * self.population_size)\n            elite_indices = fitness.argsort()[:elite_count]\n            elites = population[elite_indices]\n\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Adaptive local search for exploitation\n                for _ in range(np.random.randint(1, self.local_search_iters + 1)):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n                population[i] = trial\n                fitness[i] = trial_fit\n\n            # Reinforce the population with elites\n            population[:elite_count] = elites\n            fitness[:elite_count] = [func(ind) for ind in elites]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 30, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "062a9d27-768c-47aa-98bc-1e73562c2887", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid optimizer integrating adaptive differential evolution with an entropy-based local search for improved convergence.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f_min, self.f_max = 0.5, 1.0  # Adaptive differential mutation factor bounds\n        self.cr_min, self.cr_max = 0.1, 1.0  # Adaptive crossover probability bounds\n        self.local_search_iters = 10\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            entropy = -np.sum((fitness / np.sum(fitness)) * np.log(fitness / np.sum(fitness)))\n            f = self.f_min + (self.f_max - self.f_min) * entropy\n            cr = self.cr_min + (self.cr_max - self.cr_min) * (1 - entropy)\n\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Entropy-based local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    delta = np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = trial + delta * (1 - entropy)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 31, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "eb5bd6e0-2fc0-4a3d-a659-08a76550d122", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced local search strategy within a hybrid Differential Evolution framework for improved exploitation.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Enhanced local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.normal(0, 0.05, self.dim) * (ub - lb)  # Changed line\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 32, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "32c61d2f-c339-41e9-b070-4c009aa70197", "fitness": 0.05489238978066418, "name": "ImprovedHybridDEOptimizer", "description": "Introducing adaptive parameter control and enhanced local search to improve exploration-exploitation balance in the Hybrid Differential Evolution Optimizer.", "code": "import numpy as np\n\nclass ImprovedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.initial_f = 0.8  # Initial differential mutation factor\n        self.initial_cr = 0.9  # Initial crossover probability\n        self.local_search_iters = 10  # Increased local search iterations\n        self.adaptation_factor = 0.95  # Adaptation factor for f and cr\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        f = self.initial_f\n        cr = self.initial_cr\n\n        while evaluations < self.budget:\n            # Adaptive Differential Evolution - selection, mutation, and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n                else:\n                    # Adaptive parameter control\n                    f *= self.adaptation_factor\n                    cr *= self.adaptation_factor\n\n                # Enhanced Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.05, 0.05, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 33, "feedback": "The algorithm ImprovedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "f20ff456-5704-4f8c-9fff-64bc8608401e", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "An enhanced hybrid metaheuristic with adaptive parameters to balance exploration and exploitation more efficiently.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb) * np.random.rand(self.dim)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 34, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "1b44db2f-87c6-4dec-a761-0911b8cba1a2", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced differential evolution with adaptive mutation to improve convergence precision.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                f_adaptive = self.f + np.random.uniform(-0.1, 0.1)  # Adaptive mutation factor\n                mutant = np.clip(a + f_adaptive * (b - c), lb, ub)  # Use adaptive f\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 35, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "347707d3-d5fe-49e4-93fd-19eebe3555a1", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced hybrid optimizer with adaptive mutation factor and elitism mechanism for improved convergence.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Adaptive mutation factor\n                self.f = 0.5 + 0.3 * np.random.rand()\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Elitism - Keep the best individual\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 36, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "10bc0935-4637-46d7-a19c-3016178ade50", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced HybridDEOptimizer using adaptive mutation factor and probability crossover for improved convergence.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                \n                # Change 1: Adaptive mutation factor\n                self.f = 0.5 + 0.3 * np.random.rand()\n                \n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                # Change 2: Probabilistic crossover with feedback\n                cross_points = np.random.rand(self.dim) < (self.cr + 0.1 * np.random.rand())\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 37, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "241da5de-2026-4f7e-8e5c-27472bd320c9", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid metaheuristic that incorporates adaptive differential evolution with dynamic local search for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.adaptive_strategy = True\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        \n        def adaptive_parameters(evals):\n            return max(0.5, 1.0 - evals / self.budget), min(0.9, 0.5 + evals / self.budget)\n        \n        while evaluations < self.budget:\n            if self.adaptive_strategy:\n                self.f, self.cr = adaptive_parameters(evaluations)\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    local_step_size = np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = trial + local_step_size\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 38, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "9d292389-bebd-4ef6-ae48-cc10234bc6d6", "fitness": 0.05489238978066418, "name": "AdaptiveHybridDEOptimizer", "description": "An adaptive hybrid optimizer combining differential evolution's exploration with a tuned local search strategy and dynamic parameter adjustment for enhanced optimization performance.", "code": "import numpy as np\n\nclass AdaptiveHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        adaptive_f = self.f\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n                    adaptive_f = min(1.0, adaptive_f * 1.05)  # Increase mutation factor if successful\n                else:\n                    adaptive_f = max(0.1, adaptive_f * 0.95)  # Decrease mutation factor if not successful\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    step_size = np.random.uniform(0.05, 0.15) * (ub - lb)\n                    neighbor = trial + np.random.uniform(-1, 1, self.dim) * step_size\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 39, "feedback": "The algorithm AdaptiveHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "3201a03b-0f85-451f-88b8-652126d3827d", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "An enhanced hybrid metaheuristic optimizing both exploration and exploitation by utilizing adaptive mutation and crossover strategies.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                self.f = 0.5 + np.random.rand() * 0.5  # Adaptive mutation\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                self.cr = 0.9 - 0.4 * (evaluations / self.budget)  # Adaptive crossover\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 40, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "b5558343-be96-4b1b-93a5-5b416a27f5b1", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "A refined hybrid metaheuristic enhancing differential evolution with adaptive mutation and crossover for improved optimization efficiency.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                # Line 1 changed: Adaptive crossover rate\n                adaptive_cr = self.cr * (1 - (evaluations / self.budget))\n                cross_points = np.random.rand(self.dim) < adaptive_cr  \n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    # Line 2 changed: Adaptive mutation factor\n                    adaptive_f = self.f * (1 - (evaluations / self.budget))  \n                    \n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 41, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "fc2616d1-0086-4cfc-8881-bf7f1ba0a1e3", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Improved exploration through adaptive mutation and crossover rates in a hybrid metaheuristic.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation, and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                self.f = np.random.uniform(0.5, 1.0)  # Adaptive mutation factor\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                self.cr = np.random.uniform(0.8, 1.0)  # Adaptive crossover probability\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 42, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "082af0c0-5a36-4699-9e9f-5064325cb312", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "An enhanced hybrid metaheuristic that combines differential evolution and local search with adaptive mutation scaling for improved optimization.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                adaptive_f = self.f * np.random.rand()  # Adaptive mutation scaling\n                mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 43, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "db0c134d-68a2-477c-8c4b-6b6d4beef552", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "An enhanced hybrid metaheuristic that fine-tunes exploration and exploitation balance by adjusting mutation factor and crossover probability dynamically.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                self.f = np.random.uniform(0.6, 0.9)  # Adjust mutation factor dynamically\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                self.cr = 0.8 if evaluations < self.budget / 2 else 0.95  # Adjust crossover probability dynamically\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 44, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "02876764-40d9-49b1-bc9f-dc369e13d995", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "An improved HybridDEOptimizer with adaptive mutation factor adjustment based on fitness diversity.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Calculate fitness diversity\n            fitness_diversity = np.std(fitness) / np.mean(fitness)\n            # Adjust mutation factor based on fitness diversity\n            self.f = 0.5 + 0.3 * (fitness_diversity > 0.1)\n\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 45, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "99e8d9c7-769f-4871-a446-e4eed0385724", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid metaheuristic using adaptive differential evolution for exploration and stochastic local search for dynamic exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Initial differential mutation factor\n        self.cr = 0.9  # Initial crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Adjust mutation factor and crossover probability dynamically\n            self.f = 0.5 + 0.3 * np.random.rand()\n            self.cr = 0.8 + 0.2 * np.random.rand()\n\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Stochastic local search for dynamic exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    step_size = (ub - lb) * 0.1 * np.random.rand(self.dim)\n                    neighbor = trial + np.random.uniform(-1, 1, self.dim) * step_size\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 46, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "04d9af87-3ee6-471d-8105-699423223be3", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "Enhanced hybrid optimizer integrating adaptive differential evolution with stochastic local search for improved convergence and efficiency.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f_base = 0.5  # Base differential mutation factor\n        self.cr_base = 0.9  # Base crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Adaptive differential evolution - selection, mutation, and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                f = self.f_base + np.random.rand() * 0.3  # Adaptive mutation factor\n                cr = self.cr_base + np.random.rand() * 0.1  # Adaptive crossover probability\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Stochastic local search for exploitation\n                local_step = np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + local_step\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 47, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "fca7a12e-51f6-46f7-9365-8568b942d1c7", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "An enhanced hybrid metaheuristic improving differential evolution with adaptive mutation and crossover rates for better optimization.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                self.f = 0.5 + 0.3 * np.random.rand()  # Adaptive mutation factor\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                self.cr = 0.8 + 0.2 * np.random.rand()  # Adaptive crossover probability\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 48, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "e0873a27-94b4-4013-b769-19fa2024c449", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid metaheuristic integrating differential evolution's exploration with adaptive local search's dynamic step size for improved optimization efficiency.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.adaptive_step = 0.1  # Initial step size for local search\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation, and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n                    self.adaptive_step *= 1.1  # Increase step size if improved\n                else:\n                    self.adaptive_step *= 0.9  # Decrease step size if not improved\n                \n                self.adaptive_step = np.clip(self.adaptive_step, 0.01, 0.2)  # Limit step size range\n\n                # Adaptive local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    # Dynamic step size based on adaptive_step\n                    neighbor = trial + np.random.uniform(-self.adaptive_step, self.adaptive_step, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n                        self.adaptive_step *= 1.1  # Increase step size if improved\n                    else:\n                        self.adaptive_step *= 0.9  # Decrease step size if not improved\n\n                    self.adaptive_step = np.clip(self.adaptive_step, 0.01, 0.2)  # Limit step size range\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 49, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "c3e9cc02-84ce-4bed-98a8-3ac4c1e2edef", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced hybrid metaheuristic using adaptive mutation factor and crossover probability for improved convergence.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                self.f = 0.5 + 0.3 * np.random.rand()  # Adaptive mutation factor\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                self.cr = 0.6 + 0.3 * np.random.rand()  # Adaptive crossover probability\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 50, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "5259435e-db7a-4c26-8060-847e3ee0806e", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced local search phase by incorporating adaptive step sizes for better exploration-exploitation balance.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                step_size = 0.1 * (ub - lb) * (1 - evaluations / self.budget)  # Adaptive step size\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-step_size, step_size, self.dim)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 51, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "12128d8b-7729-43c3-952d-56fa06b397d1", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "A hybrid metaheuristic combining differential evolution's exploration with enhanced local search exploitation to improve optimization efficiency.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 7  # Increased from 5 to 7 for more extensive local search\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.2, 0.2, self.dim) * (ub - lb)  # Increased step size from 0.1 to 0.2\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 52, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "6875385a-b49a-4d2a-9312-0baa82a11200", "fitness": 0.05489238978066418, "name": "AdaptiveHybridDEOptimizer", "description": "A novel adaptive hybrid metaheuristic blending differential evolution's exploration with a dynamic local search intensity based on convergence rate for enhanced optimization.", "code": "import numpy as np\n\nclass AdaptiveHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.initial_local_search_iters = 5\n        self.min_local_search_iters = 1\n        self.max_local_search_iters = 10\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        previous_best_fitness = np.min(fitness)\n\n        while evaluations < self.budget:\n            # Adaptive local search intensity based on convergence\n            current_best_fitness = np.min(fitness)\n            if current_best_fitness < previous_best_fitness:\n                local_search_iters = self.initial_local_search_iters\n                previous_best_fitness = current_best_fitness\n            else:\n                local_search_iters = max(self.min_local_search_iters, self.initial_local_search_iters - (self.budget - evaluations) // (self.budget // self.initial_local_search_iters))\n\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Adaptive local search for exploitation\n                for _ in range(local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 53, "feedback": "The algorithm AdaptiveHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "8c87ebf9-f798-45c0-ab2a-f474dc2942d9", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid optimization algorithm combining adaptive differential evolution with a dynamic local search strategy for superior exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.dynamic_local_search_iters = max(1, dim // 10)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        archive = []\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n                    archive.append((trial, trial_fit))\n\n                local_search_iters = min(self.dynamic_local_search_iters, len(archive))\n                for _ in range(local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n                        archive.append((neighbor, neighbor_fit))\n\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 54, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "a58e1271-9409-4e49-95d8-deead9352dfa", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhance local search by dynamically adjusting its intensity based on current solution quality.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                adaptative_iters = self.local_search_iters + int(0.1 * (evaluations / self.budget))\n                for _ in range(adaptative_iters):  # Change to adaptive local search\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 55, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "180108b0-1e54-4286-b732-887eea9c7a4a", "fitness": 0.05489238978066418, "name": "RefinedHybridDEOptimizer", "description": "A refined hybrid metaheuristic enhancing differential evolution with adaptive mutation and intensified local search for improved convergence.", "code": "import numpy as np\n\nclass RefinedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.mutation_decay = 0.99  # Decay factor for adaptive mutation\n        self.local_search_intensity = 0.05  # Initial intensity for local search step size\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                adaptive_f = self.f * (1 - evaluations / self.budget)  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Enhanced local search for exploitation\n                step_size = self.local_search_intensity * (ub - lb)\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-step_size, step_size)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n                        step_size *= self.mutation_decay  # Reduce step size as we get better solutions\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 56, "feedback": "The algorithm RefinedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "b34ffc0e-46ca-4a4f-86b7-f096995ef042", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "Enhanced HybridDEOptimizer with adaptive parameter control and elitist selection for improved convergence.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f_min, self.f_max = 0.5, 0.9  # Adaptive mutation factor boundaries\n        self.cr_min, self.cr_max = 0.1, 0.9  # Adaptive crossover probability boundaries\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                f = np.random.uniform(self.f_min, self.f_max)\n                mutant = np.clip(a + f * (b - c), lb, ub)\n\n                cr = np.random.uniform(self.cr_min, self.cr_max)\n                cross_points = np.random.rand(self.dim) < cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                # Elitist selection\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update the population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 57, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "36029d93-c9a5-495c-9d5a-0bdd4954c570", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced HybridDEOptimizer that fine-tunes the crossover rate adaptively and employs Gaussian perturbation in local search to improve convergence.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                self.cr = 0.5 + np.random.rand() * 0.5  # Adaptive crossover probability\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.normal(0, 0.1, self.dim) * (ub - lb)  # Gaussian perturbation\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 58, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "e7371b15-1b88-479a-837d-94ef2d9b2824", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "Integrate adaptive mutation strategies and elitism to enhance exploration and exploitation balance in differential evolution.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f_min, self.f_max = 0.5, 0.9  # Adaptive mutation factors\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Sort population for elitism\n            indices = np.argsort(fitness)\n            population = population[indices]\n            fitness = fitness[indices]\n\n            # Adaptive mutation based on population diversity\n            diversity = np.std(population, axis=0).mean()\n            f = self.f_min + (self.f_max - self.f_min) * (1 - diversity / (ub - lb).mean())\n\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            if trial_fit < fitness[i]:\n                population[i] = trial\n                fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 59, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "e62d3e23-3490-4add-a5da-5e24b600e8a8", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "A synergistic optimizer that enhances differential evolution with adaptive parameter control and a strategic local search to boost convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f_min = 0.5  # Min differential mutation factor\n        self.f_max = 0.9  # Max differential mutation factor\n        self.cr_min = 0.1  # Min crossover probability\n        self.cr_max = 0.9  # Max crossover probability\n        self.local_search_iters = 10\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Adaptive parameters\n            f = self.f_min + (self.f_max - self.f_min) * evaluations / self.budget\n            cr = self.cr_max - (self.cr_max - self.cr_min) * evaluations / self.budget\n\n            # Differential evolution - selection, mutation, and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Strategic local search with adaptive step size\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    step_size = 0.1 * (1 - evaluations / self.budget)\n                    neighbor = trial + np.random.uniform(-step_size, step_size, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 60, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "9f88ee59-94dc-4f9e-8406-bf6e95af5b58", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "An enhanced hybrid DE optimizer with adaptive control of mutation and crossover parameters for improved convergence speed and solution quality.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                self.f = 0.5 + 0.3 * np.random.rand()  # Adaptive mutation factor\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                self.cr = 0.8 + 0.1 * np.random.rand()  # Adaptive crossover probability\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 61, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "ac54e47f-12f8-4c25-8020-895a563f5248", "fitness": -Infinity, "name": "HybridDEOptimizer", "description": "Enhanced HybridDEOptimizer incorporating adaptive mutation rate and elitism for improved convergence.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                \n                # Adaptive mutation rate\n                f_adaptive = self.f * (1 - evaluations / self.budget)\n                mutant = np.clip(a + f_adaptive * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n            # Elitism: Keep the best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n\n        # Return the best solution found\n        return best_solution", "configspace": "", "generation": 62, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'best_solution' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'best_solution' referenced before assignment\")", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {}}
{"id": "feabdf40-7ecc-4ba8-8f27-f95670f4b1a4", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced HybridDEOptimizer by introducing adaptive mutation factor for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                \n                # Adaptive mutation factor\n                adaptive_f = self.f * (1 - evaluations / self.budget)\n\n                mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 63, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "2d31de32-2f91-45e5-a364-5e8e7fc86631", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid optimizer combining adaptive differential evolution with elitist local search for improved convergence and exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.epsilon = 1e-6  # Small value to prevent stagnation\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution with adaptive strategy\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                f_adaptive = self.f + np.random.uniform(-0.1, 0.1)  # Adaptive factor\n                mutant = np.clip(a + f_adaptive * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n                elif np.random.rand() < self.epsilon:\n                    # Introduce diversity to avoid local optima\n                    population[i] = np.random.uniform(lb, ub, self.dim)\n                    fitness[i] = func(population[i])\n                    evaluations += 1\n\n                # Elitist local search\n                best_in_neighborhood = trial\n                best_fit = trial_fit\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.05, 0.05, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < best_fit:\n                        best_in_neighborhood = neighbor\n                        best_fit = neighbor_fit\n\n                # Update population with the best found in local search\n                population[i] = best_in_neighborhood\n                fitness[i] = best_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 64, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "d31e8697-e208-4d7b-bed0-e900262d56b7", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced differential evolution with adaptive mutation factor for dynamic exploration-exploitation balance.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        best_fit = np.min(fitness)\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Adaptive mutation factor\n                self.f = 0.5 + (0.5 * (best_fit - np.min(fitness)) / (best_fit - np.max(fitness) + 1e-10))\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n                    if trial_fit < best_fit:  # Update best fitness\n                        best_fit = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n                        if trial_fit < best_fit:  # Update best fitness\n                            best_fit = trial_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 65, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "66ee434f-a980-496c-9cc7-618a9cd66843", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid metaheuristic combining differential evolution with adaptive mutation strategies and strategic local search for superior black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Initial differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.tau1 = 0.1  # Probability for adjusting F\n        self.tau2 = 0.1  # Probability for adjusting CR\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptive mutation factor and crossover rate\n                if np.random.rand() < self.tau1:\n                    self.f = np.random.uniform(0.5, 0.9)\n                if np.random.rand() < self.tau2:\n                    self.cr = np.random.uniform(0.4, 1.0)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Strategic local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    step_size = (ub - lb) * 0.1 * np.random.rand(self.dim)\n                    neighbor = trial + np.random.choice([-1, 1], self.dim) * step_size\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 66, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "18c658b3-111c-47c3-baf9-20a3dc5cb626", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced hybrid metaheuristic with adaptive mutation factor for improved differential evolution.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                self.f = 0.5 + np.random.rand() * 0.5  # Adaptive mutation factor\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 67, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "c6666656-5631-416a-8ecd-3a96a3ec0593", "fitness": 0.05489238978066418, "name": "RefinedHybridDEOptimizer", "description": "A refined hybrid metaheuristic enhancing diversity and convergence by incorporating adaptive mutation and dynamic local search for improved optimization of black box problems.", "code": "import numpy as np\n\nclass RefinedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f_min = 0.5  # Minimum differential mutation factor\n        self.f_max = 0.9  # Maximum differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def adaptive_mutation_factor(self, fitness, best_fitness):\n        # Adapt the mutation factor based on fitness improvement\n        return self.f_max - (self.f_max - self.f_min) * (fitness - best_fitness) / (np.max(fitness) - best_fitness + 1e-9)\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        best_fitness = np.min(fitness)\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation, and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                f = self.adaptive_mutation_factor(fitness[i], best_fitness)\n                mutant = np.clip(a + f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n                    if trial_fit < best_fitness:\n                        best_fitness = trial_fit\n\n                # Dynamic local search for exploitation\n                dynamic_iters = int(self.local_search_iters * (1 - (evaluations / self.budget)))\n                for _ in range(dynamic_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n                        if trial_fit < best_fitness:\n                            best_fitness = trial_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 68, "feedback": "The algorithm RefinedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "d65cc22a-302b-44af-9652-5c5eee1dccaf", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced HybridDEOptimizer with adaptive differential mutation factor for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                adaptive_f = self.f + np.random.uniform(-0.1, 0.1)  # Change 1: Adaptive mutation factor\n                mutant = np.clip(a + adaptive_f * (b - c), lb, ub)  # Change 2: Use adaptive_f\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 69, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "1bf81a11-5c7e-4bba-b172-b03e6c3f1538", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "A refined hybrid metaheuristic using adaptive mutation factor in differential evolution and enhanced local search for boosted optimization efficiency.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                \n                # Adaptive mutation factor\n                adapt_f = self.f * (1 - evaluations/self.budget)\n                mutant = np.clip(a + adapt_f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Enhanced local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    step_size = np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb) * (1 - evaluations/self.budget)\n                    neighbor = trial + step_size\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 70, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "ddf274ed-19c9-4582-b4e8-7f8797ca00f6", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced exploitation by increasing local search iterations based on current fitness improvement.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation with adaptive iterations\n                improvement = fitness[i] - trial_fit\n                adaptive_iters = self.local_search_iters + int(improvement > 0)  # Change line\n                for _ in range(adaptive_iters):  # Change line\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 71, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "2fa95856-9b9b-4f27-8bcb-4c48dff187fa", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid optimizer combining adaptive differential evolution for robust exploration and dynamic local search for optimized exploitation in complex search spaces.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.5  # Initial differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.adaptation_rate = 0.1\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Adaptive mutation factor based on success\n                if trial_fit < fitness[i]:\n                    self.f = min(1, self.f + self.adaptation_rate)\n                else:\n                    self.f = max(0.1, self.f - self.adaptation_rate)\n\n                # Dynamic local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    step_size = 0.1 * np.exp(-0.1 * evaluations/self.budget)\n                    neighbor = trial + np.random.uniform(-step_size, step_size, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 72, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "4eec4d2a-0d67-40e3-9b10-86331e1f9ec6", "fitness": 0.05489238978066418, "name": "AdaptiveDEAnnealingOptimizer", "description": "A novel metaheuristic combining adaptive differential evolution with simulated annealing to enhance exploration and exploitation, improving optimization efficiency.", "code": "import numpy as np\n\nclass AdaptiveDEAnnealingOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.initial_temperature = 1.0\n        self.cooling_rate = 0.99\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        temperature = self.initial_temperature\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                adaptive_f = self.f * (1 - (evaluations / self.budget))\n                mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                # Simulated annealing acceptance criterion\n                delta = trial_fit - fitness[i]\n                if delta < 0 or np.random.rand() < np.exp(-delta / temperature):\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                temperature *= self.cooling_rate\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 73, "feedback": "The algorithm AdaptiveDEAnnealingOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "8bf1130e-fbfe-4b18-b232-a3d0cf3309d4", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid optimizer integrating differential evolution with adaptive local search and dynamic population resizing for improved global and local search balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.population_shrink_rate = 0.95\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n                \n                local_search_improved = False\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    step_size = 0.1 * (ub - lb) * (1 - evaluations / self.budget)\n                    neighbor = trial + np.random.uniform(-step_size, step_size, self.dim)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n                        local_search_improved = True\n\n                # Apply new trial to population if improved by local search\n                if local_search_improved or trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n            # Shrink population size over time to enhance exploitation\n            population_size = max(2, int(self.initial_population_size * (self.population_shrink_rate ** (evaluations / self.budget))))\n            population = population[:population_size]\n            fitness = fitness[:population_size]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "137ab4b4-392c-48dd-a18a-813233f017fa", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "An enhanced hybrid metaheuristic that improves local search using adaptive step sizes for better optimization.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation with adaptive step size\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    step_size = np.random.uniform(0.05, 0.1)\n                    neighbor = trial + step_size * (ub - lb) * np.random.uniform(-1, 1, self.dim)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 75, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "12566a9f-22f7-4778-ad06-68e25978c522", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced the mutation strategy and local search step in HybridDEOptimizer for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.9  # Differential mutation factor (changed from 0.8)\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.15, 0.15, self.dim) * (ub - lb)  # Range changed from -0.1 to 0.1\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 76, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "cd12ae41-5786-48bd-bf96-7b0fad6dbd4c", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Optimized mutation strategy and adaptive local search step size in HybridDEOptimizer to enhance convergence speed.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.9  # Differential mutation factor (changed)\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                step_size = np.random.uniform(-0.05, 0.05, self.dim)  # Adaptive step size (changed)\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + step_size * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 77, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "9cd037c2-fb01-49ab-b9b3-6b9a0ed7d91f", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "Introducing adaptive parameters and a greedy random sampling strategy to enhance exploration and exploitation balance in differential evolution.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f_initial = 0.5  # Initial differential mutation factor\n        self.cr_initial = 0.7  # Initial crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Dynamically adjust mutation factor and crossover probability\n            f = self.f_initial * (1.0 - evaluations / self.budget)\n            cr = self.cr_initial * (1.0 - evaluations / self.budget)\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Greedy random sampling strategy\n                indices = np.random.permutation(self.population_size)\n                a, b, c = population[indices[:3]]\n                mutant = np.clip(a + f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "814fde33-38c8-49d8-9c41-772d1897cb53", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "Enhanced HybridDE using adaptive mutation factor and dynamic local search step size for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f_min = 0.5  # Minimum differential mutation factor\n        self.f_max = 1.0  # Maximum differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.local_search_step_size = 0.1\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        adaptive_f = self.f_max\n\n        while evaluations < self.budget:\n            # Adaptive differential evolution\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                adaptive_f = self.f_min + (self.f_max - self.f_min) * (1 - evaluations / self.budget)\n                mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Dynamic local search\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    step_size = self.local_search_step_size * (1 - evaluations / self.budget)\n                    neighbor = trial + np.random.uniform(-step_size, step_size, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 79, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "956470b5-c425-435c-b5fe-e52b96c785eb", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "A refined hybrid metaheuristic that enhances local search diversity and adaptively tunes parameters for improved optimization performance.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.15, 0.15, self.dim) * (ub - lb)  # Increased diversity\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n            # Adaptive tuning for mutation factor\n            self.f = 0.5 + 0.3 * (self.budget - evaluations) / self.budget  # Adaptive mutation factor\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 80, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "15124f3b-ba65-4c7f-a4c8-93b2732ccc73", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "A refined hybrid metaheuristic enhancing diversity through adaptive mutation and incorporating a restart strategy for improved exploration.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                self.f = 0.5 + 0.5 * np.random.rand()  # Adaptive mutation factor\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            population[i] = trial\n            fitness[i] = trial_fit\n\n            if evaluations > 0.8 * self.budget:  # Restart strategy\n                population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n                fitness = np.array([func(ind) for ind in population])\n                evaluations += self.population_size\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 81, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "b6e64484-fb17-4c63-879f-0835e2879ae0", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "A hybrid metaheuristic that enhances differential evolution with adaptive parameter tuning and dynamic local search for improved optimization efficiency.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution with adaptive parameters\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adapt mutation factor and crossover probability\n                self.f = 0.5 + 0.3 * np.random.rand()\n                self.cr = 0.6 + 0.3 * np.random.rand()\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Dynamic local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.normal(0, 0.05, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 82, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "ae729648-6190-49cb-87f0-772587a3f189", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "A hybrid algorithm integrating diversity-controlled differential evolution with adaptive local search for enhanced exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.mutation_strategy = 'rand-to-best'  # Additional mutation strategy\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Track diversity in population\n            diversity = np.std(population, axis=0).mean()\n\n            # Differential evolution - selection, mutation, and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n\n                # Apply different mutation strategies based on diversity\n                if diversity > 0.1:\n                    # Classic DE/rand/1 strategy\n                    mutant = np.clip(a + self.f * (b - c), lb, ub)\n                else:\n                    # DE/rand-to-best/1 strategy to enhance convergence\n                    best = population[np.argmin(fitness)]\n                    mutant = np.clip(a + self.f * (best - a) + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Adaptive local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    step_size = np.random.uniform(0.05, 0.15) * (ub - lb)\n                    neighbor = trial + np.random.uniform(-1, 1, self.dim) * step_size\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "a8e9cf5f-a38f-41fb-9a51-92d695f64d1e", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced HybridDEOptimizer with dynamic mutation factor for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            current_f = self.f * (1 - evaluations / self.budget)  # Line 1 change: Dynamic mutation factor\n            \n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + current_f * (b - c), lb, ub)  # Line 2 change: Use current_f instead of self.f\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 84, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "1cc344ad-3afd-4c47-9e85-68dba5d8ca80", "fitness": 0.05489238978066418, "name": "RefinedHybridDEOptimizer", "description": "A refined hybrid metaheuristic enhancing differential evolution with adaptive local search and elitism for improved optimization performance.", "code": "import numpy as np\n\nclass RefinedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.elitism_rate = 0.1  # Proportion of elite individuals\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            new_population = []\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    new_population.append((trial, trial_fit))\n                else:\n                    new_population.append((population[i], fitness[i]))\n\n                # Adaptive local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    step_size = 0.1 * (ub - lb) / (1 + evaluations/self.budget)\n                    neighbor = trial + np.random.uniform(-step_size, step_size, self.dim)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n                        new_population[-1] = (trial, trial_fit)\n\n            # Select the elite individuals\n            new_population.sort(key=lambda x: x[1])\n            elite_count = int(self.elitism_rate * self.population_size)\n            population, fitness = zip(*new_population[:self.population_size - elite_count])\n\n            # Add elite individuals\n            elite_individuals = [(ind, fit) for ind, fit in new_population[:elite_count]]\n            population = list(population) + [ind for ind, _ in elite_individuals]\n            fitness = list(fitness) + [fit for _, fit in elite_individuals]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 85, "feedback": "The algorithm RefinedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "63daf2a0-7de1-4e95-87f2-635e2e9d87f4", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid optimizer integrating adaptive differential evolution with a stochastic hill-climbing local search for robust global optimization.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f_min, self.f_max = 0.5, 0.9  # Adaptive mutation factor range\n        self.cr_min, self.cr_max = 0.1, 0.9  # Adaptive crossover probability range\n        self.local_search_iters = 5\n        self.evaluations = 0\n\n    def adaptive_parameters(self):\n        t = self.evaluations / self.budget\n        f = self.f_min + (self.f_max - self.f_min) * (1 - t)\n        cr = self.cr_min + (self.cr_max - self.cr_min) * t\n        return f, cr\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations = self.population_size\n\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                f, cr = self.adaptive_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                self.evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                for _ in range(self.local_search_iters):\n                    if self.evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.normal(0, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    self.evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n                population[i] = trial\n                fitness[i] = trial_fit\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 86, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "7acd39f3-fb12-40e6-9726-9bc973b30b88", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced metaheuristic algorithm that integrates dynamic parameter adaptation and a chaotic local search mechanism into the HybridDEOptimizer for improved convergence and solution quality.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.chaos_beta = 0.9  # Initial chaos parameter\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Dynamic parameter adaptation\n            self.f = 0.5 + (0.5 * evaluations / self.budget)\n            self.cr = 0.9 - (0.5 * evaluations / self.budget)\n\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Chaotic local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + (np.random.rand(self.dim) - 0.5) * 2 * self.chaos_beta * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n                    \n                    # Update chaos parameter\n                    self.chaos_beta *= 0.98\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "9249bb1d-efa6-4b05-a61b-437e679e605a", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "Enhanced HybridDEOptimizer with adaptive crossover probability for improved convergence.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                # Adaptive crossover probability\n                self.cr = 0.5 + 0.5 * (fitness[i] / np.max(fitness))\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 88, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "13ab71e6-823b-4915-964b-6445e8b1e41a", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "Introducing adaptive mutation and dynamic local search intensity in DE to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f_min, self.f_max = 0.5, 1.0  # Adaptive differential mutation factor range\n        self.cr = 0.9  # Crossover probability\n        self.min_local_search_iters, self.max_local_search_iters = 2, 10\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptive mutation factor\n                f = self.f_min + (self.f_max - self.f_min) * np.random.rand()\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Dynamic local search intensity\n                local_search_iters = self.min_local_search_iters + int(\n                    (1 - (trial_fit / fitness[i])) * (self.max_local_search_iters - self.min_local_search_iters)\n                )\n\n                # Local search for exploitation\n                for _ in range(local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 89, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "404d88af-4808-4e2f-89d9-ac0ca63a0a31", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid metaheuristic combining adaptive differential evolution with local search and elitism for improved exploration and exploitation in optimization.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f_min, self.f_max = 0.5, 1.0  # Adaptive differential mutation factor range\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.elitism = True\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Adaptive mutation factor\n                f = self.f_min + (self.f_max - self.f_min) * np.random.rand()\n                mutant = np.clip(a + f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Elitism: Preserve the best individual\n            if self.elitism:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                best_fitness = fitness[best_idx]\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n            # Restore the best individual if necessary\n            if self.elitism and best_fitness < np.min(fitness):\n                worst_idx = np.argmax(fitness)\n                population[worst_idx] = best_solution\n                fitness[worst_idx] = best_fitness\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 90, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "70cbc3d0-7923-44b0-b834-87201501b29a", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "Introducing adaptive parameter control and a dynamic population size in the HybridDEOptimizer to balance exploration and exploitation effectively for enhanced optimization performance.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.f_min, self.f_max = 0.5, 0.9  # Adaptive mutation factor range\n        self.cr_min, self.cr_max = 0.1, 0.9  # Adaptive crossover probability range\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = evaluations / self.budget\n            self.f = self.f_min + adaptive_factor * (self.f_max - self.f_min)\n            self.cr = self.cr_max - adaptive_factor * (self.cr_max - self.cr_min)\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.05, 0.05, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            if evaluations < self.budget:\n                population[i] = trial\n                fitness[i] = trial_fit\n\n            if evaluations >= 0.7 * self.budget:\n                # Reduce population size towards the end to focus on exploitation\n                self.population_size = max(5, int(self.initial_population_size * (1 - adaptive_factor)))\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 91, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "f72b7db9-e999-4a2b-a131-097207a580dd", "fitness": 0.05489238978066418, "name": "AdaptiveDEOptimizer", "description": "A novel optimizer using adaptive differential evolution with crowding distance for diversity and convergence improvement.", "code": "import numpy as np\n\nclass AdaptiveDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f_base = 0.5  # Base mutation factor\n        self.cr_base = 0.9  # Base crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Adaptive DE - selection, mutation, crossover with crowding distance\n            new_population = np.copy(population)\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adapt mutation factor and crossover rate\n                f = self.f_base + np.random.rand() * 0.5\n                cr = self.cr_base - np.random.rand() * 0.1\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                # Crowding distance for diversity\n                distances = np.linalg.norm(population - trial, axis=1)\n                nearest_idx = np.argmin(distances)\n\n                if trial_fit < fitness[i] and trial_fit < fitness[nearest_idx]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            population = new_population\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 92, "feedback": "The algorithm AdaptiveDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "b9db436c-9238-4903-8797-a6bd26421892", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "An enhanced hybrid metaheuristic that improves local exploration through adaptive search radius and feedback-driven search space reduction.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                adaptive_radius = 0.1 * (ub - lb) * (1 - evaluations / self.budget)  # Adaptive radius\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-adaptive_radius, adaptive_radius, self.dim)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n                        adaptive_radius *= 0.9  # Reduce radius upon improvement\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 93, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "26d40453-6aaf-4dc9-9b3b-acc5c553447e", "fitness": 0.05489238978066418, "name": "AdaptiveHybridDEOptimizer", "description": "An enhanced hybrid metaheuristic using adaptive parameter control and dynamic local search step-size for improved exploration and exploitation balance in differential evolution.", "code": "import numpy as np\n\nclass AdaptiveHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f_min, self.f_max = 0.5, 0.9  # Adaptive differential mutation factor range\n        self.cr_min, self.cr_max = 0.3, 0.9  # Adaptive crossover probability range\n        self.local_search_iters = 5\n\n    def adaptive_params(self, evaluations):\n        # Adapt mutation factor and crossover probability based on the progress\n        progress = evaluations / self.budget\n        f = self.f_min + progress * (self.f_max - self.f_min)\n        cr = self.cr_max - progress * (self.cr_max - self.cr_min)\n        return f, cr\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            f, cr = self.adaptive_params(evaluations)\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                dynamic_step_size = 0.1 * (1 - evaluations / self.budget)\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-dynamic_step_size, dynamic_step_size, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n                population[i] = trial\n                fitness[i] = trial_fit\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 94, "feedback": "The algorithm AdaptiveHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "ad179d75-bc56-43ea-83d3-3ad034dc0ecf", "fitness": 0.05489238978066418, "name": "EnhancedHybridDEOptimizer", "description": "An enhanced hybrid optimization algorithm integrating adaptive differential evolution with a dynamically-tuned local search strategy for robust performance across varied black box functions.", "code": "import numpy as np\n\nclass EnhancedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Initial differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Adaptive differential evolution\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                f_adaptive = self.f * (0.5 + np.random.rand() / 2)  # Adaptive mutation factor\n                mutant = np.clip(a + f_adaptive * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Dynamically tuned local search based on diversity\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    diversity_factor = np.std(population, axis=0) / (ub - lb)\n                    tuning_parameter = max(0.05, 1 - np.mean(diversity_factor))  # dynamic tuning\n                    neighbor = trial + np.random.uniform(-tuning_parameter, tuning_parameter, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 95, "feedback": "The algorithm EnhancedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "679e93af-a160-4831-b1a1-1743e20e06ca", "fitness": 0.05489238978066418, "name": "RefinedHybridOptimizer", "description": "A multi-phase hybrid optimizer integrating adaptive differential evolution with strategic local search and dynamic population resizing for improved convergence efficiency.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.shrink_factor = 0.95  # Factor to reduce population size dynamically\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Adaptive differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Strategic local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    step_size = 0.1 * (1 - evaluations / self.budget)  # Gradually reduce step size\n                    neighbor = trial + np.random.uniform(-step_size, step_size, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n            # Dynamically shrink population size\n            self.population_size = max(5, int(self.shrink_factor * self.population_size))\n            population = population[:self.population_size]\n            fitness = fitness[:self.population_size]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 96, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "9bc64394-1fc2-4643-83dd-317bb8afd2d9", "fitness": 0.05489238978066418, "name": "HybridDEOptimizer", "description": "An enhanced hybrid metaheuristic with adaptive mutation factor for improved optimization performance.", "code": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Initial differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                self.f = 0.5 + np.random.rand() * 0.5  # Adaptive mutation factor\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Local search for exploitation\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 97, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
{"id": "7f8f2121-e7f6-40f4-b41b-dbf1a67e003d", "fitness": 0.05444071257874165, "name": "ImprovedHybridDEOptimizer", "description": "An improved hybrid metaheuristic that enhances exploration via adaptive differential evolution and optimizes exploitation through dynamic local search for better convergence.", "code": "import numpy as np\n\nclass ImprovedHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.5 + np.random.rand() * 0.5  # Adaptive mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                \n                # Adaptive strategy for mutation factor\n                f_adaptive = self.f * (1.2 - fitness[i] / (fitness.max() + 1e-6))\n                mutant = np.clip(a + f_adaptive * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Dynamic local search for exploitation\n                dynamic_steps = int(self.local_search_iters * (1.0 - evaluations / self.budget))\n                for _ in range(dynamic_steps):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 98, "feedback": "The algorithm ImprovedHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05444 with standard deviation 0.00390.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.058870218861461265, 0.05617229535635393, 0.05155150480852089, 0.06028486258848531, 0.057518496332854, 0.052773815040258354, 0.053986171559361695, 0.051512537674584746, 0.04729651098679466]}}
{"id": "518639fe-d7a6-4c02-9a65-7cd128840212", "fitness": 0.05489238978066418, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer mixing dynamic differential evolution with adaptive local search for robust exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.8  # Differential mutation factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_iters = 5\n        self.adaptive_scale = 0.1  # Scale for adaptive local search\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Differential evolution - selection, mutation and crossover\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.f * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fit = func(trial)\n                evaluations += 1\n\n                if trial_fit < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fit\n\n                # Adaptive local search for exploitation\n                adaptive_factor = (ub - lb) * (1 - evaluations / self.budget) * self.adaptive_scale\n                for _ in range(self.local_search_iters):\n                    if evaluations >= self.budget:\n                        break\n\n                    neighbor = trial + np.random.uniform(-adaptive_factor, adaptive_factor, self.dim)\n                    neighbor = np.clip(neighbor, lb, ub)\n                    neighbor_fit = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fit < trial_fit:\n                        trial = neighbor\n                        trial_fit = neighbor_fit\n\n            # Update population with the best found in local search\n            population[i] = trial\n            fitness[i] = trial_fit\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 99, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05489 with standard deviation 0.00347.", "error": "", "parent_ids": ["34a5cf32-bb23-450e-83ac-6799a70d0f57"], "operator": null, "metadata": {"aucs": [0.05482600253385128, 0.05931884302051005, 0.05383063082911632, 0.0561300982562537, 0.06075396533724031, 0.05510932980575278, 0.05030207318240343, 0.054370626377065134, 0.04938993868378461]}}
