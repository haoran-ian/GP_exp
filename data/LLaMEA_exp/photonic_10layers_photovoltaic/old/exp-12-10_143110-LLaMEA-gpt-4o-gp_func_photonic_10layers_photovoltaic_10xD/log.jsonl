{"id": "baef5bbb-eea8-45df-b8b9-e726d39bc4b8", "fitness": 0.05638793421035241, "name": "HybridMetaheuristicOptimizer", "description": "A novel hybrid metaheuristic combining adaptive differential evolution with local search for efficient exploration and exploitation.", "code": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            if func(trial_vector) < func(self.population[i]):\n                new_population[i] = trial_vector\n                if func(trial_vector) < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = func(trial_vector)\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        step_size = 0.1 * (bounds.ub - bounds.lb)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                candidate = np.copy(self.population[i])\n                candidate[j] += step_size[j] * np.random.uniform(-1, 1)\n                candidate = np.clip(candidate, bounds.lb, bounds.ub)\n                if func(candidate) < func(self.population[i]):\n                    self.population[i] = candidate\n                    if func(candidate) < self.best_fitness:\n                        self.best_solution = candidate\n                        self.best_fitness = func(candidate)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 0, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05639 with standard deviation 0.00186.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.05889220975039733, 0.05954955709965393, 0.05670311143818796, 0.056301043050963284, 0.05691534739501647, 0.05422830573148307, 0.05544561411145221, 0.056046396024982514, 0.05340982329103494]}}
{"id": "b9260548-dd0a-41f8-89ce-dd9850e219d3", "fitness": 0.05638793421035241, "name": "HybridMetaheuristicOptimizer", "description": "A novel hybrid metaheuristic combining adaptive differential evolution with local search and adaptive crossover probability for efficient exploration and exploitation.", "code": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            if func(trial_vector) < func(self.population[i]):\n                new_population[i] = trial_vector\n                if func(trial_vector) < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = func(trial_vector)\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        step_size = 0.1 * (bounds.ub - bounds.lb)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                candidate = np.copy(self.population[i])\n                candidate[j] += step_size[j] * np.random.uniform(-1, 1)\n                candidate = np.clip(candidate, bounds.lb, bounds.ub)\n                if func(candidate) < func(self.population[i]):\n                    self.population[i] = candidate\n                    if func(candidate) < self.best_fitness:\n                        self.best_solution = candidate\n                        self.best_fitness = func(candidate)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.local_search(bounds, func)\n                evaluations += self.pop_size\n            # Adaptive crossover probability adjustment\n            self.cr = 0.9 * (1 - (evaluations / self.budget)) + 0.1\n        return self.best_solution", "configspace": "", "generation": 1, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05639 with standard deviation 0.00186.", "error": "", "parent_ids": ["baef5bbb-eea8-45df-b8b9-e726d39bc4b8"], "operator": null, "metadata": {"aucs": [0.05889220975039733, 0.05954955709965393, 0.05670311143818796, 0.056301043050963284, 0.05691534739501647, 0.05422830573148307, 0.05544561411145221, 0.056046396024982514, 0.05340982329103494]}}
{"id": "9f8e4ec2-e830-49dd-9a59-a3cf9108e7b2", "fitness": 0.05463130068662698, "name": "HybridMetaheuristicOptimizer", "description": "A novel hybrid metaheuristic combining adaptive differential evolution with local search for efficient exploration and exploitation, now with dynamic mutation factor adjustment based on population diversity.", "code": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            if func(trial_vector) < func(self.population[i]):\n                new_population[i] = trial_vector\n                if func(trial_vector) < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = func(trial_vector)\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        step_size = 0.1 * (bounds.ub - bounds.lb)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                candidate = np.copy(self.population[i])\n                candidate[j] += step_size[j] * np.random.uniform(-1, 1)\n                candidate = np.clip(candidate, bounds.lb, bounds.ub)\n                if func(candidate) < func(self.population[i]):\n                    self.population[i] = candidate\n                    if func(candidate) < self.best_fitness:\n                        self.best_solution = candidate\n                        self.best_fitness = func(candidate)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            diversity = np.std(self.population, axis=0).mean() / (bounds.ub - bounds.lb).mean()\n            self.mutation_factor = 0.5 + 0.3 * diversity  # Change applied here\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 2, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05463 with standard deviation 0.00199.", "error": "", "parent_ids": ["baef5bbb-eea8-45df-b8b9-e726d39bc4b8"], "operator": null, "metadata": {"aucs": [0.0556788139134301, 0.05863236734625654, 0.055359235980735555, 0.053255192927819994, 0.05603523757647255, 0.052944371969708026, 0.0524530594252135, 0.05517826875796672, 0.052145158282039805]}}
{"id": "4549f32b-0414-4af1-b9f2-4b68986d2084", "fitness": 0.05280144794246866, "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic with adaptive crossover and mutation strategies for improved convergence.", "code": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.5 + np.random.rand() * 0.5  # Adaptive crossover probability\n        self.mutation_factor = 0.5 + np.random.rand() * 0.5  # Adaptive mutation factor\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            if func(trial_vector) < func(self.population[i]):\n                new_population[i] = trial_vector\n                if func(trial_vector) < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = func(trial_vector)\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        step_size = 0.1 * (bounds.ub - bounds.lb)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                candidate = np.copy(self.population[i])\n                candidate[j] += step_size[j] * np.random.uniform(-1, 1)\n                candidate = np.clip(candidate, bounds.lb, bounds.ub)\n                if func(candidate) < func(self.population[i]):\n                    self.population[i] = candidate\n                    if func(candidate) < self.best_fitness:\n                        self.best_solution = candidate\n                        self.best_fitness = func(candidate)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 3, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05280 with standard deviation 0.00374.", "error": "", "parent_ids": ["baef5bbb-eea8-45df-b8b9-e726d39bc4b8"], "operator": null, "metadata": {"aucs": [0.05976113798773208, 0.051737144991802375, 0.05247156074249715, 0.05713590511251754, 0.0494838726459953, 0.050187351749166, 0.05626937619407779, 0.048736592664473055, 0.049430089393956655]}}
{"id": "06f890d3-2727-48ee-a532-4e99d1e885af", "fitness": 0.05736185718057791, "name": "EnhancedHybridOptimizer", "description": "A synergistic hybrid optimizer that enhances adaptive differential evolution with gradient-based local search for superior exploration and fine-tuned exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["baef5bbb-eea8-45df-b8b9-e726d39bc4b8"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "9a184f4d-f91b-4d5e-8805-b0d029d02823", "fitness": 0.05623905682974956, "name": "EnhancedHybridOptimizer", "description": "Utilize a self-adaptive mutation factor and dynamic crossover to improve exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  \n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            self.mutation_factor = np.random.rand()  # Self-adaptive mutation\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05624 with standard deviation 0.00268.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.055861043447196, 0.06142838534062889, 0.05737616413577595, 0.05342955173986552, 0.05870636118385686, 0.05487189713873564, 0.052624885173843006, 0.05780928134107388, 0.054043941966770315]}}
{"id": "a73f404d-3f72-479e-bd72-c992b985c849", "fitness": 0.05437065978698696, "name": "EnhancedHybridOptimizer", "description": "A hybrid optimizer integrating adaptive differential evolution with stochastic tunneling and adaptive gradient-based local search to enhance convergence speed and accuracy.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if np.random.rand() < np.exp(-(trial_fitness - func(self.population[i])) / (abs(func(self.population[i])) + 1e-9)):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient * np.random.uniform(0.8, 1.2)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05437 with standard deviation 0.00225.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.055623757265452434, 0.05879259225961231, 0.054440410754432955, 0.05320321187296717, 0.05619528827872766, 0.05206369899202523, 0.052402056331450675, 0.05533812788225001, 0.051276794445964224]}}
{"id": "2eed6180-cc11-4979-bb06-6c42f8c2d570", "fitness": 0.05679611235139906, "name": "EnhancedHybridOptimizer", "description": "A refined hybrid optimizer that incorporates adaptive mutation scaling and dynamic crossover probability to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            # Adaptive mutation factor based on best fitness\n            self.mutation_factor = 0.5 + 0.3 * np.exp(-0.1 * self.best_fitness)\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            # Dynamic crossover rate\n            self.cr = 0.9 if np.random.rand() < 0.5 else 0.2\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05680 with standard deviation 0.00467.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.05606428288468979, 0.06508569151005017, 0.055563647246712256, 0.05380337022235904, 0.06496827820613615, 0.05322344688345804, 0.05270735935309834, 0.05744184966873278, 0.05230708518735494]}}
{"id": "f8185039-69a7-4ed6-95e4-4297a7502778", "fitness": 0.05736185718057791, "name": "AdaptiveEnhancedHybridOptimizer", "description": "An adaptive hybrid optimizer integrating a self-adjusting mutation factor in differential evolution with variance-based dynamic local search for enhanced convergence.", "code": "import numpy as np\n\nclass AdaptiveEnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func, evaluations):\n        new_population = np.copy(self.population)\n        scale = (self.budget - evaluations) / self.budget\n        self.mutation_factor = 0.5 + 0.3 * scale  # Adaptive mutation factor\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def variance_based_local_search(self, bounds, func):\n        variance = np.var(self.population, axis=0)\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient * (1 + variance)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func, evaluations)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.variance_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveEnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "b5459124-2612-47f5-82ea-43deef9de800", "fitness": 0.05736185718057791, "name": "EnhancedHybridOptimizerV2", "description": "A differentiated hybrid optimizer combining enhanced adaptive differential evolution with dynamic adaptive local search and mutation factor control, aiming for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def dynamic_adaptive_local_search(self, bounds, func):\n        learning_rate = 0.001\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = learning_rate * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def update_mutation_factor(self, evaluations):\n        self.mutation_factor = 0.8 - 0.6 * (evaluations / self.budget)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            self.update_mutation_factor(evaluations)\n            if evaluations < self.budget:\n                self.dynamic_adaptive_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 9, "feedback": "The algorithm EnhancedHybridOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "d263b01e-f16e-4c52-aa24-cd2e04b137c4", "fitness": 0.05736185718057791, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer that combines adaptive differential evolution and refined local search with dynamic step size adjustment for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb) * (1 - self.best_fitness / float('inf'))  # Dynamic adjustment\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 10, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "d6046821-4063-412a-9ad6-91e03a6a7d8c", "fitness": 0.05601437997704117, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer employing adaptive mutation factor and dynamic step size in local search for improved balance between exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            self.mutation_factor = 0.5 + np.random.rand() * 0.5  # Adaptive mutation factor\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            progress_ratio = (self.budget - func.evaluations) / self.budget\n            step_size = (0.01 + 0.09 * progress_ratio) * (bounds.ub - bounds.lb)  # Dynamic step size\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 11, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05601 with standard deviation 0.00244.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.057236262519946846, 0.06079411751759767, 0.05594887771160639, 0.05473477604376187, 0.0580964652450906, 0.05350601757422635, 0.05390778102028404, 0.057207284660102165, 0.05269783750075463]}}
{"id": "11b6517a-5c33-4035-b0c8-c3edc67e5a7a", "fitness": 0.05231302542173043, "name": "EnhancedHybridOptimizer", "description": "Enhanced Hybrid Optimizer with Adaptive Population Size and Momentum-Based Local Search for improved convergence and exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.momentum = 0.9  # Momentum for local search\n        self.velocity = np.zeros(self.dim)  # Initialize velocity for momentum-based local search\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        pop_size = self.adaptive_pop_size()\n        self.population = np.random.rand(pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_pop_size(self):\n        # Adaptive population size based on remaining budget\n        remaining_budget_ratio = self.budget / (self.init_pop_size * self.dim)\n        return max(5, int(self.init_pop_size * remaining_budget_ratio))\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(len(self.population)):\n            indices = np.random.choice(range(len(self.population)), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def momentum_based_local_search(self, bounds, func):\n        for i in range(len(self.population)):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            self.velocity = self.momentum * self.velocity - 0.01 * gradient\n            candidate += self.velocity\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += len(self.population)\n            if evaluations < self.budget:\n                self.momentum_based_local_search(bounds, func)\n                evaluations += len(self.population)\n        return self.best_solution", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05231 with standard deviation 0.00198.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.05558300572357555, 0.05478804836286799, 0.052055632271873264, 0.05316216409715291, 0.052408994374527285, 0.04979374958635063, 0.052360947877616026, 0.05162109244387392, 0.049043594057736284]}}
{"id": "a4de387e-82d4-41fb-b1cf-9607a08a0ef4", "fitness": 0.05736185718057791, "name": "EnhancedHybridOptimizer", "description": "A synergistic hybrid optimizer that enhances adaptive differential evolution with gradient-based local search for superior exploration and fine-tuned exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 13, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "ce240a78-70f7-4c29-af6a-6c20f48a8be3", "fitness": 0.05736185718057791, "name": "AdvancedHybridOptimizer", "description": "An advanced hybrid optimizer integrating adaptive differential evolution with a dynamic adaptive local search that adjusts step size based on function landscape for enhanced convergence.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def adaptive_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            adapt_step_size = np.clip(0.01 / (np.linalg.norm(gradient) + 1e-8), 1e-5, 0.1)\n            step_size = adapt_step_size * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.adaptive_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 14, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "1342e654-b436-47eb-8df7-f394f2ba179a", "fitness": 0.056108786644193956, "name": "AdaptiveChaosHybridOptimizer", "description": "An adaptive hybrid optimizer that dynamically adjusts mutation rate and incorporates chaos theory for diversified exploration and refined exploitation.", "code": "import numpy as np\n\nclass AdaptiveChaosHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            adaptive_mutation_factor = self.mutation_factor * (1 - (self.best_fitness / (self.best_fitness + func(x0))))\n            mutant_vector = x0 + adaptive_mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def chaos_map(self, x, a=4.0):\n        return a * x * (1 - x)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        chaos_factor = np.random.rand()\n        while evaluations < self.budget:\n            chaos_factor = self.chaos_map(chaos_factor)\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                for i in range(self.pop_size):\n                    if np.random.rand() < chaos_factor:\n                        self.population[i] = np.random.rand(self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 15, "feedback": "The algorithm AdaptiveChaosHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05611 with standard deviation 0.00345.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.05579600452628686, 0.06270451271477984, 0.05576449052863408, 0.05336670044299918, 0.05992197593699955, 0.053331142959679, 0.05256274957511575, 0.05900550949566452, 0.05252599361758681]}}
{"id": "fba6e6f6-19b5-4d85-b4e0-22b773e451d7", "fitness": 0.05528672315121202, "name": "EnhancedHybridOptimizer", "description": "Improved exploration by introducing random perturbations in the differential evolution step for enhanced diversity.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2) + np.random.uniform(-0.1, 0.1, self.dim)  # Perturbation added\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 16, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05529 with standard deviation 0.00251.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.05557608625852484, 0.06028740626709561, 0.05585023296783209, 0.053154693846358625, 0.05760980666863691, 0.053413975430757876, 0.05235331278629174, 0.05672711770479566, 0.05260787643061482]}}
{"id": "83436e33-670d-4944-b320-f2d6c05e1271", "fitness": 0.05736185718057791, "name": "AdaptiveSynergyOptimizer", "description": "Adaptive synergy optimizer with stochastic tunneling incorporates stochastic tunneling within adaptive differential evolution and gradient-based local search to escape local optima and improve convergence speed.", "code": "import numpy as np\n\nclass AdaptiveSynergyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.st_tunneling_prob = 0.1  # Probability of applying stochastic tunneling\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def stochastic_tunneling(self, candidate, func):\n        if np.random.rand() < self.st_tunneling_prob:\n            perturbation = np.random.normal(0, 0.1, self.dim) * (func.bounds.ub - func.bounds.lb)\n            candidate += perturbation\n            candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n        return candidate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n            if evaluations < self.budget:\n                for i in range(self.pop_size):\n                    self.population[i] = self.stochastic_tunneling(self.population[i], func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 17, "feedback": "The algorithm AdaptiveSynergyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "4508c77b-6df3-4268-a61c-0feeb52a63c8", "fitness": 0.055859549624455905, "name": "EnhancedHybridOptimizer", "description": "An improved hybrid optimizer that incorporates adaptive population size and dynamic mutation factor adjustments for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 3 * dim  # Adjusted population size\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.mutation_factor = 0.5 + 0.4 * (self.budget - evaluations) / self.budget  # Dynamic mutation factor\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05586 with standard deviation 0.00240.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.05648654312419921, 0.060667596689268355, 0.05633291007144603, 0.05402847660584997, 0.057976334958082965, 0.053876170839133986, 0.05321524170588399, 0.05708920118833716, 0.053063471437901466]}}
{"id": "47643c17-c7b2-4dfa-8b2d-dcd2d8ea49cc", "fitness": 0.05623479322447643, "name": "AdaptiveHybridOptimizer", "description": "Incorporating adaptive population resizing and elitist selection into a hybrid algorithm that combines differential evolution with enhanced local search, to improve convergence efficiency and exploration balance.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.elite_size = max(1, int(0.1 * self.initial_pop_size))  # 10% elites\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.initial_pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = []\n        for i in range(self.current_pop_size):\n            indices = np.random.choice(range(self.current_pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population.append((trial_vector, trial_fitness))\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                new_population.append((self.population[i], func(self.population[i])))\n        new_population.sort(key=lambda x: x[1])\n        self.population = np.array([x[0] for x in new_population[:self.current_pop_size]])\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.current_pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            self.current_pop_size = int(self.initial_pop_size * (1 - evaluations / self.budget))\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.current_pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.current_pop_size\n        return self.best_solution", "configspace": "", "generation": 19, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05623 with standard deviation 0.00213.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.0584383416718518, 0.06010788751421092, 0.056122729020930406, 0.05587031572383028, 0.05744720596284281, 0.05367257797855485, 0.05502225802656213, 0.05656978268362478, 0.0528620404378799]}}
{"id": "7a14e638-d3a0-49c9-b836-8d125e40ba21", "fitness": 0.05736185718057791, "name": "ImprovedHybridOptimizer", "description": "An improved hybrid optimizer enhancing adaptive differential evolution with momentum-based local search for more efficient convergence.", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.momentum = np.zeros((self.pop_size, self.dim))\n        self.beta = 0.9  # Momentum factor\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def momentum_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            self.momentum[i] = self.beta * self.momentum[i] + (1 - self.beta) * gradient\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * self.momentum[i]\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.momentum_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 20, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "6f80b4ae-9bfa-4c87-8975-6f90f76de121", "fitness": 0.05601437997704117, "name": "EnhancedAdaptiveHybridOptimizer", "description": "A dual-phase adaptive strategy combining an enhanced differential evolution framework with dynamic local search adjustments for improved convergence and robustness.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.local_search_probability = 0.3  # Probability of performing local search\n        self.mutation_factor_adaptive_range = (0.5, 1.0)\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            F = np.random.uniform(*self.mutation_factor_adaptive_range)\n            mutant_vector = x0 + F * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            if np.random.rand() < self.local_search_probability:\n                candidate = np.copy(self.population[i])\n                gradient = self.estimate_gradient(candidate, func)\n                step_size = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n                candidate -= step_size * gradient\n                candidate = np.clip(candidate, bounds.lb, bounds.ub)\n                candidate_fitness = func(candidate)\n                if candidate_fitness < func(self.population[i]):\n                    self.population[i] = candidate\n                    if candidate_fitness < self.best_fitness:\n                        self.best_solution = candidate\n                        self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 21, "feedback": "The algorithm EnhancedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05601 with standard deviation 0.00244.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.057236262519946846, 0.06079411751759767, 0.05594887771160639, 0.05473477604376187, 0.0580964652450906, 0.05350601757422635, 0.05390778102028404, 0.057207284660102165, 0.05269783750075463]}}
{"id": "b0f52482-78f5-433d-982a-a70c87b2c0fb", "fitness": 0.055929070138783815, "name": "RefinedHybridOptimizer", "description": "A refined hybrid optimizer integrating adaptive differential evolution with a stochastic local search and adaptive mutation scaling to enhance exploration, convergence, and adaptability.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.base_mutation_factor = 0.5\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_mutation_factor(self, func_evals):\n        # Decrease mutation factor as evaluations increase\n        return self.base_mutation_factor * (1 - func_evals / self.budget)\n\n    def differential_evolution_step(self, bounds, func, func_evals):\n        new_population = np.copy(self.population)\n        mutation_factor = self.adaptive_mutation_factor(func_evals)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def stochastic_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            perturbation = np.random.normal(0, 0.01, self.dim) * (bounds.ub - bounds.lb)\n            candidate += perturbation\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func, evaluations)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.stochastic_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 22, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05593 with standard deviation 0.00211.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.05778433744683964, 0.059946211105002756, 0.055986505421391164, 0.05525677733050782, 0.05728098117779867, 0.053545369749931626, 0.054421451266261855, 0.05640231486335057, 0.052737682887970205]}}
{"id": "0ca60c8b-0de6-4725-9f33-ca845ff9dc2a", "fitness": 0.05736185718057791, "name": "EnhancedHybridOptimizer", "description": "An improved hybrid optimizer that incorporates adaptive scaling of mutation and crossover probabilities for enhanced convergence rates.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n        # Adaptive adjustment of mutation and crossover rates\n        self.mutation_factor = 0.5 + 0.3 * np.random.rand()\n        self.cr = 0.8 + 0.2 * np.random.rand()\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 23, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "1f25d6d7-a205-4c3f-8d9f-ba7816b795aa", "fitness": 0.05736185718057791, "name": "DynamicHybridOptimizer", "description": "A dynamic hybrid optimizer that merges adaptive differential evolution with adaptive gradient descent, leveraging fitness landscape analysis for enhanced exploration-exploitation balance and precision.", "code": "import numpy as np\n\nclass DynamicHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def adaptive_gradient_descent(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            fitness_landscape = self.estimate_fitness_landscape(candidate, func)\n            step_size = 0.01 * fitness_landscape * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def estimate_fitness_landscape(self, x, func, epsilon=1e-8):\n        variance = 0\n        fx = func(x)\n        for _ in range(5):\n            perturbation = np.random.normal(0, epsilon, size=self.dim)\n            variance += (func(x + perturbation) - fx) ** 2\n        return np.sqrt(variance / 5)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.adaptive_gradient_descent(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 24, "feedback": "The algorithm DynamicHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "6734a229-2784-42a9-84a0-a1f83ee569a4", "fitness": 0.05527272922973522, "name": "DynamicAdaptiveHybridOptimizer", "description": "A dynamic adaptive hybrid optimizer that incorporates a self-tuning differential evolution strategy and adaptive gradient-based local search for enhanced exploration and exploitation.  ", "code": "import numpy as np\n\nclass DynamicAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Initial crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutation_factor_dynamic = self.mutation_factor + 0.1 * np.random.rand()\n            mutant_vector = x0 + mutation_factor_dynamic * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n        self.population = new_population\n\n    def adaptive_gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size_dynamic = 0.01 * np.random.rand() * (bounds.ub - bounds.lb)\n            candidate -= step_size_dynamic * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        while self.evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            if self.evaluations < self.budget:\n                self.adaptive_gradient_based_local_search(bounds, func)\n        return self.best_solution", "configspace": "", "generation": 25, "feedback": "The algorithm DynamicAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05527 with standard deviation 0.00241.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.05567928350487461, 0.06009789699251611, 0.05589348870247823, 0.05325476146249353, 0.05742667443258331, 0.05345536830854347, 0.0524523544707346, 0.05654606676579055, 0.05264866842760252]}}
{"id": "9891516c-569e-4958-8032-12acaef3615a", "fitness": 0.05736185718057791, "name": "DynamicHybridOptimizer", "description": "A dynamic hybrid optimizer that enhances adaptive differential evolution with gradient-based local search and a dynamic hyperparameter adjustment mechanism for better exploration-exploitation balance.", "code": "import numpy as np\n\nclass DynamicHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Initial crossover probability\n        self.mutation_factor = 0.8  # Initial mutation factor\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n        self.evaluations += self.pop_size\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.evaluations += self.pop_size\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def adjust_parameters(self):\n        progress = self.evaluations / self.budget\n        self.cr = 0.9 - progress * 0.5  # Decreasing crossover rate over time\n        self.mutation_factor = 0.8 + progress * 0.2  # Increasing mutation factor for diversity\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        while self.evaluations < self.budget:\n            self.adjust_parameters()\n            self.differential_evolution_step(bounds, func)\n            if self.evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n        return self.best_solution", "configspace": "", "generation": 26, "feedback": "The algorithm DynamicHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "33adebb5-32d5-45e6-9efa-d0be70db5286", "fitness": 0.055502400578746114, "name": "AdvancedHybridOptimizer", "description": "An innovative evolution-inspired optimizer that integrates adaptive differential evolution with stochastic gradient-free local search and dynamic population adaptation for robust global exploration and effective local exploitation.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = max(10, 5 * dim)\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def stochastic_gradient_free_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            random_step = np.random.uniform(-1, 1, self.dim) * (0.1 * (bounds.ub - bounds.lb))\n            candidate += random_step\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def adapt_population_size(self):\n        if self.best_fitness < float('inf'):\n            self.pop_size = max(10, int(self.pop_size * 0.9))\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.stochastic_gradient_free_local_search(bounds, func)\n                evaluations += self.pop_size\n                self.adapt_population_size()\n        return self.best_solution", "configspace": "", "generation": 27, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05550 with standard deviation 0.00252.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.05571633810519938, 0.06052133729904752, 0.05614443781753864, 0.05329053564253161, 0.05783442766781832, 0.05369431731347707, 0.0524877112938239, 0.056948724416682084, 0.0528837756525965]}}
{"id": "5e0fbea7-8a60-40e4-b032-0ef6d2cb01a0", "fitness": 0.05736185718057791, "name": "EnhancedHybridOptimizerV2", "description": "An enhanced hybrid optimizer that dynamically adjusts exploration-exploitation balance by incorporating adaptive mutation rates in differential evolution and gradient-based local search for improved convergence.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.initial_mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_mutation_factor(self, current_eval):\n        # Dynamic adjustment of the mutation factor based on evaluations\n        return self.initial_mutation_factor * (1 - current_eval / self.budget)\n    \n    def differential_evolution_step(self, bounds, func, current_eval):\n        new_population = np.copy(self.population)\n        mutation_factor = self.adaptive_mutation_factor(current_eval)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func, evaluations)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 28, "feedback": "The algorithm EnhancedHybridOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "7adf93d1-767d-4553-92c4-38382d7e8a9c", "fitness": 0.055148975819649365, "name": "EnhancedAdaptiveHybridOptimizer", "description": "Enhanced Adaptive Hybrid Optimizer leveraging adaptive mutation and dynamic crossover integrated with stochastic gradient local search for improved convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.base_cr = 0.9  # Base Crossover probability\n        self.base_mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_parameters(self, iteration, max_iterations):\n        # Adaptively adjust crossover and mutation parameters based on progress\n        progress = iteration / max_iterations\n        cr = self.base_cr - 0.4 * progress  # Decrease crossover probability over time\n        mutation_factor = self.base_mutation_factor + 0.2 * (1 - progress)  # Increase mutation factor over time\n        return cr, mutation_factor\n\n    def differential_evolution_step(self, bounds, func, iteration, max_iterations):\n        new_population = np.copy(self.population)\n        cr, mutation_factor = self.adaptive_parameters(iteration, max_iterations)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def stochastic_gradient_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        iterations = 0\n        max_iterations = self.budget // self.pop_size\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func, iterations, max_iterations)\n            evaluations += self.pop_size\n            iterations += 1\n            if evaluations < self.budget:\n                self.stochastic_gradient_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 29, "feedback": "The algorithm EnhancedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05515 with standard deviation 0.00250.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.05564805723326682, 0.0601325582656983, 0.05550642765305769, 0.053224547141938916, 0.05746125043694894, 0.05308335035723166, 0.05242247097458086, 0.056580596256365356, 0.05228152405775577]}}
{"id": "1d684706-31f7-48fe-bb4e-c5f096fe508d", "fitness": 0.05736185718057791, "name": "AdvancedCooperativeOptimizer", "description": "An advanced cooperative optimizer that integrates speculative search with adaptive local refinement for dynamic problem-solving and enhanced convergence.", "code": "import numpy as np\n\nclass AdvancedCooperativeOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def speculative_search_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def adaptive_local_refinement(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb) * np.random.rand() * (self.best_fitness - func(candidate))\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.speculative_search_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.adaptive_local_refinement(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 30, "feedback": "The algorithm AdvancedCooperativeOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "8aba6a1d-f362-49fb-a094-0fff183eeead", "fitness": 0.05541582393747668, "name": "EnhancedHybridOptimizer", "description": "A synergistic hybrid optimizer that enhances adaptive differential evolution with adaptive parameter control and gradient-based local search for superior exploration and fine-tuned exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)  # Change step size\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def adaptive_parameter_control(self):\n        self.mutation_factor = np.random.uniform(0.5, 1.0)  # Add adaptive control\n        self.cr = np.random.uniform(0.7, 1.0)  # Add adaptive control\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_parameter_control()  # Apply adaptive control\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 31, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05542 with standard deviation 0.00219.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.05614792004916813, 0.059839573737632934, 0.05613211839113741, 0.05370447628934072, 0.057178490320359376, 0.053677327811479936, 0.05289595181397555, 0.05630116722616341, 0.05286538979803268]}}
{"id": "df13adeb-dc96-423e-971c-53bf7a1fba9d", "fitness": 0.05633489969875064, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer that incorporates dynamic mutation and adaptive crossover strategies to improve convergence and solution quality.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            dynamic_mutation_factor = self.mutation_factor * np.random.rand()  # Changed line\n            mutant_vector = x0 + dynamic_mutation_factor * (x1 - x2)  # Modified line\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            adaptive_cr = self.cr * (1 - (func(self.population[i]) / self.best_fitness))  # Changed line\n            cross_points = np.random.rand(self.dim) < adaptive_cr  # Modified line\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05633 with standard deviation 0.00281.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.058889473312535134, 0.06110102863702482, 0.05502988450317725, 0.05630169500053839, 0.0583413097121237, 0.05263192194576605, 0.055447263744055175, 0.05743341830170834, 0.051838102131826935]}}
{"id": "54b0b403-ea76-43a3-a706-6cad21292e35", "fitness": 0.05527699085307017, "name": "EnhancedHybridOptimizer", "description": "A synergistic hybrid optimizer improves exploration-exploitation balance by adapting mutation factor dynamically.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            # Dynamic adaptation of mutation factor\n            self.mutation_factor = 0.5 + np.random.rand() * 0.3\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 33, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05528 with standard deviation 0.00264.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.05561050405855705, 0.060508623996208266, 0.05556365584276346, 0.05318813474895867, 0.05782336143316125, 0.05313829768973688, 0.052386432559846874, 0.05693817458059358, 0.052335732767805454]}}
{"id": "2fe1a8fb-c376-451c-987d-b57f8192df3c", "fitness": 0.05736185718057791, "name": "AdaptiveHybridOptimizer", "description": "An adaptive hybrid optimizer integrating differential evolution with stochastic gradient descent and diversity-preserving mutation for balanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_mutation_factor(self):\n        return self.mutation_factor * (1 - self.evaluations / self.budget)\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        mutation_factor = self.adaptive_mutation_factor()\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            self.evaluations += 1\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def stochastic_gradient_descent(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient * np.random.rand(self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            self.evaluations += 1\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n            self.evaluations += 1\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        while self.evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evaluations < self.budget:\n                self.stochastic_gradient_descent(bounds, func)\n        return self.best_solution", "configspace": "", "generation": 34, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "7a372c31-d5e0-4be8-ae1e-1c45b4bd66c6", "fitness": 0.05527699085307017, "name": "EnhancedHybridOptimizerV2", "description": "EnhancedHybridOptimizerV2 utilizes a dynamic mutation factor and adaptive crossover to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            dynamic_mutation_factor = 0.5 + np.random.rand() * 0.3  # Dynamic mutation factor\n            mutant_vector = x0 + dynamic_mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 35, "feedback": "The algorithm EnhancedHybridOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05528 with standard deviation 0.00264.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.05561050405855705, 0.060508623996208266, 0.05556365584276346, 0.05318813474895867, 0.05782336143316125, 0.05313829768973688, 0.052386432559846874, 0.05693817458059358, 0.052335732767805454]}}
{"id": "3f35f1e9-e8e8-4d51-8cbd-9e0426843754", "fitness": 0.05736185718057791, "name": "EnhancedHybridOptimizerV2", "description": "An enhanced hybrid optimizer that combines adaptive differential evolution with stochastic gradient descent for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def stochastic_gradient_descent(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = np.random.uniform(0.001, 0.01) * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.stochastic_gradient_descent(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 36, "feedback": "The algorithm EnhancedHybridOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "2f9003bd-8ade-4b5c-bda2-00fb1e3613a3", "fitness": 0.05736185718057791, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer integrating adaptive differential evolution with stochastic local search for improved convergence and exploration.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def stochastic_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate += perturbation\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.stochastic_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 37, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "5ab4c79d-d679-4a22-b4e2-ecf966166764", "fitness": 0.05708061654515119, "name": "AdvancedHybridOptimizer", "description": "An advanced hybrid optimizer that integrates adaptive differential evolution with stochastic gradient estimation and dynamic crossover for enhanced exploration and robust exploitation.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Initial crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func, evaluations):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            self.cr = 0.5 + 0.4 * np.random.rand()  # Dynamic crossover probability\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def stochastic_gradient_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * np.random.rand() * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func, evaluations)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.stochastic_gradient_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 38, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05708 with standard deviation 0.00280.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06064509489496528, 0.061070388789941976, 0.05560744711136467, 0.05795002419302453, 0.05836318143218222, 0.05317992878333655, 0.05706192334850746, 0.05747088599854333, 0.05237667435449467]}}
{"id": "ab03dcaa-0a98-4c26-ba68-c32c3f3d6ea1", "fitness": 0.05736185718057791, "name": "RefinedHybridOptimizer", "description": "A synergistic hybrid optimizer that improves adaptive differential evolution using adaptive population resizing and gradient-based local search tailored for diverse exploration and precise exploitation.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = 10 + 5 * dim\n        self.pop_size = self.initial_pop_size\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def resize_population(self):\n        self.pop_size = max(4, int(self.pop_size * 0.95))\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n            if evaluations < self.budget * 0.5:\n                self.resize_population()  # Gradually reduce population size during early convergence\n        return self.best_solution", "configspace": "", "generation": 39, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "8d83cb94-f5bd-47b0-b34f-da25cf67ee9c", "fitness": 0.05501392483547174, "name": "EnhancedHybridOptimizer", "description": "Augmented hybrid optimizer that integrates adaptive differential evolution with dynamic mutation control and gradient-based local search for enhanced global and local search balance.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            # Dynamic mutation factor based on population diversity\n            diversity = np.std(self.population, axis=0).mean()\n            adaptive_mutation_factor = self.mutation_factor + 0.1 * diversity\n            mutant_vector = x0 + adaptive_mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05501 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.055694411777724495, 0.059697120887631794, 0.0554792150184662, 0.0532693777069112, 0.0570408553061138, 0.05305699113227458, 0.05246680365266965, 0.05616510380883033, 0.052255444228623604]}}
{"id": "167cb523-9ebc-4244-8da4-31273a706177", "fitness": 0.055627754149505194, "name": "EnhancedHybridOptimizerV2", "description": "Introduces adaptive learning rates and opposition-based learning to enhance exploratory capabilities and convergence speed in solving black box optimization problems.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rates = np.full(self.pop_size, 0.01)  # Adaptive learning rates\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            \n            # Opposition-based learning\n            opposite_vector = lb + ub - trial_vector\n            opposite_vector = np.clip(opposite_vector, lb, ub)\n            \n            trial_fitness = func(trial_vector)\n            opposite_fitness = func(opposite_vector)\n            \n            # Choose the better of the trial or opposite vector\n            if trial_fitness < opposite_fitness:\n                selected_vector, selected_fitness = trial_vector, trial_fitness\n            else:\n                selected_vector, selected_fitness = opposite_vector, opposite_fitness\n\n            if selected_fitness < func(self.population[i]):\n                new_population[i] = selected_vector\n                if selected_fitness < self.best_fitness:\n                    self.best_solution = selected_vector\n                    self.best_fitness = selected_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        lb, ub = bounds.lb, bounds.ub\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = self.learning_rates[i] * (ub - lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n                self.learning_rates[i] *= 1.1  # Increase learning rate on success\n            else:\n                self.learning_rates[i] *= 0.9  # Decrease learning rate on failure\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 41, "feedback": "The algorithm EnhancedHybridOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05563 with standard deviation 0.00185.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.057307352187235816, 0.05921168219976669, 0.05625120481261148, 0.05479677290834262, 0.05659813396013891, 0.05379690877004706, 0.05396701794952219, 0.055735703296161176, 0.05298501126172084]}}
{"id": "903b77c2-a1aa-4250-8a3e-98e3cbab255c", "fitness": 0.05542946736968088, "name": "AugmentedHybridOptimizer", "description": "An augmented hybrid optimizer incorporating nonlinear inertia weight in differential evolution and adaptive gradient descent for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AugmentedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.evaluations = 0\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def nonlinear_inertia(self, progress):\n        return 0.9 - 0.5 * (progress ** 2)\n    \n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            progress = self.evaluations / self.budget\n            inertia_weight = self.nonlinear_inertia(progress)\n            mutant_vector = x0 + inertia_weight * self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n        self.evaluations += self.pop_size\n\n    def adaptive_gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb) / (np.linalg.norm(gradient) + 1e-8)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.evaluations += self.pop_size\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        while self.evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evaluations < self.budget:\n                self.adaptive_gradient_based_local_search(bounds, func)\n        return self.best_solution", "configspace": "", "generation": 42, "feedback": "The algorithm AugmentedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05543 with standard deviation 0.00222.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.05572647821351606, 0.05986371083800113, 0.05656587915692912, 0.05330030957863696, 0.05720181317206885, 0.05410046968235871, 0.0524973661278747, 0.05632422441771123, 0.053284955140031176]}}
{"id": "f6de616b-5dd9-4baf-82c5-105e38998809", "fitness": 0.05736185718057791, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer integrating adaptive differential evolution with momentum-based local search for improved convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.momentum = np.zeros(dim)  # Added for momentum-based search\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            self.momentum = 0.9 * self.momentum + gradient  # Added momentum term\n            candidate -= step_size * self.momentum  # Adjusted to use momentum\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 43, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "cf479b34-3f32-4141-a325-7748c51a508a", "fitness": 0.05581292038142554, "name": "EnhancedHybridOptimizer", "description": "Introduce adaptive mutation factor scaling in differential evolution to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            adaptive_factor = self.mutation_factor * np.exp(-0.05 * i / self.pop_size)  # Adaptive scaling\n            mutant_vector = x0 + adaptive_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05581 with standard deviation 0.00267.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.05597692987962377, 0.06109784354993497, 0.05626674191342207, 0.05353929504338906, 0.05838950969413903, 0.053813945697379184, 0.05273267323562636, 0.05749685031789642, 0.053002494101418995]}}
{"id": "06107322-3f2a-42e2-9658-fb1cbadb06e6", "fitness": 0.055929070138783815, "name": "AdvancedHybridOptimizer", "description": "An advanced adaptive hybrid optimizer that integrates dynamic parameter tuning and stochastic gradient descent into differential evolution for enhanced exploration and precise exploitation.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Initial crossover probability\n        self.mutation_factor = 0.8  # Initial mutation factor\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def adaptive_parameter_tuning(self, iteration, max_iterations):\n        # Dynamically adjust mutation factor and crossover probability\n        self.mutation_factor = 0.5 + 0.3 * np.sin(np.pi * iteration / max_iterations)\n        self.cr = 0.5 + 0.4 * np.cos(np.pi * iteration / max_iterations)\n\n    def stochastic_gradient_descent(self, bounds, func):\n        learning_rate = 0.01\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            candidate -= learning_rate * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        max_iterations = self.budget // self.pop_size\n        iteration = 0\n        \n        while evaluations < self.budget:\n            self.adaptive_parameter_tuning(iteration, max_iterations)\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.stochastic_gradient_descent(bounds, func)\n                evaluations += self.pop_size\n            iteration += 1\n        \n        return self.best_solution", "configspace": "", "generation": 45, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05593 with standard deviation 0.00211.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.05778433744683964, 0.059946211105002756, 0.055986505421391164, 0.05525677733050782, 0.05728098117779867, 0.053545369749931626, 0.054421451266261855, 0.05640231486335057, 0.052737682887970205]}}
{"id": "1bd568da-939e-495b-a4e9-6dcdebec01cf", "fitness": 0.05736185718057791, "name": "AdvancedCoEvolutionaryHybridOptimizer", "description": "Advanced Co-evolutionary Hybrid Optimizer combines adaptive differential evolution with co-evolutionary local search to enhance both exploration and exploitation.", "code": "import numpy as np\n\nclass AdvancedCoEvolutionaryHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            mutant_vector = x0 + self.mutation_factor * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def co_evolutionary_local_search(self, bounds, func):\n        # Introduce diverse local searches for individuals to co-evolve\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            # Choose random individuals to influence local search\n            collaborators = np.random.choice(np.delete(np.arange(self.pop_size), i), size=3, replace=False)\n            # Use weighted collaboration with random individuals\n            influence = np.mean(self.population[collaborators], axis=0)\n            candidate = 0.7 * candidate + 0.3 * influence\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.co_evolutionary_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 46, "feedback": "The algorithm AdvancedCoEvolutionaryHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05736 with standard deviation 0.00233.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.06036787577819158, 0.06104075156003308, 0.056773854550320135, 0.05770203568277377, 0.058334764690166185, 0.05429566208017178, 0.056822811020227126, 0.05744286346031613, 0.05347609580300139]}}
{"id": "ac8d0f56-6a9f-4981-9844-8dd5855bfb5b", "fitness": 0.055445034007666476, "name": "EnhancedHybridOptimizer", "description": "Enhanced differential evolution with adaptive mutation scaling and dynamic crossover for improved exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = 0.9  # Crossover probability\n        self.mutation_factor = 0.8\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            adaptive_mutation = np.random.rand() * (0.5 + 0.5 * (i / self.pop_size))\n            mutant_vector = x0 + adaptive_mutation * (x1 - x2)  # Adjusted line\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            dynamic_cr = 0.5 + 0.5 * (np.random.rand() ** 2)    # Adjusted line\n            cross_points = np.random.rand(self.dim) < dynamic_cr\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = 0.01 * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 47, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05545 with standard deviation 0.00246.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.05650405800288827, 0.06029287533598082, 0.05541279953455369, 0.054039677871033365, 0.057615043113476316, 0.052992483592046, 0.053224526755292656, 0.05673227830797889, 0.05219156355574828]}}
{"id": "fa4e5bc7-537a-4544-adaa-30b16512316e", "fitness": 0.057734642715580674, "name": "ImprovedDynamicHybridOptimizer", "description": "A dynamic adaptive hybrid optimizer that utilizes adaptive differential evolution with dynamic crossover and mutation strategies, combined with a stochastic gradient-driven local search for enhanced flexibility and precision in optimization.", "code": "import numpy as np\n\nclass ImprovedDynamicHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = np.linspace(0.5, 0.9, self.pop_size)  # Dynamic crossover probability\n        self.mutation_factor = np.linspace(0.5, 0.9, self.pop_size)\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            mutant_vector = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def stochastic_gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.stochastic_gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 48, "feedback": "The algorithm ImprovedDynamicHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05773 with standard deviation 0.00284.", "error": "", "parent_ids": ["06f890d3-2727-48ee-a532-4e99d1e885af"], "operator": null, "metadata": {"aucs": [0.0627303478929574, 0.06003977717074893, 0.05657430933275842, 0.059958364452230795, 0.05737063405032583, 0.054109055901293446, 0.05904500803544299, 0.05649070812376822, 0.053293579480700015]}}
{"id": "2a45a288-d183-4c74-be7b-734a69271476", "fitness": 0.05706555318236292, "name": "ImprovedDynamicHybridOptimizer", "description": "An adaptive hybrid optimizer enhancing its evolutionary process by incorporating elitism and adaptive mutation scaling based on fitness variability to improve convergence speed and solution quality.", "code": "import numpy as np\n\nclass ImprovedDynamicHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = np.linspace(0.5, 0.9, self.pop_size)  # Dynamic crossover probability\n        self.mutation_factor = np.linspace(0.5, 0.9, self.pop_size)\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n        self.elite = np.copy(self.population[0])  # Add elite population initialization\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        fitness_values = np.array([func(ind) for ind in self.population])\n        fitness_variability = np.std(fitness_values)  # Calculate fitness variability\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i] * (1 + fitness_variability)  # Adapt mutation factor\n            mutant_vector = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n        self.population[0] = self.elite  # Apply elitism by retaining best solution\n\n    def stochastic_gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.stochastic_gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 49, "feedback": "The algorithm ImprovedDynamicHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05707 with standard deviation 0.00297.", "error": "", "parent_ids": ["fa4e5bc7-537a-4544-adaa-30b16512316e"], "operator": null, "metadata": {"aucs": [0.05738335544523987, 0.06288072614455364, 0.05698389669229653, 0.05488040684814033, 0.06007949764590348, 0.05449715730805982, 0.05405284603166993, 0.0591572593768791, 0.053674833148523615]}}
{"id": "253189cd-31bc-451d-889d-8ad4f70e9996", "fitness": 0.057734642715580674, "name": "ImprovedDynamicHybridOptimizer", "description": "An enhanced hybrid optimizer that leverages adaptive differential evolution with dynamic crossover and mutation, complemented by a refined stochastic gradient-driven local search with improved step size scaling for robustness in optimization.", "code": "import numpy as np\n\nclass ImprovedDynamicHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = np.linspace(0.5, 0.9, self.pop_size)  # Dynamic crossover probability\n        self.mutation_factor = np.linspace(0.5, 0.9, self.pop_size)\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            mutant_vector = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n        self.population = new_population\n\n    def stochastic_gradient_based_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.003, 0.015) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.stochastic_gradient_based_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 50, "feedback": "The algorithm ImprovedDynamicHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05773 with standard deviation 0.00284.", "error": "", "parent_ids": ["fa4e5bc7-537a-4544-adaa-30b16512316e"], "operator": null, "metadata": {"aucs": [0.0627303478929574, 0.06003977717074893, 0.05657430933275842, 0.059958364452230795, 0.05737063405032583, 0.054109055901293446, 0.05904500803544299, 0.05649070812376822, 0.053293579480700015]}}
{"id": "aba35156-3f5f-465d-a855-f8a10e93e3f9", "fitness": 0.0587252690446254, "name": "HyperAdaptiveHybridOptimizer", "description": "A hyper-adaptive hybrid optimizer that integrates learning-based parameter tuning with a modified differential evolution and a simulated annealing-inspired local search to enhance convergence speed and solution robustness.", "code": "import numpy as np\n\nclass HyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 + 5 * dim\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)  # Dynamic crossover probability\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for temperature\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            mutant_vector = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)  # Tuning crossover rate\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.1, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)  # Tuning crossover rate\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n        return self.best_solution", "configspace": "", "generation": 51, "feedback": "The algorithm HyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05873 with standard deviation 0.00306.", "error": "", "parent_ids": ["fa4e5bc7-537a-4544-adaa-30b16512316e"], "operator": null, "metadata": {"aucs": [0.06007332749816896, 0.06450375877955794, 0.057846236223427216, 0.05743346824589779, 0.06162448267051279, 0.055320698099519894, 0.056562250954374504, 0.06067728457636368, 0.05448591435380579]}}
{"id": "edebc3d3-3a2e-4aa2-9b3d-384ff93b0854", "fitness": 0.059456538453540896, "name": "EnhancedHyperAdaptiveHybridOptimizer", "description": "Enhanced HyperAdaptiveHybridOptimizer with a dynamic population resizing and multi-mutation strategy for improved exploration-exploitation balance and convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Multi-mutation strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x1)\n            mutant_vector = mutant_vector_1 if np.random.rand() > 0.5 else mutant_vector_2\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.1, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 52, "feedback": "The algorithm EnhancedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05946 with standard deviation 0.00291.", "error": "", "parent_ids": ["aba35156-3f5f-465d-a855-f8a10e93e3f9"], "operator": null, "metadata": {"aucs": [0.06216604754068489, 0.06438585996971158, 0.058155398391527724, 0.059421692430533524, 0.06151217445443313, 0.05561208452767541, 0.05851715653768108, 0.060566766482824885, 0.05477166574679582]}}
{"id": "fd6cc755-1a09-43d6-8205-267316342b3a", "fitness": 0.059456538453540896, "name": "EnhancedHyperAdaptiveHybridOptimizer", "description": "Enhanced HyperAdaptiveHybridOptimizer with improved cooling rate in simulated annealing for finer convergence control.", "code": "import numpy as np\n\nclass EnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.995  # Adjusted cooling rate\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Multi-mutation strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x1)\n            mutant_vector = mutant_vector_1 if np.random.rand() > 0.5 else mutant_vector_2\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.1, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 53, "feedback": "The algorithm EnhancedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05946 with standard deviation 0.00291.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.06216604754068489, 0.06438585996971158, 0.058155398391527724, 0.059421692430533524, 0.06151217445443313, 0.05561208452767541, 0.05851715653768108, 0.060566766482824885, 0.05477166574679582]}}
{"id": "ab893acf-3562-4bdb-8dcd-421e94e368cd", "fitness": 0.059456538453540896, "name": "EnhancedHyperAdaptiveHybridOptimizer", "description": "Introduced a new adaptive cooling rate strategy to enhance the simulated annealing local search process by dynamically adjusting the cooling rate based on the current temperature and fitness improvements.", "code": "import numpy as np\n\nclass EnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Multi-mutation strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x1)\n            mutant_vector = mutant_vector_1 if np.random.rand() > 0.5 else mutant_vector_2\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.1, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate * (1 - self.best_fitness / func(self.best_solution)) # Changed line\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 54, "feedback": "The algorithm EnhancedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05946 with standard deviation 0.00291.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.06216604754068489, 0.06438585996971158, 0.058155398391527724, 0.059421692430533524, 0.06151217445443313, 0.05561208452767541, 0.05851715653768108, 0.060566766482824885, 0.05477166574679582]}}
{"id": "b306535a-04b3-4642-8f0e-db69682ec32d", "fitness": 0.059456538453540896, "name": "EnhancedEntropyAdaptiveHybridOptimizer", "description": "Enhanced HyperAdaptiveHybridOptimizer with a novel entropy-based dynamic strategy, leveraging information gain for superior exploration-exploitation balance and convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedEntropyAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x1)\n            mutant_vector = mutant_vector_1 if np.random.rand() > 0.5 else mutant_vector_2\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.1, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n        self.population = new_population\n\n    def entropy_based_simulated_annealing(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            fitness_diff = candidate_fitness - func(self.population[i])\n            entropy = np.abs(fitness_diff) / (np.abs(func(self.population[i])) + 1e-8)\n            acceptance_prob = np.exp(-entropy / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.entropy_based_simulated_annealing(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 55, "feedback": "The algorithm EnhancedEntropyAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05946 with standard deviation 0.00291.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.06216604754068489, 0.06438585996971158, 0.058155398391527724, 0.059421692430533524, 0.06151217445443313, 0.05561208452767541, 0.05851715653768108, 0.060566766482824885, 0.05477166574679582]}}
{"id": "eea42bcf-4dae-4500-9930-4a29c986b954", "fitness": 0.059209698686899216, "name": "EnhancedHyperAdaptiveHybridOptimizer", "description": "Introduced a stochastic element to mutation factor update for enhanced exploration potential.", "code": "import numpy as np\n\nclass EnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Multi-mutation strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x1)\n            mutant_vector = mutant_vector_1 if np.random.rand() > 0.5 else mutant_vector_2\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + np.random.uniform(0.05, 0.15), 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 56, "feedback": "The algorithm EnhancedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05921 with standard deviation 0.00288.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.0613938196156576, 0.06438585996971158, 0.058155398391527724, 0.05869050802395759, 0.06151217445443313, 0.05561208452767541, 0.057799010969509235, 0.060566766482824885, 0.05477166574679582]}}
{"id": "a1f0f024-387b-48ef-8f85-0b88fd239361", "fitness": 0.05864578923336783, "name": "EnhancedHyperAdaptiveHybridOptimizer", "description": "Further enhancement of Enhanced HyperAdaptiveHybridOptimizer with synergy-based crossover and mutation control for improved convergence and adaptability.", "code": "import numpy as np\n\nclass EnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.98  # Adjusted cooling rate\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        synergy_factor = 0.7  # Introduce synergy factor\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i] * (1 + synergy_factor * np.random.rand()) # Synergy in crossover\n            mut_factor_i = self.mutation_factor[i]\n            mutant_vector = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.1, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 57, "feedback": "The algorithm EnhancedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05865 with standard deviation 0.00307.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.06044671239806465, 0.06426361486950394, 0.05747380815888903, 0.0577812874739404, 0.06139445727248738, 0.054964575777667446, 0.05690209281951619, 0.06045051958258052, 0.054135034747660904]}}
{"id": "2284ced5-521a-48a5-888f-1856477a3d52", "fitness": 0.0587252690446254, "name": "OptimizedAdaptiveHybridEvolutionaryAlgorithm", "description": "Optimized Adaptive Hybrid Evolutionary Algorithm integrating enhanced learning strategies, dynamic population control, and an adaptive stochastic gradient descent to improve convergence speed and robustness.", "code": "import numpy as np\n\nclass OptimizedAdaptiveHybridEvolutionaryAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def enhanced_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            mutant_vector = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.05, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.05, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.05, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.05, 0.5)\n        self.population = new_population\n\n    def adaptive_stochastic_gradient_descent(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            step_size = np.random.uniform(0.001, 0.01) * (bounds.ub - bounds.lb)\n            candidate -= step_size * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_control(self, evaluations):\n        if evaluations > self.budget * 0.6:\n            self.pop_size = max(int(self.init_pop_size * 0.3), 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.enhanced_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.adaptive_stochastic_gradient_descent(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_control(evaluations)\n        return self.best_solution", "configspace": "", "generation": 58, "feedback": "The algorithm OptimizedAdaptiveHybridEvolutionaryAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05873 with standard deviation 0.00306.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.06007332749816896, 0.06450375877955794, 0.057846236223427216, 0.05743346824589779, 0.06162448267051279, 0.055320698099519894, 0.056562250954374504, 0.06067728457636368, 0.05448591435380579]}}
{"id": "a44087cb-9184-4dc6-b0f1-9258ca64cd79", "fitness": 0.059456538453540896, "name": "EnhancedHyperAdaptiveHybridOptimizer", "description": "Enhanced HyperAdaptiveHybridOptimizer with dynamic adaptive parameter tuning and improved local search step for better convergence.", "code": "import numpy as np\n\nclass EnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.97  # Changed from 0.99 to 0.97\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x1)\n            mutant_vector = mutant_vector_1 if np.random.rand() > 0.5 else mutant_vector_2\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.1, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.03) * (bounds.ub - bounds.lb)  # Changed upper limit from 0.02 to 0.03\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 59, "feedback": "The algorithm EnhancedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05946 with standard deviation 0.00291.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.06216604754068489, 0.06438585996971158, 0.058155398391527724, 0.059421692430533524, 0.06151217445443313, 0.05561208452767541, 0.05851715653768108, 0.060566766482824885, 0.05477166574679582]}}
{"id": "af6f01e8-5618-4be8-a65d-5fd6543c6180", "fitness": 0.05674333156873807, "name": "EnhancedHyperAdaptiveHybridOptimizer", "description": "Improved convergence efficiency by enhancing the population initialization with Sobol sequence for better diversity and coverage.", "code": "import numpy as np\nfrom scipy.stats import qmc\n\nclass EnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        samples = sampler.random(n=self.pop_size)\n        self.population = qmc.scale(samples, lb, ub)\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x1)\n            mutant_vector = mutant_vector_1 if np.random.rand() > 0.5 else mutant_vector_2\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.1, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 60, "feedback": "The algorithm EnhancedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05674 with standard deviation 0.00286.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.059528377056596415, 0.0578471237661522, 0.05929868368123692, 0.060385648504160505, 0.05131092348072985, 0.055198595754960444, 0.05601872937211394, 0.05336861332107934, 0.05773328918161302]}}
{"id": "8af00a67-2b56-4406-8d09-21024ed03ca4", "fitness": 0.05751935406894742, "name": "EnhancedHyperAdaptiveHybridOptimizer", "description": "Introduced a small random perturbation to the trial vector for further exploration and diversity enhancement in solutions.", "code": "import numpy as np\n\nclass EnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Multi-mutation strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x1)\n            mutant_vector = mutant_vector_1 if np.random.rand() > 0.5 else mutant_vector_2\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            # Adding random perturbation\n            perturbation = np.random.uniform(-0.001, 0.001, trial_vector.shape)\n            trial_vector = np.clip(trial_vector + perturbation, bounds.lb, bounds.ub)\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.1, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05752 with standard deviation 0.00371.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.056539427835045064, 0.06452945674563326, 0.057594399891145565, 0.054073385273420205, 0.06164992110140877, 0.055079341627320755, 0.053257692659969824, 0.060702624631892355, 0.05424793685469098]}}
{"id": "336b650e-ce29-4a17-94b9-e6821589aba9", "fitness": 0.05619050904168632, "name": "EnhancedHyperAdaptiveHybridOptimizer", "description": "Refined Enhanced HyperAdaptiveHybridOptimizer with adaptive crossover and mutation, improved local search, and enhanced population resizing for better diversity and convergence.", "code": "import numpy as np\n\nclass EnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.3, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.3, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Multi-mutation strategy\n            mutant_vector = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            self.cr[i] = np.clip(self.cr[i] + 0.1 * (trial_fitness < func(self.population[i])), 0.3, 0.9)\n            self.mutation_factor[i] = np.clip(mut_factor_i + 0.1 * (trial_fitness < func(self.population[i])), 0.3, 0.9)\n        self.population = new_population\n\n    def improved_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            stochastic_step = np.random.uniform(-0.02, 0.02, self.dim) * (bounds.ub - bounds.lb)\n            candidate += stochastic_step\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(self.population[i]):\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.improved_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 62, "feedback": "The algorithm EnhancedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05619 with standard deviation 0.00409.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.05382784346294911, 0.06344776115070838, 0.05725439172687796, 0.05148600545785542, 0.06061722910207079, 0.05475582636848875, 0.05071016474958712, 0.0596856590438406, 0.053929700312798756]}}
{"id": "2b70a430-720b-4254-8ac5-2535108fc121", "fitness": 0.05780203495156392, "name": "RefinedHyperAdaptiveHybridOptimizer", "description": "Introducing a novel adaptive learning rate mechanism with chaos-based mutation strategies for refined exploration and exploitation balance in black box optimization.", "code": "import numpy as np\n\nclass RefinedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.learning_rate = np.random.uniform(0.01, 0.05, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.995\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Chaos-based mutation strategy\n            chaotic_factor = np.sin(self.learning_rate[i] * np.pi * np.random.rand())\n            mutant_vector = x0 + chaotic_factor * mut_factor_i * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.1, 0.9)\n                self.learning_rate[i] = min(self.learning_rate[i] * 1.1, 0.1)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n                self.learning_rate[i] = max(self.learning_rate[i] * 0.9, 0.01)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = self.learning_rate[i] * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.learning_rate = np.resize(self.learning_rate, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 63, "feedback": "The algorithm RefinedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05780 with standard deviation 0.00225.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.06188179036824948, 0.05758956935033921, 0.06006880951431359, 0.05915073843129259, 0.055075911318312576, 0.05741561524485572, 0.05825047985129206, 0.05424493985638401, 0.05654046062903606]}}
{"id": "62a1250f-cabc-45c6-b1f9-0d1027dd6442", "fitness": 0.05708961525576542, "name": "EnhancedHyperAdaptiveHybridOptimizer", "description": "Enhanced HyperAdaptiveHybridOptimizer with refined mutation factor scaling for fine-tuned exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i] * 0.95  # Change: Refined mutation factor scaling\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x1)\n            mutant_vector = mutant_vector_1 if np.random.rand() > 0.5 else mutant_vector_2\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.1, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 64, "feedback": "The algorithm EnhancedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05709 with standard deviation 0.00405.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.05535292780507217, 0.06452409802039094, 0.05744825383280516, 0.05294172712048539, 0.06164614274711189, 0.05493985237623411, 0.052143617823788846, 0.060699334625517465, 0.054110582950482766]}}
{"id": "2593213d-f400-44e3-a913-314dc9c09d03", "fitness": -Infinity, "name": "EnhancedHyperAdaptiveHybridOptimizer", "description": "Enhanced HyperAdaptiveHybridOptimizer with an intelligent multi-objective adaptation mechanism to refine exploration-exploitation balance and dynamic parameter control for convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.objectives = np.full(self.pop_size, np.inf)\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x1)\n            mutant_vector = mutant_vector_1 if np.random.rand() > 0.5 else mutant_vector_2\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.1, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def update_objectives(self, func):\n        for i in range(self.pop_size):\n            self.objectives[i] = func(self.population[i])\n\n    def intelligent_adaptation(self, evaluations):\n        # Adopt a strategy based on fitness diversity\n        fitness_diff = np.std([func(self.population[i]) for i in range(self.pop_size)])\n        adaptation_rate = max(0.1, min(1.0, 1.0 - (fitness_diff / (np.max(self.objectives) - np.min(self.objectives) + 1e-8))))\n        self.cr += adaptation_rate * (np.random.rand(self.pop_size) - 0.5)\n        self.mutation_factor += adaptation_rate * (np.random.rand(self.pop_size) - 0.5)\n        self.cr = np.clip(self.cr, 0.5, 0.9)\n        self.mutation_factor = np.clip(self.mutation_factor, 0.5, 0.9)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.update_objectives(func)\n            self.intelligent_adaptation(evaluations)\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 65, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {}}
{"id": "ba63cdbb-5632-41d2-b52b-11cb0bcf4458", "fitness": 0.059456538453540896, "name": "EnhancedHyperAdaptiveHybridOptimizer", "description": "Introduce an elite preservation and adaptive cooling strategy to enhance convergence speed and solution robustness in the Enhanced HyperAdaptiveHybridOptimizer.", "code": "import numpy as np\n\nclass EnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.elite = None\n        self.elite_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x1)\n            mutant_vector = mutant_vector_1 if np.random.rand() > 0.5 else mutant_vector_2\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.1, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n            if trial_fitness < self.elite_fitness:\n                self.elite = trial_vector\n                self.elite_fitness = trial_fitness\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            # Adaptive cooling strategy\n            self.temperature = max(0.1, self.temperature * self.cooling_rate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n            # Elite preservation\n            if self.elite is not None and self.elite_fitness < self.best_fitness:\n                self.best_solution = self.elite\n                self.best_fitness = self.elite_fitness\n        return self.best_solution", "configspace": "", "generation": 66, "feedback": "The algorithm EnhancedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05946 with standard deviation 0.00291.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.06216604754068489, 0.06438585996971158, 0.058155398391527724, 0.059421692430533524, 0.06151217445443313, 0.05561208452767541, 0.05851715653768108, 0.060566766482824885, 0.05477166574679582]}}
{"id": "a9a03d90-758a-4f1b-a75f-2c25e5f56956", "fitness": 0.059456538453540896, "name": "EnhancedHyperAdaptiveHybridOptimizer", "description": "EnhancedHyperAdaptiveHybridOptimizer with improved simulated annealing step to increase exploitation efficiency.", "code": "import numpy as np\n\nclass EnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Multi-mutation strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x1)\n            mutant_vector = mutant_vector_1 if np.random.rand() > 0.5 else mutant_vector_2\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.1, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.1) * (bounds.ub - bounds.lb)  # Increase step size\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05946 with standard deviation 0.00291.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.06216604754068489, 0.06438585996971158, 0.058155398391527724, 0.059421692430533524, 0.06151217445443313, 0.05561208452767541, 0.05851715653768108, 0.060566766482824885, 0.05477166574679582]}}
{"id": "15fe66d7-a010-4eff-8279-65b3ae1e883f", "fitness": 0.05303339228004264, "name": "EnhancedHyperAdaptiveHybridOptimizer", "description": "Enhanced population diversity by introducing Gaussian mutation for broader exploration in early iterations.", "code": "import numpy as np\n\nclass EnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Multi-mutation strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x1)\n            mutant_vector = mutant_vector_1 if np.random.rand() > 0.5 else mutant_vector_2\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.1, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            if evaluations < self.budget * 0.1:  # Introduce Gaussian mutation in early iterations\n                self.population += np.random.normal(0, 0.1, self.population.shape) * (bounds.ub - bounds.lb)\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 68, "feedback": "The algorithm EnhancedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05303 with standard deviation 0.00314.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.0546813697149332, 0.05143273805456561, 0.05860895201261085, 0.0522793125446086, 0.04919326000944746, 0.056013240343542936, 0.05148456831846615, 0.04845039960129294, 0.05515668992091605]}}
{"id": "4d8a2790-bb7d-40ab-b63d-9a7a2010aecf", "fitness": 0.059456538453540896, "name": "EnhancedHyperAdaptiveHybridOptimizer", "description": "Enhanced HyperAdaptiveHybridOptimizer with refined convergence rate by adjusting mutation factor and crossover rate adaptively based on current iteration.", "code": "import numpy as np\n\nclass EnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Multi-mutation strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x1)\n            mutant_vector = mutant_vector_1 if np.random.rand() > 0.5 else mutant_vector_2\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.05, 0.9)  # Adjusted increment to 0.05\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.05, 0.9)  # Adjusted increment to 0.05\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.05, 0.5)  # Adjusted decrement to 0.05\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.05, 0.5)  # Adjusted decrement to 0.05\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 69, "feedback": "The algorithm EnhancedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05946 with standard deviation 0.00291.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.06216604754068489, 0.06438585996971158, 0.058155398391527724, 0.059421692430533524, 0.06151217445443313, 0.05561208452767541, 0.05851715653768108, 0.060566766482824885, 0.05477166574679582]}}
{"id": "8be8766d-199b-4e6f-a32e-6768929e3602", "fitness": 0.059456538453540896, "name": "EnhancedHyperAdaptiveHybridOptimizer", "description": "Improved reinitialization strategy to enhance exploration in the later stages of optimization.", "code": "import numpy as np\n\nclass EnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Multi-mutation strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x1)\n            mutant_vector = mutant_vector_1 if np.random.rand() > 0.5 else mutant_vector_2\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.1, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            if evaluations % (self.budget // 10) == 0:  # Reinitialize part of the population\n                lb, ub = self.population[0].bounds.lb, self.population[0].bounds.ub\n                self.population[:self.pop_size // 4] = np.random.rand(self.pop_size // 4, self.dim) * (ub - lb) + lb\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 70, "feedback": "The algorithm EnhancedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05946 with standard deviation 0.00291.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.06216604754068489, 0.06438585996971158, 0.058155398391527724, 0.059421692430533524, 0.06151217445443313, 0.05561208452767541, 0.05851715653768108, 0.060566766482824885, 0.05477166574679582]}}
{"id": "6fc9cc9e-cb08-4560-8a77-12f027270e36", "fitness": 0.059456538453540896, "name": "OptimizedEnhancedHyperAdaptiveHybridOptimizer", "description": "Optimized Enhanced HyperAdaptiveHybridOptimizer introducing adaptive learning rates for parameter tuning and strategic diversity preservation to enhance solution accuracy and convergence efficiency.", "code": "import numpy as np\n\nclass OptimizedEnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.adaptive_learning_rate = 0.1\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Multi-mutation strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x1)\n            mutant_vector = mutant_vector_1 if np.random.rand() > 0.5 else mutant_vector_2\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + self.adaptive_learning_rate, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + self.adaptive_learning_rate, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - self.adaptive_learning_rate, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - self.adaptive_learning_rate, 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def maintain_population_diversity(self):\n        # Introduce random solutions if diversity is too low\n        diversity_threshold = 0.05 * (func.bounds.ub - func.bounds.lb)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                if np.std(self.population[:, j]) < diversity_threshold[j]:\n                    self.population[i, j] = np.random.uniform(func.bounds.lb[j], func.bounds.ub[j])\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n            self.maintain_population_diversity()\n        return self.best_solution", "configspace": "", "generation": 71, "feedback": "The algorithm OptimizedEnhancedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05946 with standard deviation 0.00291.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.06216604754068489, 0.06438585996971158, 0.058155398391527724, 0.059421692430533524, 0.06151217445443313, 0.05561208452767541, 0.05851715653768108, 0.060566766482824885, 0.05477166574679582]}}
{"id": "4ab6dcf8-546e-4849-9fed-f96a630e458c", "fitness": 0.059456538453540896, "name": "EnhancedHyperAdaptiveHybridOptimizer", "description": "Introduce a slight cooling rate adjustment in the simulated annealing process to enhance convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.985  # Changed from 0.99 for improved cooling strategy\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Multi-mutation strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x1)\n            mutant_vector = mutant_vector_1 if np.random.rand() > 0.5 else mutant_vector_2\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.1, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 72, "feedback": "The algorithm EnhancedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05946 with standard deviation 0.00291.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.06216604754068489, 0.06438585996971158, 0.058155398391527724, 0.059421692430533524, 0.06151217445443313, 0.05561208452767541, 0.05851715653768108, 0.060566766482824885, 0.05477166574679582]}}
{"id": "8f9d6d58-d0bf-4550-897c-b07089f1910c", "fitness": -Infinity, "name": "ImprovedEnhancedHyperAdaptiveHybridOptimizer", "description": "Improved Enhanced HyperAdaptiveHybridOptimizer using a hybrid strategy combining adaptive differential evolution, simulated annealing, and self-adaptive parameter tuning for robust exploration and rapid convergence.", "code": "import numpy as np\n\nclass ImprovedEnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.98  # Slightly faster cooling\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 4, replace=False)\n            x0, x1, x2, x3 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Improved Multi-mutation strategy with an additional choice\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x3)\n            mutant_vector = np.random.choice([mutant_vector_1, mutant_vector_2])\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.05, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.05, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.05, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.05, 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.015) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget * 0.75:  # Resize later in the process\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 73, "feedback": "An exception occurred: ValueError('a must be 1-dimensional').", "error": "ValueError('a must be 1-dimensional')", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {}}
{"id": "438d8364-381b-43a9-902c-62f6a4ba905c", "fitness": 0.05765717153895627, "name": "EnhancedHyperAdaptiveHybridOptimizer", "description": "Improved population initialization for better diversity by adjusting population size based on dimensionality.", "code": "import numpy as np\n\nclass EnhancedHyperAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * int(np.sqrt(dim))  # Adjust population size for better diversity.\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x1)\n            mutant_vector = mutant_vector_1 if np.random.rand() > 0.5 else mutant_vector_2\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] = min(self.cr[i] + 0.1, 0.9)\n                self.mutation_factor[i] = min(self.mutation_factor[i] + 0.1, 0.9)\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] = max(self.cr[i] - 0.1, 0.5)\n                self.mutation_factor[i] = max(self.mutation_factor[i] - 0.1, 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedHyperAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05766 with standard deviation 0.00294.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.058613944230426096, 0.06332778940387751, 0.0571459047141164, 0.05604465569143169, 0.060514574887122974, 0.054653730550572543, 0.05519598968709394, 0.059588329556817254, 0.05382962512914802]}}
{"id": "0a7cc509-13c2-4fd5-987e-842502ce74c4", "fitness": 0.05964441008728795, "name": "EnhancedDynamicLearningHybridOptimizer", "description": "An enhanced optimizer incorporating a dynamic learning strategy with adaptive parameter tuning and hybrid search methods to improve convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.1\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Dynamic learning strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x0)\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_1, mutant_vector_2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 75, "feedback": "The algorithm EnhancedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05964 with standard deviation 0.00297.", "error": "", "parent_ids": ["edebc3d3-3a2e-4aa2-9b3d-384ff93b0854"], "operator": null, "metadata": {"aucs": [0.0627665704326339, 0.06438585996971158, 0.058155398391527724, 0.05997428365400925, 0.06151217445443313, 0.05561208452767541, 0.05905488712597984, 0.060566766482824885, 0.05477166574679582]}}
{"id": "7c4bdf19-e88e-48e6-9150-70ab8b6865af", "fitness": 0.05964441008728795, "name": "EnhancedDynamicLearningHybridOptimizer", "description": "A novel optimizer integrating adaptive differential evolution, hybrid local search, and dynamic population resizing with a temperature-based learning rate to enhance exploration-exploitation balance and convergence speed.", "code": "import numpy as np\n\nclass EnhancedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.97\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.1\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        temperature_based_lr = self.learning_rate * self.temperature\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x0)\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_1, mutant_vector_2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += temperature_based_lr * (0.9 - self.cr[i])\n                self.mutation_factor[i] += temperature_based_lr * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= temperature_based_lr * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= temperature_based_lr * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def hybrid_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate += np.random.normal(scale=stochastic_step, size=self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.hybrid_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 76, "feedback": "The algorithm EnhancedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05964 with standard deviation 0.00297.", "error": "", "parent_ids": ["0a7cc509-13c2-4fd5-987e-842502ce74c4"], "operator": null, "metadata": {"aucs": [0.0627665704326339, 0.06438585996971158, 0.058155398391527724, 0.05997428365400925, 0.06151217445443313, 0.05561208452767541, 0.05905488712597984, 0.060566766482824885, 0.05477166574679582]}}
{"id": "784e935e-0734-4fae-b379-d5fa11a10245", "fitness": 0.05809867890125039, "name": "EnhancedDynamicLearningHybridOptimizer", "description": "A refined optimizer that enhances adaptive learning and hybrid search by introducing elite selection and dynamic feedback for mutation and crossover rates to improve convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.1\n        self.elite_fraction = 0.2\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        elite_count = max(1, int(self.pop_size * self.elite_fraction))\n        new_population = np.copy(self.population)\n        fitness = np.apply_along_axis(func, 1, self.population)\n        elite_indices = np.argsort(fitness)[:elite_count]\n        elite_solutions = self.population[elite_indices]\n\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n\n            # Use elite solution for differential evolution\n            elite_choice = elite_solutions[np.random.choice(elite_count)]\n            mutant_vector = x0 + mut_factor_i * (elite_choice - x2)\n\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n\n            if trial_fitness < fitness[i]:\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 77, "feedback": "The algorithm EnhancedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05810 with standard deviation 0.00372.", "error": "", "parent_ids": ["0a7cc509-13c2-4fd5-987e-842502ce74c4"], "operator": null, "metadata": {"aucs": [0.057344122938468356, 0.06516007662683487, 0.05795838296655231, 0.05484131156886374, 0.06225251617325045, 0.05542886731970398, 0.054013826017526134, 0.061296222254757726, 0.0545927842452959]}}
{"id": "f48cb6b0-0afc-45e5-aed3-94a78f1d38b6", "fitness": 0.05964441008728795, "name": "EnhancedDynamicLearningHybridOptimizer", "description": "Introduce a self-adaptive learning mechanism with an exploration-exploitation balance and dynamic population strategy to enhance convergence and performance.", "code": "import numpy as np\n\nclass EnhancedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.95  # Enhanced cooling rate for faster convergence\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.1\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Dynamic learning strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x0)\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_1, mutant_vector_2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.002, 0.015) * (bounds.ub - bounds.lb)  # Refined step size\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        # Improved population resizing strategy\n        if evaluations > self.budget / 2:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05964 with standard deviation 0.00297.", "error": "", "parent_ids": ["0a7cc509-13c2-4fd5-987e-842502ce74c4"], "operator": null, "metadata": {"aucs": [0.0627665704326339, 0.06438585996971158, 0.058155398391527724, 0.05997428365400925, 0.06151217445443313, 0.05561208452767541, 0.05905488712597984, 0.060566766482824885, 0.05477166574679582]}}
{"id": "70ccf431-e085-4801-bc74-32d1b40b6760", "fitness": 0.05964441008728795, "name": "EnhancedDynamicLearningHybridOptimizer", "description": "A slight adjustment to the learning rate aims to enhance the algorithm's adaptability and convergence performance.", "code": "import numpy as np\n\nclass EnhancedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.15  # Adjusted learning rate from 0.1 to 0.15\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Dynamic learning strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x0)\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_1, mutant_vector_2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 79, "feedback": "The algorithm EnhancedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05964 with standard deviation 0.00297.", "error": "", "parent_ids": ["0a7cc509-13c2-4fd5-987e-842502ce74c4"], "operator": null, "metadata": {"aucs": [0.0627665704326339, 0.06438585996971158, 0.058155398391527724, 0.05997428365400925, 0.06151217445443313, 0.05561208452767541, 0.05905488712597984, 0.060566766482824885, 0.05477166574679582]}}
{"id": "834dd831-e1f2-4fa3-87e5-674df71bebe5", "fitness": 0.05964441008728795, "name": "EnhancedDynamicLearningHybridOptimizer", "description": "Modify the dynamic learning strategy by adjusting the learning rate based on the best fitness improvement, aiming to enhance the adaptation of control parameters.", "code": "import numpy as np\n\nclass EnhancedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.1\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Dynamic learning strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x0)\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_1, mutant_vector_2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                fitness_improvement = self.best_fitness - trial_fitness\n                adjusted_learning_rate = self.learning_rate * (1 + fitness_improvement if fitness_improvement > 0 else 0)  # Adjust learning rate\n                self.cr[i] += adjusted_learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += adjusted_learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 80, "feedback": "The algorithm EnhancedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05964 with standard deviation 0.00297.", "error": "", "parent_ids": ["0a7cc509-13c2-4fd5-987e-842502ce74c4"], "operator": null, "metadata": {"aucs": [0.0627665704326339, 0.06438585996971158, 0.058155398391527724, 0.05997428365400925, 0.06151217445443313, 0.05561208452767541, 0.05905488712597984, 0.060566766482824885, 0.05477166574679582]}}
{"id": "b2b53362-a253-4835-bd9f-4158cd365124", "fitness": 0.0587252690446254, "name": "RefinedDynamicLearningHybridOptimizer", "description": "A refined optimizer with enhanced diversity control and learning rate adaptation, leveraging historical information to improve convergence and solution accuracy.", "code": "import numpy as np\n\nclass RefinedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.98\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.1\n        self.history = []\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        diversity_threshold = 1e-5\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Enhanced diversity strategy\n            mutant_vector = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n            if np.linalg.norm(mutant_vector - self.population[i]) < diversity_threshold:\n                self.cr[i], self.mutation_factor[i] = np.random.uniform(0.5, 0.9, 2)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 81, "feedback": "The algorithm RefinedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05873 with standard deviation 0.00306.", "error": "", "parent_ids": ["0a7cc509-13c2-4fd5-987e-842502ce74c4"], "operator": null, "metadata": {"aucs": [0.06007332749816896, 0.06450375877955794, 0.057846236223427216, 0.05743346824589779, 0.06162448267051279, 0.055320698099519894, 0.056562250954374504, 0.06067728457636368, 0.05448591435380579]}}
{"id": "848bd0ca-bfc6-49cb-a2d5-c16908ba2bee", "fitness": 0.0587252690446254, "name": "RefinedDynamicLearningHybridOptimizer", "description": "A refined hybrid optimizer with an advanced dynamic learning strategy, enhanced adaptive parameters, and a novel local search mechanism to boost convergence speed and solution quality.", "code": "import numpy as np\n\nclass RefinedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.995\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.05\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Advanced dynamic learning strategy\n            mutant_vector = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def novel_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim) * (bounds.ub - bounds.lb)\n            candidate += perturbation\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.75:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.novel_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 82, "feedback": "The algorithm RefinedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05873 with standard deviation 0.00306.", "error": "", "parent_ids": ["0a7cc509-13c2-4fd5-987e-842502ce74c4"], "operator": null, "metadata": {"aucs": [0.06007332749816896, 0.06450375877955794, 0.057846236223427216, 0.05743346824589779, 0.06162448267051279, 0.055320698099519894, 0.056562250954374504, 0.06067728457636368, 0.05448591435380579]}}
{"id": "1d534010-0822-4a38-8fed-c6105c0b7a5c", "fitness": 0.05964441008728795, "name": "EnhancedDynamicLearningHybridOptimizer", "description": "Improved adaptive strategy with early stopping to enhance convergence speed and optimization performance.", "code": "import numpy as np\n\nclass EnhancedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.1\n        self.early_stopping_threshold = 1e-5  # New line for early stopping\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x0)\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_1, mutant_vector_2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n            if self.best_fitness < self.early_stopping_threshold:  # New condition for early stopping\n                break  # New line for breaking the loop\n        return self.best_solution", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05964 with standard deviation 0.00297.", "error": "", "parent_ids": ["0a7cc509-13c2-4fd5-987e-842502ce74c4"], "operator": null, "metadata": {"aucs": [0.0627665704326339, 0.06438585996971158, 0.058155398391527724, 0.05997428365400925, 0.06151217445443313, 0.05561208452767541, 0.05905488712597984, 0.060566766482824885, 0.05477166574679582]}}
{"id": "41e6247c-7636-4e5d-a645-34950cab5769", "fitness": 0.05964441008728795, "name": "EnhancedDynamicLearningHybridOptimizer", "description": "Introduce adaptive mutation factor adjustment based on fitness improvement to enhance convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.1\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Dynamic learning strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x0)\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_1, mutant_vector_2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                # Introduce adaptive mutation factor adjustment\n                self.mutation_factor[i] = max(0.5, min(0.9, mut_factor_i * (1 + (self.best_fitness - trial_fitness) / abs(self.best_fitness + 1e-9))))\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 84, "feedback": "The algorithm EnhancedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05964 with standard deviation 0.00297.", "error": "", "parent_ids": ["0a7cc509-13c2-4fd5-987e-842502ce74c4"], "operator": null, "metadata": {"aucs": [0.0627665704326339, 0.06438585996971158, 0.058155398391527724, 0.05997428365400925, 0.06151217445443313, 0.05561208452767541, 0.05905488712597984, 0.060566766482824885, 0.05477166574679582]}}
{"id": "1ff77705-b66b-4722-82f7-b89a337e8793", "fitness": 0.0587252690446254, "name": "RefinedHybridOptimizer", "description": "A hybrid optimizer that integrates adaptive differential evolution with a variable learning rate and a multi-scale simulated annealing strategy to enhance convergence speed and solution quality.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.98\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = np.linspace(0.05, 0.2, self.pop_size)\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Dynamic learning strategy\n            mutant_vector = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate[i] * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate[i] * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate[i] * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate[i] * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def multi_scale_simulated_annealing(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.multi_scale_simulated_annealing(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 85, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05873 with standard deviation 0.00306.", "error": "", "parent_ids": ["0a7cc509-13c2-4fd5-987e-842502ce74c4"], "operator": null, "metadata": {"aucs": [0.06007332749816896, 0.06450375877955794, 0.057846236223427216, 0.05743346824589779, 0.06162448267051279, 0.055320698099519894, 0.056562250954374504, 0.06067728457636368, 0.05448591435380579]}}
{"id": "553f3a6c-8e03-4d10-8ae7-099ff8aded2e", "fitness": 0.05964441008728795, "name": "EnhancedDynamicLearningHybridOptimizer", "description": "Enhanced optimizer with refined adaptive parameters and perturbation strategy for improved solution exploration.", "code": "import numpy as np\n\nclass EnhancedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.98  # Adjusted cooling rate\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.15  # Adjusted learning rate\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x0)\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_1, mutant_vector_2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.95 - self.cr[i])  # Adjusted target cr\n                self.mutation_factor[i] += self.learning_rate * (0.95 - self.mutation_factor[i])  # Adjusted target mutation factor\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 86, "feedback": "The algorithm EnhancedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05964 with standard deviation 0.00297.", "error": "", "parent_ids": ["0a7cc509-13c2-4fd5-987e-842502ce74c4"], "operator": null, "metadata": {"aucs": [0.0627665704326339, 0.06438585996971158, 0.058155398391527724, 0.05997428365400925, 0.06151217445443313, 0.05561208452767541, 0.05905488712597984, 0.060566766482824885, 0.05477166574679582]}}
{"id": "c5dd8859-cf80-4adc-a113-da50e1b9257d", "fitness": 0.05964441008728795, "name": "RefinedDynamicLearningHybridOptimizer", "description": "A refined optimizer using an adaptive hyperparameter tuning mechanism and probabilistic clustering for enhanced exploration and exploitation balance.", "code": "import numpy as np\n\nclass RefinedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.1\n        self.clustering_threshold = 0.01\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Dynamic learning strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x0)\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_1, mutant_vector_2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def probabilistic_clustering(self):\n        # Measure the diversity of the population\n        diversity = np.std(self.population, axis=0)\n        if np.any(diversity < self.clustering_threshold):\n            # Increase mutation factor and crossover rate to enhance exploration\n            self.mutation_factor = np.random.uniform(0.7, 1.0, self.pop_size)\n            self.cr = np.random.uniform(0.7, 1.0, self.pop_size)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.probabilistic_clustering()\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 87, "feedback": "The algorithm RefinedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05964 with standard deviation 0.00297.", "error": "", "parent_ids": ["0a7cc509-13c2-4fd5-987e-842502ce74c4"], "operator": null, "metadata": {"aucs": [0.0627665704326339, 0.06438585996971158, 0.058155398391527724, 0.05997428365400925, 0.06151217445443313, 0.05561208452767541, 0.05905488712597984, 0.060566766482824885, 0.05477166574679582]}}
{"id": "3fdb1c59-cc03-4460-8d56-0908b461ebbc", "fitness": 0.05964441008728795, "name": "EnhancedDynamicLearningHybridOptimizer", "description": "Enhanced optimizer with an improved learning rate adaptation strategy for better convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.15  # Modified line: Adjusted learning rate for improved adaptation\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Dynamic learning strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x0)\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_1, mutant_vector_2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 88, "feedback": "The algorithm EnhancedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05964 with standard deviation 0.00297.", "error": "", "parent_ids": ["0a7cc509-13c2-4fd5-987e-842502ce74c4"], "operator": null, "metadata": {"aucs": [0.0627665704326339, 0.06438585996971158, 0.058155398391527724, 0.05997428365400925, 0.06151217445443313, 0.05561208452767541, 0.05905488712597984, 0.060566766482824885, 0.05477166574679582]}}
{"id": "6e3f1f00-d23a-4722-a51b-1be7dfaeb3dc", "fitness": 0.05964441008728795, "name": "EnhancedDynamicLearningHybridOptimizer", "description": "Enhanced optimizer with precise gradient estimation for improved convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.1\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Dynamic learning strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x0)\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_1, mutant_vector_2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-10):  # Increased precision in gradient estimation\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 89, "feedback": "The algorithm EnhancedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05964 with standard deviation 0.00297.", "error": "", "parent_ids": ["0a7cc509-13c2-4fd5-987e-842502ce74c4"], "operator": null, "metadata": {"aucs": [0.0627665704326339, 0.06438585996971158, 0.058155398391527724, 0.05997428365400925, 0.06151217445443313, 0.05561208452767541, 0.05905488712597984, 0.060566766482824885, 0.05477166574679582]}}
{"id": "3689756d-35d5-4955-aa33-ba8e5846d801", "fitness": 0.05822929823730717, "name": "EnhancedDynamicLearningHybridOptimizer", "description": "Incorporating an elite preservation strategy in adaptive differential evolution to enhance the exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.1\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Dynamic learning strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x0)\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_1, mutant_vector_2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n                \n            # Elite preservation strategy\n            if np.random.rand() < 0.1:  # With a small probability, preserve elites\n                new_population[i] = self.best_solution\n\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 90, "feedback": "The algorithm EnhancedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05823 with standard deviation 0.00324.", "error": "", "parent_ids": ["0a7cc509-13c2-4fd5-987e-842502ce74c4"], "operator": null, "metadata": {"aucs": [0.057537777571199045, 0.06444875390574156, 0.0588993577347624, 0.055011692158076886, 0.061572911716810896, 0.05632053866122588, 0.054177090928477556, 0.06062680052067915, 0.05546876093879116]}}
{"id": "f970c8c8-52c8-4203-9851-1a74105b391b", "fitness": 0.05964441008728795, "name": "EnhancedDynamicLearningHybridOptimizer", "description": "Improved convergence by tweaking the cooling rate for simulated annealing step.", "code": "import numpy as np\n\nclass EnhancedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.97  # slightly reduced to enhance exploration\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.1\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Dynamic learning strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x0)\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_1, mutant_vector_2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 91, "feedback": "The algorithm EnhancedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05964 with standard deviation 0.00297.", "error": "", "parent_ids": ["0a7cc509-13c2-4fd5-987e-842502ce74c4"], "operator": null, "metadata": {"aucs": [0.0627665704326339, 0.06438585996971158, 0.058155398391527724, 0.05997428365400925, 0.06151217445443313, 0.05561208452767541, 0.05905488712597984, 0.060566766482824885, 0.05477166574679582]}}
{"id": "3bf98213-6529-4724-8053-a0043996eddb", "fitness": 0.05964441008728795, "name": "EnhancedDynamicLearningHybridOptimizer", "description": "Introduced a minimal adjustment in the dynamic learning strategy to slightly enhance the adaptation dynamics of the mutation factor.", "code": "import numpy as np\n\nclass EnhancedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.1\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            # Dynamic learning strategy\n            mutant_vector_1 = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_2 = x0 + mut_factor_i * (x2 - x0)\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_1, mutant_vector_2)\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += 1.1 * self.learning_rate * (0.9 - self.mutation_factor[i])  # Slightly increased learning impact\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05964 with standard deviation 0.00297.", "error": "", "parent_ids": ["0a7cc509-13c2-4fd5-987e-842502ce74c4"], "operator": null, "metadata": {"aucs": [0.0627665704326339, 0.06438585996971158, 0.058155398391527724, 0.05997428365400925, 0.06151217445443313, 0.05561208452767541, 0.05905488712597984, 0.060566766482824885, 0.05477166574679582]}}
{"id": "e4cd830d-9f05-4860-9f6e-1a1fafc159ed", "fitness": 0.06105495582155467, "name": "EnhancedDynamicLearningHybridOptimizer", "description": "Enhanced optimizer utilizing adaptive learning rates, dynamic crossover, and mutation strategies combined with a diversified search approach to optimize convergence and solution quality.", "code": "import numpy as np\n\nclass EnhancedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.98  # Slightly faster cooling rate\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.05  # Lowered learning rate for stability\n        self.diversification_factor = 0.1  # New factor to enhance exploration\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            \n            # Dynamic learning strategy with diversification\n            mutant_vector_base = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_alt = x0 + mut_factor_i * (x2 - x0) + self.diversification_factor * np.random.normal(size=self.dim)\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_base, mutant_vector_alt)\n            \n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            \n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            \n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 93, "feedback": "The algorithm EnhancedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06105 with standard deviation 0.00580.", "error": "", "parent_ids": ["0a7cc509-13c2-4fd5-987e-842502ce74c4"], "operator": null, "metadata": {"aucs": [0.05549613652316776, 0.06475756044064374, 0.0694881595698037, 0.05307688446636649, 0.06187073144754318, 0.06632268817043996, 0.05227622822713185, 0.060921044778190825, 0.06528516877070456]}}
{"id": "d0ad4b63-93ed-46ea-83d4-c90138c5b349", "fitness": 0.06105495582155467, "name": "EnhancedDynamicLearningHybridOptimizer", "description": "Enhanced optimizer combining adaptive learning rates, dynamic crossover, mutation strategies, and refined dynamic population resizing to optimize convergence and solution quality.", "code": "import numpy as np\n\nclass EnhancedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.98  # Slightly faster cooling rate\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.05  # Lowered learning rate for stability\n        self.diversification_factor = 0.1  # New factor to enhance exploration\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            \n            # Dynamic learning strategy with diversification\n            mutant_vector_base = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_alt = x0 + mut_factor_i * (x2 - x0) + self.diversification_factor * np.random.normal(size=self.dim)\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_base, mutant_vector_alt)\n            \n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            \n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            \n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 2.0:  # Modified resizing threshold\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 94, "feedback": "The algorithm EnhancedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06105 with standard deviation 0.00580.", "error": "", "parent_ids": ["e4cd830d-9f05-4860-9f6e-1a1fafc159ed"], "operator": null, "metadata": {"aucs": [0.05549613652316776, 0.06475756044064374, 0.0694881595698037, 0.05307688446636649, 0.06187073144754318, 0.06632268817043996, 0.05227622822713185, 0.060921044778190825, 0.06528516877070456]}}
{"id": "6d3eed3c-ab8e-419b-bedd-82894ee5634e", "fitness": 0.05879617893194971, "name": "EnhancedDynamicLearningHybridOptimizer", "description": "Enhanced optimizer with modified adaptive learning rates, dynamic crossover, mutation, and integrated swarm intelligence for improved convergence and solution quality.", "code": "import numpy as np\n\nclass EnhancedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.95  # Improved cooling rate for better local search\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.1  # Increased learning rate for faster adaptation\n        self.diversification_factor = 0.2  # Enhanced to increase exploration\n        self.inertia_weight = 0.5  # New parameter for incorporating swarm intelligence\n        self.cognitive_coeff = 1.5  # Cognitive coefficient for individual best\n        self.social_coeff = 1.5  # Social coefficient for global best\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n        self.velocity = np.random.rand(self.pop_size, self.dim) * (ub - lb) * 0.1  # Initialize velocity\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            \n            mutant_vector_base = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_alt = x0 + mut_factor_i * (x2 - x0) + self.diversification_factor * np.random.normal(size=self.dim)\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_base, mutant_vector_alt)\n            \n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            \n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n\n        # Swarm Intelligence addition\n        personal_best = np.copy(new_population)\n        global_best = self.best_solution\n        for i in range(self.pop_size):\n            rp, rg = np.random.rand(self.dim), np.random.rand(self.dim)\n            self.velocity[i] = (self.inertia_weight * self.velocity[i] +\n                                self.cognitive_coeff * rp * (personal_best[i] - new_population[i]) +\n                                self.social_coeff * rg * (global_best - new_population[i]))\n            new_population[i] += self.velocity[i]\n            new_population[i] = np.clip(new_population[i], bounds.lb, bounds.ub)\n\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            \n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n            self.velocity = self.velocity[:self.pop_size]  # Resize velocity accordingly\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 95, "feedback": "The algorithm EnhancedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05880 with standard deviation 0.00311.", "error": "", "parent_ids": ["e4cd830d-9f05-4860-9f6e-1a1fafc159ed"], "operator": null, "metadata": {"aucs": [0.05797550132794771, 0.06472008365758941, 0.05994900065573194, 0.05543283882212591, 0.061834171208071886, 0.057321844386588605, 0.05459280675072553, 0.06088479210537434, 0.05645457147339206]}}
{"id": "b5292fd0-1a68-4cc7-ac0e-28052e9d42bd", "fitness": 0.059198021668135294, "name": "RefinedDynamicLearningHybridOptimizer", "description": "Optimizer leveraging adaptive learning rates, multi-phase mutation strategies, and stochastic gradient-based local fine-tuning to enhance global search efficiency and convergence stability.", "code": "import numpy as np\n\nclass RefinedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.97  # Further refined cooling rate\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.03  # Refined learning rate for better adaptation\n        self.exploration_factor = 0.15  # Enhanced exploration for diversity\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def enhanced_mutation_strategy(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            \n            base_vector = x0 + mut_factor_i * (x1 - x2)\n            alt_vector = x0 + mut_factor_i * (x2 - x0) + self.exploration_factor * np.random.normal(size=self.dim)\n            combined_vector = 0.6 * base_vector + 0.4 * alt_vector  # Combine strategies\n            \n            mutant_vector = np.clip(combined_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            \n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def stochastic_gradient_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            \n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.enhanced_mutation_strategy(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.stochastic_gradient_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 96, "feedback": "The algorithm RefinedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05920 with standard deviation 0.00402.", "error": "", "parent_ids": ["e4cd830d-9f05-4860-9f6e-1a1fafc159ed"], "operator": null, "metadata": {"aucs": [0.05592259452125026, 0.06483135847178867, 0.06314584127617873, 0.05348385974349268, 0.06194163856756918, 0.0603542778175451, 0.052676955959449545, 0.060991015637495316, 0.05943465301844819]}}
{"id": "56ec366d-e90d-4b04-a464-d5ab0064f91e", "fitness": 0.06105495582155467, "name": "EnhancedDynamicLearningHybridOptimizer", "description": "Refined optimizer introducing dynamic elite retention and adaptive mutation scaling to further enhance convergence precision and solution robustness.", "code": "import numpy as np\n\nclass EnhancedDynamicLearningHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.98\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.05\n        self.diversification_factor = 0.1\n        self.elite_fraction = 0.1  # Fraction of elite solutions retained\n        self.adaptive_mutation_scale = 0.5\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            \n            # Dynamic learning strategy with diversification\n            mutant_vector_base = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_alt = x0 + mut_factor_i * (x2 - x0) + self.diversification_factor * np.random.normal(size=self.dim)\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_base, mutant_vector_alt)\n            \n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            \n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            \n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def retain_elites(self, func):\n        elite_count = max(int(self.elite_fraction * self.pop_size), 1)\n        fitness_scores = np.array([func(ind) for ind in self.population])\n        elite_indices = fitness_scores.argsort()[:elite_count]\n        elites = self.population[elite_indices]\n        return elites\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            \n            # Retain elite solutions with reduced mutation for stability\n            elites = self.retain_elites(func)\n            self.population[:len(elites)] = elites\n            self.mutation_factor[:len(elites)] *= self.adaptive_mutation_scale\n            \n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 97, "feedback": "The algorithm EnhancedDynamicLearningHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06105 with standard deviation 0.00580.", "error": "", "parent_ids": ["e4cd830d-9f05-4860-9f6e-1a1fafc159ed"], "operator": null, "metadata": {"aucs": [0.05549613652316776, 0.06475756044064374, 0.0694881595698037, 0.05307688446636649, 0.06187073144754318, 0.06632268817043996, 0.05227622822713185, 0.060921044778190825, 0.06528516877070456]}}
{"id": "feeeaba2-b91c-41de-91da-0f211dafdd30", "fitness": 0.06143657484682874, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Advanced Hybrid Metaheuristic Optimizer employing adaptive features, dynamic population resizing, and a novel harmony search-inspired diversification mechanism for robust exploration and convergence enhancement.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate = 0.97  # Adjusted for gradual cooling\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.05  # Lowered learning rate for stability\n        self.diversification_factor = 0.15  # Increased for enhanced exploration\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            \n            # Harmony search-inspired diversification\n            harmony_memory = np.mean(self.population, axis=0)\n            random_memory = harmony_memory + self.diversification_factor * np.random.normal(size=self.dim)\n            mutant_vector_base = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_alt = random_memory\n            mutant_vector = np.where(np.random.rand() > 0.5, mutant_vector_base, mutant_vector_alt)\n            \n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            \n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            \n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= self.cooling_rate\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, 4)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = self.population[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 98, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06144 with standard deviation 0.00530.", "error": "", "parent_ids": ["e4cd830d-9f05-4860-9f6e-1a1fafc159ed"], "operator": null, "metadata": {"aucs": [0.0567904458067332, 0.0645871437952753, 0.06956396846930712, 0.05429707290755337, 0.0617063136139816, 0.06639555491259874, 0.053473011734298526, 0.06075858027330483, 0.06535708210840596]}}
{"id": "a7b51752-2753-4d46-8559-129a2923de57", "fitness": 0.06090501490138421, "name": "EnhancedAdaptiveMetaheuristicOptimizer", "description": "Enhanced Adaptive Metaheuristic Optimizer using a refined mutation strategy, adaptive cooling schedule, and elite preservation to boost exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 10 + 5 * dim\n        self.pop_size = self.init_pop_size\n        self.cr = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.mutation_factor = np.random.uniform(0.5, 0.9, self.pop_size)\n        self.temperature = 1.0\n        self.cooling_rate_initial = 0.97\n        self.cooling_rate_final = 0.99\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.learning_rate = 0.05\n        self.diversification_factor = 0.15\n        self.elite_size = max(2, int(0.05 * self.init_pop_size))\n        self.elite_pool = []\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.copy(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(range(self.pop_size), 3, replace=False)\n            x0, x1, x2 = self.population[indices]\n            cr_i = self.cr[i]\n            mut_factor_i = self.mutation_factor[i]\n            harmony_memory = np.mean(self.population, axis=0)\n            random_memory = harmony_memory + self.diversification_factor * np.random.normal(size=self.dim)\n            mutant_vector_base = x0 + mut_factor_i * (x1 - x2)\n            mutant_vector_alt = random_memory\n            if np.random.rand() < 0.7:\n                mutant_vector = mutant_vector_base\n            else:\n                mutant_vector = mutant_vector_alt\n            mutant_vector = np.clip(mutant_vector, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < cr_i\n            trial_vector = np.where(cross_points, mutant_vector, self.population[i])\n            trial_fitness = func(trial_vector)\n            if trial_fitness < func(self.population[i]):\n                new_population[i] = trial_vector\n                self.cr[i] += self.learning_rate * (0.9 - self.cr[i])\n                self.mutation_factor[i] += self.learning_rate * (0.9 - self.mutation_factor[i])\n                if trial_fitness < self.best_fitness:\n                    self.best_solution = trial_vector\n                    self.best_fitness = trial_fitness\n            else:\n                self.cr[i] -= self.learning_rate * (self.cr[i] - 0.5)\n                self.mutation_factor[i] -= self.learning_rate * (self.mutation_factor[i] - 0.5)\n        self.population = new_population\n\n    def simulated_annealing_local_search(self, bounds, func):\n        for i in range(self.pop_size):\n            candidate = np.copy(self.population[i])\n            gradient = self.estimate_gradient(candidate, func)\n            stochastic_step = np.random.uniform(0.005, 0.02) * (bounds.ub - bounds.lb)\n            candidate -= stochastic_step * gradient\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            candidate_fitness = func(candidate)\n            acceptance_prob = np.exp(-(candidate_fitness - func(self.population[i])) / self.temperature)\n            if candidate_fitness < func(self.population[i]) or np.random.rand() < acceptance_prob:\n                self.population[i] = candidate\n                if candidate_fitness < self.best_fitness:\n                    self.best_solution = candidate\n                    self.best_fitness = candidate_fitness\n        self.temperature *= (self.cooling_rate_initial + (self.cooling_rate_final - self.cooling_rate_initial) * (self.budget - self.budget_remaining) / self.budget)\n\n    def estimate_gradient(self, x, func, epsilon=1e-8):\n        gradient = np.zeros_like(x)\n        fx = func(x)\n        for i in range(self.dim):\n            x_step = np.copy(x)\n            x_step[i] += epsilon\n            gradient[i] = (func(x_step) - fx) / epsilon\n        return gradient\n\n    def dynamic_population_resizing(self, evaluations):\n        if evaluations > self.budget / 1.5:\n            self.pop_size = max(self.init_pop_size // 2, self.elite_size)\n            self.cr = np.resize(self.cr, self.pop_size)\n            self.mutation_factor = np.resize(self.mutation_factor, self.pop_size)\n            self.population = np.vstack((self.population[:self.elite_size], self.population[:self.pop_size - self.elite_size]))\n            self.elite_pool = sorted(self.population, key=lambda x: func(x))[:self.elite_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.budget_remaining = self.budget\n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_differential_evolution_step(bounds, func)\n            evaluations += self.pop_size\n            self.budget_remaining -= self.pop_size\n            if evaluations < self.budget:\n                self.simulated_annealing_local_search(bounds, func)\n                evaluations += self.pop_size\n                self.budget_remaining -= self.pop_size\n            self.dynamic_population_resizing(evaluations)\n        return self.best_solution", "configspace": "", "generation": 99, "feedback": "The algorithm EnhancedAdaptiveMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06091 with standard deviation 0.00594.", "error": "", "parent_ids": ["feeeaba2-b91c-41de-91da-0f211dafdd30"], "operator": null, "metadata": {"aucs": [0.055245457837552525, 0.06434788637694555, 0.0696839369760468, 0.05283892514451183, 0.06147519440943239, 0.06651066883352796, 0.05204232324541935, 0.060530114129045076, 0.06547062715997642]}}
