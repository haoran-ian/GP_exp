{"id": "24c20614-606e-47f9-a11d-d90fbc5eea15", "fitness": -0.12936889217251893, "name": "AdaptiveDifferentialMigration", "description": "Adaptive Differential Migration (ADM) combines differential evolution and particle swarm concepts with adaptive strategies to effectively traverse search spaces.", "code": "import numpy as np\n\nclass AdaptiveDifferentialMigration:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim  # Population size scaling with dimension\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n\n    def __call__(self, func):\n        eval_count = 0\n        fitness = np.apply_along_axis(func, 1, self.population)\n        eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n                # Select three random indices different from i\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n\n                # Differential mutation\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                eval_count += 1\n                if trial_fitness < fitness[i]:\n                    self.population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Update the best solution\n                if trial_fitness < func(self.best):\n                    self.best = trial\n\n        return self.best", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialMigration got an average Area over the convergence curve (AOCC, 1.0 is the best) score of -0.12937 with standard deviation 0.24888.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08253869520449197, 0.08392319549533256, 0.1640353658314183, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.027111444057190126, 0.025216886049310383, 0.020000000000000018, 0.020000000000000018, 0.03170594370801405, 0.0579039478994946, 0.051441224640525474, 0.05180914224875566, 0.020144607785814284, 0.055878197540796704, 0.020000000000000018, 0.03266933197704547, 0.08594148016760361, 0.057929235093625686, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.09994410407015442, 0.1008057409439922, 0.13084903150896143, 0.020000000000000018, 0.020000000000000018, 0.030708686037580812, 0.10538398238180946, 0.08959867197400018, 0.07015575039186472, 0.12990506644848054, 0.13323277534341382, 0.11856502333523733, 0.060570239005714344, 0.07990382579724953, 0.05610455226583122, 0.1255331358040932, 0.10698248325040538, 0.12191976134240401, 0.10691965183577112, 0.020000000000000018, 0.020000000000000018, 0.10819636878037442, 0.07292120586292017, 0.0895133107273659, 0.06527120746322768, 0.04616535580723691, 0.056492493752391515, 0.16261696303253914, 0.13589170657517036, 0.15475448018179316, 0.04858445501710462, 0.03392426887360889, 0.02657375009670604, 0.029583864287932138, 0.050668765511528435, 0.04040435222851424, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010555936431259405, 0.010921107350638004, 0.010413481130909341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07653964318343431, 0.07443205724495594, 0.06681753000357316, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05459995317003097, 0.06147825531143891, 0.05528095097707919, 0.11869262382913237, 0.11318731240470836, 0.10281775224528589, 0.05756835603815702, 0.04897156462936447, 0.053936809511723194, 0.07896945735270666, 0.07884622233717176, 0.08807105303750307, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02803951838058183, 0.02907108857626084, 0.0272480973576803, 0.024895242935785933, 0.022872037771315634, 0.024733098257367914, 0.13237515317305115, 0.1312439877380549, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.43846442735072544, -0.4503110828321184, -0.4479407442556911, -0.5, -0.5, -0.5, -0.44756305275314046, -0.44329719927389344, -0.44166077909996027, -0.3781667908591828, -0.38034405222277257, -0.3692925263809974, -0.46153779116946114, -0.4635035548426063, -0.45733794516959203, -0.40143418949605825, -0.41119658258351044, -0.4006152758026389, -0.5, -0.5, -0.5, -0.48441151589447196, -0.4835146242764512, -0.4858024530793119, -0.48886659315022474, -0.48459384938123584, -0.48800423502670953, -0.32741446184340584, -0.3036434880944845, -0.34047527933627575, -0.5, -0.5, -0.5]}}
{"id": "b680c808-8d52-4712-88e9-746cecbc1517", "fitness": -0.12922686857314944, "name": "AdaptiveDifferentialMigrationDL", "description": "Adaptive Differential Migration with Dynamic Learning (ADM-DL) introduces dynamic parameter adaptation and a learning phase to enhance exploration and exploitation capabilities.", "code": "import numpy as np\n\nclass AdaptiveDifferentialMigrationDL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.F = 0.5\n        self.CR = 0.9\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n\n    def __call__(self, func):\n        eval_count = 0\n        fitness = np.apply_along_axis(func, 1, self.population)\n        eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Dynamic adjustment of F and CR\n                self.F = self.learning_rate * np.random.rand() + 0.5\n                self.CR = self.learning_rate * np.random.rand() + 0.8\n\n                # Select three random indices different from i\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n\n                # Differential mutation\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                eval_count += 1\n                if trial_fitness < fitness[i]:\n                    self.population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Update the best solution\n                if trial_fitness < func(self.best):\n                    self.best = trial\n\n            # Learning phase to exploit best solutions\n            if eval_count < self.budget:\n                # Generate a new population around the current best\n                new_population = self.best + np.random.normal(0, 0.1, (self.population_size, self.dim))\n                new_population = np.clip(new_population, self.lb, self.ub)\n                new_fitness = np.apply_along_axis(func, 1, new_population)\n                eval_count += self.population_size\n\n                # Combine old and new population\n                combined_population = np.vstack((self.population, new_population))\n                combined_fitness = np.hstack((fitness, new_fitness))\n\n                # Select the best individuals for the next generation\n                best_indices = np.argsort(combined_fitness)[:self.population_size]\n                self.population = combined_population[best_indices]\n                fitness = combined_fitness[best_indices]\n                self.best = self.population[np.argmin(fitness)]\n\n        return self.best", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDifferentialMigrationDL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of -0.12923 with standard deviation 0.24872.", "error": "", "parent_ids": ["24c20614-606e-47f9-a11d-d90fbc5eea15"], "operator": null, "metadata": {"aucs": [0.08557087258794327, 0.07959473907647463, 0.15148323741579883, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02904739967341352, 0.020623621721961194, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07354552473193277, 0.04089031357367734, 0.04956030618232932, 0.020000000000000018, 0.06614643324397007, 0.020000000000000018, 0.03216612426525134, 0.08594148016760361, 0.057929235093625686, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.11874890998849419, 0.08712346351932165, 0.13084903150896143, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.08072694858327945, 0.0863116411064907, 0.0852763695874601, 0.14036971104528195, 0.13323277534341382, 0.1160417955509998, 0.0746982959243031, 0.07990382579724953, 0.05766593281472043, 0.11205586464406281, 0.12493682176449505, 0.12191976134240401, 0.020000000000000018, 0.020000000000000018, 0.12187300796160339, 0.10819636878037442, 0.07292120586292017, 0.0895133107273659, 0.06837325976443287, 0.05591327863915696, 0.056492493752391515, 0.1438526290318487, 0.13589170657517036, 0.15475448018179316, 0.038669801718039354, 0.03392426887360889, 0.030024217552354404, 0.02422007577324936, 0.031052079376680974, 0.04106241539000055, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010017863920444903, 0.013528836119916954, 0.013167386806783088, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010401520835176203, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07667461189471292, 0.07827900972143231, 0.07010173284017707, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05526352629019671, 0.06225062436504525, 0.07856387474570325, 0.12007973363605384, 0.09792691493023686, 0.10290821022441288, 0.058325196373856136, 0.041391008018738185, 0.05423547519278815, 0.07937121808640024, 0.07893186116002926, 0.08856785991990623, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.028155629150906414, 0.03242202974301289, 0.029007026238937028, 0.024901771521379845, 0.022517298895109006, 0.02470027110581352, 0.13185458194505073, 0.13123409970527322, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, -0.5, -0.4959809507606159, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.43846442735072544, -0.4420358640573776, -0.45288588190259116, -0.5, -0.5, -0.5, -0.4443249850508273, -0.44309242274136884, -0.44763794975557536, -0.37477013598542896, -0.37945921414430095, -0.3692925263809974, -0.46544483854174556, -0.4627857240877298, -0.45733794516959203, -0.40143418949605825, -0.41112296026678163, -0.40722204537292495, -0.5, -0.5, -0.5, -0.48441151589447196, -0.4830489997607641, -0.4855594861087258, -0.4888165829240132, -0.48459384938123584, -0.4878399819688197, -0.3262084071747027, -0.3036434880944845, -0.30570321592869276, -0.5, -0.5, -0.5]}}
{"id": "673f5ee9-5fee-44dd-ab74-13d0bb099d8d", "fitness": -0.130082700709283, "name": "AdaptiveDifferentialMigrationDL", "description": "Enhanced Adaptive Differential Migration with Dynamic Learning uses improved learning rate adaptation and dynamic population size for better convergence.", "code": "import numpy as np\n\nclass AdaptiveDifferentialMigrationDL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.F = 0.5\n        self.CR = 0.9\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.02  # Modified learning rate for parameter adaptation\n\n    def __call__(self, func):\n        eval_count = 0\n        fitness = np.apply_along_axis(func, 1, self.population)\n        eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Dynamic adjustment of F and CR\n                self.F = self.learning_rate * np.random.rand() + 0.5\n                self.CR = self.learning_rate * np.random.rand() + 0.9  # Slight adjustment here\n\n                # Select three random indices different from i\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n\n                # Differential mutation\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                eval_count += 1\n                if trial_fitness < fitness[i]:\n                    self.population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Update the best solution\n                if trial_fitness < func(self.best):\n                    self.best = trial\n\n            # Learning phase to exploit best solutions\n            if eval_count < self.budget:\n                # Generate a new population around the current best\n                new_population_size = self.population_size + 1  # Dynamic population size\n                new_population = self.best + np.random.normal(0, 0.1, (new_population_size, self.dim))\n                new_population = np.clip(new_population, self.lb, self.ub)\n                new_fitness = np.apply_along_axis(func, 1, new_population)\n                eval_count += new_population_size\n\n                # Combine old and new population\n                combined_population = np.vstack((self.population, new_population))\n                combined_fitness = np.hstack((fitness, new_fitness))\n\n                # Select the best individuals for the next generation\n                best_indices = np.argsort(combined_fitness)[:self.population_size]\n                self.population = combined_population[best_indices]\n                fitness = combined_fitness[best_indices]\n                self.best = self.population[np.argmin(fitness)]\n\n        return self.best", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDifferentialMigrationDL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of -0.13008 with standard deviation 0.24844.", "error": "", "parent_ids": ["b680c808-8d52-4712-88e9-746cecbc1517"], "operator": null, "metadata": {"aucs": [0.09703918414615431, 0.07941839134038586, 0.14801719543592018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020724190876082682, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07201633036105193, 0.04089031357367734, 0.04956030618232932, 0.020000000000000018, 0.03882600544383974, 0.020000000000000018, 0.03919156133131796, 0.08594148016760361, 0.057929235093625686, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12994995767983053, 0.10556040469924421, 0.13084903150896143, 0.020093426231662437, 0.020000000000000018, 0.020000000000000018, 0.08072694858327945, 0.0863116411064907, 0.07015575039186472, 0.13975343220572878, 0.13323277534341382, 0.12361021445981579, 0.0700478572829516, 0.0869815167455289, 0.07138451868318119, 0.11205586464406281, 0.10957545026570958, 0.12191976134240401, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.10819636878037442, 0.07292120586292017, 0.0895133107273659, 0.06527120746322768, 0.04616535580723691, 0.056492493752391515, 0.14776908841192116, 0.13589170657517036, 0.15475448018179316, 0.038669801718039354, 0.03392426887360889, 0.02657375009670604, 0.02422007577324936, 0.03324288647283591, 0.04785968875150215, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010034787037931259, 0.010797282916247486, 0.011458408962175093, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010401520835176203, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07667461189471292, 0.07788503703327476, 0.06715538063535087, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05526352629019671, 0.06366630479692248, 0.06178605092188949, 0.12007973363605384, 0.09693116994761142, 0.10290821022441288, 0.058325196373856136, 0.036190218017786324, 0.05423547519278815, 0.07937121808640024, 0.07893186116002926, 0.08992700095650585, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.028155629150906414, 0.02916227965883944, 0.02678557325072295, 0.024901771521379845, 0.022517298895109006, 0.02470027110581352, 0.13185458194505073, 0.13123409970527322, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.43846442735072544, -0.4459173997887016, -0.45288588190259116, -0.5, -0.5, -0.5, -0.44980472772044156, -0.4420373570393943, -0.44763794975557536, -0.3776395423593295, -0.37919027532641336, -0.3692925263809974, -0.4677062745359464, -0.4615856393947184, -0.45733794516959203, -0.40143418949605825, -0.41119658258351044, -0.40722204537292495, -0.5, -0.5, -0.5, -0.48441151589447196, -0.4829639563035135, -0.4847412625595957, -0.48879774220379324, -0.48459384938123584, -0.48837751112411976, -0.33106793221044395, -0.3036434880944845, -0.3307160326802936, -0.5, -0.5, -0.5]}}
{"id": "f533e4b2-1c42-4bf3-89b2-bb4ad8fcc22f", "fitness": -0.12923037722798217, "name": "AdaptiveDifferentialMigrationDL", "description": "Enhanced Adaptive Differential Migration with a modified learning phase for better convergence.", "code": "import numpy as np\n\nclass AdaptiveDifferentialMigrationDL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.F = 0.5\n        self.CR = 0.9\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n\n    def __call__(self, func):\n        eval_count = 0\n        fitness = np.apply_along_axis(func, 1, self.population)\n        eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Dynamic adjustment of F and CR\n                self.F = self.learning_rate * np.random.rand() + 0.5\n                self.CR = self.learning_rate * np.random.rand() + 0.8\n\n                # Select three random indices different from i\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n\n                # Differential mutation\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                eval_count += 1\n                if trial_fitness < fitness[i]:\n                    self.population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Update the best solution\n                if trial_fitness < func(self.best):\n                    self.best = trial\n\n            # Modified learning phase to exploit best solutions\n            if eval_count < self.budget:\n                # Generate a new population with reduced variance\n                new_population = self.best + np.random.normal(0, 0.05, (self.population_size, self.dim))\n                new_population = np.clip(new_population, self.lb, self.ub)\n                new_fitness = np.apply_along_axis(func, 1, new_population)\n                eval_count += self.population_size\n\n                # Combine old and new population\n                combined_population = np.vstack((self.population, new_population))\n                combined_fitness = np.hstack((fitness, new_fitness))\n\n                # Select the best individuals for the next generation\n                best_indices = np.argsort(combined_fitness)[:self.population_size]\n                self.population = combined_population[best_indices]\n                fitness = combined_fitness[best_indices]\n                self.best = self.population[np.argmin(fitness)]\n\n        return self.best", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDifferentialMigrationDL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of -0.12923 with standard deviation 0.24872.", "error": "", "parent_ids": ["b680c808-8d52-4712-88e9-746cecbc1517"], "operator": null, "metadata": {"aucs": [0.08557087258794327, 0.07959473907647463, 0.15148323741579883, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02904739967341352, 0.020623621721961194, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07354552473193277, 0.04089031357367734, 0.04956030618232932, 0.020000000000000018, 0.06614643324397007, 0.020000000000000018, 0.03216612426525134, 0.08594148016760361, 0.057929235093625686, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.11874890998849419, 0.08712346351932165, 0.13084903150896143, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.08072694858327945, 0.0863116411064907, 0.0852763695874601, 0.14036971104528195, 0.13323277534341382, 0.1160417955509998, 0.0746982959243031, 0.07990382579724953, 0.05766593281472043, 0.11205586464406281, 0.12493682176449505, 0.12191976134240401, 0.020000000000000018, 0.020000000000000018, 0.12187300796160339, 0.10819636878037442, 0.07292120586292017, 0.0895133107273659, 0.06837325976443287, 0.05591327863915696, 0.056492493752391515, 0.1438526290318487, 0.13589170657517036, 0.15475448018179316, 0.038669801718039354, 0.03392426887360889, 0.030024217552354404, 0.02419483518113308, 0.031005608100810655, 0.04100738276020577, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.013507471354536116, 0.013144782465441263, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07660972926329934, 0.07818698484313036, 0.06998403524650187, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05472170164809098, 0.06254263342541033, 0.07903247960028603, 0.11945860588129331, 0.09834495231583251, 0.10292055642596565, 0.05841246239917963, 0.04121091187571002, 0.05444828288516712, 0.07959544650101658, 0.07909317063380072, 0.08837729768542957, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.028095709181398254, 0.032347345351554924, 0.028886671625254268, 0.02489917303500566, 0.02250269626038648, 0.024671561612446546, 0.13185458194505073, 0.13129880498022284, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, -0.5, -0.4959809507606159, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.43846442735072544, -0.4420358640573776, -0.45288588190259116, -0.5, -0.5, -0.5, -0.4443249850508273, -0.44309242274136884, -0.44763794975557536, -0.37477013598542896, -0.37945921414430095, -0.3692925263809974, -0.46544483854174556, -0.4627857240877298, -0.45733794516959203, -0.40143418949605825, -0.41112296026678163, -0.40722204537292495, -0.5, -0.5, -0.5, -0.48441151589447196, -0.4830489997607641, -0.4855594861087258, -0.4888165829240132, -0.48459384938123584, -0.4878399819688197, -0.3262084071747027, -0.3036434880944845, -0.30570321592869276, -0.5, -0.5, -0.5]}}
{"id": "e1cef90f-0eb2-44bd-a53e-a8b9a6f3f702", "fitness": -0.12923037722798217, "name": "AdaptiveDifferentialMigrationDL", "description": "Enhanced ADM-DL introduces adaptive learning rate for parameter adaptation, improving balance between exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialMigrationDL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.F = 0.5\n        self.CR = 0.9\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n\n    def __call__(self, func):\n        eval_count = 0\n        fitness = np.apply_along_axis(func, 1, self.population)\n        eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Dynamic adjustment of F and CR\n                self.F = self.learning_rate * np.random.rand() + 0.5\n                self.CR = self.learning_rate * np.random.rand() + 0.8\n\n                # Select three random indices different from i\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n\n                # Differential mutation\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                eval_count += 1\n                if trial_fitness < fitness[i]:\n                    self.population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Update the best solution\n                if trial_fitness < func(self.best):\n                    self.best = trial\n\n            # Learning phase to exploit best solutions\n            if eval_count < self.budget:\n                # Generate a new population around the current best\n                new_population = self.best + np.random.normal(0, 0.05, (self.population_size, self.dim))  # Reduced variance\n                new_population = np.clip(new_population, self.lb, self.ub)\n                new_fitness = np.apply_along_axis(func, 1, new_population)\n                eval_count += self.population_size\n\n                # Combine old and new population\n                combined_population = np.vstack((self.population, new_population))\n                combined_fitness = np.hstack((fitness, new_fitness))\n\n                # Select the best individuals for the next generation\n                best_indices = np.argsort(combined_fitness)[:self.population_size]\n                self.population = combined_population[best_indices]\n                fitness = combined_fitness[best_indices]\n                self.best = self.population[np.argmin(fitness)]\n\n        return self.best", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDifferentialMigrationDL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of -0.12923 with standard deviation 0.24872.", "error": "", "parent_ids": ["b680c808-8d52-4712-88e9-746cecbc1517"], "operator": null, "metadata": {"aucs": [0.08557087258794327, 0.07959473907647463, 0.15148323741579883, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02904739967341352, 0.020623621721961194, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07354552473193277, 0.04089031357367734, 0.04956030618232932, 0.020000000000000018, 0.06614643324397007, 0.020000000000000018, 0.03216612426525134, 0.08594148016760361, 0.057929235093625686, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.11874890998849419, 0.08712346351932165, 0.13084903150896143, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.08072694858327945, 0.0863116411064907, 0.0852763695874601, 0.14036971104528195, 0.13323277534341382, 0.1160417955509998, 0.0746982959243031, 0.07990382579724953, 0.05766593281472043, 0.11205586464406281, 0.12493682176449505, 0.12191976134240401, 0.020000000000000018, 0.020000000000000018, 0.12187300796160339, 0.10819636878037442, 0.07292120586292017, 0.0895133107273659, 0.06837325976443287, 0.05591327863915696, 0.056492493752391515, 0.1438526290318487, 0.13589170657517036, 0.15475448018179316, 0.038669801718039354, 0.03392426887360889, 0.030024217552354404, 0.02419483518113308, 0.031005608100810655, 0.04100738276020577, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.013507471354536116, 0.013144782465441263, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07660972926329934, 0.07818698484313036, 0.06998403524650187, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05472170164809098, 0.06254263342541033, 0.07903247960028603, 0.11945860588129331, 0.09834495231583251, 0.10292055642596565, 0.05841246239917963, 0.04121091187571002, 0.05444828288516712, 0.07959544650101658, 0.07909317063380072, 0.08837729768542957, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.028095709181398254, 0.032347345351554924, 0.028886671625254268, 0.02489917303500566, 0.02250269626038648, 0.024671561612446546, 0.13185458194505073, 0.13129880498022284, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, -0.5, -0.4959809507606159, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.43846442735072544, -0.4420358640573776, -0.45288588190259116, -0.5, -0.5, -0.5, -0.4443249850508273, -0.44309242274136884, -0.44763794975557536, -0.37477013598542896, -0.37945921414430095, -0.3692925263809974, -0.46544483854174556, -0.4627857240877298, -0.45733794516959203, -0.40143418949605825, -0.41112296026678163, -0.40722204537292495, -0.5, -0.5, -0.5, -0.48441151589447196, -0.4830489997607641, -0.4855594861087258, -0.4888165829240132, -0.48459384938123584, -0.4878399819688197, -0.3262084071747027, -0.3036434880944845, -0.30570321592869276, -0.5, -0.5, -0.5]}}
{"id": "20d16c0e-76a5-4425-8c51-59767d129e1f", "fitness": -0.1298347443006202, "name": "AdaptiveDifferentialMigrationDL", "description": "Enhanced ADM-DL with strategic learning and mutation rate adaptation for balanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialMigrationDL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.F = 0.5\n        self.CR = 0.9\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.mutation_adjustment_factor = 1.2  # New: Mutation rate adjustment factor\n\n    def __call__(self, func):\n        eval_count = 0\n        fitness = np.apply_along_axis(func, 1, self.population)\n        eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Dynamic adjustment of F and CR\n                self.F = (self.learning_rate * np.random.rand() + 0.5) * self.mutation_adjustment_factor  # Modified\n                self.CR = self.learning_rate * np.random.rand() + 0.8\n\n                # Select three random indices different from i\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n\n                # Differential mutation\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                eval_count += 1\n                if trial_fitness < fitness[i]:\n                    self.population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Update the best solution\n                if trial_fitness < func(self.best):\n                    self.best = trial\n\n            # Learning phase to exploit best solutions\n            if eval_count < self.budget:\n                # Generate a new population around the current best\n                new_population = self.best + np.random.normal(0, 0.1, (self.population_size, self.dim))\n                new_population = np.clip(new_population, self.lb, self.ub)\n                new_fitness = np.apply_along_axis(func, 1, new_population)\n                eval_count += self.population_size\n\n                # Combine old and new population\n                combined_population = np.vstack((self.population, new_population))\n                combined_fitness = np.hstack((fitness, new_fitness))\n\n                # Select the best individuals for the next generation\n                best_indices = np.argsort(combined_fitness)[:self.population_size]\n                self.population = combined_population[best_indices]\n                fitness = combined_fitness[best_indices]\n                self.best = self.population[np.argmin(fitness)]\n\n        return self.best", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDifferentialMigrationDL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of -0.12983 with standard deviation 0.24849.", "error": "", "parent_ids": ["b680c808-8d52-4712-88e9-746cecbc1517"], "operator": null, "metadata": {"aucs": [0.08354909139681244, 0.07695563204450684, 0.14612577632232837, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02636270528546769, 0.028489569306986873, 0.020000000000000018, 0.020000000000000018, 0.02490669699696557, 0.08368972029841193, 0.04089031357367734, 0.04956030618232932, 0.020000000000000018, 0.05905019121810373, 0.020000000000000018, 0.04367198897702673, 0.08594148016760361, 0.06689734475951059, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.11789235296808509, 0.09160817724475467, 0.13084903150896143, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.08072694858327945, 0.0863116411064907, 0.07015575039186472, 0.15177348546413638, 0.13323277534341382, 0.1160417955509998, 0.07210093937066031, 0.10004853830117666, 0.05967298994113135, 0.11205586464406281, 0.10411907530727893, 0.12191976134240401, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.10819636878037442, 0.07292120586292017, 0.0895133107273659, 0.07763161950565833, 0.05207374437078738, 0.056492493752391515, 0.1438526290318487, 0.13589170657517036, 0.15475448018179316, 0.038669801718039354, 0.03392426887360889, 0.03539422855864305, 0.02422007577324936, 0.031052079376680974, 0.03921106253134132, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010590870823900533, 0.013692233358456152, 0.013231254506543277, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010401520835176203, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07667461189471292, 0.07720245475965692, 0.06762363109034486, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05526352629019671, 0.06225062436504525, 0.05395939349942802, 0.12007973363605384, 0.09874496095217211, 0.10290821022441288, 0.058325196373856136, 0.0375945059754359, 0.05423547519278815, 0.07937121808640024, 0.07893186116002926, 0.08989251873472304, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02813913321564332, 0.031230777380094388, 0.030226096060278596, 0.024901771521379845, 0.022517298895109006, 0.02470027110581352, 0.13628994243074655, 0.13123409970527322, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, -0.5, -0.49834030747432734, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.43846442735072544, -0.44165720330882086, -0.45288588190259116, -0.5, -0.5, -0.5, -0.45242434131350584, -0.4415172028791823, -0.44763794975557536, -0.37712671134495657, -0.38034405222277257, -0.3692925263809974, -0.46033972676147217, -0.4635035548426063, -0.45733794516959203, -0.40143418949605825, -0.41019361518077146, -0.40722204537292495, -0.5, -0.5, -0.5, -0.48441151589447196, -0.4823985505820092, -0.48582597307690145, -0.48888791146107313, -0.48459384938123584, -0.48848726001003806, -0.32709510203516534, -0.3036434880944845, -0.3340914158305417, -0.5, -0.5, -0.5]}}
{"id": "376827af-ac47-4f31-95c0-c51d6914d569", "fitness": 0.03603077262534481, "name": "EnhancedAdaptiveDifferentialMigrationDL", "description": "Enhanced Adaptive Differential Migration with Multi-Phase Learning integrates adaptive mutation strategies and phased learning to boost convergence and robustness.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigrationDL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n\n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._multi_phase_learning(func, fitness)\n\n        return self.best\n\n    def _multi_phase_learning(self, func, fitness):\n        new_population = self.best + np.random.normal(0, 0.1, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationDL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03603 with standard deviation 0.04226.", "error": "", "parent_ids": ["b680c808-8d52-4712-88e9-746cecbc1517"], "operator": null, "metadata": {"aucs": [0.11277385199925305, 0.08029635503814836, 0.1542441629715363, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02986989055281497, 0.02064960596037624, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07543265310792524, 0.04398270078091626, 0.050271698969439726, 0.020000000000000018, 0.07306839823056577, 0.020000000000000018, 0.04672172961017795, 0.09058152385779406, 0.07652705478500788, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12073096847005371, 0.08853616282159349, 0.15460510068643485, 0.021651211951709493, 0.020000000000000018, 0.020000000000000018, 0.08478129756274089, 0.09527066942209439, 0.0893812978034978, 0.14537869465225983, 0.13323277534341382, 0.12375848023854952, 0.07757113476168054, 0.0819677528194862, 0.07058629960187057, 0.12149531372979194, 0.12946509895629044, 0.1321444811294049, 0.020000000000000018, 0.020000000000000018, 0.12187300796160339, 0.12055172671401448, 0.07531830161208541, 0.10018128823221639, 0.07225933709368038, 0.05737546706394481, 0.061026617304125064, 0.1438526290318487, 0.15479764415700514, 0.15475448018179316, 0.046159963587498964, 0.04015081472753124, 0.030024217552354404, 0.02455511657750309, 0.03179362779588635, 0.04213258142683829, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.01022337427820641, 0.014426916168644333, 0.014064125902461866, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012409125011057509, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0775284906267355, 0.07838566242202494, 0.0702532270306101, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.059871740272339435, 0.06809335077036816, 0.080067303911171, 0.12470343299245823, 0.09967436255825168, 0.10670079250887765, 0.06172526342188267, 0.043411224812001215, 0.05646305997225032, 0.08197334416317448, 0.08221062664957335, 0.0968149870152124, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0287140384479756, 0.03276770956225217, 0.02996738343707961, 0.025103736673561916, 0.02269080999406836, 0.025445363767953588, 0.13424220451493152, 0.13123409970527322, 0.14322508399413603, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0020095246196936145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.036085974579406654, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03527148404808844, 0.03768368840796099, 0.034215952184717624, 0.08242611036104264, 0.08010083873818208, 0.08430029781132409, 0.02223543249795701, 0.02455136423359594, 0.02590229237562558, 0.06501016208547095, 0.057793244132968935, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010851029183329697, 0.009566395153418505, 0.0073964092768795675, 0.010051284513262426, 0.007915286035893976, 0.11456134532055307, 0.1304161139054778, 0.12264961247578765, 0.0, 0.0, 0.0]}}
{"id": "075cce95-76aa-4286-ab17-7757eb0774dc", "fitness": 0.03603071732097849, "name": "EnhancedAdaptiveDifferentialMigrationDL", "description": "Enhanced Adaptive Differential Migration with Multi-Phase Learning and Swarm Synchronization incorporates swarm-based parameter tuning to improve search efficiency and adaptability.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigrationDL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n\n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) \n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._multi_phase_learning(func, fitness)\n\n        return self.best\n\n    def _multi_phase_learning(self, func, fitness):\n        new_population = self.best + np.random.normal(0, 0.1, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n        # Swarm synchronization\n        if np.random.rand() < 0.1:  # New condition to occasionally fine-tune\n            sync_vector = np.mean(self.population - self.best, axis=0) * 0.05\n            self.population += sync_vector\n            self.population = np.clip(self.population, self.lb, self.ub)", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationDL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03603 with standard deviation 0.04226.", "error": "", "parent_ids": ["376827af-ac47-4f31-95c0-c51d6914d569"], "operator": null, "metadata": {"aucs": [0.11277385199925305, 0.08029635503814836, 0.1542441629715363, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02986989055281497, 0.02064960596037624, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07543265310792524, 0.04398270078091626, 0.050271698969439726, 0.020000000000000018, 0.07306839823056577, 0.020000000000000018, 0.04672172961017795, 0.09058152385779406, 0.07652705478500788, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12073096847005371, 0.08853616282159349, 0.15460510068643485, 0.021651211951709493, 0.020000000000000018, 0.020000000000000018, 0.08478129756274089, 0.09527066942209439, 0.0893812978034978, 0.14537869465225983, 0.13323277534341382, 0.12375848023854952, 0.07757113476168054, 0.0819677528194862, 0.07058629960187057, 0.12149531372979194, 0.12946509895629044, 0.1321444811294049, 0.020000000000000018, 0.020000000000000018, 0.12187300796160339, 0.12055172671401448, 0.07531830161208541, 0.10018128823221639, 0.07225933709368038, 0.05737546706394481, 0.061026617304125064, 0.1438526290318487, 0.15479764415700514, 0.15475448018179316, 0.046159963587498964, 0.04015081472753124, 0.030024217552354404, 0.02455511657750309, 0.03179362779588635, 0.04208080620755705, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010235310025455302, 0.014426916168644333, 0.014064125902461866, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012409125011057509, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0775284906267355, 0.07838566242202494, 0.0702532270306101, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.059871740272339435, 0.06809335077036816, 0.080067303911171, 0.12470343299245823, 0.09967436255825168, 0.10670079250887765, 0.06172526342188267, 0.043411224812001215, 0.05646305997225032, 0.0820307779431062, 0.08221062664957335, 0.0968149870152124, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.028706287824827537, 0.03274592013437738, 0.02996738343707961, 0.025103736673561916, 0.02269080999406836, 0.025445363767953588, 0.13424220451493152, 0.13123409970527322, 0.14322508399413603, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0020095246196936145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.036085974579406654, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03527148404808844, 0.03768368840796099, 0.034215952184717624, 0.08242611036104264, 0.08010083873818208, 0.08430029781132409, 0.02223543249795701, 0.02455136423359594, 0.02590229237562558, 0.06501016208547095, 0.057793244132968935, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010851029183329697, 0.009566395153418505, 0.0073964092768795675, 0.010051284513262426, 0.007915286035893976, 0.11456134532055307, 0.1304161139054778, 0.12264961247578765, 0.0, 0.0, 0.0]}}
{"id": "001ac407-f388-4bab-bd86-f3d77077b117", "fitness": 0.03421705029970376, "name": "HybridDifferentialEvolutionAGM", "description": "Hybrid Differential Evolution with Adaptive Gaussian Mutation refines exploration with Gaussian perturbations and adaptive strategies for enhanced convergence.", "code": "import numpy as np\n\nclass HybridDifferentialEvolutionAGM:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 20 + 3 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self):\n        # Adaptive scaling factor and crossover rate\n        F = np.random.normal(0.5, self.learning_rate)\n        CR = 0.9 + self.learning_rate * np.random.rand()\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n    \n    def _gaussian_mutation(self, target):\n        mutation_strength = 0.1 * (self.ub - self.lb)\n        mutated = target + np.random.normal(0, mutation_strength, target.shape)\n        mutated = np.clip(mutated, self.lb, self.ub)\n        return mutated\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                # Apply Gaussian mutation for further diversification\n                trial = self._gaussian_mutation(trial)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._multi_phase_learning(func, fitness)\n\n        return self.best\n\n    def _multi_phase_learning(self, func, fitness):\n        new_population = self.best + np.random.normal(0, 0.1, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 8, "feedback": "The algorithm HybridDifferentialEvolutionAGM got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03422 with standard deviation 0.03974.", "error": "", "parent_ids": ["376827af-ac47-4f31-95c0-c51d6914d569"], "operator": null, "metadata": {"aucs": [0.08834408403525362, 0.09675422033931169, 0.12111305630268887, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02444539175305327, 0.020385699327755913, 0.02315741011791672, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.06638588840156712, 0.06220949562150968, 0.04956030618232932, 0.03598182131293015, 0.020000000000000018, 0.020000000000000018, 0.023245346750239593, 0.08594148016760361, 0.057929235093625686, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.09994410407015442, 0.11614906076298637, 0.13084903150896143, 0.020000000000000018, 0.020000000000000018, 0.024675429448468567, 0.08072694858327945, 0.10163963843403134, 0.07015575039186472, 0.14403035701408895, 0.13323277534341382, 0.1218431588130483, 0.09216168762091825, 0.07990382579724953, 0.05967998823180265, 0.11205586464406281, 0.09781866273457496, 0.12191976134240401, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.10819636878037442, 0.08101141654542143, 0.09133372603177148, 0.0677162831653646, 0.05729790989079797, 0.056492493752391515, 0.1438526290318487, 0.14494024533966976, 0.15475448018179316, 0.05158449799566678, 0.03547757673771412, 0.03301244985217211, 0.032802519635247385, 0.03213429873971396, 0.03910155267332782, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010731553714081965, 0.01953157633528635, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07671023818575795, 0.07443205724495594, 0.07567138436101706, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05845415184112057, 0.06147825531143891, 0.05956498678980093, 0.11869262382913237, 0.10076998253991287, 0.10281775224528589, 0.05756835603815702, 0.04429996998255048, 0.053936809511723194, 0.08921490962938794, 0.08116443580824961, 0.08999225630690677, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.032149717924335075, 0.031007513658429686, 0.026266331051900682, 0.024895242935785933, 0.025355053999832067, 0.024804088146004766, 0.13185458194505073, 0.1387391153171349, 0.13812263609415998, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.030773878152534073, 0.0, 0.0, 0.0, 0.031938464217375784, 0.046343363419250605, 0.034215952184717624, 0.0811281329954987, 0.07965841969894782, 0.08430029781132409, 0.023108303425455445, 0.024192448856156457, 0.027679073480967542, 0.06528231691020692, 0.05775643297460331, 0.05988166818894314, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.009520483419696135, 0.007432833609379963, 0.010051284513262426, 0.007591647015283476, 0.11282773061013285, 0.1304161139054778, 0.107909048612855, 0.0, 0.0, 0.0]}}
{"id": "3c70f052-63e8-40a1-aacc-bdc9633e8327", "fitness": 0.03577599988852209, "name": "EnhancedAdaptiveDifferentialMigrationDL", "description": "Enhanced Adaptive Differential Migration with Multi-Phase Learning and Gaussian Mutations utilizes Gaussian mutations and adaptive strategies for improved performance and convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigrationDL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        gaussian_mutation = np.random.normal(0, 0.1, self.dim)  # Added Gaussian mutation\n        mutant = a + F * (b - c) + gaussian_mutation  # Apply Gaussian mutation\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._multi_phase_learning(func, fitness)\n\n        return self.best\n\n    def _multi_phase_learning(self, func, fitness):\n        new_population = self.best + np.random.normal(0, 0.1, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 9, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationDL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03578 with standard deviation 0.04160.", "error": "", "parent_ids": ["376827af-ac47-4f31-95c0-c51d6914d569"], "operator": null, "metadata": {"aucs": [0.10506621366100877, 0.07769223348327103, 0.12410963532673269, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.03210044899581643, 0.034532551970030356, 0.034160002661927535, 0.020000000000000018, 0.020000000000000018, 0.033658351016203514, 0.05148249477307443, 0.04341651515749534, 0.05097097300814912, 0.07500422462425504, 0.020000000000000018, 0.020000000000000018, 0.0387967787455753, 0.09092523079780823, 0.07618169192531044, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1275496618462978, 0.09958240512167915, 0.1351424716664732, 0.03377852230006484, 0.029815890534840128, 0.020000000000000018, 0.09454466963551156, 0.10167266980604306, 0.08181261857336009, 0.1476023667276558, 0.14603272699200232, 0.1299158667053788, 0.08286277525611507, 0.08125615010114584, 0.06931447758837772, 0.13173106858960437, 0.11119091436358164, 0.13747274171788904, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12226631481933703, 0.09052918145887234, 0.10081400411651653, 0.07448264956747674, 0.06136757245886937, 0.06059529203081293, 0.16397797755429255, 0.13589170657517036, 0.15475448018179316, 0.05571003697818089, 0.040631383884118244, 0.033701822936556214, 0.030042875931600155, 0.03186696954905299, 0.0440102921508706, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.01062231366023747, 0.011108430299969108, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012808809885362482, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08107053855488078, 0.07627638737041953, 0.0845863135931616, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.060140409006525286, 0.08428897172016758, 0.06058779691880478, 0.12224653021215826, 0.09825119631779566, 0.10589528180503605, 0.06212602130301792, 0.04053013505067282, 0.05913857430640013, 0.08466164001809318, 0.08879512811695378, 0.0959790318524888, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02860207748713195, 0.029806167162338637, 0.03099800113083362, 0.02506286018891979, 0.02268157920670022, 0.025225336969342038, 0.1333179285572057, 0.13123409970527322, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.033655649458314896, 0.03753213255583987, 0.034215952184717624, 0.08081782478307176, 0.07986500665121665, 0.08430029781132409, 0.020806169371402228, 0.024792955874405997, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.060153614422910895, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010913316582679022, 0.009473444514923823, 0.007379149371542604, 0.010051284513262426, 0.007813196303673609, 0.11482801153612743, 0.1304161139054778, 0.11165992115999546, 0.0, 0.0, 0.0]}}
{"id": "b1458b48-865c-4984-8184-72e3d93d9776", "fitness": 0.03568572045254609, "name": "EnhancedAdaptiveDifferentialMigrationDL", "description": "Refined Adaptive Differential Migration with a subtle correction factor for improved exploration.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigrationDL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n\n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.01 * np.random.randn(self.dim)  # Added correction factor\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._multi_phase_learning(func, fitness)\n\n        return self.best\n\n    def _multi_phase_learning(self, func, fitness):\n        new_population = self.best + np.random.normal(0, 0.1, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 10, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationDL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03569 with standard deviation 0.04176.", "error": "", "parent_ids": ["376827af-ac47-4f31-95c0-c51d6914d569"], "operator": null, "metadata": {"aucs": [0.10528810344002026, 0.07769223348327103, 0.1220700022896557, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.031302734704895974, 0.03282808095997125, 0.03585372239409379, 0.020000000000000018, 0.020000000000000018, 0.03619393647308189, 0.05148249477307443, 0.043096377876575165, 0.05098404896134667, 0.07136393892169135, 0.020000000000000018, 0.020000000000000018, 0.039128368594140595, 0.09092523079780823, 0.07618169192531044, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12564856969402227, 0.09684502087345903, 0.1351424716664732, 0.031661566303292976, 0.023790190914249942, 0.020000000000000018, 0.09454466963551156, 0.10231717811120389, 0.07605639792164742, 0.1476023667276558, 0.1506549324423514, 0.12984583427212892, 0.07270815177035383, 0.08125615010114584, 0.06450907333324485, 0.13319361927596363, 0.1275718236249821, 0.13202214855149264, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12226631481933703, 0.08413880525949169, 0.10081400411651653, 0.07448264956747674, 0.06136757245886937, 0.06059529203081293, 0.16265023252788435, 0.1468716785193135, 0.15475448018179316, 0.04443175033661939, 0.040845271728030874, 0.033701822936556214, 0.028629517176187558, 0.03182180701077131, 0.04437998405333898, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010553867067196743, 0.01110604494227152, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012808809885362482, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08194881162719947, 0.07627638737041953, 0.08423771743533792, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.059529685632775386, 0.08767615898661074, 0.06054721194299573, 0.12224653021215826, 0.09855771691524029, 0.10715598697959006, 0.0622534866587402, 0.04144274586854568, 0.05913857430640013, 0.08466164001809318, 0.07909209211964252, 0.09742426364065859, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02860207748713195, 0.029806167162338637, 0.031127702802366986, 0.02506286018891979, 0.02267049822675693, 0.025225336969342038, 0.13986228295429692, 0.13519665846541085, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03674240117378147, 0.03779512806001928, 0.034215952184717624, 0.08083849559056677, 0.07965841969894782, 0.08430029781132409, 0.0208836242783359, 0.024576169135275716, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010909668831920571, 0.009470975175057239, 0.007354325247402649, 0.010051284513262426, 0.007839357359119337, 0.11148380199769503, 0.1304161139054778, 0.11330467923280241, 0.0, 0.0, 0.0]}}
{"id": "9bf9d405-a7ff-4591-abde-06cca2245a48", "fitness": 0.03424834635229437, "name": "QuantumInspiredAdaptiveDifferentialMigration", "description": "Quantum-Inspired Adaptive Differential Migration integrates quantum bit representation and dynamic parameter control to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass QuantumInspiredAdaptiveDifferentialMigration:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.eval_count = 0\n        self.phi = np.random.rand(self.population_size, dim) * 2 * np.pi  # Quantum phase\n        self.alpha = 0.01  # Learning rate for parameter adaptation\n\n    def _dynamic_parameters(self):\n        F = self.alpha * np.random.rand() + 0.5\n        CR = self.alpha * np.random.rand() + 0.8\n        return F, CR\n\n    def _quantum_transition(self):\n        self.phi += np.random.normal(0, np.pi / 4, self.phi.shape)\n        q_population = np.tanh(np.cos(self.phi))\n        q_population = self.lb + (q_population + 1) * (self.ub - self.lb) / 2\n        return q_population\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            q_population = self._quantum_transition()\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._multi_phase_learning(func, fitness, q_population)\n\n        return self.best\n\n    def _multi_phase_learning(self, func, fitness, q_population):\n        new_population = self.best + np.random.normal(0, 0.1, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        combined_population = np.vstack((self.population, q_population, new_population))\n        \n        new_fitness = np.apply_along_axis(func, 1, combined_population)\n        self.eval_count += len(combined_population)\n\n        best_indices = np.argsort(new_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = new_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 11, "feedback": "The algorithm QuantumInspiredAdaptiveDifferentialMigration got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03425 with standard deviation 0.04076.", "error": "", "parent_ids": ["376827af-ac47-4f31-95c0-c51d6914d569"], "operator": null, "metadata": {"aucs": [0.09454264451334182, 0.08178397656863357, 0.12111305630268887, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.040130006723072875, 0.020000000000000018, 0.05102699366819863, 0.04286095772362952, 0.04956030618232932, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.03170765472986847, 0.08594148016760361, 0.057929235093625686, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1310236456258923, 0.13862721837909386, 0.13084903150896143, 0.020000000000000018, 0.020000000000000018, 0.02993936427500532, 0.08072694858327945, 0.0863116411064907, 0.10125855512702375, 0.15177524750329918, 0.13337900018349502, 0.1160417955509998, 0.05943272979281122, 0.07990382579724953, 0.05610455226583122, 0.11373107423343665, 0.09722152176762422, 0.14213259860468463, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.10819636878037442, 0.10017661270022782, 0.0895133107273659, 0.06713007348213129, 0.057644762505781544, 0.056492493752391515, 0.16598201501700827, 0.15246802458013464, 0.15475448018179316, 0.04006091328448913, 0.03654961974533788, 0.02657375009670604, 0.024656986846063322, 0.031099988888414143, 0.03910155267332782, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012013151227594276, 0.010731553714081965, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07692371913899687, 0.07613624134179009, 0.07629103724805619, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05459995317003097, 0.06147825531143891, 0.054962711635669126, 0.11869262382913237, 0.10321231886834581, 0.10281775224528589, 0.05756835603815702, 0.04271791495017996, 0.053936809511723194, 0.0841163844284476, 0.07884622233717176, 0.08807105303750307, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.028141730996487535, 0.03042314508689259, 0.026018756973356538, 0.024895242935785933, 0.02411104321967361, 0.024640285494433645, 0.13417186945822157, 0.13123409970527322, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0006229524388885466, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030455326353685863, 0.03337473306870198, 0.0, 0.0, 0.0, 0.032947471309702214, 0.03753213255583987, 0.03531852765101495, 0.08303211034073354, 0.08289724630940032, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011467273694831581, 0.009433151669329232, 0.007354325247402649, 0.010051284513262426, 0.007591647015283476, 0.11253771713739313, 0.1304161139054778, 0.1086065354157375, 0.0, 0.0, 0.0]}}
{"id": "9d6920bc-ec29-487b-9ca4-cfd409f011e1", "fitness": 0.03595076336392399, "name": "EnhancedAdaptiveDifferentialMigrationDL", "description": "Enhanced Adaptive Differential Migration with Precision-Driven Exploration leverages adaptive exploration strategies to fine-tune accuracy and convergence in high-dimensional spaces.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigrationDL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n\n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._precision_driven_exploration(func, fitness)  # Changed method call\n\n        return self.best\n\n    def _precision_driven_exploration(self, func, fitness):  # Renamed method and updated logic\n        new_population = self.best + np.random.normal(0, 0.05, (self.population_size, self.dim))  # Adjusted exploration scale\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationDL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03595 with standard deviation 0.04225.", "error": "", "parent_ids": ["376827af-ac47-4f31-95c0-c51d6914d569"], "operator": null, "metadata": {"aucs": [0.11277385199925305, 0.08029635503814836, 0.1542441629715363, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02986989055281497, 0.02064960596037624, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07498530185635133, 0.04398270078091626, 0.0499385739386885, 0.020000000000000018, 0.07306839823056577, 0.020000000000000018, 0.04672172961017795, 0.08594148016760361, 0.06507721972068947, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.11989963117532387, 0.08783537396029684, 0.15460510068643485, 0.020392483878820866, 0.020000000000000018, 0.020000000000000018, 0.09172722920661702, 0.09969167860580341, 0.0893812978034978, 0.14132104237226373, 0.13410580523490234, 0.12616951534006138, 0.07757113476168054, 0.08559080626900228, 0.06644524406423968, 0.12231896638945117, 0.12946509895629044, 0.14428113185301294, 0.020000000000000018, 0.020000000000000018, 0.12187300796160339, 0.1153168663498686, 0.07418131343350476, 0.09455385818271167, 0.06942938930521791, 0.05737546706394481, 0.05965121121253991, 0.1438526290318487, 0.15479764415700514, 0.15475448018179316, 0.05088531677178776, 0.040557244701638795, 0.030024217552354404, 0.0243635230057625, 0.031377598614934166, 0.04208080620755705, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010200756742018147, 0.014426916168644333, 0.014064125902461866, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010273963799457442, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07705105802532719, 0.07838566242202494, 0.0702532270306101, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05668964852944958, 0.06656883798799895, 0.080067303911171, 0.12251300163080037, 0.10082552509408671, 0.1051851908421193, 0.06291011160605164, 0.04358204475664307, 0.056233474993098054, 0.08416767530135472, 0.08083976001721271, 0.09668427850082639, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.028406230513458253, 0.03268354369436932, 0.02996738343707961, 0.02500251063487946, 0.022590906476697037, 0.02567851633096052, 0.13220039704239073, 0.1337143977431856, 0.14230019269912342, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0020095246196936145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.036085974579406654, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03527148404808844, 0.03768368840796099, 0.034215952184717624, 0.08242611036104264, 0.08010083873818208, 0.08430029781132409, 0.02223543249795701, 0.02455136423359594, 0.02590229237562558, 0.06501016208547095, 0.057793244132968935, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010851029183329697, 0.009566395153418505, 0.0073964092768795675, 0.010051284513262426, 0.007915286035893976, 0.11456134532055307, 0.1304161139054778, 0.12264961247578765, 0.0, 0.0, 0.0]}}
{"id": "c2513b2e-3608-43d2-8b85-2f334998cb5b", "fitness": 0.035149993076665824, "name": "EnhancedAdaptiveDifferentialMigrationDL", "description": "Enhanced Adaptive Differential Migration now includes dynamic learning rate adjustment and selective learning to improve convergence and robustness.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigrationDL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n\n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                # Dynamic learning rate adjustment\n                improvement = np.min(fitness) - np.min(self._evaluate_population(func))\n                self.learning_rate = max(0.001, self.learning_rate * (1 + 0.1 * improvement))\n                self._multi_phase_learning(func, fitness)\n\n        return self.best\n\n    def _multi_phase_learning(self, func, fitness):\n        # Selective learning\n        selected = self.population[np.argsort(fitness)[:self.population_size//2]]\n        new_population = selected + np.random.normal(0, 0.1, (selected.shape[0], self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += new_population.shape[0]\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 13, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationDL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03515 with standard deviation 0.04119.", "error": "", "parent_ids": ["376827af-ac47-4f31-95c0-c51d6914d569"], "operator": null, "metadata": {"aucs": [0.11277385199925305, 0.08029635503814836, 0.1542441629715363, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02986989055281497, 0.02064960596037624, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07454594742043275, 0.04398270078091626, 0.04956030618232932, 0.020000000000000018, 0.07306839823056577, 0.020000000000000018, 0.04672172961017795, 0.08594148016760361, 0.057929235093625686, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.11898755996471433, 0.08712346351932165, 0.15460510068643485, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.08072694858327945, 0.0863116411064907, 0.0893812978034978, 0.14132104237226373, 0.13323277534341382, 0.1160417955509998, 0.07757113476168054, 0.07990382579724953, 0.05770813228901461, 0.11205586464406281, 0.12946509895629044, 0.12191976134240401, 0.020000000000000018, 0.020000000000000018, 0.12187300796160339, 0.10819636878037442, 0.07292120586292017, 0.0895133107273659, 0.0686013518454035, 0.05737546706394481, 0.056492493752391515, 0.1438526290318487, 0.15479764415700514, 0.15475448018179316, 0.038669801718039354, 0.03392426887360889, 0.030024217552354404, 0.02416775099637991, 0.03097080387136797, 0.04208080620755705, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.01001485887348652, 0.014447140853809382, 0.014064125902461866, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07656742437039799, 0.07858788512589099, 0.0702532270306101, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05459995317003097, 0.06153899866496171, 0.080067303911171, 0.12007973363605384, 0.09976687710696408, 0.10290821022441288, 0.05756835603815702, 0.043523378172233285, 0.05412987240628142, 0.07928696307603156, 0.07991875184274566, 0.09011061003415821, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.028109523367630618, 0.03268354369436932, 0.030129148939333827, 0.024895242935785933, 0.02249108056179483, 0.02470027110581352, 0.13185458194505073, 0.13123409970527322, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0020095246196936145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.036085974579406654, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03527148404808844, 0.03768368840796099, 0.034215952184717624, 0.08242611036104264, 0.08010083873818208, 0.08430029781132409, 0.02223543249795701, 0.02455136423359594, 0.02590229237562558, 0.06501016208547095, 0.057793244132968935, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010851029183329697, 0.009566395153418505, 0.0073964092768795675, 0.010051284513262426, 0.007915286035893976, 0.11456134532055307, 0.1304161139054778, 0.12264961247578765, 0.0, 0.0, 0.0]}}
{"id": "3f42b3ff-9fcf-4e74-b814-2811936f6a1d", "fitness": 0.03569944474490274, "name": "EnhancedAdaptiveDifferentialMigrationSA", "description": "Enhanced Adaptive Differential Migration with Simulated Annealing integrates adaptive mutation strategies and annealing-inspired probabilistic acceptance to boost exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigrationSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n        self.temperature = 1.0  # Initial temperature for simulated annealing\n\n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _acceptance_probability(self, current_fitness, trial_fitness):\n        if trial_fitness < current_fitness:\n            return 1.0\n        else:\n            return np.exp((current_fitness - trial_fitness) / self.temperature)\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        ap = self._acceptance_probability(fitness[i], trial_fitness)\n        if trial_fitness < fitness[i] or np.random.rand() < ap:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            self.temperature *= 0.99  # Cooling schedule\n\n            if self.eval_count < self.budget:\n                self._multi_phase_learning(func, fitness)\n\n        return self.best\n\n    def _multi_phase_learning(self, func, fitness):\n        new_population = self.best + np.random.normal(0, 0.1, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 14, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03570 with standard deviation 0.04190.", "error": "", "parent_ids": ["376827af-ac47-4f31-95c0-c51d6914d569"], "operator": null, "metadata": {"aucs": [0.08333356892919996, 0.07825650118413785, 0.1542441629715363, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.021385804915965756, 0.02991868579325374, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.08505901692060969, 0.062249866350812755, 0.050301066867307176, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02992813681616835, 0.09069715334080031, 0.0769154197670886, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.11789658864485286, 0.10694968558504203, 0.13627987168776334, 0.02190952661551704, 0.020000000000000018, 0.020000000000000018, 0.08950212599085117, 0.11159323908790908, 0.07233993519752568, 0.1421840714063327, 0.13358431384454517, 0.12578618125753938, 0.07396968841465457, 0.08329102686086987, 0.06827874675223555, 0.12377949441564962, 0.10293953053551153, 0.14080625661574464, 0.020000000000000018, 0.020000000000000018, 0.12187300796160339, 0.11689265046211272, 0.07531830161208541, 0.09494688108651717, 0.07803981020638051, 0.0646517335919583, 0.06489793433812052, 0.1438526290318487, 0.14147473347510298, 0.15475448018179316, 0.04304063234191602, 0.04015081472753124, 0.04034982823416133, 0.024955168536601402, 0.03198229838747946, 0.04002632390179228, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010322295692304806, 0.01829485803504194, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.01042691773442772, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07781747498935554, 0.08870442906426712, 0.06744005158389033, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06387077286537446, 0.07401877979269866, 0.080067303911171, 0.12408145755058597, 0.0993988698132735, 0.10644713839252662, 0.06373567358265131, 0.04156439160998315, 0.05602825492379149, 0.08607170215707582, 0.08169216671068158, 0.09475301356479959, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.028624504483894198, 0.02993301493033218, 0.02995422830832306, 0.02680522618573744, 0.024123084621852642, 0.02553354769819305, 0.13344605791605735, 0.13446054345524028, 0.15241422406711136, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.040787826525341186, 0.03139537345274035, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03313373231614225, 0.03753213255583987, 0.034215952184717624, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.021697834432119145, 0.02474491208114027, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.0105988461542047, 0.007354325247402649, 0.010051284513262426, 0.007591647015283476, 0.11467109705082523, 0.1304161139054778, 0.12261526629297714, 0.0, 0.0, 0.0]}}
{"id": "3d146110-ce90-453a-84a7-2d86ce8d0261", "fitness": 0.03602911273501675, "name": "EnhancedAdaptiveDifferentialMigrationDL", "description": "Enhanced Adaptive Differential Migration with Multi-Phase Learning and Elite Preservation introduces elite preservation to improve convergence and robustness.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigrationDL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n\n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._multi_phase_learning(func, fitness)\n\n        return self.best\n\n    def _multi_phase_learning(self, func, fitness):\n        elite = self.best.copy()  # Preserve the elite individual\n        new_population = self.best + np.random.normal(0, 0.1, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.population[0] = elite  # Restore the elite individual\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 15, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationDL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03603 with standard deviation 0.04226.", "error": "", "parent_ids": ["376827af-ac47-4f31-95c0-c51d6914d569"], "operator": null, "metadata": {"aucs": [0.11277385199925305, 0.08029635503814836, 0.1542441629715363, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02986989055281497, 0.02064960596037624, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07543265310792524, 0.04398270078091626, 0.050271698969439726, 0.020000000000000018, 0.07306839823056577, 0.020000000000000018, 0.04672172961017795, 0.09058152385779406, 0.07652705478500788, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12073096847005371, 0.08853616282159349, 0.15460510068643485, 0.021651211951709493, 0.020000000000000018, 0.020000000000000018, 0.08478129756274089, 0.09527066942209439, 0.0893812978034978, 0.14537869465225983, 0.13323277534341382, 0.12375848023854952, 0.07757113476168054, 0.0819677528194862, 0.07058629960187057, 0.12149531372979194, 0.12946509895629044, 0.1321444811294049, 0.020000000000000018, 0.020000000000000018, 0.12187300796160339, 0.12055172671401448, 0.07531830161208541, 0.10018128823221639, 0.07225933709368038, 0.05737546706394481, 0.061026617304125064, 0.1438526290318487, 0.15479764415700514, 0.15475448018179316, 0.046159963587498964, 0.04015081472753124, 0.030024217552354404, 0.02455511657750309, 0.03179362779588635, 0.04208080620755705, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.014426916168644333, 0.014064125902461866, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012409125011057509, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0775284906267355, 0.07838566242202494, 0.0702532270306101, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.059871740272339435, 0.06809335077036816, 0.080067303911171, 0.12470343299245823, 0.09967436255825168, 0.10670079250887765, 0.0617337930995413, 0.043411224812001215, 0.05646305997225032, 0.08197334416317448, 0.08221062664957335, 0.0968149870152124, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.028706287824827537, 0.03268354369436932, 0.02996738343707961, 0.025103736673561916, 0.02269080999406836, 0.025445363767953588, 0.13424220451493152, 0.13123409970527322, 0.14322508399413603, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0020095246196936145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.036085974579406654, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03527148404808844, 0.03768368840796099, 0.034215952184717624, 0.08242611036104264, 0.08010083873818208, 0.08430029781132409, 0.02223543249795701, 0.02455136423359594, 0.02590229237562558, 0.06501016208547095, 0.057793244132968935, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010851029183329697, 0.009566395153418505, 0.0073964092768795675, 0.010051284513262426, 0.007915286035893976, 0.11456134532055307, 0.1304161139054778, 0.12264961247578765, 0.0, 0.0, 0.0]}}
{"id": "7501b813-67c3-445a-8545-127ba0bbb22c", "fitness": 0.033023233430531304, "name": "AdaptiveDifferentialEvolutionChaotic", "description": "Adaptive Differential Evolution with Chaotic Initialization and Periodic Local Search integrates chaotic maps to enhance exploration and incorporates periodic local searches to refine solutions and improve convergence.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionChaotic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = self._chaotic_initialization()\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n        self.local_search_frequency = 5  # Perform local search every few generations\n\n    def _chaotic_initialization(self):\n        # Use a logistic map for chaotic initialization\n        x = np.random.rand(self.population_size, self.dim)\n        for _ in range(100):  # Iterate to get chaotic sequence\n            x = 4 * x * (1 - x)\n        return self.lb + (self.ub - self.lb) * x\n\n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        generation = 0\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            # Periodic local search\n            if generation % self.local_search_frequency == 0 and self.eval_count < self.budget:\n                self._local_search(func, fitness)\n\n            generation += 1\n\n        return self.best\n\n    def _local_search(self, func, fitness):\n        # Perform a local search around the best known solution\n        search_radius = 0.1\n        new_solutions = self.best + np.random.uniform(-search_radius, search_radius, (self.population_size, self.dim))\n        new_solutions = np.clip(new_solutions, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_solutions)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_solutions))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 16, "feedback": "The algorithm AdaptiveDifferentialEvolutionChaotic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03302 with standard deviation 0.04027.", "error": "", "parent_ids": ["376827af-ac47-4f31-95c0-c51d6914d569"], "operator": null, "metadata": {"aucs": [0.13879233498190968, 0.06849975296788746, 0.09477538201558189, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.03230834188271614, 0.02185176349490503, 0.020000000000000018, 0.03054108197952643, 0.020000000000000018, 0.0815954930600179, 0.060217116324959186, 0.08481846778235724, 0.020000000000000018, 0.03253638244171442, 0.020000000000000018, 0.06524299774160525, 0.06466089669585828, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12449505922321147, 0.08829422150274835, 0.11904429356377677, 0.020000000000000018, 0.020000000000000018, 0.025074348564692706, 0.08343635254630755, 0.08076980330110983, 0.08499584028371698, 0.14978091043070296, 0.1348960076871052, 0.10285999965556725, 0.0974967852978017, 0.08434534895234447, 0.04663721371705354, 0.10724318721039283, 0.14083463010509778, 0.12007064605404394, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07546784049226563, 0.08235179746267618, 0.08556895210654636, 0.04284119582372259, 0.15309246989000214, 0.06977693593904011, 0.14521120604486637, 0.14472495005157748, 0.1379613728678839, 0.04567575522025047, 0.025455598508221544, 0.030606688933812576, 0.015481558102576232, 0.02120781976889996, 0.02316107823850344, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.026020270934271394, 0.014439157757289545, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07975624569589146, 0.07263563417827823, 0.06381471864360966, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05436859852019649, 0.07725272777582304, 0.05386214100020714, 0.08982044224919361, 0.11388682389114724, 0.09675494242586169, 0.03868925154450542, 0.04766564939669149, 0.04208706978777921, 0.045931729689004275, 0.06451919958669705, 0.04955206574698334, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.03644789852288932, 0.031713782420921266, 0.02510809094151778, 0.02069633285893313, 0.040117896591917335, 0.019831184635227705, 0.12828538061091233, 0.158378935778567, 0.13985550314664097, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02251289937891021, 0.02202722283232883, 0.027233018956246635, 0.0, 0.0, 0.0, 0.042960402555829225, 0.04418495543307632, 0.03502221829093832, 0.07103703676248518, 0.08571981781535598, 0.07749012169187108, 0.008574585316759475, 0.023763185042047774, 0.01870859636264255, 0.022252587276494706, 0.0185359253485603, 0.021458523804370078, 0.0, 0.0, 0.0, 0.007632009510945559, 0.008078892729523335, 0.008031975525389368, 0.006902795070554912, 0.006937345567235242, 0.006627109058988223, 0.12740552423747842, 0.137037974119102, 0.11669214506111214, 0.0, 0.0, 0.0]}}
{"id": "4e88c8e6-b58b-4345-869f-d0867094c0a0", "fitness": 0.03603077262534481, "name": "EnhancedAdaptiveDifferentialMigrationRD", "description": "Enhanced Adaptive Differential Migration with Reinforced Diversity introduces a dynamic diversity mechanism to maintain a diverse population and prevent premature convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigrationRD:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n        self.diversity_threshold = 0.1\n\n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def _measure_diversity(self):\n        distances = np.sqrt(np.sum((self.population - self.best) ** 2, axis=1))\n        mean_distance = np.mean(distances)\n        return mean_distance\n\n    def _reinforce_diversity(self):\n        if self._measure_diversity() < self.diversity_threshold:\n            noise = np.random.normal(0, 0.1, self.population.shape)\n            self.population = np.clip(self.population + noise, self.lb, self.ub)\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self._reinforce_diversity()\n\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._multi_phase_learning(func, fitness)\n\n        return self.best\n\n    def _multi_phase_learning(self, func, fitness):\n        new_population = self.best + np.random.normal(0, 0.1, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationRD got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03603 with standard deviation 0.04226.", "error": "", "parent_ids": ["376827af-ac47-4f31-95c0-c51d6914d569"], "operator": null, "metadata": {"aucs": [0.11277385199925305, 0.08029635503814836, 0.1542441629715363, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02986989055281497, 0.02064960596037624, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07543265310792524, 0.04398270078091626, 0.050271698969439726, 0.020000000000000018, 0.07306839823056577, 0.020000000000000018, 0.04672172961017795, 0.09058152385779406, 0.07652705478500788, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12073096847005371, 0.08853616282159349, 0.15460510068643485, 0.021651211951709493, 0.020000000000000018, 0.020000000000000018, 0.08478129756274089, 0.09527066942209439, 0.0893812978034978, 0.14537869465225983, 0.13323277534341382, 0.12375848023854952, 0.07757113476168054, 0.0819677528194862, 0.07058629960187057, 0.12149531372979194, 0.12946509895629044, 0.1321444811294049, 0.020000000000000018, 0.020000000000000018, 0.12187300796160339, 0.12055172671401448, 0.07531830161208541, 0.10018128823221639, 0.07225933709368038, 0.05737546706394481, 0.061026617304125064, 0.1438526290318487, 0.15479764415700514, 0.15475448018179316, 0.046159963587498964, 0.04015081472753124, 0.030024217552354404, 0.02455511657750309, 0.03179362779588635, 0.04213258142683829, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.01022337427820641, 0.014426916168644333, 0.014064125902461866, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012409125011057509, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0775284906267355, 0.07838566242202494, 0.0702532270306101, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.059871740272339435, 0.06809335077036816, 0.080067303911171, 0.12470343299245823, 0.09967436255825168, 0.10670079250887765, 0.06172526342188267, 0.043411224812001215, 0.05646305997225032, 0.08197334416317448, 0.08221062664957335, 0.0968149870152124, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0287140384479756, 0.03276770956225217, 0.02996738343707961, 0.025103736673561916, 0.02269080999406836, 0.025445363767953588, 0.13424220451493152, 0.13123409970527322, 0.14322508399413603, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0020095246196936145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.036085974579406654, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03527148404808844, 0.03768368840796099, 0.034215952184717624, 0.08242611036104264, 0.08010083873818208, 0.08430029781132409, 0.02223543249795701, 0.02455136423359594, 0.02590229237562558, 0.06501016208547095, 0.057793244132968935, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010851029183329697, 0.009566395153418505, 0.0073964092768795675, 0.010051284513262426, 0.007915286035893976, 0.11456134532055307, 0.1304161139054778, 0.12264961247578765, 0.0, 0.0, 0.0]}}
{"id": "d2f8327c-bb8b-4335-8f82-d76abb6a48ac", "fitness": 0.03602294979772437, "name": "EnhancedAdaptiveDifferentialMigrationDL", "description": "Enhanced Adaptive Differential Migration with Exploration-Exploitation Balance improves convergence by integrating dynamic mutation scaling and neighborhood exploration.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigrationDL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n\n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() * (0.9 - 0.4) + 0.4  # Adjusted scaling\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._multi_phase_learning(func, fitness)\n\n        return self.best\n\n    def _multi_phase_learning(self, func, fitness):\n        new_population = self.best + np.random.normal(0, 0.1, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationDL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03602 with standard deviation 0.04190.", "error": "", "parent_ids": ["376827af-ac47-4f31-95c0-c51d6914d569"], "operator": null, "metadata": {"aucs": [0.09985770462418297, 0.08144671287112804, 0.1595618191060617, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02085436559911369, 0.024015071148783096, 0.020457309725126827, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07062048777766239, 0.04404028330027421, 0.050271698969439726, 0.020000000000000018, 0.07742815101462641, 0.020000000000000018, 0.0401236969943668, 0.09058152385779406, 0.07652705478500788, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12178068508371365, 0.08853616282159349, 0.15210950357189612, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.08478129756274089, 0.09527066942209439, 0.09449522417788703, 0.14434955616776024, 0.13323277534341382, 0.12649420981545378, 0.08108298559479232, 0.0819677528194862, 0.07016430485892766, 0.12185641763009625, 0.10631327057333317, 0.1321444811294049, 0.020000000000000018, 0.020000000000000018, 0.11209960796582807, 0.12055172671401448, 0.10609764971912172, 0.10018128823221639, 0.07898172593318409, 0.052336726797786115, 0.06864798702576225, 0.1438526290318487, 0.13589170657517036, 0.15475448018179316, 0.046159963587498964, 0.04015081472753124, 0.06879547150507037, 0.025091519607849166, 0.03179362779588635, 0.05277380334972481, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.014159591515218617, 0.014167618216854416, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012409125011057509, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07752873958436435, 0.07640484767599498, 0.07231845784512203, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.060485139994378434, 0.06809335077036816, 0.06066486783919667, 0.12470343299245823, 0.10015180455666017, 0.10670079250887765, 0.06169898017330355, 0.0406183622934726, 0.05646305997225032, 0.08206684442548962, 0.08225891643706706, 0.09672191483528314, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.028710132766953778, 0.033821307991913185, 0.02808639352919029, 0.025103736673561916, 0.02270080726216206, 0.025759513964489167, 0.13424220451493152, 0.13634125789114104, 0.14322508399413603, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0021460867133324957, 0.0005242861332735993, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.03657248278474101, 0.029890571760543994, 0.0, 0.0, 0.0, 0.032549588259195184, 0.03837653691170073, 0.034215952184717624, 0.08422504959956345, 0.08008209631664276, 0.08430029781132409, 0.024764259127404098, 0.024638222997613313, 0.02590229237562558, 0.06501016208547095, 0.0598604310028672, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.009764730869593286, 0.007467614814213874, 0.010051284513262426, 0.007798554103335409, 0.1122663355774206, 0.1304161139054778, 0.10779535324930867, 0.0, 0.0, 0.0]}}
{"id": "5d9654bf-35a0-4fc3-9bd8-ea970fe2fb7c", "fitness": 0.03470006326068728, "name": "EnhancedAdaptiveDifferentialMigrationDL", "description": "Augmented Differential Migration using Adaptive Gradient Boosting fuses gradual parameter adaptation with hybrid heuristic learning to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigrationDL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n        self.adapt_factor = 1.0\n\n    def _dynamic_parameters(self):\n        F = (0.5 + self.adapt_factor) * np.random.rand() + 0.3\n        CR = (0.8 + self.adapt_factor) * np.random.rand() + 0.7\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._multi_phase_learning(func, fitness)\n\n        return self.best\n\n    def _multi_phase_learning(self, func, fitness):\n        new_population = self.best + np.random.normal(0, 0.1, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n        self.adapt_factor *= 0.95  # Gradual adaptation for stability", "configspace": "", "generation": 19, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationDL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03470 with standard deviation 0.04068.", "error": "", "parent_ids": ["376827af-ac47-4f31-95c0-c51d6914d569"], "operator": null, "metadata": {"aucs": [0.10351288164360806, 0.07773696034045796, 0.12400420288242653, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020342186796969752, 0.03526784127867644, 0.02288693666616015, 0.020000000000000018, 0.020000000000000018, 0.07246282727229425, 0.0441758234464662, 0.056118287845541515, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02971649113483632, 0.09058152385779406, 0.07652705478500788, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.11941864692153903, 0.10978477662207087, 0.13450160934735866, 0.021780712704513983, 0.020000000000000018, 0.020000000000000018, 0.08478129756274089, 0.09527066942209439, 0.07029426724381915, 0.1424512895335447, 0.13323277534341382, 0.1304029739470849, 0.06834446280604001, 0.0819677528194862, 0.07264501622337038, 0.12149531372979194, 0.10521493093233147, 0.1321444811294049, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12055172671401448, 0.07531830161208541, 0.10018128823221639, 0.07157506085076759, 0.05228945923025918, 0.061026617304125064, 0.1438526290318487, 0.13589170657517036, 0.15475448018179316, 0.046159963587498964, 0.04015081472753124, 0.02657375009670604, 0.024572907012445633, 0.03185504913930881, 0.04612713494700715, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0225747087274093, 0.013022417090299832, 0.018042671306581437, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012409125011057509, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07756403055246774, 0.07558951958760785, 0.068661651677559, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0618690729268947, 0.06809335077036816, 0.05780096867424933, 0.12475714066449939, 0.10107813926154885, 0.10670079250887765, 0.06167383983980812, 0.040314196983968764, 0.05646305997225032, 0.08256536055463182, 0.0815010258457518, 0.09672191483528314, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.028725307416470502, 0.03026037879140886, 0.03207458408007169, 0.025103736673561916, 0.022711491360470126, 0.025445363767953588, 0.13914734531026474, 0.13123409970527322, 0.1441868812257462, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03141516645577269, 0.03753213255583987, 0.0363501355321848, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.010651086139957466, 0.007354325247402649, 0.010051284513262426, 0.007591647015283476, 0.1235391876804175, 0.1304161139054778, 0.11946066381694753, 0.0, 0.0, 0.0]}}
{"id": "ba444e5b-ab26-40c7-80a8-47d0a96ffaa5", "fitness": 0.03602615843816014, "name": "EnhancedAdaptiveDifferentialMigrationDL", "description": "Enhanced Adaptive Differential Migration with Multi-Phase Learning and Dynamic Niching Strategy integrates adaptive mutation with phased learning and niche-based diversity to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigrationDL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n        self.niche_radius = 0.1  # Added niche radius for diversity\n\n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._multi_phase_learning(func, fitness)\n\n            # Apply dynamic niching strategy\n            if self.eval_count < self.budget:\n                self._dynamic_niching(func, fitness)\n\n        return self.best\n\n    def _multi_phase_learning(self, func, fitness):\n        new_population = self.best + np.random.normal(0, 0.1, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _dynamic_niching(self, func, fitness):\n        # Introduce diversity by creating niches\n        new_population = np.random.uniform(self.lb, self.ub, (self.population_size, self.dim))\n        new_population = np.clip(new_population + np.random.normal(0, self.niche_radius, new_population.shape), self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 20, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationDL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03603 with standard deviation 0.04225.", "error": "", "parent_ids": ["376827af-ac47-4f31-95c0-c51d6914d569"], "operator": null, "metadata": {"aucs": [0.11277385199925305, 0.08029635503814836, 0.1542441629715363, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02986989055281497, 0.02064960596037624, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07543265310792524, 0.04398270078091626, 0.050271698969439726, 0.020000000000000018, 0.07306839823056577, 0.020000000000000018, 0.04672172961017795, 0.09058152385779406, 0.07652705478500788, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12073096847005371, 0.08853616282159349, 0.15460510068643485, 0.021651211951709493, 0.020000000000000018, 0.020000000000000018, 0.08478129756274089, 0.09527066942209439, 0.0893812978034978, 0.14537869465225983, 0.13323277534341382, 0.12375848023854952, 0.07757113476168054, 0.0819677528194862, 0.07058629960187057, 0.12149531372979194, 0.12946509895629044, 0.1321444811294049, 0.020000000000000018, 0.020000000000000018, 0.12187300796160339, 0.12055172671401448, 0.07531830161208541, 0.10018128823221639, 0.07225933709368038, 0.05737546706394481, 0.061026617304125064, 0.1438526290318487, 0.15479764415700514, 0.15475448018179316, 0.046159963587498964, 0.04015081472753124, 0.030024217552354404, 0.02455027646349417, 0.03179362779588635, 0.04208080620755705, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.014426916168644333, 0.014064125902461866, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012409125011057509, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0774781176425573, 0.07838566242202494, 0.0702532270306101, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.059871740272339435, 0.06809335077036816, 0.080067303911171, 0.12470343299245823, 0.09947107922519915, 0.10670079250887765, 0.061970055717875394, 0.043411224812001215, 0.05646305997225032, 0.08197334416317448, 0.0815010258457518, 0.0968149870152124, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.028692369013279206, 0.03268354369436932, 0.02996738343707961, 0.025103736673561916, 0.022798435301315756, 0.025445363767953588, 0.13424220451493152, 0.13123409970527322, 0.14322508399413603, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0020095246196936145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.036085974579406654, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03527148404808844, 0.03768368840796099, 0.034215952184717624, 0.08242611036104264, 0.08010083873818208, 0.08430029781132409, 0.02223543249795701, 0.02455136423359594, 0.02590229237562558, 0.06501016208547095, 0.057793244132968935, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010851029183329697, 0.009566395153418505, 0.0073964092768795675, 0.010051284513262426, 0.007915286035893976, 0.11456134532055307, 0.1304161139054778, 0.12264961247578765, 0.0, 0.0, 0.0]}}
{"id": "7548044c-789b-4d96-8644-bb252ae4e6c1", "fitness": 0.03603077262534481, "name": "EnhancedAdaptiveDifferentialMigrationDL", "description": "Enhanced Adaptive Differential Migration with Diversity-Driven Multi-Phase Learning leverages population diversity measures to dynamically adapt exploration and exploitation phases for improved convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigrationDL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n\n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def _compute_diversity(self):\n        if self.population_size > 1:\n            mean_position = np.mean(self.population, axis=0)\n            diversity = np.mean(np.linalg.norm(self.population - mean_position, axis=1))\n        else:\n            diversity = 0\n        return diversity\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n        \n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._multi_phase_learning(func, fitness)\n\n        return self.best\n\n    def _multi_phase_learning(self, func, fitness):\n        diversity = self._compute_diversity()\n        exploration_factor = np.clip(diversity, 0.1, 1.0)\n        \n        new_population = self.best + exploration_factor * np.random.normal(0, 0.1, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 21, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationDL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03603 with standard deviation 0.04226.", "error": "", "parent_ids": ["376827af-ac47-4f31-95c0-c51d6914d569"], "operator": null, "metadata": {"aucs": [0.11277385199925305, 0.08029635503814836, 0.1542441629715363, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02986989055281497, 0.02064960596037624, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07543265310792524, 0.04398270078091626, 0.050271698969439726, 0.020000000000000018, 0.07306839823056577, 0.020000000000000018, 0.04672172961017795, 0.09058152385779406, 0.07652705478500788, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12073096847005371, 0.08853616282159349, 0.15460510068643485, 0.021651211951709493, 0.020000000000000018, 0.020000000000000018, 0.08478129756274089, 0.09527066942209439, 0.0893812978034978, 0.14537869465225983, 0.13323277534341382, 0.12375848023854952, 0.07757113476168054, 0.0819677528194862, 0.07058629960187057, 0.12149531372979194, 0.12946509895629044, 0.1321444811294049, 0.020000000000000018, 0.020000000000000018, 0.12187300796160339, 0.12055172671401448, 0.07531830161208541, 0.10018128823221639, 0.07225933709368038, 0.05737546706394481, 0.061026617304125064, 0.1438526290318487, 0.15479764415700514, 0.15475448018179316, 0.046159963587498964, 0.04015081472753124, 0.030024217552354404, 0.02455511657750309, 0.03179362779588635, 0.04213258142683829, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.01022337427820641, 0.014426916168644333, 0.014064125902461866, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012409125011057509, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0775284906267355, 0.07838566242202494, 0.0702532270306101, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.059871740272339435, 0.06809335077036816, 0.080067303911171, 0.12470343299245823, 0.09967436255825168, 0.10670079250887765, 0.06172526342188267, 0.043411224812001215, 0.05646305997225032, 0.08197334416317448, 0.08221062664957335, 0.0968149870152124, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0287140384479756, 0.03276770956225217, 0.02996738343707961, 0.025103736673561916, 0.02269080999406836, 0.025445363767953588, 0.13424220451493152, 0.13123409970527322, 0.14322508399413603, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0020095246196936145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.036085974579406654, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03527148404808844, 0.03768368840796099, 0.034215952184717624, 0.08242611036104264, 0.08010083873818208, 0.08430029781132409, 0.02223543249795701, 0.02455136423359594, 0.02590229237562558, 0.06501016208547095, 0.057793244132968935, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010851029183329697, 0.009566395153418505, 0.0073964092768795675, 0.010051284513262426, 0.007915286035893976, 0.11456134532055307, 0.1304161139054778, 0.12264961247578765, 0.0, 0.0, 0.0]}}
{"id": "3d5e35c5-c506-49d5-8af4-94049961d25c", "fitness": 0.03551444708301944, "name": "QuantumInspiredDifferentialEvolutionAQ", "description": "Quantum-Inspired Differential Evolution with Adaptive Quantum Rotation uses quantum-inspired rotation and adaptive strategies to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass QuantumInspiredDifferentialEvolutionAQ:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _quantum_rotate(self, vector, F):\n        rotation_matrix = np.eye(self.dim) + F * (np.random.rand(self.dim, self.dim) - 0.5)\n        rotated_vector = np.dot(rotation_matrix, vector)\n        return np.clip(rotated_vector, self.lb, self.ub)\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = self._quantum_rotate(a + F * (b - c), F)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._multi_phase_learning(func, fitness)\n\n        return self.best\n\n    def _multi_phase_learning(self, func, fitness):\n        new_population = self.best + np.random.normal(0, 0.1, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 22, "feedback": "The algorithm QuantumInspiredDifferentialEvolutionAQ got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03551 with standard deviation 0.04093.", "error": "", "parent_ids": ["376827af-ac47-4f31-95c0-c51d6914d569"], "operator": null, "metadata": {"aucs": [0.08372349182314276, 0.07683821898492882, 0.12286054675979696, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.03515318871749651, 0.030558423836970716, 0.03675517952763996, 0.04626118104436172, 0.020000000000000018, 0.020000000000000018, 0.049954410614479805, 0.07073517688194586, 0.05957693802737096, 0.05967173811537363, 0.020000000000000018, 0.020000000000000018, 0.0385216524993065, 0.09050122202632427, 0.07612134119070224, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.10419704995661305, 0.0911141107282859, 0.1602855317937183, 0.020160895130938772, 0.020000000000000018, 0.020000000000000018, 0.09207637614954278, 0.10233287322192008, 0.07830401084137018, 0.14327592298235103, 0.13323277534341382, 0.12337339122402502, 0.08605875605318725, 0.07990382579724953, 0.069330099056222, 0.11460272154103013, 0.09666343675419486, 0.1320619586848264, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12049329590014224, 0.10580750224249447, 0.09449527213025577, 0.06989479386947484, 0.0868102511087292, 0.09823351785650647, 0.14564249033465004, 0.13806691247290903, 0.15475448018179316, 0.050739729601364414, 0.03392426887360889, 0.034229756790522536, 0.02801938026182027, 0.03191137269759792, 0.04064250197683261, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.011039908974019297, 0.01238274278042073, 0.024851069003838377, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0806795841696214, 0.07768345327335391, 0.06777054072257205, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05628397062001511, 0.06624253546308678, 0.05694683413943025, 0.12271248890090458, 0.10271704963414496, 0.10818572759673761, 0.06328411457838168, 0.041164405780711344, 0.05710364948670654, 0.08894298765191366, 0.08071953542160581, 0.09359093849904587, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.028630448757994142, 0.03559130091459195, 0.02727451739127007, 0.02524125795600496, 0.031316790358144275, 0.02516984363703645, 0.13650919256287086, 0.13123409970527322, 0.13897483207246908, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03408131938147274, 0.038057525974555695, 0.03547565832809729, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.021376791420215646, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.00951741893222291, 0.0073575193451411725, 0.010051284513262426, 0.007591647015283476, 0.1178567176355575, 0.1304161139054778, 0.11341408835076994, 0.0, 0.0, 0.0]}}
{"id": "b94a05fc-605c-43c8-91f9-38e657e9b386", "fitness": 0.03489054984764041, "name": "EnhancedAdaptiveDifferentialMigrationDL", "description": "Improved Adaptive Mutation Strategy with Dynamic Learning Rate Amplitude tunes mutation scale based on performance to enhance exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigrationDL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n\n    def _dynamic_parameters(self):\n        F = (0.5 + 0.1 * (self.eval_count / self.budget)) * np.random.rand() + 0.5  # Adjusted mutation scale\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._multi_phase_learning(func, fitness)\n\n        return self.best\n\n    def _multi_phase_learning(self, func, fitness):\n        new_population = self.best + np.random.normal(0, 0.1, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 23, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationDL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03489 with standard deviation 0.04089.", "error": "", "parent_ids": ["376827af-ac47-4f31-95c0-c51d6914d569"], "operator": null, "metadata": {"aucs": [0.11408742052479126, 0.07773696034045796, 0.12400420288242653, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02752664589605136, 0.021309443734179423, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.08907867247845869, 0.043465294927959475, 0.050271698969439726, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.04981353576111813, 0.09058152385779406, 0.07652705478500788, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12497774433782982, 0.09270311140804921, 0.14037085867563415, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.08478129756274089, 0.10225397234852196, 0.07015575039186472, 0.14332097922036213, 0.13877072519838518, 0.12375848023854952, 0.07519030252779368, 0.0819677528194862, 0.0703732032376827, 0.12149531372979194, 0.10674270742503833, 0.1321444811294049, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12055172671401448, 0.08255140482503376, 0.10018128823221639, 0.0721111829544171, 0.05210208729479893, 0.061026617304125064, 0.15428451508427743, 0.13589170657517036, 0.15475448018179316, 0.04637986586935594, 0.042586149501467774, 0.028999097659091477, 0.024564678966497167, 0.03179362779588635, 0.0528512890831615, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.013500964250817704, 0.015610459570701174, 0.015593509207276979, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012409125011057509, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07752945464895444, 0.07548360529646558, 0.07141106067371505, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.060892239733153275, 0.06809335077036816, 0.06110745988245658, 0.1247050678492786, 0.09860254842218974, 0.10670079250887765, 0.06176535660114635, 0.04095033359501188, 0.05646305997225032, 0.08214428879271551, 0.08200425679485068, 0.09672191483528314, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0287324086092009, 0.0301084172387317, 0.033093788780834466, 0.025103736673561916, 0.022700977537266742, 0.025445363767953588, 0.13425992325970915, 0.13123409970527322, 0.14322508399413603, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03281678172320046, 0.03850025253628242, 0.034215952184717624, 0.08138321045466934, 0.07965841969894782, 0.08430029781132409, 0.026267626083041895, 0.024641737084208692, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.009433151669329232, 0.007354325247402649, 0.010051284513262426, 0.007731587408227547, 0.11783149568311346, 0.1304161139054778, 0.10519528647380538, 0.0, 0.0, 0.0]}}
{"id": "aeb9ca5d-390a-4b6b-8a3e-3df4331c5b44", "fitness": 0.03660839774767006, "name": "AdvancedAdaptiveDifferentialMigrationLE", "description": "Advanced Adaptive Differential Migration with Learning and Exploration integrates adaptive mutation, learning phases, and enhanced exploration to achieve improved convergence and robustness.", "code": "import numpy as np\n\nclass AdvancedAdaptiveDifferentialMigrationLE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n\n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._learning_and_exploration(func, fitness)\n\n        return self.best\n\n    def _learning_and_exploration(self, func, fitness):\n        exploration_sigma = 0.2  # Increased exploration\n        new_population = self.best + np.random.normal(0, exploration_sigma, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 24, "feedback": "The algorithm AdvancedAdaptiveDifferentialMigrationLE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03661 with standard deviation 0.04277.", "error": "", "parent_ids": ["376827af-ac47-4f31-95c0-c51d6914d569"], "operator": null, "metadata": {"aucs": [0.11277385199925305, 0.08029635503814836, 0.1542441629715363, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02986989055281497, 0.02064960596037624, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07635255678945219, 0.04398270078091626, 0.05056467120786989, 0.020000000000000018, 0.07306839823056577, 0.020000000000000018, 0.04672172961017795, 0.09084427410143459, 0.08507623860946945, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.06423244067131761, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1220941195379257, 0.08989330259161254, 0.15460510068643485, 0.022425154143736026, 0.020000000000000018, 0.020000000000000018, 0.08998123208858999, 0.09158750097584356, 0.0893812978034978, 0.15137779787341943, 0.13323277534341382, 0.11650585914577904, 0.08484633364506633, 0.07990382579724953, 0.0780074468079921, 0.1243051218187824, 0.12946509895629044, 0.1330601960238389, 0.020000000000000018, 0.020000000000000018, 0.12187300796160339, 0.14498719193911525, 0.07725412544249155, 0.10850277284028753, 0.07668771129650764, 0.059065562664433635, 0.06057614412370382, 0.1438526290318487, 0.15479764415700514, 0.15475448018179316, 0.05110799405613364, 0.035349964616254836, 0.03204790339825325, 0.024895357405848584, 0.032606923307988644, 0.043092619369950014, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.01026633931132137, 0.014426916168644333, 0.014064125902461866, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010123501622536946, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07829914256098336, 0.07838566242202494, 0.0702532270306101, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06839158211522856, 0.07010872741405505, 0.080067303911171, 0.12330760951501918, 0.09962205083002185, 0.10587287859002226, 0.06252170790457168, 0.043411224812001215, 0.055524008228311605, 0.08488769623326864, 0.0825981452658503, 0.097408993112435, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.029780537359053172, 0.03298635495241786, 0.02996738343707961, 0.025337865621081423, 0.02287247591732766, 0.02590693805790434, 0.13185458194505073, 0.1318325445365015, 0.13806045750856666, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0020095246196936145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.036085974579406654, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03527148404808844, 0.03768368840796099, 0.034215952184717624, 0.08242611036104264, 0.08010083873818208, 0.08430029781132409, 0.02223543249795701, 0.02455136423359594, 0.02590229237562558, 0.06501016208547095, 0.057793244132968935, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010851029183329697, 0.009566395153418505, 0.0073964092768795675, 0.010051284513262426, 0.007915286035893976, 0.11456134532055307, 0.1304161139054778, 0.12264961247578765, 0.0, 0.0, 0.0]}}
{"id": "1ffc6ccf-c572-48bc-b72c-5c81ac9d97bf", "fitness": 0.0365049514524899, "name": "EnhancedAdaptiveDifferentialMigrationWithReinforcement", "description": "Enhanced Adaptive Differential Migration with Reinforcement introduces a reinforcement mechanism to dynamically adjust exploration and exploitation, improving adaptability and convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigrationWithReinforcement:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n        self.success_count = 0  # Track successful trials\n    \n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            self.success_count += 1  # Increment on success\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n            \n            if self.eval_count < self.budget:\n                self._adaptive_learning_and_exploration(func, fitness)\n\n        return self.best\n\n    def _adaptive_learning_and_exploration(self, func, fitness):\n        exploration_sigma = 0.2 + (0.05 * self.success_count / self.population_size)  # Adaptive exploration\n        new_population = self.best + np.random.normal(0, exploration_sigma, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 25, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithReinforcement got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03650 with standard deviation 0.04304.", "error": "", "parent_ids": ["aeb9ca5d-390a-4b6b-8a3e-3df4331c5b44"], "operator": null, "metadata": {"aucs": [0.11277385199925305, 0.0803308565810974, 0.1542441629715363, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02986989055281497, 0.02064960596037624, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07673038596940951, 0.04398270078091626, 0.05066627703655424, 0.020000000000000018, 0.07306839823056577, 0.020000000000000018, 0.04672172961017795, 0.09084427410143459, 0.08496211283347621, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12253060397177651, 0.09028598345901007, 0.15460510068643485, 0.022506742070251606, 0.020000000000000018, 0.020000000000000018, 0.09312837679027552, 0.09281460548961518, 0.09762100134603369, 0.14863903849136895, 0.13323277534341382, 0.11890017085354088, 0.07757113476168054, 0.07990382579724953, 0.0766060924198183, 0.14012365751479194, 0.12946509895629044, 0.13224573798067407, 0.020000000000000018, 0.020000000000000018, 0.12187300796160339, 0.1483834395390755, 0.07759111906774896, 0.10976181518166939, 0.07788138745149809, 0.060522177593536286, 0.06062080198138997, 0.1438526290318487, 0.15479764415700514, 0.1589202745203997, 0.04422901994561246, 0.03392426887360889, 0.03105460130583504, 0.024949041638574032, 0.03279309501221861, 0.04325425393415616, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.01036786586025673, 0.014426916168644333, 0.014064125902461866, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.01120887228500822, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07846020361177286, 0.07838566242202494, 0.07029708792282396, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06771207070420016, 0.07138687221598206, 0.080067303911171, 0.1221460350972492, 0.10008738888785729, 0.10481640985040586, 0.061516767793435734, 0.043411224812001215, 0.05638368023565843, 0.08328576241418062, 0.07917681593867609, 0.09818936588804117, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.029665972013107167, 0.03302298185350372, 0.02996738343707961, 0.025315854538448646, 0.022942612285666297, 0.02592240021132941, 0.13270592373315993, 0.13503774224529053, 0.13719258208757434, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0020095246196936145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.036085974579406654, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03527148404808844, 0.03768368840796099, 0.034215952184717624, 0.08242611036104264, 0.08010083873818208, 0.08430029781132409, 0.02223543249795701, 0.02455136423359594, 0.02590229237562558, 0.06501016208547095, 0.057793244132968935, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010851029183329697, 0.009566395153418505, 0.0073964092768795675, 0.010051284513262426, 0.007915286035893976, 0.11456134532055307, 0.1304161139054778, 0.12264961247578765, 0.0, 0.0, 0.0]}}
{"id": "31da597d-1b69-4a5b-9637-597980c22a81", "fitness": 0.03565690130165536, "name": "EnhancedAdaptiveDifferentialMigrationSRM", "description": "Enhanced Adaptive Differential Migration with Strategic Recombination and Memory-based Exploration employs strategic recombination, memory mechanisms, and refined exploration techniques to improve convergence efficiency and solution quality.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigrationSRM:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.memory = np.zeros((self.population_size, dim))\n        self.learning_rate = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + self.memory[indices[0]]\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n                self.memory[i] = trial - self.population[i]  # Update memory with the difference\n\n            if self.eval_count < self.budget:\n                self._strategic_recombination_and_exploration(func, fitness)\n\n        return self.best\n\n    def _strategic_recombination_and_exploration(self, func, fitness):\n        exploration_sigma = 0.1  # Refined exploration\n        new_population = self.best + np.random.normal(0, exploration_sigma, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 26, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationSRM got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03566 with standard deviation 0.04150.", "error": "", "parent_ids": ["aeb9ca5d-390a-4b6b-8a3e-3df4331c5b44"], "operator": null, "metadata": {"aucs": [0.09237729774635506, 0.07773696034045796, 0.1542441629715363, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02986989055281497, 0.02064960596037624, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07543265310792524, 0.04398270078091626, 0.05745189786745353, 0.020000000000000018, 0.07306839823056577, 0.020000000000000018, 0.03466394196945455, 0.09058152385779406, 0.07652705478500788, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12095076279028716, 0.08853616282159349, 0.13450160934735866, 0.021651211951709493, 0.020000000000000018, 0.020000000000000018, 0.08478129756274089, 0.09527066942209439, 0.07061591167304127, 0.1424512895335447, 0.13323277534341382, 0.12375848023854952, 0.06665717430891693, 0.0819677528194862, 0.07058629960187057, 0.12149531372979194, 0.12946509895629044, 0.1321444811294049, 0.020000000000000018, 0.020000000000000018, 0.12187300796160339, 0.12055172671401448, 0.0793106346763156, 0.10018128823221639, 0.07225933709368038, 0.05210208729479893, 0.061026617304125064, 0.14596757113298908, 0.13589170657517036, 0.15475448018179316, 0.046159963587498964, 0.04015081472753124, 0.030024217552354404, 0.029105898387001616, 0.03179362779588635, 0.039804524913626516, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012070564781289761, 0.014454105216844093, 0.02070510223854971, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012409125011057509, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0774781176425573, 0.07838566242202494, 0.07837597118183126, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.059871740272339435, 0.06809335077036816, 0.08017985072713929, 0.12470343299245823, 0.09947107922519915, 0.10670079250887765, 0.06172526342188267, 0.043411224812001215, 0.05646305997225032, 0.08207665724469504, 0.0815010258457518, 0.0968149870152124, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.028709807915362262, 0.03268354369436932, 0.027712032592936753, 0.025103736673561916, 0.023956115727746696, 0.025445363767953588, 0.13424220451493152, 0.13123409970527322, 0.14322508399413603, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0020095246196936145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.036085974579406654, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03527148404808844, 0.03769008422648701, 0.035255134529590726, 0.08242611036104264, 0.08010083873818208, 0.08430029781132409, 0.022195405643112065, 0.02455136423359594, 0.02590229237562558, 0.06501016208547095, 0.057793244132968935, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010180009909299792, 0.010851029183329697, 0.009433151669329232, 0.0073964092768795675, 0.010051284513262426, 0.007591647015283476, 0.11456134532055307, 0.1304161139054778, 0.12264961247578765, 0.0, 0.0, 0.0]}}
{"id": "47cddf00-51ad-4b1a-b8e2-01d60b3ad82c", "fitness": 0.036856072195246586, "name": "AdvancedAdaptiveDifferentialMigrationLE", "description": "Enhanced Exploration and Adaptive Learning uses improved exploration with dynamic sigma and adaptive learning rates to boost convergence speed in complex landscapes.", "code": "import numpy as np\n\nclass AdvancedAdaptiveDifferentialMigrationLE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n\n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._learning_and_exploration(func, fitness)\n\n        return self.best\n\n    def _learning_and_exploration(self, func, fitness):\n        exploration_sigma = 0.1 + 0.3 * (1 - self.eval_count / self.budget)  # Dynamic sigma\n        new_population = self.best + np.random.normal(0, exploration_sigma, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 27, "feedback": "The algorithm AdvancedAdaptiveDifferentialMigrationLE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03686 with standard deviation 0.04333.", "error": "", "parent_ids": ["aeb9ca5d-390a-4b6b-8a3e-3df4331c5b44"], "operator": null, "metadata": {"aucs": [0.11277385199925305, 0.08132892453002005, 0.1542441629715363, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.024788137669295218, 0.02986989055281497, 0.02064960596037624, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07711411932233359, 0.04398270078091626, 0.050836536203181626, 0.020000000000000018, 0.07306839823056577, 0.020000000000000018, 0.04672172961017795, 0.09853707300579773, 0.08371152097207357, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.03302020713768372, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1228356061353657, 0.09092232338616302, 0.15460510068643485, 0.023850267331645192, 0.024393753252040296, 0.020000000000000018, 0.10322859798965056, 0.09508246298618006, 0.09244862645502705, 0.15531656801065774, 0.13323277534341382, 0.12095345136966695, 0.0780034435220911, 0.07990382579724953, 0.07997272149164614, 0.12733225127917702, 0.12946509895629044, 0.15188899261438893, 0.020000000000000018, 0.020000000000000018, 0.12187300796160339, 0.15221567577126183, 0.07850533384231095, 0.11232285992511548, 0.07911862574579287, 0.06300031679246043, 0.06065472680563522, 0.1438526290318487, 0.15479764415700514, 0.15475448018179316, 0.052403191392077164, 0.03392426887360889, 0.030024217552354404, 0.02776440687663395, 0.03276660505918738, 0.04325089842734031, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010346979785383903, 0.014426916168644333, 0.014064125902461866, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012358528887337572, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07844805219859641, 0.07838566242202494, 0.0702532270306101, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0673582447418063, 0.0715466922621617, 0.080067303911171, 0.12242047193925887, 0.10056994010780518, 0.10510900309714666, 0.0616657379508142, 0.043411224812001215, 0.05598629913151432, 0.08360359680688045, 0.08158180646163204, 0.09704462806465897, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.029665972013107167, 0.0330262761881136, 0.02996738343707961, 0.025318539575902244, 0.02291951933184433, 0.025925510339530278, 0.13356606548284788, 0.13184964562173296, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0020095246196936145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.036085974579406654, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03527148404808844, 0.03768368840796099, 0.034215952184717624, 0.08242611036104264, 0.08010083873818208, 0.08430029781132409, 0.02223543249795701, 0.02455136423359594, 0.02590229237562558, 0.06501016208547095, 0.057793244132968935, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010851029183329697, 0.009566395153418505, 0.0073964092768795675, 0.010051284513262426, 0.007915286035893976, 0.11456134532055307, 0.1304161139054778, 0.12264961247578765, 0.0, 0.0, 0.0]}}
{"id": "d9983ae8-784a-45a7-a72b-5ecf6f516672", "fitness": 0.036560974932191424, "name": "AdvancedAdaptiveDifferentialMigrationLE", "description": "Improved Exploration Dynamics by incorporating variable inertia factor for adaptive exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdvancedAdaptiveDifferentialMigrationLE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n\n    def _dynamic_parameters(self):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters()\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._learning_and_exploration(func, fitness)\n\n        return self.best\n\n    def _learning_and_exploration(self, func, fitness):\n        exploration_sigma = 0.1 + 0.3 * (1 - self.eval_count / self.budget)  # Dynamic sigma\n        inertia = 0.9 - 0.5 * (self.eval_count / self.budget)  # New line\n        new_population = self.best + inertia * np.random.normal(0, exploration_sigma, (self.population_size, self.dim))  # Modified line\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 28, "feedback": "The algorithm AdvancedAdaptiveDifferentialMigrationLE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03656 with standard deviation 0.04300.", "error": "", "parent_ids": ["47cddf00-51ad-4b1a-b8e2-01d60b3ad82c"], "operator": null, "metadata": {"aucs": [0.11277385199925305, 0.08029635503814836, 0.1542441629715363, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02986989055281497, 0.02064960596037624, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07631509159821026, 0.04398270078091626, 0.050566602021685836, 0.020000000000000018, 0.07306839823056577, 0.020000000000000018, 0.04672172961017795, 0.09084427410143459, 0.08507623860946945, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.03302222923920983, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12204832773721574, 0.08984038310300224, 0.15460510068643485, 0.022296335191344996, 0.020000000000000018, 0.020000000000000018, 0.09044974721981991, 0.09269626057182179, 0.0893812978034978, 0.1524193820452533, 0.13323277534341382, 0.11830648067333982, 0.08475744404503782, 0.07990382579724953, 0.0805837633258436, 0.12830491325662807, 0.12946509895629044, 0.1443906962547571, 0.020000000000000018, 0.020000000000000018, 0.12187300796160339, 0.14408485417654626, 0.07718478252890404, 0.10828190184020059, 0.07653915536560474, 0.05884622143753182, 0.060560643980068773, 0.1438526290318487, 0.15479764415700514, 0.15475448018179316, 0.052178879596770256, 0.03659062630106813, 0.03275790532954881, 0.024667776969734323, 0.03205651477730853, 0.042199998121236204, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010110835930404405, 0.014426916168644333, 0.014064125902461866, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.011936777346096217, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07775384484067105, 0.07838566242202494, 0.0702532270306101, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06237030313619352, 0.06768413036900656, 0.080067303911171, 0.12454812080244759, 0.09947107922519915, 0.10728152863754015, 0.06230239656388137, 0.04341751421597184, 0.05535368471573676, 0.0842564069602445, 0.07886625262063474, 0.09887257398212934, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.029018456302827955, 0.0328459876099747, 0.02996738343707961, 0.02516511181718517, 0.022755367314347463, 0.02564605342123283, 0.13497566648660353, 0.13932126314945348, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0020095246196936145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.036085974579406654, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03527148404808844, 0.03768368840796099, 0.034215952184717624, 0.08242611036104264, 0.08010083873818208, 0.08430029781132409, 0.02223543249795701, 0.02455136423359594, 0.02590229237562558, 0.06501016208547095, 0.057793244132968935, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010851029183329697, 0.009566395153418505, 0.0073964092768795675, 0.010051284513262426, 0.007915286035893976, 0.11456134532055307, 0.1304161139054778, 0.12264961247578765, 0.0, 0.0, 0.0]}}
{"id": "3a0109c4-9621-4697-855e-fa6ddb6e8790", "fitness": 0.041918202053637735, "name": "AdvancedAdaptiveDifferentialMigrationLE", "description": "Introduced adaptive mutation scaling based on fitness variance to enhance convergence precision and exploration efficiency.", "code": "import numpy as np\n\nclass AdvancedAdaptiveDifferentialMigrationLE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._learning_and_exploration(func, fitness)\n\n        return self.best\n\n    def _learning_and_exploration(self, func, fitness):\n        exploration_sigma = 0.1 + 0.3 * (1 - self.eval_count / self.budget)  # Dynamic sigma\n        new_population = self.best + np.random.normal(0, exploration_sigma, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 29, "feedback": "The algorithm AdvancedAdaptiveDifferentialMigrationLE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04192 with standard deviation 0.08026.", "error": "", "parent_ids": ["47cddf00-51ad-4b1a-b8e2-01d60b3ad82c"], "operator": null, "metadata": {"aucs": [0.08700553136296374, 0.08069109183758949, 0.12791440330453607, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.024788137669295218, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772668992085076, 0.05326824029141619, 0.7265412280365338, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.035878608803082024, 0.09853707300579773, 0.08371152097207357, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.03302020713768372, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.11173186629462695, 0.129125337995751, 0.14147492680476093, 0.022296395430967286, 0.024393753252040296, 0.020000000000000018, 0.09857294095494029, 0.09508246298618006, 0.08983671895322687, 0.15256353812142553, 0.13323277534341382, 0.12095345136966695, 0.07497094371073221, 0.07990382579724953, 0.0795929262229974, 0.12733225127917702, 0.101953845680271, 0.15188899261438893, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.15221567577126183, 0.07908623558423011, 0.11232285992511548, 0.07843434950288009, 0.06105073222607671, 0.06065472680563522, 0.1438526290318487, 0.14188549594006206, 0.15475448018179316, 0.052403191392077164, 0.03392426887360889, 0.028494842509084073, 0.024959643715871338, 0.03276660505918738, 0.040637402431462744, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029046811574076403, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012358528887337572, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07839997763932871, 0.07635567993856862, 0.06868863424364124, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06895741749927697, 0.0715466922621617, 0.05921809090486563, 0.12242047193925887, 0.09707729838113921, 0.10510900309714666, 0.0616657379508142, 0.03791201343887707, 0.05598629913151432, 0.08168051980549718, 0.08158180646163204, 0.09695515271364497, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.029941500408319777, 0.0303582639454808, 0.027291124695688618, 0.025318539575902244, 0.022940228892196046, 0.02592241153755237, 0.1337958871889815, 0.13184964562173296, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03155776558395129, 0.03992221998646983, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011162431641798642, 0.009433151669329232, 0.007391477193524176, 0.010051284513262426, 0.007591647015283476, 0.11144320441440769, 0.1304161139054778, 0.10919098792292203, 0.0, 0.0, 0.0]}}
{"id": "06191a60-950d-4e51-ad25-02e3506896f4", "fitness": 0.0417381756280598, "name": "AdvancedAdaptiveDifferentialMigrationLE", "description": "Enhanced exploration by introducing variable population size and improved parameter adaptation strategy.", "code": "import numpy as np\n\nclass AdvancedAdaptiveDifferentialMigrationLE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        # Adjust learning rate dynamically based on fitness variance\n        self.learning_rate = 0.005 + 0.01 * fitness_variance\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._learning_and_exploration(func, fitness)\n\n        return self.best\n\n    def _learning_and_exploration(self, func, fitness):\n        # Dynamic population size for exploration\n        population_variation = int(self.population_size * 0.1 * (1 - self.eval_count / self.budget))\n        exploration_sigma = 0.1 + 0.3 * (1 - self.eval_count / self.budget)  # Dynamic sigma\n        new_population = self.best + np.random.normal(0, exploration_sigma, (self.population_size + population_variation, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size + population_variation\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 30, "feedback": "The algorithm AdvancedAdaptiveDifferentialMigrationLE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04174 with standard deviation 0.08021.", "error": "", "parent_ids": ["3a0109c4-9621-4697-855e-fa6ddb6e8790"], "operator": null, "metadata": {"aucs": [0.08700553136296374, 0.08069109183758949, 0.12791440330453607, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.024788137669295218, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7769535543773954, 0.04280593446108427, 0.7265412280365338, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.035878608803082024, 0.09853707300579773, 0.08371152097207357, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.03302020713768372, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.11173186629462695, 0.09168756216451834, 0.14147492680476093, 0.022296395430967286, 0.024393753252040296, 0.020000000000000018, 0.10442295997871387, 0.1022675103292775, 0.09592604507574687, 0.15256353812142553, 0.13323277534341382, 0.12095345136966695, 0.07497094371073221, 0.07990382579724953, 0.0795929262229974, 0.12733225127917702, 0.101953845680271, 0.15188899261438893, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.15221567577126183, 0.07850533384231095, 0.11232285992511548, 0.07843434950288009, 0.06105073222607671, 0.06065472680563522, 0.1438526290318487, 0.13589170657517036, 0.15475448018179316, 0.052403191392077164, 0.03392426887360889, 0.028494842509084073, 0.024959643715871338, 0.03276660505918738, 0.040637402431462744, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.038322182090691714, 0.0209932577200308, 0.019499156994452704, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012358528887337572, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07839997763932871, 0.07635567993856862, 0.06868863424364124, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0673582447418063, 0.0715466922621617, 0.05965547606729582, 0.12242047193925887, 0.09707729838113921, 0.10510900309714666, 0.0616657379508142, 0.03791201343887707, 0.05598629913151432, 0.08168051980549718, 0.08158180646163204, 0.09695515271364497, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.029649793611427944, 0.030317622474169026, 0.026760328688753776, 0.025318539575902244, 0.022907346571155895, 0.02592241153755237, 0.13356606548284788, 0.13659429757470865, 0.14316920301768965, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03496852941966522, 0.0383207134947513, 0.034215952184717624, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011081968609510273, 0.009433151669329232, 0.007392022414376864, 0.010051284513262426, 0.007591647015283476, 0.11315416184101079, 0.13301638678057348, 0.10714242148153164, 0.0, 0.0, 0.0]}}
{"id": "204a3280-5767-4ef3-bd63-0829cdf005bb", "fitness": 0.035670228042347346, "name": "AdvancedAdaptiveDifferentialMigrationLE", "description": "Enhanced mutation scaling with local search to improve exploitation capability and convergence speed.", "code": "import numpy as np\n\nclass AdvancedAdaptiveDifferentialMigrationLE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance)\n        F = np.clip(F, 0.4, 0.9)  # Enhanced mutation factor range\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        local_search = trial + np.random.normal(0, 0.001, self.dim)  # Local search enhancement\n        return np.clip(local_search, self.lb, self.ub)\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._learning_and_exploration(func, fitness)\n\n        return self.best\n\n    def _learning_and_exploration(self, func, fitness):\n        exploration_sigma = 0.1 + 0.3 * (1 - self.eval_count / self.budget)  # Dynamic sigma\n        new_population = self.best + np.random.normal(0, exploration_sigma, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 31, "feedback": "The algorithm AdvancedAdaptiveDifferentialMigrationLE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03567 with standard deviation 0.04168.", "error": "", "parent_ids": ["3a0109c4-9621-4697-855e-fa6ddb6e8790"], "operator": null, "metadata": {"aucs": [0.09136860312600548, 0.0868385587122037, 0.13169853677886545, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02555720802686101, 0.020000000000000018, 0.030234363205843717, 0.020000000000000018, 0.10614703465113218, 0.052670680400630276, 0.05131994222323244, 0.020000000000000018, 0.061190471769599575, 0.020000000000000018, 0.0381762511647884, 0.0948372695330656, 0.07650382834824254, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.10956284766968027, 0.0991637369675048, 0.14106327618902514, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.09224340565524425, 0.0863116411064907, 0.0909998197056151, 0.14023436600653494, 0.13581402449700963, 0.12529010194640788, 0.06919123469115807, 0.07990382579724953, 0.06994078817341376, 0.1320909653725718, 0.09139717483204546, 0.1303841559006852, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12524145157617672, 0.08890577792319476, 0.11481707797507157, 0.09168908461820136, 0.09164318297867813, 0.07990545000858051, 0.16561522731933986, 0.13589170657517036, 0.15475448018179316, 0.04724786217895138, 0.03392426887360889, 0.034463600070400835, 0.0251937963923764, 0.03264412110463044, 0.04150020024808099, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010261424815313336, 0.01685410528684117, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010278189027039808, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07871771145174844, 0.0772105879484909, 0.07171237415891896, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06334944626786021, 0.07007804632542336, 0.06035470196626702, 0.12346403857696364, 0.09692506587463856, 0.10761748745592026, 0.06229198401436886, 0.04119192204422317, 0.057276494600690064, 0.08642202873037153, 0.08080817571837162, 0.09695776730731853, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.029647524885874077, 0.043709271396558336, 0.027894442276075182, 0.025538488697654738, 0.023133734308971143, 0.025900089709316787, 0.13817951011494412, 0.13456201916884114, 0.14103035418829735, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.0326016256620959, 0.042141183308109564, 0.03787766629148592, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.009433151669329232, 0.007354325247402649, 0.010051284513262426, 0.007928143779864616, 0.11642000709264633, 0.13191579079808524, 0.10749282915013036, 0.0, 0.0, 0.0]}}
{"id": "9a5246cd-2393-44fe-bbf4-344c5ab118b3", "fitness": 0.04019912582447384, "name": "EnhancedAdaptiveDifferentialMigration", "description": "Leveraged self-adaptive learning rates and covariances, dynamically tuning exploration and exploitation balance to improve convergence and precision.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigration:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Initial learning rate for parameter adaptation\n        self.eval_count = 0\n\n    def _adaptive_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.sqrt(np.random.rand()) + 0.5 * (1 + 0.1 * fitness_variance)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._adaptive_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._adaptive_exploration(func, fitness)\n\n        return self.best\n\n    def _adaptive_exploration(self, func, fitness):\n        exploration_sigma = 0.1 + 0.2 * (1 - self.eval_count / self.budget)  # Dynamic sigma\n        exploration_rate = self.learning_rate * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, exploration_sigma, (self.population_size, self.dim)) * exploration_rate\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigration got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04020 with standard deviation 0.07940.", "error": "", "parent_ids": ["3a0109c4-9621-4697-855e-fa6ddb6e8790"], "operator": null, "metadata": {"aucs": [0.08256024734572553, 0.07604574770542138, 0.12115348456810437, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772668997109866, 0.05326824029141619, 0.7265412280365338, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023245346750239593, 0.08594148016760361, 0.057929235093625686, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.09999503343662264, 0.129125337995751, 0.13089607404122783, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.08873853495766515, 0.07171728634208596, 0.13021216610679331, 0.13347435603909863, 0.11637962056371465, 0.0595249934722234, 0.08062211020524268, 0.056716656940308585, 0.11900993840370933, 0.09396814628646866, 0.13281324970848418, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.10841458061611631, 0.0760988336939542, 0.08963326985605957, 0.06537622460944992, 0.04624770139285883, 0.05659469123046823, 0.15336018532653017, 0.14163092929153143, 0.16597395342124832, 0.04069938591087696, 0.03456374421552544, 0.02697714387621375, 0.02421243600506584, 0.031073390114061628, 0.03921133154128331, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0272474001957278, 0.028176322700732448, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07730882678337436, 0.07521617800079472, 0.06703261505435354, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.062416811056665966, 0.05494699646205137, 0.1189125269444965, 0.09688417020279061, 0.10301797318529482, 0.057880257505949384, 0.03597855785630799, 0.054090591421674605, 0.0832951149627591, 0.07930353574550142, 0.09190228370382547, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02804559926550798, 0.029048845997853623, 0.027176396991222873, 0.024897325477046595, 0.02248882522847151, 0.024646373533826593, 0.1340282694522993, 0.144322376090258, 0.13833723250528973, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03165583923185211, 0.03992588295155475, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011158717194208911, 0.009433151669329232, 0.0073903766425093, 0.010051284513262426, 0.007591647015283476, 0.11176804961832787, 0.1304161139054778, 0.10942991026495863, 0.0, 0.0, 0.0]}}
{"id": "9844ea8b-8ea8-4ed2-92cd-d78cc194e624", "fitness": 0.0420248624355915, "name": "AdvancedAdaptiveDifferentialMigrationLE", "description": "Enhanced exploration by incorporating adaptive global best perturbation to maintain diversity and improve convergence.", "code": "import numpy as np\n\nclass AdvancedAdaptiveDifferentialMigrationLE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._learning_and_exploration(func, fitness)\n\n        return self.best\n\n    def _learning_and_exploration(self, func, fitness):\n        exploration_sigma = 0.1 + 0.3 * (1 - self.eval_count / self.budget)  # Dynamic sigma\n        new_population = self.best + np.random.normal(0, exploration_sigma, (self.population_size, self.dim))\n        perturbation_strength = 0.5 * (1 - self.eval_count / self.budget)  # Line 1 of 6 changed\n        perturbed_population = new_population + perturbation_strength * np.random.normal(0, 1, new_population.shape)  # Line 2 of 6 changed\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)  # Line 3 of 6 changed\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)  # Line 4 of 6 changed\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))  # Line 5 of 6 changed\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]  # Line 6 of 6 changed", "configspace": "", "generation": 33, "feedback": "The algorithm AdvancedAdaptiveDifferentialMigrationLE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04202 with standard deviation 0.08022.", "error": "", "parent_ids": ["3a0109c4-9621-4697-855e-fa6ddb6e8790"], "operator": null, "metadata": {"aucs": [0.08724094883748135, 0.08178854087292098, 0.12838442301604036, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.027440568871929805, 0.02140639674363587, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.022649697787595957, 0.7772668992085076, 0.05326824029141619, 0.7265412280365338, 0.0366291945850199, 0.02999759003356328, 0.020000000000000018, 0.041376663444003814, 0.09635318262522674, 0.10280789871688223, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1278738453701369, 0.129125337995751, 0.13642369974074298, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10490209931562477, 0.08195305379908491, 0.1448911004763792, 0.1375488526032238, 0.12622408201090307, 0.07746393498784387, 0.07990382579724953, 0.06631484726990267, 0.12664500666936762, 0.0907369174111986, 0.13559206926217016, 0.020000000000000018, 0.020000000000000018, 0.026557996229800596, 0.14057578842581364, 0.0760988336939542, 0.1093632089435278, 0.0979514785527088, 0.06084404105517971, 0.05842518601271218, 0.1438526290318487, 0.15094016447900682, 0.15475448018179316, 0.04041227739607589, 0.035743658968060865, 0.028134221931844117, 0.02581941658222786, 0.03387051739684177, 0.040617536730723214, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.028514004089269074, 0.029398460800452275, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010028865930576059, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07905629092300104, 0.0779931773846605, 0.06887261401726252, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06333034664221249, 0.07421067684891003, 0.05640257411388849, 0.12367852298765503, 0.09919700014855526, 0.1054789781402099, 0.061049555077913875, 0.039211180660104206, 0.05605534315601046, 0.08415783169537094, 0.07884622233717176, 0.09716128182579042, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.03031117875634759, 0.03180539287527395, 0.027728485563494165, 0.025337018500884212, 0.02344542832892893, 0.025730630126186482, 0.13249075356767692, 0.13142452654672554, 0.13751648809304573, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03155776558395129, 0.03992221998646983, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011162431641798642, 0.009433151669329232, 0.007391477193524176, 0.010051284513262426, 0.007591647015283476, 0.11144320441440769, 0.1304161139054778, 0.10919098792292203, 0.0, 0.0, 0.0]}}
{"id": "e658d886-0115-43e1-b5f1-a209eb64eef0", "fitness": 0.03844085231921941, "name": "AdvancedAdaptiveDifferentialMigrationLE", "description": "Refined dynamic parameters and exploration strategy to enhance balance between exploration and exploitation while maintaining diversity.", "code": "import numpy as np\n\nclass AdvancedAdaptiveDifferentialMigrationLE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01  # Learning rate for parameter adaptation\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.05 * fitness_variance)  # Line 1 of 6 changed\n        CR = self.learning_rate * np.random.rand() + 0.9  # Line 2 of 6 changed\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._learning_and_exploration(func, fitness)\n\n        return self.best\n\n    def _learning_and_exploration(self, func, fitness):\n        exploration_sigma = 0.1 + 0.25 * (1 - self.eval_count / self.budget)  # Line 3 of 6 changed\n        new_population = self.best + np.random.normal(0, exploration_sigma, (self.population_size, self.dim))\n        perturbation_strength = 0.3 * (1 - self.eval_count / self.budget)  # Line 4 of 6 changed\n        perturbed_population = new_population + perturbation_strength * np.random.normal(0, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 34, "feedback": "The algorithm AdvancedAdaptiveDifferentialMigrationLE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03844 with standard deviation 0.06250.", "error": "", "parent_ids": ["9844ea8b-8ea8-4ed2-92cd-d78cc194e624"], "operator": null, "metadata": {"aucs": [0.08577636698344515, 0.07989271107191709, 0.12770209568227164, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02667707461406077, 0.0239016900541279, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.0823850815695295, 0.04151035640846823, 0.7275826370492389, 0.031135479649595066, 0.024678986453874785, 0.020000000000000018, 0.038853896657796705, 0.09357279871458535, 0.08371152097207357, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.11747821831008876, 0.09347076178086955, 0.1372778180754345, 0.021794694554010063, 0.020000000000000018, 0.020000000000000018, 0.09222255113729294, 0.09937175270058829, 0.09592604507574687, 0.14065393707053575, 0.13323277534341382, 0.12274606485931006, 0.08042857561257799, 0.07990382579724953, 0.07254130477106713, 0.12801840581528978, 0.09647143942251035, 0.1312987114788976, 0.020000000000000018, 0.020000000000000018, 0.03869705041262117, 0.16046517757770118, 0.0760988336939542, 0.11116156814590095, 0.08623199435725715, 0.05868856793166255, 0.06675543911988058, 0.1438526290318487, 0.14344476702333453, 0.15475448018179316, 0.05220097518367228, 0.03392426887360889, 0.027344151227903235, 0.0253133643169402, 0.03321861519925573, 0.04040949168718666, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.02512769762409417, 0.012170726316088176, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010028865930576059, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07846322496921454, 0.07698528486904843, 0.06862929178503085, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.060094839730698624, 0.07370688148114679, 0.05777294168660119, 0.12340104130088403, 0.09811439825325707, 0.10596488233363754, 0.061242922305352776, 0.039509394184385926, 0.05630407308970231, 0.0859504256747059, 0.07964405376682915, 0.09535342502371624, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.029956968302053633, 0.03096733547441377, 0.028458302288062676, 0.025315937802874, 0.02319231129576993, 0.025494135024258235, 0.13487527597162707, 0.13868579109568724, 0.14101336090296546, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03616995198183137, 0.039328507619665776, 0.041375730189195936, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011234582102183865, 0.009698620452813445, 0.007404249635587057, 0.010051284513262426, 0.007650084807698687, 0.11181450310727004, 0.1304161139054778, 0.10579291562689674, 0.0, 0.0, 0.0]}}
{"id": "3c1c3cc7-b3b1-4f85-a5b0-8740f18f612d", "fitness": 0.03540161103542671, "name": "AdvancedAdaptiveDifferentialMigrationLE", "description": "Introduced a dynamic learning rate and adaptive exploration mechanisms to enhance global convergence and adapt better to diverse problem landscapes.", "code": "import numpy as np\n\nclass AdvancedAdaptiveDifferentialMigrationLE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.initial_learning_rate = 0.05  # Initial learning rate for adaptive exploration\n        self.learning_rate_decay = 0.995  # Learning rate decay factor\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_std = np.std(fitness)\n        F = self.initial_learning_rate * (1 - self.eval_count / self.budget) + 0.4 * (1 + 0.1 * fitness_std)\n        CR = 0.9 - 0.5 * fitness_std  # More adaptive crossover\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._adaptive_exploration(func, fitness)\n\n        return self.best\n\n    def _adaptive_exploration(self, func, fitness):\n        exploration_sigma = 0.1 + 0.3 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, exploration_sigma, (self.population_size, self.dim))\n        perturbation_strength = 0.3 * (1 - self.eval_count / self.budget) * np.random.rand()  # Dynamic perturbation\n        perturbed_population = new_population + perturbation_strength * np.random.normal(0, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 35, "feedback": "The algorithm AdvancedAdaptiveDifferentialMigrationLE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03540 with standard deviation 0.04176.", "error": "", "parent_ids": ["9844ea8b-8ea8-4ed2-92cd-d78cc194e624"], "operator": null, "metadata": {"aucs": [0.09285102918770183, 0.0769474687973053, 0.14490528652196744, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.024015985434672982, 0.020000000000000018, 0.020540079348797757, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.0519296061511102, 0.04291477584264891, 0.06053495343897264, 0.020000000000000018, 0.03034625543545244, 0.020000000000000018, 0.0356357389871067, 0.09150167701026757, 0.08630397141066615, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.10709981775693511, 0.09442089493391903, 0.13940905842178386, 0.021147456412443533, 0.020000000000000018, 0.03519100915515627, 0.0853544550159765, 0.09708986057473967, 0.07777632521977951, 0.1448261311760034, 0.13323277534341382, 0.12029901717760083, 0.07832668013240207, 0.07990382579724953, 0.06441298044051669, 0.13039844756345753, 0.10013374170338607, 0.1298445677979675, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12098907873350073, 0.08332913152150379, 0.11570403527630968, 0.06980596120805682, 0.10725752950449774, 0.13571584203323483, 0.14386510657607687, 0.13589170657517036, 0.16033375220406298, 0.04323777104705262, 0.03861702626351449, 0.03554851221433042, 0.02847489754586774, 0.035840071260028195, 0.04074106791963472, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010277298470296348, 0.012194993325675973, 0.010254708670136226, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07853040034643055, 0.0776664602603403, 0.07997749686909339, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.057073496574423066, 0.070037954256846, 0.05460550751282067, 0.12939044318781912, 0.10280212030544034, 0.10821790513598717, 0.07101969814099618, 0.03708762956808187, 0.05592810553342287, 0.0861586299863718, 0.07980430131796812, 0.1005419202795017, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.029109113833355904, 0.029939563514107315, 0.026429977169126984, 0.025489060009235343, 0.023162943949766035, 0.025918476423576253, 0.13967734505004514, 0.1322232387115625, 0.14040074175898454, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03233432113280488, 0.03753213255583987, 0.03609404914906511, 0.08072778292416571, 0.07986956631737951, 0.08430029781132409, 0.023814244174877097, 0.025461509552512362, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.009890871457180483, 0.007354325247402649, 0.010234331346775316, 0.007915327319814813, 0.11723738468626155, 0.1304161139054778, 0.11007108965768031, 0.0, 0.0, 0.0]}}
{"id": "9bb4103c-e767-4df9-9958-6022b4ced77a", "fitness": 0.042470496391623855, "name": "AdvancedAdaptiveDifferentialMigrationLE", "description": "Introduce a dynamic population replacement strategy with adaptive mutation scaling to enhance convergence speed and solution accuracy.", "code": "import numpy as np\n\nclass AdvancedAdaptiveDifferentialMigrationLE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.6 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.normal(0, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 36, "feedback": "The algorithm AdvancedAdaptiveDifferentialMigrationLE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04247 with standard deviation 0.08037.", "error": "", "parent_ids": ["9844ea8b-8ea8-4ed2-92cd-d78cc194e624"], "operator": null, "metadata": {"aucs": [0.08836375165201327, 0.08300522928671072, 0.13085959218805954, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.022921133803709193, 0.020000000000000018, 0.022315228685375832, 0.7772668992085076, 0.05326824029141619, 0.7265412280365338, 0.03772551537477564, 0.03369191505592739, 0.020000000000000018, 0.04209681495792661, 0.09681133249915608, 0.10280789871688223, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.13666670368749834, 0.129125337995751, 0.13846496350167314, 0.023779214777780444, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.11083411723254155, 0.07402806370622639, 0.14571722251998265, 0.14532220557062037, 0.1252045611884871, 0.08495543485449497, 0.08169643571784801, 0.07107687854769895, 0.12748156988662318, 0.09884378001066063, 0.12713986514334663, 0.020000000000000018, 0.020000000000000018, 0.05788840403464457, 0.14054696162639757, 0.0760988336939542, 0.10464573982547654, 0.10552709526886006, 0.06305210052984644, 0.06133160114510827, 0.14611775740439126, 0.14188549594006206, 0.15475448018179316, 0.04347938064227064, 0.03578827546700181, 0.029393656978159544, 0.026221063331097416, 0.03458650441673272, 0.041017203734857066, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027675589216613616, 0.02938497149495578, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010304038965074036, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07998845508781205, 0.07873211719894269, 0.06947103116173281, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.07119740286155751, 0.06341533504278629, 0.12322813996812043, 0.099155460023307, 0.10464546867182789, 0.06207829982701407, 0.038859631808177086, 0.05712920993962034, 0.08922662667464298, 0.08343043553315477, 0.09604936318466728, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.031330011784732825, 0.032781452762095764, 0.02821207303684259, 0.025536784688458436, 0.023728143384570632, 0.025377339047017222, 0.13264569586331343, 0.1344562619981824, 0.13670632571501273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03155776558395129, 0.03992221998646983, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011162431641798642, 0.009433151669329232, 0.007391477193524176, 0.010051284513262426, 0.007591647015283476, 0.11144320441440769, 0.1304161139054778, 0.10919098792292203, 0.0, 0.0, 0.0]}}
{"id": "d374a66f-f208-45a6-a811-73ec7a83382d", "fitness": 0.042514635515492755, "name": "AdvancedAdaptiveDifferentialMigrationLE", "description": "Enhance the adaptive mutation and crossover strategy by introducing guided exploration toward the best solution to improve convergence and solution accuracy.", "code": "import numpy as np\n\nclass AdvancedAdaptiveDifferentialMigrationLE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)  # Modified to guide mutation\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.6 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.normal(0, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 37, "feedback": "The algorithm AdvancedAdaptiveDifferentialMigrationLE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04251 with standard deviation 0.08040.", "error": "", "parent_ids": ["9bb4103c-e767-4df9-9958-6022b4ced77a"], "operator": null, "metadata": {"aucs": [0.08836375165201327, 0.08300522928671072, 0.13085959218805954, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.022921133803709193, 0.020000000000000018, 0.022315228685375832, 0.7772763319637979, 0.05326824029141619, 0.7265412280365338, 0.03772551537477564, 0.03369191505592739, 0.020000000000000018, 0.04209681495792661, 0.09681133249915608, 0.10280789871688223, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.13666670368749834, 0.129125337995751, 0.13846496350167314, 0.023779214777780444, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.11083411723254155, 0.07402806370622639, 0.14571722251998265, 0.14532220557062037, 0.1252045611884871, 0.08495543485449497, 0.08169643571784801, 0.07107687854769895, 0.12748156988662318, 0.09884378001066063, 0.12713986514334663, 0.020000000000000018, 0.020000000000000018, 0.05788840403464457, 0.14054696162639757, 0.0760988336939542, 0.10464573982547654, 0.10552709526886006, 0.06305210052984644, 0.06133160114510827, 0.14611775740439126, 0.13589170657517036, 0.15475448018179316, 0.04347938064227064, 0.03578827546700181, 0.029393656978159544, 0.02624903294270664, 0.034616071794115966, 0.041017203734857066, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027675589216613616, 0.02938497149495578, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010304038965074036, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07998061227330056, 0.07873211719894269, 0.06947103116173281, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.07119740286155751, 0.06466930764950729, 0.12322813996812043, 0.09921308883675695, 0.10464546867182789, 0.06207829982701407, 0.03850060310767234, 0.05712920993962034, 0.08922662667464298, 0.08328484926542723, 0.09604936318466728, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.031330011784732825, 0.032781452762095764, 0.027866719312096833, 0.02552096964412809, 0.02372995427321667, 0.025377339047017222, 0.13915274629809504, 0.1371318859334676, 0.13670632571501273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.032473795341348644, 0.040365543622892686, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011292806613968476, 0.010014655897381708, 0.007425983519209645, 0.010051284513262426, 0.007734785985316517, 0.1128495875007629, 0.1304161139054778, 0.11137413719603517, 0.0, 0.0, 0.0]}}
{"id": "f1426763-07e6-47f0-8386-1271b385d27f", "fitness": 0.035855941165610486, "name": "EnhancedDifferentialMigration", "description": "Introduce stochastic learning rates and adaptive population diversity strategies to enhance convergence speed and robustness against local optima.", "code": "import numpy as np\n\nclass EnhancedDifferentialMigration:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        stochastic_lr = np.random.uniform(0.001, 0.02)\n        F = stochastic_lr * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance)\n        CR = stochastic_lr * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._adaptive_population_replacement(func, fitness)\n\n        return self.best\n\n    def _adaptive_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.6 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.normal(0, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 38, "feedback": "The algorithm EnhancedDifferentialMigration got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03586 with standard deviation 0.04218.", "error": "", "parent_ids": ["d374a66f-f208-45a6-a811-73ec7a83382d"], "operator": null, "metadata": {"aucs": [0.09215398981716716, 0.08050829788528258, 0.13470715206467565, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02684288522060685, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07789000616501884, 0.0887312175487095, 0.05949048973683746, 0.03842063362476689, 0.020000000000000018, 0.020000000000000018, 0.0420957422856203, 0.09078479694278452, 0.09580319085070343, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.03541730574606927, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.13767410143059078, 0.10375510556444512, 0.1446970575163422, 0.023924569192310563, 0.020000000000000018, 0.020000000000000018, 0.08072694858327945, 0.0863116411064907, 0.07486341088122206, 0.1461937302385491, 0.13601756848317204, 0.12663779635909211, 0.08431683105388288, 0.08843656866622995, 0.06557902025507434, 0.12803652975765123, 0.10052960198007266, 0.1278641629758499, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.19249441058267958, 0.08357522428934483, 0.09218662900615582, 0.07581586466376933, 0.08141899454538315, 0.06272290886508869, 0.14620836253929292, 0.13850637017577927, 0.15475448018179316, 0.04364223244834031, 0.036765261250308434, 0.02657375009670604, 0.028045455246206052, 0.035552160206344885, 0.04172140250269618, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.011225882123801934, 0.03556370356588645, 0.034611616909260956, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010856759592771548, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07976344531027568, 0.07821325594979656, 0.06779028460366132, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05641628706513291, 0.069614181305935, 0.05395939349942802, 0.12385573392381188, 0.09974155829298137, 0.1045158507688706, 0.06268794275561729, 0.038335705958932764, 0.05414061053173613, 0.08696422032188866, 0.08055045732262045, 0.09968673378790982, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.030367434007232408, 0.032578659804929244, 0.027237024856878955, 0.025645658976070962, 0.023115690229087882, 0.028434534214676566, 0.13185458194505073, 0.13123409970527322, 0.1433595186271206, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.03166088988302318, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03338726802380765, 0.03753213255583987, 0.034215952184717624, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.010007123073578317, 0.00805659801989167, 0.010051284513262426, 0.007591647015283476, 0.12030342258201432, 0.1304161139054778, 0.10449493478821081, 0.0, 0.0, 0.0]}}
{"id": "809dc9c1-fc68-4452-b264-298f24f1ae64", "fitness": 0.03618334213514827, "name": "EnhancedAdaptiveDifferentialMigration", "description": "Introduce environmental selection pressures by incorporating fitness diversity and adaptive learning rates to enhance convergence robustness and solution quality.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigration:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = 0.5 * (1 + np.tanh(fitness_variance)) * np.random.rand() + 0.5\n        CR = 0.8 * (1 - fitness_variance) + 0.1 * np.random.rand()\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.6 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.normal(0, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        # Introduce diversity preserving mechanism\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        diversity_indices = self._preserve_diversity(combined_population, combined_fitness)\n        best_indices = diversity_indices[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n        \n    def _preserve_diversity(self, population, fitness):\n        sorted_indices = np.argsort(fitness)\n        elite_indices = sorted_indices[:self.population_size // 2]\n        remaining_indices = sorted_indices[self.population_size // 2:]\n        np.random.shuffle(remaining_indices)\n        diversity_indices = np.concatenate((elite_indices, remaining_indices[:self.population_size // 2]))\n        return diversity_indices", "configspace": "", "generation": 39, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigration got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03618 with standard deviation 0.04293.", "error": "", "parent_ids": ["d374a66f-f208-45a6-a811-73ec7a83382d"], "operator": null, "metadata": {"aucs": [0.0911363009683882, 0.09065845374807013, 0.19285746289471573, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.036680845774171744, 0.03831997092227657, 0.020000000000000018, 0.022006777715720904, 0.020000000000000018, 0.020000000000000018, 0.05330726655721074, 0.04578235187164781, 0.055819380758839854, 0.020000000000000018, 0.0663183903198733, 0.020000000000000018, 0.0502928520794137, 0.09708155335338409, 0.08102841730505905, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.13656279363420087, 0.13921116014930324, 0.14714130415736493, 0.025279057506280456, 0.020000000000000018, 0.038091373494540615, 0.09899743773406788, 0.0940927971432618, 0.0895431737884107, 0.14495757317778812, 0.14695346819131017, 0.12439450711038913, 0.07592053237995344, 0.08195694181814839, 0.07131707331602188, 0.13097513773517522, 0.11056841481755453, 0.12191976134240401, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1480413481217121, 0.08729613911498912, 0.0982710438025961, 0.07225171760314941, 0.075992721293479, 0.06531721714090077, 0.14473560328757518, 0.13589170657517036, 0.15475448018179316, 0.047782291303498425, 0.03392426887360889, 0.02740078020105874, 0.0274286956980333, 0.033282580553781305, 0.04174985421478017, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.011142167905969225, 0.01269350828803173, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012150569561146463, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08079951335042679, 0.08012916936476233, 0.07541536088309986, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05806951401312954, 0.06933912966594946, 0.05395939349942802, 0.12390133709821327, 0.09810048159078477, 0.10627947769005974, 0.06212889060473392, 0.04362441512012272, 0.05556840093200743, 0.08886118052367498, 0.07939764703207786, 0.09887486354877695, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.030876631112040087, 0.03167355028043661, 0.02716925124909575, 0.025360080823472853, 0.02346545789094745, 0.028665038652783803, 0.1370841617532118, 0.13655798558610344, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030333655821893513, 0.02989668063988815, 0.0, 0.0, 0.0, 0.03270055623381074, 0.03753213255583987, 0.03426829600135961, 0.08099279898401368, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.02424674653206027, 0.02597125382412213, 0.06700315798755696, 0.058590564779008814, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.009737888070539324, 0.007354325247402649, 0.010051284513262426, 0.008790383402653812, 0.11266399911795077, 0.1304161139054778, 0.11126058916559689, 0.0, 0.0, 0.0]}}
{"id": "61e19228-9f85-4616-a7af-bb21b7de50c8", "fitness": 0.03576282560128764, "name": "CooperativeAdaptiveDifferentialMigration", "description": "Integrate a cooperative co-evolutionary strategy and adaptive parameter control to enhance exploration and exploitation in high-dimensional search spaces.", "code": "import numpy as np\n\nclass CooperativeAdaptiveDifferentialMigration:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n        self.subcomponent_size = max(1, dim // 5)  # Divide dimensions into subcomponents\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        dynamic_factor = np.random.rand()\n        F = self.learning_rate * dynamic_factor + 0.5 * (1 + 0.1 * fitness_variance)\n        CR = self.learning_rate * dynamic_factor + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.6 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.normal(0, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 40, "feedback": "The algorithm CooperativeAdaptiveDifferentialMigration got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03576 with standard deviation 0.04164.", "error": "", "parent_ids": ["d374a66f-f208-45a6-a811-73ec7a83382d"], "operator": null, "metadata": {"aucs": [0.08662598653177045, 0.08491190523003045, 0.13597376667579852, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.026663904993784793, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.09998337588775708, 0.0708524447872958, 0.05376284078525828, 0.020000000000000018, 0.02682701992374148, 0.020000000000000018, 0.0476633364793656, 0.08819775663806961, 0.08167584877274503, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.04744108966852423, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.11901234456578602, 0.11259460792995013, 0.15530914244632643, 0.02741418427765263, 0.020267175680844463, 0.020000000000000018, 0.08072694858327945, 0.0863116411064907, 0.08183944067326987, 0.14099193798852805, 0.1422491549952568, 0.1208377166927268, 0.07500008993587892, 0.0833333354164596, 0.07093631149999713, 0.1389323465835729, 0.09667460717539611, 0.13144845544935924, 0.020000000000000018, 0.020000000000000018, 0.03189572474900615, 0.10819636878037442, 0.08689278348941398, 0.0895133107273659, 0.0715247176151621, 0.08438303894248667, 0.057180456460320106, 0.1603867929198981, 0.16731616763621848, 0.15475448018179316, 0.04610296058717833, 0.03392426887360889, 0.03430582817359917, 0.02710386005001808, 0.033454140380956954, 0.040611207749288836, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.030415058062216116, 0.029629600187736527, 0.03361524313101871, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.015052201288315525, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08053457131118591, 0.0776332918118604, 0.0686486622739213, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0558531353681303, 0.06862433477622076, 0.054125669942782784, 0.12323083150551395, 0.10164966686915022, 0.10537019795899072, 0.06300956875169617, 0.03795246302396438, 0.05509173459481875, 0.08629472595298737, 0.08132835957282825, 0.09277068755988638, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.031028373823447475, 0.03315021161766096, 0.0267465466859248, 0.025542560741073972, 0.023532344260025706, 0.02839621855646035, 0.1388483557764748, 0.1355101246348428, 0.14052462703396362, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03274747357300811, 0.0403360717254988, 0.034215952184717624, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010132361659671685, 0.010618216925484814, 0.009433151669329232, 0.007369111652019589, 0.010051284513262426, 0.007596092850668756, 0.1125276983722564, 0.1304161139054778, 0.10733980579631153, 0.0, 0.0, 0.0]}}
{"id": "8bf6d1e8-0e2c-46a2-94fb-5d12e3dbc4fe", "fitness": 0.03853639745516852, "name": "AdvancedAdaptiveDifferentialMigrationLE", "description": "Introduce a small mutation scaling factor to enhance exploration without deviating significantly from the existing strategy.", "code": "import numpy as np\n\nclass AdvancedAdaptiveDifferentialMigrationLE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a) + 0.005 * (np.random.rand(self.dim) - 0.5)  # Introduced small mutation scaling\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.6 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.normal(0, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 41, "feedback": "The algorithm AdvancedAdaptiveDifferentialMigrationLE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03854 with standard deviation 0.05923.", "error": "", "parent_ids": ["d374a66f-f208-45a6-a811-73ec7a83382d"], "operator": null, "metadata": {"aucs": [0.09424848988543044, 0.08297069130105028, 0.1313726839006012, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020041648952355606, 0.02169827905122601, 0.021605861362334333, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.6541884443421967, 0.15023232927742414, 0.06419905139989257, 0.020000000000000018, 0.03112705332575416, 0.020000000000000018, 0.04149745848390651, 0.10007910485894345, 0.08835433528871717, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.11912525701581667, 0.11304288705694854, 0.13165118660038888, 0.02343195035383272, 0.020000000000000018, 0.020000000000000018, 0.08072694858327945, 0.09928793661059199, 0.08114274227647977, 0.13866113569116723, 0.13323277534341382, 0.11880741032167264, 0.0748177837621502, 0.07990382579724953, 0.08077860928544922, 0.12016515454112209, 0.11490101305030864, 0.12509727045036778, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1482024710883436, 0.08551336478575022, 0.10047104153015651, 0.10125746079474685, 0.06899234283455413, 0.056492493752391515, 0.1484356551067958, 0.13746473127544623, 0.15475448018179316, 0.04999698587569468, 0.03392426887360889, 0.030826163983294963, 0.027454019855847056, 0.03476903486612226, 0.042002009017962916, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.031398185105422804, 0.01836079660097012, 0.024041295728481793, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.011281039673938231, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.01194949498791742, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07893245927225967, 0.08046197400432609, 0.06903242122961306, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05849323647296112, 0.06829764581334108, 0.06306133785331125, 0.12415414364774746, 0.09959956955852822, 0.10712256289828559, 0.062051045794548565, 0.03823406333761126, 0.05537559766350397, 0.08470028936337615, 0.08217309382447535, 0.09521290128174043, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.03289530400392848, 0.03222874234546125, 0.026634239478461663, 0.02578177309705354, 0.0233999295499262, 0.026801302615836575, 0.13392250683285323, 0.13619703405631933, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03406289658344863, 0.03753213255583987, 0.03465120371373265, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010190334510148946, 0.010618216925484814, 0.010556535090102681, 0.007354325247402649, 0.010051284513262426, 0.007644598090935983, 0.11323297275731281, 0.1304161139054778, 0.10669883815588943, 0.0, 0.0, 0.0]}}
{"id": "7ce61f0a-661b-4b78-83a2-1bdbdf128df0", "fitness": 0.04260687019350787, "name": "AdvancedAdaptiveDifferentialMigrationLE", "description": "Introduce dynamic learning rate adaptation and improved exploration through adaptive perturbation based on budget utilization.", "code": "import numpy as np\n\nclass AdvancedAdaptiveDifferentialMigrationLE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = 0.05 * (1 - self.eval_count / self.budget) + 0.01\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 42, "feedback": "The algorithm AdvancedAdaptiveDifferentialMigrationLE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04261 with standard deviation 0.08076.", "error": "", "parent_ids": ["d374a66f-f208-45a6-a811-73ec7a83382d"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772763632458853, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1438526290318487, 0.15615812512833827, 0.15475448018179316, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025123511879657667, 0.03442607084039284, 0.04233582575892825, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029001442019493506, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0790376085630684, 0.07761708387127353, 0.06959797363554598, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.058066182645662434, 0.12066694323872496, 0.09861997226191732, 0.10412992861596626, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08030967880949769, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027134114452203262, 0.026738046228849677, 0.023296039890530307, 0.025302061381791807, 0.13878268055227794, 0.13144189290443054, 0.14178701823587347, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.032499882451647255, 0.04033125431819495, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011219140077787682, 0.009991863894194042, 0.007419618365621661, 0.010051284513262426, 0.007698095895331658, 0.11238505162627432, 0.1304161139054778, 0.1154790351307663, 0.0, 0.0, 0.0]}}
{"id": "de44c6e7-1b73-41be-9d07-03245d25ec19", "fitness": 0.042569391802170536, "name": "EnhancedAdaptiveDifferentialMigrationLE", "description": "Enhance exploration and exploitation balance through adaptive control of mutation scale and selective survival to improve convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigrationLE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = 0.05 * (1 - self.eval_count / self.budget) + 0.01\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * np.sqrt(1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * np.sqrt(1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 43, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationLE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04257 with standard deviation 0.08061.", "error": "", "parent_ids": ["7ce61f0a-661b-4b78-83a2-1bdbdf128df0"], "operator": null, "metadata": {"aucs": [0.0870789704295919, 0.08485479183091937, 0.12942546026606971, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020131914521090644, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772763632458853, 0.05326824029141619, 0.7265412280365338, 0.03409640516799639, 0.03458344264109725, 0.020000000000000018, 0.033348435478049665, 0.09487991418576514, 0.07459100656595608, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1334277105611037, 0.12909707461941167, 0.1433380065455364, 0.020293703331349167, 0.02269487237438339, 0.023651374890268806, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.14422550943651657, 0.13676098116823565, 0.12492353128833256, 0.07593426727500241, 0.08352960663080011, 0.07551806081972956, 0.1172798866350524, 0.09507292234662856, 0.12612619212001075, 0.020000000000000018, 0.020000000000000018, 0.09122679571746628, 0.18650994190124304, 0.08269853731281951, 0.10205199580592716, 0.09305932275458151, 0.06731576647891235, 0.06418725236536826, 0.1438526290318487, 0.14436131932495389, 0.15475448018179316, 0.04366260218088769, 0.03903038092283817, 0.02787102570518163, 0.02538377610737752, 0.03579865707697438, 0.04337739477382596, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.02916845357189668, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0104739266992937, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08041345731526728, 0.07910589242260224, 0.07061109643835595, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06147825531143891, 0.05726735380070036, 0.12171154608901802, 0.09880971017879747, 0.10538337435651368, 0.06260737627714119, 0.03620910205209815, 0.05583827495591076, 0.08496243741362841, 0.0801674709392185, 0.09811616565227288, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.030126080765674357, 0.03296089059289509, 0.02781966964358773, 0.02554213948573969, 0.023643737978286672, 0.025593259691804593, 0.14083172857766613, 0.13479713819180494, 0.14178701823587347, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.032499882451647255, 0.04033125431819495, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011219140077787682, 0.009991863894194042, 0.007419618365621661, 0.010051284513262426, 0.007698095895331658, 0.11238505162627432, 0.1304161139054778, 0.1154790351307663, 0.0, 0.0, 0.0]}}
{"id": "0fc10d31-a1d9-43e4-9c3d-198654a64546", "fitness": 0.039042333767672, "name": "AdvancedAdaptiveDifferentialMigrationLE", "description": "Enhance diversity through improved crossover strategy and boosted exploration by fine-tuning dynamic perturbations.", "code": "import numpy as np\n\nclass AdvancedAdaptiveDifferentialMigrationLE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.2 * fitness_variance)  # Increased variance factor\n        CR = self.learning_rate * np.random.rand() + 0.85  # Adjusted crossover probability\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = 0.05 * (1 - self.eval_count / self.budget) + 0.01\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.15 + 0.5 * (1 - self.eval_count / self.budget)  # Adjusted sigma for exploration\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.9 * (1 - self.eval_count / self.budget)  # Increased perturbation\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 44, "feedback": "The algorithm AdvancedAdaptiveDifferentialMigrationLE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03904 with standard deviation 0.06327.", "error": "", "parent_ids": ["7ce61f0a-661b-4b78-83a2-1bdbdf128df0"], "operator": null, "metadata": {"aucs": [0.08652995710174394, 0.08424189209297173, 0.12930888811774988, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020842356028505704, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.08343145169755561, 0.04466327904329548, 0.7303543319781531, 0.03392309670225391, 0.03440848928771245, 0.020000000000000018, 0.038488189263308104, 0.08891962920488927, 0.07459100656595608, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.13235197599452908, 0.09586065648129671, 0.1455940543056119, 0.020621346185705547, 0.02155998185901775, 0.020792271570197585, 0.08245736705764528, 0.10159018258712615, 0.09592604507574687, 0.13787465763806295, 0.13833483721680295, 0.12958082017069505, 0.07397628851339977, 0.08400862693287425, 0.08081795195047992, 0.13281230906975994, 0.09782836549505647, 0.13066644622346468, 0.020000000000000018, 0.020000000000000018, 0.0910194964449148, 0.1901271660281768, 0.08352271973809611, 0.10288209625910572, 0.09212096090543054, 0.06629155007691301, 0.07472793987558235, 0.14964167965154673, 0.14313815431092647, 0.15475448018179316, 0.04975989973634287, 0.03816363171838977, 0.027323664047651186, 0.025241797090433837, 0.0350147576599934, 0.04422581967674544, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.022653484821051006, 0.01249858323197417, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010512088763302674, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07929594492635805, 0.07825579462909305, 0.06998838620238101, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05459995317003097, 0.06147825531143891, 0.05811167716704413, 0.12061170589719694, 0.09765816874576227, 0.10576964335054573, 0.0643842892249773, 0.03777279165797909, 0.05645135553241942, 0.0863106186264766, 0.07975364316840317, 0.09564116688930935, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.029827948086734724, 0.03211950935099628, 0.027435395454053935, 0.026903472539754447, 0.02344477978516535, 0.02542507057950716, 0.13185458194505073, 0.13513462422608613, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03600848654745681, 0.03753213255583987, 0.03765418813886978, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.009734563493259185, 0.007354325247402649, 0.010051284513262426, 0.007594494917643302, 0.11322172526137919, 0.1304161139054778, 0.10942848044107079, 0.0, 0.0, 0.0]}}
{"id": "8f6bb968-5a32-44b9-bceb-5d01e9df76e3", "fitness": 0.03703704726680074, "name": "EnhancedAdvancedAdaptiveDifferentialMigrationLE", "description": "Integrate a self-organizing map-inspired mechanism for local search refinement and adaptive diversity maintenance for improved solution precision.", "code": "import numpy as np\n\nclass EnhancedAdvancedAdaptiveDifferentialMigrationLE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def _adaptive_diversity(self, func, fitness):\n        som_radius = max(1, int(self.population_size * (1 - self.eval_count / self.budget)))\n        for i in range(self.population_size):\n            local_indices = np.argsort(np.linalg.norm(self.population - self.population[i], axis=1))[:som_radius]\n            local_best_index = local_indices[np.argmin(fitness[local_indices])]\n            direction = self.population[local_best_index] - self.population[i]\n            step_size = self.learning_rate * np.random.rand() * np.linalg.norm(direction)\n            self.population[i] += step_size * (direction / np.linalg.norm(direction))\n            self.population[i] = np.clip(self.population[i], self.lb, self.ub)\n            trial_fitness = func(self.population[i])\n            self.eval_count += 1\n            if trial_fitness < fitness[i]:\n                fitness[i] = trial_fitness\n                if trial_fitness < fitness[np.argmin(fitness)]:\n                    self.best = self.population[i]\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = 0.05 * (1 - self.eval_count / self.budget) + 0.01\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._adaptive_diversity(func, fitness)\n\n        return self.best", "configspace": "", "generation": 45, "feedback": "The algorithm EnhancedAdvancedAdaptiveDifferentialMigrationLE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03704 with standard deviation 0.07980.", "error": "", "parent_ids": ["7ce61f0a-661b-4b78-83a2-1bdbdf128df0"], "operator": null, "metadata": {"aucs": [0.08253869520449197, 0.07602287599886492, 0.12111305630268887, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772763632458853, 0.05326824029141619, 0.7265412280365338, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023245346750239593, 0.08594148016760361, 0.057929235093625686, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.09994410407015442, 0.12909707461941167, 0.13084903150896143, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.12990506644848054, 0.13323277534341382, 0.1160417955509998, 0.060118881882904374, 0.07990382579724953, 0.05610455226583122, 0.11205586464406281, 0.08680635918842228, 0.12191976134240401, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.10819636878037442, 0.0760988336939542, 0.0895133107273659, 0.06527120746322768, 0.04616535580723691, 0.056492493752391515, 0.1438526290318487, 0.13589170657517036, 0.15475448018179316, 0.038669801718039354, 0.03392426887360889, 0.02657375009670604, 0.0149680458268594, 0.021239429471109794, 0.029412867290113498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017347082641185563, 0.018480366424691863, 0.02779198263301763, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06728604198002841, 0.06517649397731107, 0.05739802026005103, 0.0, 0.0, 0.0, 0.052967038260013366, 0.05201516646173898, 0.04713316009526469, 0.10979052912033627, 0.09842292829438049, 0.09377498874368939, 0.048048844482986075, 0.027267572409494956, 0.04440552542498888, 0.06968626516247634, 0.06958276044853962, 0.07886365650931315, 0.0, 0.0, 0.0, 0.018233322045474232, 0.019235518721189893, 0.016262006849088628, 0.015046053680494387, 0.01262069732077109, 0.014789901286254237, 0.13000407512754164, 0.13855838157015743, 0.13316491109263384, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.032499882451647255, 0.04033125431819495, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011219140077787682, 0.009991863894194042, 0.007419618365621661, 0.010051284513262426, 0.007698095895331658, 0.11238505162627432, 0.1304161139054778, 0.1154790351307663, 0.0, 0.0, 0.0]}}
{"id": "b4f273bf-5ea0-414b-b9d2-0b5eec807284", "fitness": 0.04173948840543322, "name": "EnhancedAdaptiveDifferentialMigrationST", "description": "Incorporate an adaptive inertia factor and stochastic tunneling to enhance convergence speed and escape local optima in high-dimensional spaces.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigrationST:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n        self.inertia_factor = 0.9\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def _stochastic_tunneling(self, trial_fitness, current_fitness):\n        delta_f = trial_fitness - current_fitness\n        temperature = 1 / (1 + np.exp(-delta_f))\n        if np.random.rand() < temperature:\n            return True\n        return False\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = 0.05 * (1 - self.eval_count / self.budget) + 0.01\n            self.inertia_factor = 0.9 - 0.8 * (self.eval_count / self.budget)\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n\n                if trial_fitness < fitness[i] or self._stochastic_tunneling(trial_fitness, fitness[i]):\n                    self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * self.inertia_factor\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 46, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationST got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04174 with standard deviation 0.08006.", "error": "", "parent_ids": ["7ce61f0a-661b-4b78-83a2-1bdbdf128df0"], "operator": null, "metadata": {"aucs": [0.09335208075488322, 0.08198827510954687, 0.13452040055062264, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772763632458853, 0.05629221173676213, 0.7265412280365338, 0.03194153735824523, 0.03383362247326882, 0.020000000000000018, 0.0474169563474226, 0.09547333451529516, 0.06523897681438795, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12202815403155143, 0.10105118269649227, 0.15181319011503436, 0.020111120625659562, 0.020000000000000018, 0.020000000000000018, 0.10860875086484467, 0.10170468553535628, 0.07715836798429787, 0.1381914263322308, 0.1401450610380115, 0.1185661494719299, 0.07351300363132396, 0.07990382579724953, 0.07384809242559376, 0.11594856734736902, 0.09639964610897522, 0.12946068445369885, 0.020000000000000018, 0.020000000000000018, 0.027668016865110334, 0.14934068829083713, 0.08012613133277602, 0.10675426399400156, 0.07255694641648569, 0.059391585682629144, 0.09343620179296264, 0.1438526290318487, 0.13589170657517036, 0.15475448018179316, 0.043159737889990835, 0.037036919747546304, 0.027380750441896584, 0.027913676324432535, 0.03531316757039349, 0.04228917281059952, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.04020858216252865, 0.020678115379033413, 0.011260982013449161, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.013451738622504816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08120864348608725, 0.07972629941596021, 0.06841720019760666, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05459995317003097, 0.06826201122370668, 0.06114143739533229, 0.12251766487482418, 0.09806598073234152, 0.10854505648549095, 0.06152415144310153, 0.040009998268129676, 0.057583723138518894, 0.08602251281037843, 0.08599929293194608, 0.09745563398446988, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.030513353441280477, 0.03113554137938379, 0.027251452897932293, 0.02543092127121427, 0.023514573979873887, 0.027385979082557133, 0.13206819088384236, 0.13608312884155138, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.033540108900473764, 0.03753213255583987, 0.03588953830361363, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.010008569556717029, 0.007354325247402649, 0.010051284513262426, 0.0077210299262102655, 0.11494166607385936, 0.1304161139054778, 0.10616500698788311, 0.0, 0.0, 0.0]}}
{"id": "e64476e6-f8ac-4319-b910-78a6ffd8c6e4", "fitness": 0.04261107691341587, "name": "AdvancedAdaptiveDifferentialMigrationLE", "description": "Introduce diversity-enhancing strategy through periodic reinitialization of a portion of the population to improve global exploration.", "code": "import numpy as np\n\nclass AdvancedAdaptiveDifferentialMigrationLE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = 0.05 * (1 - self.eval_count / self.budget) + 0.01\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 47, "feedback": "The algorithm AdvancedAdaptiveDifferentialMigrationLE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04261 with standard deviation 0.08076.", "error": "", "parent_ids": ["7ce61f0a-661b-4b78-83a2-1bdbdf128df0"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772763632458853, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1438526290318487, 0.15615812512833827, 0.15475448018179316, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.026606333167813134, 0.03435661850729954, 0.042051776184358514, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029652857497364127, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07899672006554326, 0.07762948050820873, 0.07011712253369962, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.058066182645662434, 0.12070031667744474, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08088201076496371, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.02705920384048721, 0.025380071576639462, 0.023687384711497783, 0.025302061381791807, 0.13860969531925704, 0.13123409970527322, 0.14178701823587347, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.032499882451647255, 0.04033125431819495, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011219140077787682, 0.009991863894194042, 0.007419618365621661, 0.010051284513262426, 0.007698095895331658, 0.11238505162627432, 0.1304161139054778, 0.1154790351307663, 0.0, 0.0, 0.0]}}
{"id": "58d84849-c41a-47a4-ae76-078eda83e28c", "fitness": 0.042429091388823795, "name": "AdvancedAdaptiveDifferentialMigrationLE", "description": "Add a mechanism to enhance local exploitation by introducing a dynamic learning rate adjustment based on the fitness improvement rate.", "code": "import numpy as np\n\nclass AdvancedAdaptiveDifferentialMigrationLE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            fitness_improvement = (np.max(fitness) - np.min(fitness)) / np.max(fitness)\n            self.learning_rate = 0.05 * fitness_improvement * (1 - self.eval_count / self.budget) + 0.01\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 48, "feedback": "The algorithm AdvancedAdaptiveDifferentialMigrationLE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04243 with standard deviation 0.08062.", "error": "", "parent_ids": ["e64476e6-f8ac-4319-b910-78a6ffd8c6e4"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772763489558587, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.08701210327607578, 0.08311204573035891, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1438526290318487, 0.15615812512833827, 0.15475448018179316, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.02660928032840748, 0.03435661850729954, 0.04205194989349237, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029652832206175983, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07899672006554326, 0.0776295870996393, 0.07011776587490981, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05803147923829299, 0.12069479258994942, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08031530262302988, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027131503337911034, 0.025380071576639462, 0.023687381310450895, 0.025302061381791807, 0.13716604461357707, 0.1313625889651181, 0.1385981823357938, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03252497351175265, 0.04048024831243846, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011294937155880702, 0.010014314907776378, 0.007425902630016301, 0.010051284513262426, 0.007734225731751643, 0.11352569910994548, 0.1304161139054778, 0.10714165140998655, 0.0, 0.0, 0.0]}}
{"id": "53d962a0-a073-4f7e-99ed-144b695cb3d5", "fitness": 0.04264958893418233, "name": "EnhancedAdaptiveDifferentialMigration", "description": "Enhance global exploration by including adaptive mutation scaling based on population diversity and dynamically adjusting the learning rate to balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigration:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 49, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigration got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04265 with standard deviation 0.08081.", "error": "", "parent_ids": ["e64476e6-f8ac-4319-b910-78a6ffd8c6e4"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764051006694, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1438526290318487, 0.1580040689945994, 0.1651813961152584, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.026617307086838515, 0.03435661850729954, 0.042053944088699224, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.02965421160057302, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07899672006554326, 0.07763647486041225, 0.07011873249134382, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05802425013962076, 0.12069057232272129, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08066161903096569, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.02704862553562326, 0.025380071576639462, 0.023687392470434387, 0.025302061381791807, 0.1342962566917204, 0.13200392627091107, 0.13848544843376398, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.032563045365826326, 0.040096931256073276, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011214032346829828, 0.00998794297152883, 0.007419118329694108, 0.010051284513262426, 0.007697457172142097, 0.11198768095024969, 0.1304161139054778, 0.11920814896679355, 0.0, 0.0, 0.0]}}
{"id": "3e5bc7aa-b332-4664-a85b-ecc63edfa527", "fitness": 0.042565650789711355, "name": "EnhancedAdaptiveDifferentialMigration", "description": "Introduce a small adaptive scaling factor to the mutation process to enhance convergence precision without increasing exploration.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialMigration:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        adaptive_factor = 0.01 # small adaptive scaling factor\n        mutant = a + F * (b - c) + 0.1 * (self.best - a) * (1 + adaptive_factor)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 50, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigration got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04257 with standard deviation 0.08071.", "error": "", "parent_ids": ["53d962a0-a073-4f7e-99ed-144b695cb3d5"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764999554389, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1438526290318487, 0.15615812512833827, 0.15637337541085627, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.0266214611155422, 0.03435661850729954, 0.042055686738602716, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029654906736009568, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07899672006554326, 0.07763467135879898, 0.07012329168344522, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.058514830168307275, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08078266101869414, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027043434417486578, 0.025380071576639462, 0.02368768277428268, 0.025302061381791807, 0.13621696178406384, 0.13123409970527322, 0.13866941066984362, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03270599644229122, 0.040077544840146695, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011215599172706758, 0.009990003295825889, 0.007419260695262442, 0.010051284513262426, 0.007700203624484536, 0.1117424867008664, 0.1304161139054778, 0.10991921271226834, 0.0, 0.0, 0.0]}}
{"id": "362d6198-5b79-4f83-9956-cbe33afc8d0d", "fitness": 0.04265056114682264, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Introduce a dynamic niching mechanism to adaptively maintain diversity in the population by clustering solutions and reducing premature convergence risk.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 51, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04265 with standard deviation 0.08081.", "error": "", "parent_ids": ["53d962a0-a073-4f7e-99ed-144b695cb3d5"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764051006694, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1438526290318487, 0.1580040689945994, 0.1651813961152584, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03448118027608982, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08041442961279233, 0.07763647486041225, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05802425013962076, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.02704862553562326, 0.02546478200982516, 0.024158193542157758, 0.025302061381791807, 0.1342962566917204, 0.13129500960797114, 0.13848544843376398, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.032563045365826326, 0.040096931256073276, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011214032346829828, 0.00998794297152883, 0.007419118329694108, 0.010051284513262426, 0.007697457172142097, 0.11198768095024969, 0.1304161139054778, 0.11920814896679355, 0.0, 0.0, 0.0]}}
{"id": "a1aa0a28-cfe1-4745-9ad4-26fd850ec6a0", "fitness": 0.0361067556042925, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Slightly adjust the mutation strategy to enhance exploration by incorporating a small perturbation factor.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        perturbation = np.random.normal(0, 0.01, self.dim)  # New line with small perturbation\n        mutant = a + F * (b - c) + 0.1 * (self.best - a) + perturbation\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 52, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03611 with standard deviation 0.04221.", "error": "", "parent_ids": ["362d6198-5b79-4f83-9956-cbe33afc8d0d"], "operator": null, "metadata": {"aucs": [0.09448515559760606, 0.08328215233506342, 0.12246701317121689, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02042086803234977, 0.020866163534058346, 0.02296209531435278, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.10027649221906954, 0.06482837255792473, 0.14383237903487478, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.044336383206627694, 0.09685048167096033, 0.09131132253866181, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12254719647975865, 0.10901884629661329, 0.1407388801138274, 0.029366673641042507, 0.020000000000000018, 0.020000000000000018, 0.11111412121950726, 0.09019038903729892, 0.09977235771513226, 0.13435978003773674, 0.14264641950141543, 0.12297825100264137, 0.06915650252550565, 0.07990382579724953, 0.07403194879659059, 0.13308188929431652, 0.09882457880574547, 0.1261897328485725, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.151789686720339, 0.08693033376348813, 0.10059069355724604, 0.07187289061871027, 0.0750724020978315, 0.05691570911044297, 0.1443642337579406, 0.13589170657517036, 0.15475448018179316, 0.04638964260926459, 0.03392426887360889, 0.03583899116536282, 0.02620994144805422, 0.03372436999465833, 0.04309914329284292, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.019758957500442542, 0.03301927086831058, 0.04523557907263376, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.011333111109805172, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07857783966538279, 0.07790340848338384, 0.06827003662565534, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05703797822405643, 0.06501708990853772, 0.05395939349942802, 0.12594764536528513, 0.09920713290966165, 0.10648918264959872, 0.06398679239972216, 0.03629792186065661, 0.05859695221882588, 0.0861121218489076, 0.07884693971156276, 0.09479426992942697, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.030260350933958735, 0.032652158705660406, 0.02721186292697242, 0.025680000913383183, 0.023772350094212746, 0.025512612421487946, 0.1390579863867224, 0.1429468472936335, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.033290956410165884, 0.03753213255583987, 0.04214008256899793, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010147055213945322, 0.011285048532397646, 0.009594020109527812, 0.008017377053543662, 0.010051284513262426, 0.008015213373380736, 0.11462226523934149, 0.1304161139054778, 0.10714053124892753, 0.0, 0.0, 0.0]}}
{"id": "31dcc9be-6318-4565-96c7-043cfd8106d2", "fitness": 0.042638007487993956, "name": "EnhancedAdaptiveDifferentialMigrationWithElitePreservation", "description": "Introduce an adaptive elite preservation mechanism coupled with an enhanced clustering approach to maintain solution diversity and increase convergence speed.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithElitePreservation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.01\n        self.eval_count = 0\n        self.elite_fraction = 0.1\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        # Elite preservation\n        elite_count = max(1, int(self.elite_fraction * self.population_size))\n        elite_indices = np.argsort(fitness)[:elite_count]\n        elite_solutions = self.population[elite_indices]\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1\n\n        # Reintegrating elite solutions into the population\n        self.population[:elite_count] = elite_solutions\n        fitness[:elite_count] = np.apply_along_axis(func, 1, elite_solutions)", "configspace": "", "generation": 53, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithElitePreservation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04264 with standard deviation 0.08081.", "error": "", "parent_ids": ["362d6198-5b79-4f83-9956-cbe33afc8d0d"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764051006694, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1438526290318487, 0.1580040689945994, 0.1651813961152584, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03443175853677449, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.02922418952002004, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07899672006554326, 0.07743204129440628, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06410353092962784, 0.05802425013962076, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.02704862553562326, 0.025380071576639462, 0.02375868833819117, 0.025302061381791807, 0.1342962566917204, 0.13123409970527322, 0.13848544843376398, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.032563045365826326, 0.040096931256073276, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011214032346829828, 0.00998794297152883, 0.007419118329694108, 0.010051284513262426, 0.007697457172142097, 0.11198768095024969, 0.1304161139054778, 0.11920814896679355, 0.0, 0.0, 0.0]}}
{"id": "d59cd50d-882a-4002-b2cd-5b0363bf0a1e", "fitness": 0.03899212103288646, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Introduce adaptive learning rate scaling based on fitness variance to balance exploration and exploitation more effectively.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min + 0.1 * np.var(fitness)\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 54, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03899 with standard deviation 0.06322.", "error": "", "parent_ids": ["362d6198-5b79-4f83-9956-cbe33afc8d0d"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.0669012289244193, 0.04362191251949521, 0.7275826370492389, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.09856432687058658, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.08448894247093874, 0.11106636494337008, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08268842295005885, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1439771624291375, 0.16080394960280042, 0.15475448018179316, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.026317975012394634, 0.03435661850729954, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.03809995986846959, 0.023018421752712626, 0.01947187432485331, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07899839996921754, 0.07743204129440628, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.058314505375719805, 0.06393018834262354, 0.05725989074079196, 0.12066694323872496, 0.09855641517082314, 0.10374234178370745, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.02767850657122617, 0.025873493709327455, 0.023293579697324263, 0.025302061381791807, 0.13185458194505073, 0.13123409970527322, 0.13875194145449687, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.033287414983193875, 0.03822373976253857, 0.034215952184717624, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.009651342755202297, 0.00740365816731714, 0.010051284513262426, 0.0076024113580966945, 0.11551616061952985, 0.1304161139054778, 0.11325800037209133, 0.0, 0.0, 0.0]}}
{"id": "808c4e8a-3299-44c7-99e0-08b9003038e3", "fitness": 0.04261266133259492, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Improve convergence by refining the mutation strategy using an alternate perturbation factor.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        # Modified line to refine mutation strategy\n        mutant = a + F * (b - c) + 0.05 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 55, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04261 with standard deviation 0.08074.", "error": "", "parent_ids": ["362d6198-5b79-4f83-9956-cbe33afc8d0d"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772716755211451, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1438526290318487, 0.15680939454901588, 0.15475448018179316, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03435661850729954, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08038654786361787, 0.07771824383568582, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.056857518240637006, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09604370755014768, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027246562855639422, 0.025431837510413446, 0.024098350956931025, 0.025302061381791807, 0.14017929535828488, 0.13367765748836435, 0.1395500618872716, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.038018275990722494, 0.04053593191489013, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.01116440475654612, 0.009905725723399983, 0.007406378420948823, 0.010051284513262426, 0.007591647015283476, 0.11350766026571657, 0.1304161139054778, 0.10710881976653897, 0.0, 0.0, 0.0]}}
{"id": "9df9cf7c-4b88-4683-82b9-d78e2b829b3f", "fitness": -Infinity, "name": "EnhancedAdaptiveDifferentialMigrationWithDynamicElitism", "description": "Enhance diversity through dynamic elitism and adaptive clustering to prevent premature convergence while ensuring efficient exploration and exploitation.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithDynamicElitism:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.01\n        self.eval_count = 0\n        self.elitism_rate_initial = 0.1\n        self.elitism_rate_min = 0.05\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            elitism_rate = self.elitism_rate_initial * (1 - self.eval_count / self.budget) + self.elitism_rate_min\n\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:\n                reinit_count = max(1, int(self.population_size * elitism_rate))\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._dynamic_clustering_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _dynamic_clustering_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    reinit_index = cluster_indices[np.random.randint(len(cluster_indices))]\n                    cluster_pop[reinit_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[reinit_index] = func(cluster_pop[reinit_index])\n                    self.eval_count += 1", "configspace": "", "generation": 56, "feedback": "An exception occurred: IndexError('index 26 is out of bounds for axis 0 with size 2').", "error": "IndexError('index 26 is out of bounds for axis 0 with size 2')", "parent_ids": ["362d6198-5b79-4f83-9956-cbe33afc8d0d"], "operator": null, "metadata": {}}
{"id": "db208c67-2e52-427b-b215-39af83799c49", "fitness": 0.04257654851145237, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Slightly adjusted the learning rate calculation to reduce premature convergence and balance exploration and exploitation.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            # Adjusted learning rate calculation\n            self.learning_rate = self.learning_rate_initial * (1 - (self.eval_count / self.budget)**0.5) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 57, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04258 with standard deviation 0.08072.", "error": "", "parent_ids": ["362d6198-5b79-4f83-9956-cbe33afc8d0d"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772763954333866, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1438526290318487, 0.1567459927148298, 0.15475448018179316, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03448113402732844, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08041453401789811, 0.07763651187024923, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05802379947351055, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027075738257641535, 0.02546454149695354, 0.024158273239785033, 0.025302061381791807, 0.13185458194505073, 0.13123409970527322, 0.1420746342128082, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.0325323260624687, 0.040486410167174025, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.01125137942297516, 0.010001578932347877, 0.007422775777153556, 0.010051284513262426, 0.00770026364643317, 0.1189629853914731, 0.1304161139054778, 0.10640171306312984, 0.0, 0.0, 0.0]}}
{"id": "4cd18508-f6f8-4120-bd30-ea18d6ae5fa1", "fitness": 0.04264908375742462, "name": "EnhancedAdaptiveDifferentialMigrationWithChaos", "description": "Integrate adaptive chaos-driven perturbations to explore diverse regions and escape local optima, enhancing convergence.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithChaos:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.01\n        self.eval_count = 0\n        self.chaos_beta = 0.7\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n                \n            if self.eval_count < self.budget:\n                self._chaotic_perturbation(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1\n\n    def _chaotic_perturbation(self, func, fitness):\n        chaos_map = np.random.rand(self.population_size, self.dim)\n        chaos_map = np.sin(np.pi * chaos_map)\n        chaotic_perturbation = (1 - self.chaos_beta) * chaos_map + self.chaos_beta * (self.best - self.population)\n        perturbed_population = self.population + chaotic_perturbation\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        improved_indices = new_fitness < fitness\n        self.population[improved_indices] = perturbed_population[improved_indices]\n        fitness[improved_indices] = new_fitness[improved_indices]\n        self.best = self.population[np.argmin(fitness)]", "configspace": "", "generation": 58, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithChaos got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04265 with standard deviation 0.08081.", "error": "", "parent_ids": ["362d6198-5b79-4f83-9956-cbe33afc8d0d"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764051006694, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1438526290318487, 0.1580040689945994, 0.1651813961152584, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.027397771566107565, 0.03440802531123954, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029001442019493506, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07899672006554326, 0.07743204129440628, 0.07084791332598206, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05802425013962076, 0.12066694323872496, 0.09855641517082314, 0.10368562443914242, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08124093541354283, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.02704862553562326, 0.025380071576639462, 0.023293579697324263, 0.025302061381791807, 0.1342962566917204, 0.13123409970527322, 0.13848544843376398, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.032563045365826326, 0.040096931256073276, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011214032346829828, 0.00998794297152883, 0.007419118329694108, 0.010051284513262426, 0.007697457172142097, 0.11198768095024969, 0.1304161139054778, 0.11920814896679355, 0.0, 0.0, 0.0]}}
{"id": "41ed2355-9f98-4390-a0a7-f95529c396de", "fitness": 0.03587744079852616, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Incorporate adaptive crossover probability adjustment based on individual fitness to improve balance between exploration and exploitation.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8 * (1 - np.min(fitness) / np.max(fitness))  # Changed line\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 59, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03588 with standard deviation 0.04194.", "error": "", "parent_ids": ["362d6198-5b79-4f83-9956-cbe33afc8d0d"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.10777690683897356, 0.05470610921297914, 0.05152035510891406, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12514542373447768, 0.09500861929147542, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.10885025329262565, 0.08646923162522202, 0.09023441643909291, 0.139314121872813, 0.1339278088878052, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.12759658151928388, 0.07739769564000143, 0.10665616944652856, 0.079748997916258, 0.0815003077963451, 0.07267907351081215, 0.1438526290318487, 0.13589170657517036, 0.15475448018179316, 0.05151908562632168, 0.03392426887360889, 0.02955704161521089, 0.025819860179809973, 0.03437631541561048, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.023167299209287617, 0.02141248108651539, 0.019558313907506175, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07899672006554326, 0.07743204129440628, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05892699141628299, 0.0654587909612886, 0.06293076404540898, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02971702578779556, 0.03057037351279357, 0.026986613399970327, 0.0251590199532562, 0.02318034907653288, 0.027149079019038513, 0.14342704288613972, 0.16510196773538088, 0.13841504371640856, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03920458902302537, 0.03764837000941712, 0.03426829600135961, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.00949268892323818, 0.007354325247402649, 0.010112495634945429, 0.0076935327442280155, 0.11715399755071121, 0.13100641055141027, 0.10841993580329967, 0.0, 0.0, 0.0]}}
{"id": "46f1db46-fc19-4b34-9f02-c538aceca149", "fitness": 0.04265056114682264, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Introduce a refined niching strategy by adjusting the reinitialization condition to improve diversity without compromising convergence speed.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 20) == 0:  # Adjusted periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 60, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04265 with standard deviation 0.08081.", "error": "", "parent_ids": ["362d6198-5b79-4f83-9956-cbe33afc8d0d"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764051006694, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1438526290318487, 0.1580040689945994, 0.1651813961152584, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03448118027608982, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08041442961279233, 0.07763647486041225, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05802425013962076, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.02704862553562326, 0.02546478200982516, 0.024158193542157758, 0.025302061381791807, 0.1342962566917204, 0.13129500960797114, 0.13848544843376398, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.032563045365826326, 0.040096931256073276, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011214032346829828, 0.00998794297152883, 0.007419118329694108, 0.010051284513262426, 0.007697457172142097, 0.11198768095024969, 0.1304161139054778, 0.11920814896679355, 0.0, 0.0, 0.0]}}
{"id": "40f54fee-3823-41fc-aff6-60da2819d859", "fitness": 0.04265056114682264, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Slightly increase the periodic reinitialization condition frequency to enhance exploration and prevent premature convergence.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 20) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04265 with standard deviation 0.08081.", "error": "", "parent_ids": ["362d6198-5b79-4f83-9956-cbe33afc8d0d"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764051006694, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1438526290318487, 0.1580040689945994, 0.1651813961152584, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03448118027608982, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08041442961279233, 0.07763647486041225, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05802425013962076, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.02704862553562326, 0.02546478200982516, 0.024158193542157758, 0.025302061381791807, 0.1342962566917204, 0.13129500960797114, 0.13848544843376398, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.032563045365826326, 0.040096931256073276, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011214032346829828, 0.00998794297152883, 0.007419118329694108, 0.010051284513262426, 0.007697457172142097, 0.11198768095024969, 0.1304161139054778, 0.11920814896679355, 0.0, 0.0, 0.0]}}
{"id": "0118f823-cc05-404d-88dc-dc3268c120b7", "fitness": 0.04265056114682264, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Introduce a dynamic F scaling based on the best solution's progress to enhance exploration and exploitation balance.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        progress_factor = 1.0 if self.eval_count == 0 else fitness[np.argmin(fitness)] / np.min(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness) * progress_factor\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 62, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04265 with standard deviation 0.08081.", "error": "", "parent_ids": ["362d6198-5b79-4f83-9956-cbe33afc8d0d"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764051006694, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1438526290318487, 0.1580040689945994, 0.1651813961152584, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03448118027608982, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08041442961279233, 0.07763647486041225, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05802425013962076, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.02704862553562326, 0.02546478200982516, 0.024158193542157758, 0.025302061381791807, 0.1342962566917204, 0.13129500960797114, 0.13848544843376398, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.032563045365826326, 0.040096931256073276, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011214032346829828, 0.00998794297152883, 0.007419118329694108, 0.010051284513262426, 0.007697457172142097, 0.11198768095024969, 0.1304161139054778, 0.11920814896679355, 0.0, 0.0, 0.0]}}
{"id": "73c5e7bb-c150-48fc-aac7-587ad0272e9f", "fitness": 0.03603298274008468, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Enhance mutation strategy by incorporating scaled random perturbations to improve exploration capability.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        perturbation = np.random.normal(0, 0.05, self.dim)  # Added random perturbation\n        mutant = a + F * (b - c) + 0.1 * (self.best - a) + perturbation  # Updated line\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 63, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03603 with standard deviation 0.04205.", "error": "", "parent_ids": ["362d6198-5b79-4f83-9956-cbe33afc8d0d"], "operator": null, "metadata": {"aucs": [0.09448515559760606, 0.08328215233506342, 0.12246701317121689, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02042086803234977, 0.020866163534058346, 0.02296209531435278, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.10027649221906954, 0.06482837255792473, 0.14383237903487478, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.044336383206627694, 0.09685048167096033, 0.09131132253866181, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12254719647975865, 0.10901884629661329, 0.1407388801138274, 0.029366673641042507, 0.020000000000000018, 0.020000000000000018, 0.11078237059225027, 0.09019038903729892, 0.09977235771513226, 0.13435978003773674, 0.14264641950141543, 0.12297825100264137, 0.06915650252550565, 0.07990382579724953, 0.07403194879659059, 0.13308188929431652, 0.09882457880574547, 0.1261897328485725, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.151789686720339, 0.08693033376348813, 0.10059069355724604, 0.07187289061871027, 0.0750724020978315, 0.05691570911044297, 0.1443642337579406, 0.13701259677278754, 0.15475448018179316, 0.04638964260926459, 0.03392426887360889, 0.03583899116536282, 0.02620994144805422, 0.03372436999465833, 0.04309914329284292, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.019758957500442542, 0.03301927086831058, 0.045237811263034944, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.011333111109805172, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07857783966538279, 0.07790340848338384, 0.06827003662565534, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05703797822405643, 0.06501708990853772, 0.05395939349942802, 0.12594764536528513, 0.09920713290966165, 0.10621432251725638, 0.06398679239972216, 0.03629792186065661, 0.05859695221882588, 0.0861121218489076, 0.07884622233717176, 0.09479426992942697, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.030260350933958735, 0.032652158705660406, 0.027216303811510745, 0.025680000913383183, 0.023794548763587886, 0.025512612421487946, 0.1332981633095095, 0.13597273242869123, 0.13633318144608708, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03241597692914311, 0.03753213255583987, 0.04214008256899793, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010143551866249156, 0.011302569277142571, 0.0095929369730261, 0.008051460112031061, 0.010051284513262426, 0.008009188990606764, 0.11197834535774642, 0.1304161139054778, 0.10678158859138631, 0.0, 0.0, 0.0]}}
{"id": "421bfa6f-e29b-4e2e-be1e-89ad333c8bf8", "fitness": 0.04265056114682264, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Introduce an adaptive learning rate based on fitness variance to enhance exploration and exploitation balance.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 64, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04265 with standard deviation 0.08081.", "error": "", "parent_ids": ["362d6198-5b79-4f83-9956-cbe33afc8d0d"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764051006694, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1438526290318487, 0.1580040689945994, 0.1651813961152584, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03448118027608982, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08041442961279233, 0.07763647486041225, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05802425013962076, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.02704862553562326, 0.02546478200982516, 0.024158193542157758, 0.025302061381791807, 0.1342962566917204, 0.13129500960797114, 0.13848544843376398, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.032563045365826326, 0.040096931256073276, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011214032346829828, 0.00998794297152883, 0.007419118329694108, 0.010051284513262426, 0.007697457172142097, 0.11198768095024969, 0.1304161139054778, 0.11920814896679355, 0.0, 0.0, 0.0]}}
{"id": "20f864ce-d7ed-4a46-9ea7-0df9478a094e", "fitness": 0.042843123526639416, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Refine and adjust learning rate bounds to balance exploration and exploitation dynamically.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005  # Adjusted learning rate minimum\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 65, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04284 with standard deviation 0.08109.", "error": "", "parent_ids": ["362d6198-5b79-4f83-9956-cbe33afc8d0d"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764011901584, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.15326608350552717, 0.15634082950174188, 0.1868157148114975, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03448107488521179, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08041469598101048, 0.07763655086074461, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05808677793413519, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027059608761976328, 0.025464082750396355, 0.024158396875949117, 0.025302061381791807, 0.13185458194505073, 0.13646214324842698, 0.14997119894897937, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03255565407540251, 0.04010022698611937, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011232240268557514, 0.00999711061094799, 0.007420183032002425, 0.010051284513262426, 0.00769881736621858, 0.11366417554982933, 0.1304161139054778, 0.11543043226202898, 0.0, 0.0, 0.0]}}
{"id": "9f1dcbff-2ca5-4587-949a-b2abac03132a", "fitness": 0.042598018446872964, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Slightly increased minimum learning rate to enhance search stability.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.007  # Adjusted learning rate minimum\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 66, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04260 with standard deviation 0.08074.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764027543606, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.15022144751061917, 0.15645860825084446, 0.15475448018179316, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03448111709920254, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08041458951280744, 0.07763652047057246, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.0580712221939611, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.0270552216002673, 0.025464362228795667, 0.024158315602162528, 0.025302061381791807, 0.13185458194505073, 0.13183830573683553, 0.13868950794974855, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03255154373725089, 0.04001348482574385, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011224996179423852, 0.009993464063334345, 0.007419761827963667, 0.010051284513262426, 0.007698274429879914, 0.11454927524828162, 0.1304161139054778, 0.11261880077345976, 0.0, 0.0, 0.0]}}
{"id": "5e082891-4574-4d90-b41b-0ada81936978", "fitness": 0.04284296458746247, "name": "EnhancedAdaptiveDifferentialMigrationWithDynamicCrowding", "description": "Integrating a dynamic crowding distance-based niching approach to enhance exploration capabilities and prevent premature convergence.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\n\nclass EnhancedAdaptiveDifferentialMigrationWithDynamicCrowding:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._dynamic_crowding_niching(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _dynamic_crowding_niching(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                distances = cdist(cluster_pop, cluster_pop)\n                crowding_distances = np.sum(np.sort(distances, axis=1)[:, :2], axis=1)\n                crowded_index = np.argmax(crowding_distances)\n                if cluster_fitness[crowded_index] > np.min(fitness):\n                    cluster_pop[crowded_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[crowded_index]] = func(cluster_pop[crowded_index])\n                    self.eval_count += 1", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithDynamicCrowding got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04284 with standard deviation 0.08109.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764011901584, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.15326608350552717, 0.15634082950174188, 0.1868157148114975, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03448107488521179, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08041137770346307, 0.07763655086074461, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05808677793413519, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027059608761976328, 0.025464082750396355, 0.024157256177637865, 0.025302061381791807, 0.13185458194505073, 0.13643227136206615, 0.14997119894897937, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03255565407540251, 0.04010022698611937, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011232240268557514, 0.00999711061094799, 0.007420183032002425, 0.010051284513262426, 0.00769881736621858, 0.11366417554982933, 0.1304161139054778, 0.11543043226202898, 0.0, 0.0, 0.0]}}
{"id": "94b5f5e6-64a7-4ba3-b975-3006b898e3b2", "fitness": 0.042843123526639416, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Enhance exploration by dynamically scaling the mutation factor based on population diversity.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005  # Adjusted learning rate minimum\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 68, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04284 with standard deviation 0.08109.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764011901584, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.15326608350552717, 0.15634082950174188, 0.1868157148114975, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03448107488521179, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08041469598101048, 0.07763655086074461, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05808677793413519, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027059608761976328, 0.025464082750396355, 0.024158396875949117, 0.025302061381791807, 0.13185458194505073, 0.13646214324842698, 0.14997119894897937, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03255565407540251, 0.04010022698611937, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011232240268557514, 0.00999711061094799, 0.007420183032002425, 0.010051284513262426, 0.00769881736621858, 0.11366417554982933, 0.1304161139054778, 0.11543043226202898, 0.0, 0.0, 0.0]}}
{"id": "94d8d420-741b-41e3-956b-96076ff08cff", "fitness": 0.03620849175437834, "name": "EnhancedAdaptiveDifferentialMigrationWithElitism", "description": "Integrate adaptive mutation scaling and elitist selection strategies to enhance convergence speed and solution accuracy.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithElitism:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        diversity_factor = np.std(fitness) / (np.mean(fitness) + 1e-6)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * diversity_factor)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._elitist_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _elitist_replacement(self, func, fitness):\n        new_population = self.best + np.random.normal(0, 0.1, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 69, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithElitism got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03621 with standard deviation 0.04223.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.11923886413446838, 0.07773696034045796, 0.15425509470995902, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.026585760689376348, 0.028823121264161422, 0.020000000000000018, 0.020000000000000018, 0.02229322727173222, 0.08338672150472448, 0.043319061502764655, 0.050271698969439726, 0.020000000000000018, 0.04011735190190935, 0.020000000000000018, 0.06215304465492266, 0.09058152385779406, 0.08842566009741915, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12541579651674195, 0.10071635975218529, 0.15345658997980194, 0.02440710586027961, 0.020000000000000018, 0.020000000000000018, 0.08696495218015499, 0.09527066942209439, 0.07015575039186472, 0.14463667101782507, 0.1370986212813312, 0.12440353428193984, 0.08396528904737943, 0.09406503952012724, 0.07016430485892766, 0.12149531372979194, 0.1197510476813255, 0.1321444811294049, 0.020000000000000018, 0.11631694209325905, 0.020000000000000018, 0.12055172671401448, 0.08418508989941942, 0.10018128823221639, 0.07157506085076759, 0.06586030294442868, 0.061026617304125064, 0.1438526290318487, 0.14721737346263364, 0.15475448018179316, 0.046159963587498964, 0.04015081472753124, 0.030115894377238384, 0.024815519232679106, 0.032452315780256535, 0.04630993207431089, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.011077671414240431, 0.014539112272500043, 0.014832013708665492, 0.010000000000000009, 0.010000000000000009, 0.010014322181917779, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012409125011057509, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0774781176425573, 0.08095024789569893, 0.07561254852748489, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.059871740272339435, 0.0736007955459399, 0.061125729461460554, 0.12495235944786887, 0.10285713807489627, 0.10670079250887765, 0.061647513388715214, 0.04664367874728881, 0.05646305997225032, 0.08222509665072297, 0.08167610438318673, 0.09693264411084068, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.029917139968586026, 0.032220319521049756, 0.02947827289449545, 0.025103736673561916, 0.022800765944416934, 0.025491620171630536, 0.13425110140719, 0.13152674816808996, 0.14322508399413603, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.002298516878846857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.042145481275802976, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03151836932658281, 0.03762290540943791, 0.034506678218584996, 0.08252237682287644, 0.08105651981494699, 0.08430029781132409, 0.02962970064371906, 0.024280910037565828, 0.02590229237562558, 0.06501016208547095, 0.05810062798472171, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011307835340688577, 0.01075161040979078, 0.007426847135432735, 0.010051284513262426, 0.007710285284630913, 0.1112398058338967, 0.1304161139054778, 0.11330641211649195, 0.0, 0.0, 0.0]}}
{"id": "ab5263bb-84be-4164-9d12-4a001c07fe11", "fitness": -Infinity, "name": "EnhancedAdaptiveDifferentialMigrationWithDynamicNiching", "description": "Introduce dynamic niching with adaptive convergence pressure and mutation diversity to enhance exploration-exploitation balance.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithDynamicNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        diversity = np.std(self.population, axis=0).mean()\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * np.var(fitness)) + 0.2 * np.std(fitness) / np.mean(fitness) + 0.1 * diversity\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._dynamic_niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _dynamic_niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1\n\n                # Add diversity pressure\n                sigma = 0.1 + 0.4 * (1 - self.eval_count / self.budget)\n                for idx in cluster_indices:\n                    if idx != best_cluster_index:\n                        cluster_pop[idx] += np.random.normal(0, sigma, self.dim)\n                        cluster_pop[idx] = np.clip(cluster_pop[idx], self.lb, self.ub)\n                        fitness[idx] = func(cluster_pop[idx])\n                        self.eval_count += 1", "configspace": "", "generation": 70, "feedback": "An exception occurred: IndexError('index 2 is out of bounds for axis 0 with size 2').", "error": "IndexError('index 2 is out of bounds for axis 0 with size 2')", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {}}
{"id": "13d9b4b7-04a1-44c2-87c5-2564d508827c", "fitness": 0.03598555342197276, "name": "AdaptiveNichingDifferentialMigration", "description": "Introduce adaptive niching and dual-phase learning rate dynamics for enhanced exploration-exploitation balance.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass AdaptiveNichingDifferentialMigration:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.1\n        self.learning_rate_min = 0.01\n        self.learning_rate_phase = 0.5\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_std = np.std(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.05 * fitness_std)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = (self.learning_rate_initial * (1 - self.eval_count / (self.budget * self.learning_rate_phase))\n                                  + self.learning_rate_min)\n\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._adaptive_population_replacement(func, fitness)\n\n            if self.eval_count < self.budget:\n                self._adaptive_niching_strategy(func, fitness)\n\n        return self.best\n\n    def _adaptive_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        perturbed_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _adaptive_niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                cluster_pop[best_cluster_index] = np.clip(\n                    self.best + np.random.normal(0, 0.1, self.dim),\n                    self.lb, self.ub\n                )\n                fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                self.eval_count += 1", "configspace": "", "generation": 71, "feedback": "The algorithm AdaptiveNichingDifferentialMigration got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03599 with standard deviation 0.04212.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.09047794601333492, 0.08252079785030153, 0.12917809182035678, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.11258655283997343, 0.042046672514024164, 0.06506306684909446, 0.020000000000000018, 0.03174908012606392, 0.020000000000000018, 0.037710734443657024, 0.09428068703565484, 0.08364219803065898, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.11748205778809595, 0.09506515808460902, 0.14624434672091324, 0.02007308615606751, 0.021714164773697586, 0.020000000000000018, 0.09268353456414558, 0.08742891699810518, 0.10286530281009387, 0.1484795854101536, 0.13771369598477745, 0.13192119264315894, 0.0815411032898975, 0.08211010583830625, 0.07819910140950526, 0.12944441234991388, 0.0970313267066768, 0.12651376885650967, 0.020000000000000018, 0.020000000000000018, 0.09926651585886614, 0.16677459611561818, 0.08037632994171562, 0.11697053740824148, 0.09417722852568522, 0.06587907155172457, 0.0594267024763897, 0.1438526290318487, 0.13589170657517036, 0.15475448018179316, 0.046380056412440585, 0.04046698309681773, 0.02800028991533421, 0.02624097429794825, 0.03355087307877502, 0.041963112284759796, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.016924648094223338, 0.016610898354714854, 0.033723629832486446, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010618324256095657, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07907454281945447, 0.07687409515366783, 0.06935560265960117, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06323177019317738, 0.0721492446662727, 0.05695996873951681, 0.1236378034849861, 0.09813001577760316, 0.10520166474338066, 0.06304715615908718, 0.03807007085591674, 0.05686813505860144, 0.0823682106683159, 0.07991393896382559, 0.09542048146680304, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0307183242785648, 0.03143457007205441, 0.03015004066971072, 0.025438169469214844, 0.023148204393551697, 0.02597303740089929, 0.13185458194505073, 0.13246604565758635, 0.1379880967285878, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03390166405704842, 0.03753213255583987, 0.03542032525798933, 0.08388683988030043, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011406903325833051, 0.010113198222324704, 0.007425116167357082, 0.010051284513262426, 0.007758633541156024, 0.11584021561453728, 0.1304161139054778, 0.10676820188886316, 0.0, 0.0, 0.0]}}
{"id": "e2288e7b-be19-4a77-9b3e-9a21dbd2e401", "fitness": 0.04284226636950767, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Enhanced population niching and introduction of a distance-based competition strategy within clusters.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005  # Adjusted learning rate minimum\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                distances = np.sum((cluster_pop - self.best) ** 2, axis=1)  # Added 1\n                best_cluster_index = np.argmin(cluster_fitness - 0.1 * distances)  # Adjusted 2\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 72, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04284 with standard deviation 0.08109.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764011901584, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.15326608350552717, 0.15634082950174188, 0.1868157148114975, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03448107488521179, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.080413039007456, 0.07743204129440628, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05808677793413519, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027059608761976328, 0.02551524756520862, 0.024158124546934667, 0.025302061381791807, 0.13185458194505073, 0.13643227136206615, 0.14997119894897937, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03255565407540251, 0.04010022698611937, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011232240268557514, 0.00999711061094799, 0.007420183032002425, 0.010051284513262426, 0.00769881736621858, 0.11366417554982933, 0.1304161139054778, 0.11543043226202898, 0.0, 0.0, 0.0]}}
{"id": "5d862ddc-f97c-41b1-a471-fc292cb03dc2", "fitness": 0.041870520938216374, "name": "AdvancedAdaptiveDifferentialMigrationWithStochasticRanking", "description": "Incorporate adaptive population scaling and stochastic ranking to enhance exploration-exploitation balance and avoid premature convergence.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass AdvancedAdaptiveDifferentialMigrationWithStochasticRanking:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.3 + 0.4 * fitness_variance + 0.2 * np.std(fitness) / (np.mean(fitness) + 1e-9)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._stochastic_population_scaling(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _stochastic_population_scaling(self, func, fitness):\n        scaling_factor = 0.6 + 0.4 * np.random.rand()\n        new_population = self.best + scaling_factor * np.random.normal(0, 1, (self.population_size, self.dim))\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, new_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, new_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 73, "feedback": "The algorithm AdvancedAdaptiveDifferentialMigrationWithStochasticRanking got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04187 with standard deviation 0.08001.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.09527823240351974, 0.08211704110359674, 0.12813909105294952, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023025587131139336, 0.020000000000000018, 0.020350093979051742, 0.022586455883615852, 0.020000000000000018, 0.020000000000000018, 0.7773321587445066, 0.05326824029141619, 0.7265412280365338, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.046849845880209906, 0.08594148016760361, 0.08953202628629808, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.13585299901044545, 0.12909707461941167, 0.1394814676868532, 0.02286192940162668, 0.02321807642466356, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09609722374104246, 0.1387407823344361, 0.13323277534341382, 0.12403097626352322, 0.06841949358294941, 0.07990382579724953, 0.07742603688173688, 0.13180037913598597, 0.09455387809466875, 0.12867518022635493, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.10819636878037442, 0.0849512742762818, 0.09014348430487584, 0.06697922550297153, 0.08649355944755432, 0.06018419441372447, 0.1438526290318487, 0.13589170657517036, 0.15475448018179316, 0.048621471991493914, 0.05096232326350958, 0.027457292560682744, 0.030332878180203582, 0.03551833214863609, 0.04345715393685168, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027207838298614284, 0.028594375198898958, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012674655334591112, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08123063646799156, 0.07881824558087058, 0.07030159716714535, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06385077419332319, 0.053998221039564975, 0.1244192301433854, 0.0972397146478623, 0.10439682091410207, 0.06287237105499599, 0.03894983808011887, 0.053936809511723194, 0.08743086389066179, 0.08396725424728724, 0.09391049553092146, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.034698129110182996, 0.0355330900221662, 0.030392314711536583, 0.025388690400261038, 0.02357504056313897, 0.029789393755067395, 0.13797191582012713, 0.13812871462736043, 0.1458548658567277, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.031418562099681524, 0.03753213255583987, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.009632260251558944, 0.0074259195940054035, 0.010051284513262426, 0.0077540514880657385, 0.11332121820097474, 0.1304161139054778, 0.10661514677864514, 0.0, 0.0, 0.0]}}
{"id": "ee032f34-d95a-4540-91db-0a5acf453afb", "fitness": 0.038786721285917154, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Dynamic Perturbation with Multi-Niche Reinforcement: Enhance population diversity and maintain multi-niche adaptability through adaptive perturbations and targeted exploitation.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005  # Adjusted learning rate minimum\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.6 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.9\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.2 + 0.4 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.6 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 4)  # Reduced cluster size for sharper niches\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03879 with standard deviation 0.06286.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08748556233163951, 0.08390961216581516, 0.12785378377507817, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.021047943662810265, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.08264184430139354, 0.044740124284007865, 0.7275826370492389, 0.020000000000000018, 0.033614275492463985, 0.020000000000000018, 0.0387539297160836, 0.1013022034372344, 0.07459100656595608, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12363023000631213, 0.09818480648822936, 0.14192792259096165, 0.022096972196533504, 0.020000000000000018, 0.02418418362206609, 0.08761055544774221, 0.10159018258712615, 0.09592604507574687, 0.13897735657620858, 0.13981659611446562, 0.13490955975872854, 0.07500526035145083, 0.07990382579724953, 0.07037772553090715, 0.12107156251226603, 0.09934612337212712, 0.12404419434443037, 0.020000000000000018, 0.020000000000000018, 0.09383561509173932, 0.18195982460788018, 0.0811080680666203, 0.109925246233604, 0.08729696997085024, 0.06673886277865293, 0.0580950364677687, 0.14977550894353964, 0.13589170657517036, 0.15475448018179316, 0.04908580581348143, 0.04512602689305112, 0.03235656394442288, 0.025925405557965076, 0.03479378391278687, 0.042055662551756856, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.02251275600676328, 0.012080840302583717, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010359100431578505, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07923091193532217, 0.07738180537841688, 0.06993315136757161, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.059037797788798696, 0.06292412587237095, 0.05835340083908758, 0.12025412207441333, 0.09797785084518551, 0.10299562758206893, 0.06355129145937044, 0.0393979544231603, 0.05618419397977592, 0.0828555611461631, 0.08142850718754746, 0.09660809525503145, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.030215793951601122, 0.03166941616836505, 0.027271829092283872, 0.02544660226660067, 0.02335978299955732, 0.025275267140685287, 0.1330381963827928, 0.13151665901897003, 0.13641250395931026, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.0332656429521383, 0.03753213255583987, 0.03442458765970258, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.009556088842049393, 0.007354325247402649, 0.010051284513262426, 0.007815331413518423, 0.11699083871336025, 0.1304161139054778, 0.10869868055269893, 0.0, 0.0, 0.0]}}
{"id": "7419183e-0b37-4991-95e5-58055b0bb507", "fitness": 0.04265056114682264, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Introduced a minor adjustment by tweaking the learning rate to enhance convergence speed.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.01  # Adjusted learning rate minimum\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 75, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04265 with standard deviation 0.08081.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764051006694, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1438526290318487, 0.1580040689945994, 0.1651813961152584, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03448118027608982, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08041442961279233, 0.07763647486041225, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05802425013962076, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.02704862553562326, 0.02546478200982516, 0.024158193542157758, 0.025302061381791807, 0.1342962566917204, 0.13129500960797114, 0.13848544843376398, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.032563045365826326, 0.040096931256073276, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011214032346829828, 0.00998794297152883, 0.007419118329694108, 0.010051284513262426, 0.007697457172142097, 0.11198768095024969, 0.1304161139054778, 0.11920814896679355, 0.0, 0.0, 0.0]}}
{"id": "cf99fab7-e06d-4203-8957-05b8cf807ee3", "fitness": 0.042843123526639416, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Enhance niching by allowing periodic reseeding of niche leaders to maintain diversity.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005  # Adjusted learning rate minimum\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 76, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04284 with standard deviation 0.08109.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764011901584, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.15326608350552717, 0.15634082950174188, 0.1868157148114975, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03448107488521179, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08041469598101048, 0.07763655086074461, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05808677793413519, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027059608761976328, 0.025464082750396355, 0.024158396875949117, 0.025302061381791807, 0.13185458194505073, 0.13646214324842698, 0.14997119894897937, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03255565407540251, 0.04010022698611937, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011232240268557514, 0.00999711061094799, 0.007420183032002425, 0.010051284513262426, 0.00769881736621858, 0.11366417554982933, 0.1304161139054778, 0.11543043226202898, 0.0, 0.0, 0.0]}}
{"id": "803b964e-4fa1-45b0-9dfb-e073b1a5c179", "fitness": 0.03889327943268125, "name": "EnhancedAdaptiveDifferentialMigrationWithHierarchicalNiching", "description": "Introduce adaptive learning rates and hierarchical clustering to optimize exploration and convergence in a dynamic population.", "code": "import numpy as np\nfrom sklearn.cluster import AgglomerativeClustering\n\nclass EnhancedAdaptiveDifferentialMigrationWithHierarchicalNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.1\n        self.learning_rate_min = 0.01\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        scale_factor = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance)\n        crossover_rate = self.learning_rate * np.random.rand() + 0.9\n        return scale_factor, crossover_rate\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._hierarchical_niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.05 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.5 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _hierarchical_niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        clustering = AgglomerativeClustering(n_clusters=num_clusters)\n        labels = clustering.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 77, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithHierarchicalNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03889 with standard deviation 0.06299.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08646079076581759, 0.08234045689696057, 0.12693323986097538, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02166405995496301, 0.02149812506644866, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.0825814917356048, 0.043037289713906524, 0.7275826370492389, 0.020000000000000018, 0.03296028544238827, 0.020000000000000018, 0.0333860784485972, 0.09282742375407504, 0.07814208113188958, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.04256964947437436, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1186420045455493, 0.09658914680750885, 0.14163992913460577, 0.02265369278434548, 0.020000000000000018, 0.020000000000000018, 0.09185249914251348, 0.10159018258712615, 0.09592604507574687, 0.14500216045430792, 0.13323277534341382, 0.12358982926275941, 0.07552362473335561, 0.0861163349909847, 0.07151148869390345, 0.1279284055463954, 0.092760858621046, 0.13630035268189, 0.020000000000000018, 0.020000000000000018, 0.0915858142986733, 0.16760076456548145, 0.0813043419306243, 0.11136206971193785, 0.08604691215287363, 0.06369706643185002, 0.056492493752391515, 0.15188239564870942, 0.15013655847308593, 0.155402527167268, 0.05036493227288674, 0.03496311770090044, 0.027060417198842446, 0.025730020037478707, 0.033689003351165536, 0.04123604643340639, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.023018421752712626, 0.012080840302583717, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07868546895178286, 0.07697956297468, 0.06918383008366091, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06516867190818232, 0.06767509123823912, 0.056288691174152805, 0.12337614697648458, 0.09872009267489945, 0.10436597841269812, 0.062429050785788975, 0.04004006519432768, 0.05628521490923877, 0.08381863846670967, 0.07990839155814766, 0.09597007626609211, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.029321301912351938, 0.03087433700262221, 0.02716723038144453, 0.025356601266273038, 0.024000251752879453, 0.025305703869898943, 0.1332073135988988, 0.13480007772285008, 0.1451608152461854, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03188507976047594, 0.03864055914353215, 0.03465475529815454, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.01010004686719601, 0.0074021608212901935, 0.010051284513262426, 0.007892774051116214, 0.12053266439736443, 0.1304161139054778, 0.1150845976873035, 0.0, 0.0, 0.0]}}
{"id": "0988e917-61c3-44e4-8f77-4a07b790a5aa", "fitness": 0.04283832154084239, "name": "EnhancedAdaptiveDifferentialMigrationWithDynamicNiching", "description": "Introduce dynamic niching with adaptive clustering and diversity preservation to enhance convergence and exploration balance.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithDynamicNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._adaptive_niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _adaptive_niching_strategy(self, func, fitness):\n        diversity_threshold = np.mean(np.std(self.population, axis=0))\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1 and np.mean(np.std(cluster_pop, axis=0)) > diversity_threshold:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithDynamicNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04284 with standard deviation 0.08109.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764011901584, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.15326608350552717, 0.15634082950174188, 0.1868157148114975, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.02661880056816046, 0.03435661850729954, 0.04205405409112839, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029654183259623212, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07899672006554326, 0.07763655086074461, 0.0701190407869865, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05808677793413519, 0.12068582499171043, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08057570849960394, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027059608761976328, 0.025380071576639462, 0.023687389493517563, 0.025302061381791807, 0.13185458194505073, 0.13643227136206615, 0.14997119894897937, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03255565407540251, 0.04010022698611937, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011232240268557514, 0.00999711061094799, 0.007420183032002425, 0.010051284513262426, 0.00769881736621858, 0.11366417554982933, 0.1304161139054778, 0.11543043226202898, 0.0, 0.0, 0.0]}}
{"id": "5e40bb93-9797-4028-b4ac-455c4b631dc0", "fitness": 0.04284282028160754, "name": "EnhancedAdaptiveDifferentialMigrationWithNichingV2", "description": "\"EnhancedAdaptiveDifferentialMigrationWithNiching v2: Introduce adaptive niching with dynamic clustering and enhanced perturbation to improve convergence.\"", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNichingV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._adaptive_niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _adaptive_niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        dynamic_niches = max(2, int(num_clusters * (1 - self.eval_count / self.budget)))\n        kmeans = KMeans(n_clusters=dynamic_niches, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(dynamic_niches):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                self.eval_count += 1", "configspace": "", "generation": 79, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNichingV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04284 with standard deviation 0.08109.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764011901584, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.15326608350552717, 0.15634082950174188, 0.1868157148114975, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.02592082385777761, 0.03435661850729954, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08041689160550547, 0.07743204129440628, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05808677793413519, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027059608761976328, 0.02539010418626586, 0.024422553041345108, 0.025302061381791807, 0.13185458194505073, 0.13643227136206615, 0.14997119894897937, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03255565407540251, 0.04010022698611937, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011232240268557514, 0.00999711061094799, 0.007420183032002425, 0.010051284513262426, 0.00769881736621858, 0.11366417554982933, 0.1304161139054778, 0.11543043226202898, 0.0, 0.0, 0.0]}}
{"id": "b04d6b1a-7983-42c9-b11a-92b7f9638a65", "fitness": 0.042809653010881084, "name": "EnhancedAdaptiveDifferentialMigrationWithCompetitiveHives", "description": "Improve the balance of exploration and exploitation using an adaptive multi-phase strategy that dynamically adjusts mutation scales and applies a competitive random hive mechanism.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithCompetitiveHives:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._competitive_hive_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _competitive_hive_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        exploration_factor = 0.5 * (1 - self.eval_count / self.budget)\n        competition_vector = np.random.uniform(-exploration_factor, exploration_factor, new_population.shape)\n        competitive_population = new_population + competition_vector\n        competitive_population = np.clip(competitive_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, competitive_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, competitive_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 80, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithCompetitiveHives got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04281 with standard deviation 0.08093.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08722974887089519, 0.0831542266615809, 0.12720174039933052, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02080463312515579, 0.02538284876074437, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764011901584, 0.05326824029141619, 0.7265412280365338, 0.020000000000000018, 0.0331744644208084, 0.020000000000000018, 0.03688991079620707, 0.10143202915543592, 0.06784305070995456, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12010211400339699, 0.12909707461941167, 0.14098274732026883, 0.02040051636723972, 0.020000000000000018, 0.021868528486385985, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.1413336772994762, 0.1355630278706096, 0.12696730473100504, 0.07357831434020734, 0.08635098437339206, 0.08263743455918982, 0.12617566497817057, 0.10122891624748998, 0.14565368255610633, 0.020000000000000018, 0.020000000000000018, 0.1001094926445808, 0.17303261317452, 0.08040927679151078, 0.1117204957410658, 0.08499703007530968, 0.0655921252307291, 0.056492493752391515, 0.15326608350552717, 0.1389914527406123, 0.1868157148114975, 0.051569059366393, 0.04358749788825789, 0.02657375009670604, 0.025822637816275473, 0.034328901108401366, 0.04155440969208113, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.02980053941363603, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010282613161813337, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08030078545641328, 0.0774058652462295, 0.06951985422824469, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06542457349168918, 0.06582985416837872, 0.05755174126587137, 0.12249035421859333, 0.097827796529664, 0.10349543756434099, 0.06426800799720012, 0.040269895920013954, 0.05725388962043565, 0.08329681541283174, 0.08059761974971735, 0.09673155140055001, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.029759219691103844, 0.03117409025542528, 0.027079052166838258, 0.02539842490441946, 0.02434181911164046, 0.02531116686045254, 0.13374198193319142, 0.13952312812221346, 0.14997119894897937, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03255565407540251, 0.04010022698611937, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011232240268557514, 0.00999711061094799, 0.007420183032002425, 0.010051284513262426, 0.00769881736621858, 0.11366417554982933, 0.1304161139054778, 0.11543043226202898, 0.0, 0.0, 0.0]}}
{"id": "5953acca-7988-4213-b2b3-c4a3e3348046", "fitness": 0.035652274811642, "name": "MultiModalAdaptiveDifferentialEvolution", "description": "Integrate a multi-modal exploration strategy with adaptive boundary management to enhance solution diversity and convergence.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass MultiModalAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        F = self.learning_rate * np.random.rand() + 0.5\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._adaptive_boundary_perturbation(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _adaptive_boundary_perturbation(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        perturbed_population = self.population + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 81, "feedback": "The algorithm MultiModalAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03565 with standard deviation 0.04148.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.11717216663806962, 0.07960283872500529, 0.15309228655812634, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.0256108554204042, 0.029216666393773072, 0.020000000000000018, 0.020000000000000018, 0.024405273754948498, 0.08299352072738042, 0.04333520468376395, 0.04956030618232932, 0.020000000000000018, 0.041294810659878944, 0.020000000000000018, 0.062411214420686645, 0.08948360463960592, 0.07746977814119516, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12387073084140576, 0.10117510878561409, 0.15541699971236855, 0.020000000000000018, 0.020000000000000018, 0.02687406908776413, 0.08072694858327945, 0.0863116411064907, 0.07638339910614245, 0.14330321757502618, 0.13889338599371537, 0.1160417955509998, 0.0891446127568617, 0.07990382579724953, 0.0711065636386522, 0.11205586464406281, 0.12335039597495523, 0.12371611106918234, 0.020000000000000018, 0.020000000000000018, 0.0888879949796515, 0.10819636878037442, 0.08394309393610966, 0.0895133107273659, 0.06527120746322768, 0.07287462609487538, 0.056492493752391515, 0.1438526290318487, 0.16548368736292385, 0.15475448018179316, 0.04924692300057498, 0.03392426887360889, 0.026995655716049782, 0.02416775099637991, 0.030955726136446926, 0.04597979929511642, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.01105945241154771, 0.015226474973908832, 0.015431760499914682, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07683011727993505, 0.08314019585872601, 0.09238144349350808, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05562625497328599, 0.06688630093663706, 0.057395617943990285, 0.11869262382913237, 0.09842851237709871, 0.10358907347956858, 0.05756835603815702, 0.05205943190171336, 0.05491775604816396, 0.0799139233666133, 0.07920648517033801, 0.09190563948596664, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02991293059707334, 0.03222755296573243, 0.029013048616713055, 0.024895242935785933, 0.022684876055077297, 0.024640285494433645, 0.1326593530272695, 0.13262145721263663, 0.13681630848734616, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0023729806419032773, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.04228650788520261, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03166952680253787, 0.03761477057946516, 0.034215952184717624, 0.08570832951452956, 0.08231563333005298, 0.08430029781132409, 0.030351162781220742, 0.024192448856156457, 0.02590229237562558, 0.06547520651988936, 0.058957931324835844, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011306977896837211, 0.010753027753991673, 0.007426949381767578, 0.010051284513262426, 0.007710433797327054, 0.11423107867744253, 0.1304161139054778, 0.10595974610860337, 0.0, 0.0, 0.0]}}
{"id": "0bfcc51b-bf19-44d3-80a8-ef3ffb5b146e", "fitness": 0.0428409690715148, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Slightly refine the niching strategy to improve the reinitialization procedure by reducing the perturbation magnitude.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005  # Adjusted learning rate minimum\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.05, self.dim), self.lb, self.ub)  # Reduced perturbation\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 82, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04284 with standard deviation 0.08109.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764011901584, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.15326608350552717, 0.15634082950174188, 0.1868157148114975, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025771571051613096, 0.03447976264236696, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08041234164944644, 0.07763655086074461, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05808677793413519, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08033644404713902, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027059608761976328, 0.025463982288121523, 0.02415848569228618, 0.025302061381791807, 0.13185458194505073, 0.13654656369164997, 0.14997119894897937, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03255565407540251, 0.04010022698611937, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011232240268557514, 0.00999711061094799, 0.007420183032002425, 0.010051284513262426, 0.00769881736621858, 0.11366417554982933, 0.1304161139054778, 0.11543043226202898, 0.0, 0.0, 0.0]}}
{"id": "a80c8069-98d9-428f-a15d-ebae8ae3b3a7", "fitness": 0.03934594676320633, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Introduce dynamic learning rate decay for improved exploration-exploitation balance.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005  # Adjusted learning rate minimum\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            # Changed the learning_rate adjustment (1 line)\n            self.learning_rate = self.learning_rate_initial * (1 - (self.eval_count / self.budget) ** 2) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03935 with standard deviation 0.06336.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.08337141925907643, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.08448894247093874, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1438526290318487, 0.15615812512833827, 0.15475448018179316, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03448116123592715, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08041449999803707, 0.0776364824641802, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05803513284326778, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027036485945913902, 0.025464537754904537, 0.024158247270632538, 0.025302061381791807, 0.13846679868540912, 0.13123409970527322, 0.13817972660688382, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.032574512673802736, 0.04059730899662872, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011187417640818209, 0.009973634926686348, 0.007406627517479003, 0.010051284513262426, 0.007695399124286406, 0.11400893753982055, 0.1304161139054778, 0.11939313440336208, 0.0, 0.0, 0.0]}}
{"id": "03b06211-b36c-498a-8fb1-a258aa5a1251", "fitness": 0.04273133442350728, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Refine and adjust learning rate bounds to balance exploration and exploitation dynamically with enhanced mutation strategy.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005  # Adjusted learning rate minimum\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.15 * (self.best - a)  # Slightly increase influence of best\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 84, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04273 with standard deviation 0.08089.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772811566424498, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.15818773881148895, 0.15628034007218328, 0.15475448018179316, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.034716343887673085, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08040739327039415, 0.07755490908678686, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.056663456428387105, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027029028696444746, 0.025478181922502507, 0.024112861610791292, 0.025302061381791807, 0.13185458194505073, 0.14608011466574344, 0.13802144197877597, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.0335465406282176, 0.04103504464583485, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.01129561459032502, 0.01044926058497564, 0.00742084993746106, 0.010051284513262426, 0.007841241259399445, 0.11948300833753034, 0.1304161139054778, 0.11374694589116341, 0.0, 0.0, 0.0]}}
{"id": "a8130003-4c33-4bd8-bd17-8ae20c2bfe98", "fitness": 0.04252702886212658, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Introduce a slight adaptation in the crossover strategy to potentially improve solution diversity and convergence speed.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005  # Adjusted learning rate minimum\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target * 0.5)  # Slight change in crossover to improve diversity\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04253 with standard deviation 0.08072.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7768251513981104, 0.05791372927518179, 0.7265609640308344, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12515955457149996, 0.13598244022939865, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.08448894247093874, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08268842295005885, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1438526290318487, 0.15671084638775423, 0.15475448018179316, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025950626742530902, 0.03451257468219582, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02847469455949636, 0.02299194888349898, 0.0270049320690241, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08035473242708402, 0.07743204129440628, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05941326049859097, 0.06393018834262354, 0.056663456428387105, 0.12066694323872496, 0.09954653501364774, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08106225057290473, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.03155014532798017, 0.03146931286774579, 0.027435156056804688, 0.02544395516426423, 0.023381154650348845, 0.025302061381791807, 0.13185458194505073, 0.13381735123606175, 0.1401350905931994, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.037286840160745904, 0.04225398242463985, 0.03563011557864115, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011276734160433066, 0.010499914182309777, 0.007391419958795664, 0.010051284513262426, 0.007881630824588481, 0.12170323120229454, 0.1304161139054778, 0.10617219341898232, 0.0, 0.0, 0.0]}}
{"id": "b481e843-d2d0-440f-ae4f-0669c127aef3", "fitness": -Infinity, "name": "EnhancedAdaptiveDifferentialMigrationWithAdaptiveNiching", "description": "Introduce adaptive niching and mutation strategies based on cluster diversity to enhance exploration and exploitation balance.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithAdaptiveNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        cluster_std = np.std(self.population, axis=0)\n        adaptive_mutation = np.random.normal(0, cluster_std, self.dim) * (1 - self.eval_count / self.budget)\n        mutant += adaptive_mutation\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._adaptive_niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _adaptive_niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    diversity_factor = np.std(cluster_pop) / np.mean(cluster_fitness)\n                    adaptive_replacement = np.random.normal(0, 0.1 * diversity_factor, self.dim)\n                    cluster_pop[best_cluster_index] = np.clip(self.best + adaptive_replacement, self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 86, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {}}
{"id": "9a3bc1e1-7042-4017-b0e1-6689d1f75611", "fitness": 0.04255689202597009, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Improved adaptive learning rate calculation by incorporating fitness normalization.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005  # Adjusted learning rate minimum\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        fitness_normalized = (fitness - np.min(fitness)) / (np.max(fitness) - np.min(fitness) + 1e-9)  # Added normalization step\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness_normalized) / np.mean(fitness_normalized)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04256 with standard deviation 0.08070.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764430909043, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.1438526290318487, 0.15615812512833827, 0.15475448018179316, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.0344922796881153, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08035366301620106, 0.07752734654074145, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.058535616878416796, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.0962114662731689, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027029028696444746, 0.025486678196691592, 0.02413353813489527, 0.025302061381791807, 0.13574216747204948, 0.13123409970527322, 0.13802144197877597, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03219014237122342, 0.04023125899542035, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010975148237696408, 0.009922985730719924, 0.0073808135386970175, 0.010051284513262426, 0.007665696463729832, 0.11171214490509596, 0.1304161139054778, 0.11054006070493771, 0.0, 0.0, 0.0]}}
{"id": "00f3bc28-17bd-4183-b104-f86cd9071d3e", "fitness": 0.013911094102605383, "name": "EnhancedAdaptiveDifferentialMigrationWithDynamicNiching", "description": "Integrate a dynamic niching strategy with adaptive differential evolution for enhanced exploration and exploitation balance.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithDynamicNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 5) == 0:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1\n            \n            # Introduce a slight perturbation to cluster members to escape local optima\n            if self.eval_count < self.budget:\n                perturbation = 0.1 * (1 - self.eval_count / self.budget)\n                for idx in cluster_indices:\n                    if self.eval_count >= self.budget:\n                        break\n                    perturbed = np.clip(cluster_pop[idx] + np.random.uniform(-perturbation, perturbation, self.dim), self.lb, self.ub)\n                    perturbed_fitness = func(perturbed)\n                    self.eval_count += 1\n                    if perturbed_fitness < cluster_fitness[idx]:\n                        self.population[idx] = perturbed\n                        fitness[idx] = perturbed_fitness\n                        if perturbed_fitness < fitness[np.argmin(fitness)]:\n                            self.best = perturbed", "configspace": "", "generation": 88, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithDynamicNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.01391 with standard deviation 0.09963.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764011901584, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.15326608350552717, 0.15634082950174188, 0.1868157148114975, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025123511879657667, 0.0344263958910449, 0.042322259412286733, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029001442019493506, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07903692614494662, 0.07761145805709757, 0.06959611104282493, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05808677793413519, 0.12067792947717404, 0.09860961671371471, 0.10409660842626767, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08023010979327494, 0.09597093307074178, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027134038725848297, 0.02673993452661838, 0.02329598618386386, 0.025302061381791807, 0.13185458194505073, 0.13643227136206615, 0.14997119894897937, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, -0.10000000000000009, -0.09999918048701706, -0.10000000000000009, -0.09000000000000008, -0.06000000000000005, -0.09000000000000008, -0.08000000000000007, -0.07000000000000006, -0.09000000000000008, -0.09000000000000008, -0.09000000000000008, -0.09000000000000008, -0.10000000000000009, -0.07000000000000006, -0.09000000000000008, -0.07000000000000006, -0.06000000000000005, -0.10000000000000009, -0.08000000000000007, -0.09000000000000008, -0.08000000000000007, -0.09000000000000008, -0.10000000000000009, -0.10000000000000009, -0.10000000000000009, -0.10000000000000009, -0.08000000000000007, -0.09000000000000008, -0.08000000000000007, -0.06000000000000005, -0.09000000000000008, -0.08000000000000007, -0.08000000000000007, -0.08000000000000007, -0.08000000000000007, -0.08000000000000007, -0.10000000000000009, -0.10000000000000009, -0.10000000000000009, -0.03833739574569872, -0.04724457611874611, -0.04728025618542708, -0.10000000000000009, -0.09000000000000008, -0.10000000000000009, -0.054037869467853605, -0.04552998221745885, -0.0400269573876153, -0.0015273259382193594, 0.01525918333715226, 0.002653589456706462, -0.057267417580669555, -0.07279777237747043, -0.05046277912780095, -0.027365641223255244, -0.0357416118778362, -0.02386844859829451, -0.10000000000000009, -0.08000000000000007, -0.10000000000000009, -0.08876443067120032, -0.06769974625152675, -0.08890871788256094, -0.08177653589992873, -0.08885543404708218, -0.08157186069935363, 0.03427990195683961, 0.052285385545484675, 0.03808059998047442, -0.10000000000000009, -0.09000000000000008, -0.09000000000000008]}}
{"id": "a09216d4-46aa-45f0-a31b-db0da9d5b47b", "fitness": -Infinity, "name": "AdvancedClusterAdaptiveMigration", "description": "Introduce adaptive cluster-based mutation and refined niching to enhance exploration and local exploitation balance in black-box optimization.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass AdvancedClusterAdaptiveMigration:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variability = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variability) + 0.2 * np.std(fitness) / (np.mean(fitness) + 1e-8)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._adaptive_population_replacement(func, fitness)\n\n            if self.eval_count < self.budget:\n                self._refined_niching_strategy(func, fitness)\n\n        return self.best\n\n    def _adaptive_population_replacement(self, func, fitness):\n        sigma_scale = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, sigma_scale, (self.population_size, self.dim))\n        perturbation_strength = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + perturbation_strength * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _refined_niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                for idx in cluster_indices:\n                    if self.eval_count >= self.budget:\n                        break\n                    mutated_trial = self._mutate(cluster_indices, 0.5)\n                    mutated_trial_fitness = func(mutated_trial)\n                    self.eval_count += 1\n                    if mutated_trial_fitness < fitness[idx]:\n                        self.population[idx] = mutated_trial\n                        fitness[idx] = mutated_trial_fitness\n                        if mutated_trial_fitness < fitness[np.argmin(fitness)]:\n                            self.best = mutated_trial", "configspace": "", "generation": 89, "feedback": "An exception occurred: ValueError('too many values to unpack (expected 3)').", "error": "ValueError('too many values to unpack (expected 3)')", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {}}
{"id": "ba8f1a7a-4452-457b-a557-fa8ea2a57d5e", "fitness": 0.04282853865918982, "name": "EnhancedAdaptiveDifferentialMigrationWithAdvancedNiching", "description": "Incorporate adaptive clustering and dynamic exploration balance to enhance convergence and robustness.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithAdvancedNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._advanced_niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _advanced_niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1\n\n        # Re-cluster to adapt exploration balance\n        if self.eval_count < self.budget:\n            num_clusters = max(2, int((self.population_size * (1 + 0.1 * (1 - self.eval_count / self.budget))) // 5))\n            kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n            labels = kmeans.fit_predict(self.population)\n            for cluster_id in range(num_clusters):\n                cluster_indices = np.where(labels == cluster_id)[0]\n                cluster_pop = self.population[cluster_indices]\n                if len(cluster_pop) > 1:\n                    cluster_pop = np.clip(cluster_pop + np.random.normal(0, 0.1, cluster_pop.shape), self.lb, self.ub)\n                    fitness[cluster_indices] = np.apply_along_axis(func, 1, cluster_pop)\n                    self.eval_count += len(cluster_indices)", "configspace": "", "generation": 90, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithAdvancedNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04283 with standard deviation 0.08109.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764011901584, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.15326608350552717, 0.15634082950174188, 0.1868157148114975, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03435661850729954, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029001442019493506, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07899672006554326, 0.07743204129440628, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05808677793413519, 0.12109544334934497, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08563324977710196, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027059608761976328, 0.025380071576639462, 0.023293579697324263, 0.025302061381791807, 0.13185458194505073, 0.13643227136206615, 0.14997119894897937, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03255565407540251, 0.04010022698611937, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011232240268557514, 0.00999711061094799, 0.007420183032002425, 0.010051284513262426, 0.00769881736621858, 0.11366417554982933, 0.1304161139054778, 0.11543043226202898, 0.0, 0.0, 0.0]}}
{"id": "df23a378-57ae-4736-8f00-5cbb1faa5808", "fitness": 0.04273133442350728, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Modify mutation strategy to enhance solution diversity and exploration potential.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005  # Adjusted learning rate minimum\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.15 * (self.best - a)  # Modified mutation strategy\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 91, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04273 with standard deviation 0.08089.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772811566424498, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.15818773881148895, 0.15628034007218328, 0.15475448018179316, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.034716343887673085, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08040739327039415, 0.07755490908678686, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.056663456428387105, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027029028696444746, 0.025478181922502507, 0.024112861610791292, 0.025302061381791807, 0.13185458194505073, 0.14608011466574344, 0.13802144197877597, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.0335465406282176, 0.04103504464583485, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.01129561459032502, 0.01044926058497564, 0.00742084993746106, 0.010051284513262426, 0.007841241259399445, 0.11948300833753034, 0.1304161139054778, 0.11374694589116341, 0.0, 0.0, 0.0]}}
{"id": "3dfc3ad9-006f-4e6d-bf79-28a3d90af244", "fitness": 0.04282637985950716, "name": "EnhancedAdaptiveDifferentialMigrationWithDynamicNiching", "description": "Enhanced learning rate adaptation with dynamic niching and population perturbations to improve convergence speed and diversity.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithDynamicNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                n_best = max(1, len(cluster_pop) // 3)\n                cluster_best_indices = np.argsort(cluster_fitness)[:n_best]\n                for idx in cluster_best_indices:\n                    cluster_pop[idx] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[idx]] = func(cluster_pop[idx])\n                    self.eval_count += 1", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithDynamicNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04283 with standard deviation 0.08109.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772764011901584, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.15326608350552717, 0.15634082950174188, 0.1868157148114975, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03435661850729954, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029001442019493506, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07899672006554326, 0.07743204129440628, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05808677793413519, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027059608761976328, 0.025380071576639462, 0.023293579697324263, 0.025302061381791807, 0.13185458194505073, 0.13643227136206615, 0.14997119894897937, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03255565407540251, 0.04010022698611937, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011232240268557514, 0.00999711061094799, 0.007420183032002425, 0.010051284513262426, 0.00769881736621858, 0.11366417554982933, 0.1304161139054778, 0.11543043226202898, 0.0, 0.0, 0.0]}}
{"id": "e5790488-500c-4286-90f2-8e393f9ed74a", "fitness": -Infinity, "name": "EnhancedAdaptiveDifferentialMigrationWithDynamicNiching", "description": "Incorporate an evolving clustering-based adaptive niching mechanism to enhance local search capabilities while maintaining global exploration.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithDynamicNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._dynamic_niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _dynamic_niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    for idx in cluster_indices:\n                        if idx != cluster_indices[best_cluster_index]:\n                            cluster_pop[idx] += 0.1 * np.random.randn(self.dim)\n                            cluster_pop[idx] = np.clip(cluster_pop[idx], self.lb, self.ub)\n                            fitness[idx] = func(cluster_pop[idx])\n                            self.eval_count += 1", "configspace": "", "generation": 93, "feedback": "An exception occurred: IndexError('index 16 is out of bounds for axis 0 with size 2').", "error": "IndexError('index 16 is out of bounds for axis 0 with size 2')", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {}}
{"id": "516cfb63-0863-488a-8319-0e319c91fbb1", "fitness": 0.03892605377044417, "name": "AdaptiveDifferentialPSOMigration", "description": "Introduce a multi-strategy learning mechanism by combining differential mutation and particle swarm concepts to adaptively balance exploration and exploitation.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass AdaptiveDifferentialPSOMigration:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.global_best = None\n        self.personal_best = np.copy(self.population)\n        self.velocity = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + 0.1 * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            self.personal_best[i] = trial\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n                if self.global_best is None or trial_fitness < func(self.global_best):\n                    self.global_best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        self.global_best = self.best\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n                # Particle Swarm component\n                self.velocity[i] = (\n                    0.7 * self.velocity[i]\n                    + 1.4 * np.random.rand() * (self.personal_best[i] - self.population[i])\n                    + 1.4 * np.random.rand() * (self.global_best - self.population[i])\n                )\n                self.population[i] = np.clip(self.population[i] + self.velocity[i], self.lb, self.ub)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 94, "feedback": "The algorithm AdaptiveDifferentialPSOMigration got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03893 with standard deviation 0.06448.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08612914305891861, 0.08180883142443207, 0.12402409543361526, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.022796920719286518, 0.020000000000000018, 0.023430346288511483, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7572405856457866, 0.06005593446108404, 0.07655684747993152, 0.020000000000000018, 0.030527396208224533, 0.020000000000000018, 0.04931835872345347, 0.08594148016760361, 0.07952053312762186, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.13089469046689606, 0.10044716682140509, 0.13368689601875927, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.08072694858327945, 0.0863116411064907, 0.07723478603166545, 0.14089266606103767, 0.13323277534341382, 0.12266603533218712, 0.07552518554259746, 0.07990382579724953, 0.07109985110015915, 0.11820236845903764, 0.08814167214928914, 0.13591221049938662, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.18302787909529938, 0.07292120586292017, 0.09599991795884, 0.08903467391782716, 0.06969537218052091, 0.056492493752391515, 0.16611917517935115, 0.13589170657517036, 0.15475448018179316, 0.04678888877755982, 0.037188242162974894, 0.02657375009670604, 0.027064553532614166, 0.032906851565652206, 0.0436743817039954, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06680892324428667, 0.02235679954989589, 0.058066089954170064, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07970500743905296, 0.08576844551114138, 0.06915688786656116, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.055296434508717573, 0.0664164203013553, 0.06579025760553647, 0.12410220944957406, 0.10379977408666918, 0.10353731469702443, 0.06383736694156017, 0.03842865407631679, 0.05554094614613059, 0.08185084275606913, 0.07884622233717176, 0.09721920573303788, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02821529510918619, 0.031002056024104663, 0.02749119339412187, 0.024895242935785933, 0.024000211153078266, 0.02539894825358968, 0.14219200728956938, 0.1491174835617275, 0.15588472174798995, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.030499991321740527, 0.03753213255583987, 0.0358438645518766, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010262756793778749, 0.010618216925484814, 0.009939749270507336, 0.007884272456932573, 0.010051284513262426, 0.007989610381719414, 0.11469780441079847, 0.1304161139054778, 0.11133128777443702, 0.0, 0.0, 0.0]}}
{"id": "8702b2f2-60d9-4efd-b8cb-24c6d7cc8cc2", "fitness": 0.03444081566023928, "name": "EnhancedAdaptiveDifferentialMigrationWithCooperativeNiching", "description": "Implement structured cooperative co-evolution by optimizing subgroups of dimensions in parallel, enhancing convergence through niche-based adaptive learning rates.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithCooperativeNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n        self.group_size = max(1, dim // 5)  # Divide dimensions into smaller subgroups for cooperative co-evolution\n\n    def _dynamic_parameters(self, fitness, subgroup):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F, subgroup):\n        a, b, c = self.population[indices]\n        mutant = a.copy()\n        mutant[subgroup] = a[subgroup] + F * (b[subgroup] - c[subgroup]) + 0.1 * (self.best[subgroup] - a[subgroup])\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR, subgroup):\n        crossover = np.random.rand(self.dim) < CR\n        crossover[subgroup] = True\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n\n            subgroups = [range(i, min(i + self.group_size, self.dim)) for i in range(0, self.dim, self.group_size)]\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                for subgroup in subgroups:\n                    F, CR = self._dynamic_parameters(fitness, subgroup)\n                    indices = [idx for idx in range(self.population_size) if idx != i]\n                    mutant = self._mutate(np.random.choice(indices, 3, replace=False), F, subgroup)\n                    trial = self._crossover(self.population[i], mutant, CR, subgroup)\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 95, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithCooperativeNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03444 with standard deviation 0.04027.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08253869520449197, 0.08158530434779176, 0.12111305630268887, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.022857595869758107, 0.020000000000000018, 0.02217722041342851, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07895005721532411, 0.04952397814118625, 0.08073774148722479, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.07597397724732469, 0.08594148016760361, 0.057929235093625686, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1231138929509834, 0.10623281857737565, 0.13084903150896143, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.08072694858327945, 0.09105103225069744, 0.07015575039186472, 0.13249469539453773, 0.1443188362296164, 0.1384676701201525, 0.07356052667823831, 0.07990382579724953, 0.06821434766040013, 0.11205586464406281, 0.10476143960980522, 0.12191976134240401, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.10819636878037442, 0.08722376894404049, 0.0895133107273659, 0.06527120746322768, 0.07167160177393361, 0.056492493752391515, 0.1468197049435832, 0.14602186013284812, 0.16605505026914869, 0.038669801718039354, 0.03392426887360889, 0.02657375009670604, 0.024565517351058497, 0.030955726136446926, 0.03910603137012658, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.01842334125267986, 0.016385718593170395, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07653964318343431, 0.07443205724495594, 0.07346847924754751, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05640767434173821, 0.06147825531143891, 0.06878091052267377, 0.11869262382913237, 0.09667501666422984, 0.10281775224528589, 0.05756835603815702, 0.04161431982127084, 0.053936809511723194, 0.07896945735270666, 0.07884622233717176, 0.08807105303750307, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.029822626412836284, 0.03622111797174454, 0.027103157301287983, 0.024962596079276977, 0.025573398560690075, 0.024640285494433645, 0.13185458194505073, 0.1335453054786393, 0.14243101524799295, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03311823872685349, 0.04331291052923414, 0.03650939484072058, 0.08072778292416571, 0.07974355253469767, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.009469467152884326, 0.007825060212838797, 0.010051284513262426, 0.007591647015283476, 0.11381477495680725, 0.1304161139054778, 0.11293027449087423, 0.0, 0.0, 0.0]}}
{"id": "cde16bab-e99a-47f8-aef7-a1871d9c4c33", "fitness": 0.036030212559539476, "name": "EnhancedAdaptiveDifferentialMigrationWithAdaptiveNiching", "description": "Introduce adaptive niching and dynamic scaling of mutation factors to enhance diversity and convergence in multi-modal landscapes.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithAdaptiveNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + F * (b - c) + np.random.uniform(0.1, 0.3) * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._adaptive_niching(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _adaptive_niching(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    adaptive_shift = np.random.uniform(0.05, 0.2) * (self.best - cluster_pop[best_cluster_index])\n                    cluster_pop[best_cluster_index] = np.clip(cluster_pop[best_cluster_index] + adaptive_shift, self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 96, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithAdaptiveNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03603 with standard deviation 0.04218.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08674124081184009, 0.08279539458356278, 0.13261927366443427, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.02719664620814599, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.0760574161273504, 0.0809119305886189, 0.08781731924355174, 0.041653927620843634, 0.020000000000000018, 0.020000000000000018, 0.038835946984728476, 0.10118167417794577, 0.06523466749497009, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12237549110703383, 0.10232362128130446, 0.15138803901976572, 0.023823899835749618, 0.020000000000000018, 0.020000000000000018, 0.08072694858327945, 0.09314440531273216, 0.0932529041381005, 0.14412729806964764, 0.1445462937236992, 0.1293424360437473, 0.07662430054010427, 0.07990382579724953, 0.07286696333717768, 0.12018809056794988, 0.11019187748382109, 0.1372510710248599, 0.020000000000000018, 0.020000000000000018, 0.020462454350587245, 0.1508507890211993, 0.09385169541336547, 0.09631354167210959, 0.06753784122391404, 0.09726649304458979, 0.08791643558663509, 0.1508817692335327, 0.13589170657517036, 0.15475448018179316, 0.059233500718068766, 0.036741958836930766, 0.0275853361105447, 0.029031323031593836, 0.03400888272141189, 0.04211250448470738, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.014771921574212588, 0.02251275600676328, 0.02890371547055859, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.011536416876584354, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07940352396246397, 0.07736885087088441, 0.06801243009408608, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.05892699141628299, 0.07112587176811713, 0.06798299284070641, 0.1229882672712107, 0.09844421129228753, 0.10420941308404896, 0.062047189286472904, 0.039888853822544146, 0.05439960425492307, 0.08604360704369451, 0.08060372037019914, 0.09278237074586893, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.03150374210900364, 0.03155070125486592, 0.029432980989672175, 0.025250939652846527, 0.02287645135548655, 0.02569159143923261, 0.13185458194505073, 0.14807416428992282, 0.1362397668968629, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03207937047830445, 0.03753213255583987, 0.034215952184717624, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.010389622153919587, 0.0073861732624994625, 0.010051284513262426, 0.007591647015283476, 0.12428811162843745, 0.1304161139054778, 0.10996939875761347, 0.0, 0.0, 0.0]}}
{"id": "2fc6640c-878d-4c5d-a22c-edebeaaba0e5", "fitness": 0.034656489502364676, "name": "RefinedAdaptiveDifferentialMigration", "description": "Enhance exploration and convergence by incorporating adaptive clustering and mutation based on fitness diversity.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass RefinedAdaptiveDifferentialMigration:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 3 * dim  # Increased population size for broader search\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        diversity_factor = np.std(fitness) / np.mean(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + diversity_factor)  # Adjusted F\n        CR = self.learning_rate * np.random.rand() + 0.8  # Consistent CR\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutation_vector = F * (b - c) + 0.1 * (self.best - a)\n        mutation_vector += np.random.normal(0, 0.1, self.dim)  # Added random mutation\n        mutant = a + mutation_vector\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        perturbed_population = new_population + np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=10)  # Increased n_init for stability\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 97, "feedback": "The algorithm RefinedAdaptiveDifferentialMigration got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03466 with standard deviation 0.04050.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.10150985022353565, 0.10871931985819838, 0.12111305630268887, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.026328134200817588, 0.021476816620658257, 0.024393273648868452, 0.020000000000000018, 0.02144990425992266, 0.020000000000000018, 0.05917010718029814, 0.0687000101711015, 0.05384300042492729, 0.023918021736954787, 0.020000000000000018, 0.020000000000000018, 0.025035830870257736, 0.08594148016760361, 0.0799048033988764, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.12425896891414567, 0.11806291189982177, 0.13767932490965407, 0.020120073172986586, 0.020000000000000018, 0.03227774948292905, 0.08072694858327945, 0.0863116411064907, 0.09439493988036707, 0.12990506644848054, 0.13323277534341382, 0.1218431588130483, 0.06179752923454518, 0.07990382579724953, 0.05967998823180265, 0.11205586464406281, 0.11386301028336565, 0.12191976134240401, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.10819636878037442, 0.08329118532076984, 0.1036604702270324, 0.0677162831653646, 0.04616535580723691, 0.06166853860087118, 0.15570364193617947, 0.1432657907135403, 0.15475448018179316, 0.038669801718039354, 0.03392426887360889, 0.030676352879577595, 0.03598505719886136, 0.03423368787068359, 0.0399142141400507, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010499427046716248, 0.013339425899535184, 0.01955909634542441, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.012252000097976001, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.07843067779979052, 0.07821504536562518, 0.06723045597131905, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.054799941877618163, 0.06180838992975057, 0.05395939349942802, 0.12004530203756669, 0.10052556392271828, 0.10338982593383095, 0.06071872093130504, 0.045551906259181596, 0.05498245528131507, 0.0883755224616144, 0.08284323703035268, 0.09188268466619232, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.030496572098197228, 0.03580949652188736, 0.026590521658305555, 0.025245098581063163, 0.028445458270497825, 0.02464859111870976, 0.1343718032217308, 0.13996407218701634, 0.14318583547692798, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.03332200680586128, 0.046343363419250605, 0.03567391839154599, 0.08072778292416571, 0.08039161768195524, 0.08430029781132409, 0.021347126797240046, 0.02433884464022551, 0.027679073480967542, 0.06501016208547095, 0.057942926538117945, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.009433151669329232, 0.007423215196630251, 0.010051284513262426, 0.007606799475879611, 0.11297078138527172, 0.1304161139054778, 0.10803324298620576, 0.0, 0.0, 0.0]}}
{"id": "c2ad9096-9e03-4702-bfc8-c9017cafe048", "fitness": 0.042560668251060606, "name": "EnhancedAdaptiveDifferentialMigrationWithNiching", "description": "Slightly adjust the scaling factor in mutation to enhance diversity and convergence.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005  # Adjusted learning rate minimum\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        mutant = a + 1.2 * F * (b - c) + 0.1 * (self.best - a)  # Slightly adjust the scaling factor\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:  # Periodic reinitialization condition\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1", "configspace": "", "generation": 98, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04256 with standard deviation 0.08073.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772899374770461, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.14882367212630354, 0.15753250465500657, 0.15475448018179316, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03450646402805657, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0803387971950168, 0.07750495500124177, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.056663456428387105, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.0963636848903664, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.02718824602314529, 0.02549457329346505, 0.024112822061942873, 0.025302061381791807, 0.13185458194505073, 0.13130527161252104, 0.13802144197877597, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.030499991321740527, 0.03841184610356674, 0.036428195381652984, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.010618216925484814, 0.009822288070418628, 0.007372700755524986, 0.010051284513262426, 0.007610951428424828, 0.11468055374740216, 0.1304161139054778, 0.1112348877979612, 0.0, 0.0, 0.0]}}
{"id": "86d332cf-86b0-4918-8039-d13a86c145f8", "fitness": 0.04262124467414485, "name": "EnhancedAdaptiveDifferentialMigrationWithChaoticNiching", "description": "Improve convergence by incorporating chaotic maps for enhanced diversity and adaptive mutation scaling.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveDifferentialMigrationWithChaoticNiching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population_size = 10 + 2 * dim\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, dim))\n        self.best = None\n        self.learning_rate_initial = 0.05\n        self.learning_rate_min = 0.005\n        self.eval_count = 0\n\n    def _dynamic_parameters(self, fitness):\n        fitness_variance = np.var(fitness)\n        F = self.learning_rate * np.random.rand() + 0.5 * (1 + 0.1 * fitness_variance) + 0.2 * np.std(fitness) / np.mean(fitness)\n        CR = self.learning_rate * np.random.rand() + 0.8\n        return F, CR\n\n    def _mutate(self, indices, F):\n        a, b, c = self.population[indices]\n        chaotic_factor = self._chaotic_map(self.eval_count / self.budget)\n        mutant = a + F * (b - c) + 0.1 * chaotic_factor * (self.best - a)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def _crossover(self, target, mutant, CR):\n        crossover = np.random.rand(self.dim) < CR\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover, mutant, target)\n        return trial\n\n    def _selection(self, fitness, trial, trial_fitness, i):\n        if trial_fitness < fitness[i]:\n            self.population[i] = trial\n            fitness[i] = trial_fitness\n            if trial_fitness < fitness[np.argmin(fitness)]:\n                self.best = trial\n\n    def _evaluate_population(self, func):\n        fitness = np.apply_along_axis(func, 1, self.population)\n        self.eval_count += self.population_size\n        self.best = self.population[np.argmin(fitness)]\n        return fitness\n\n    def __call__(self, func):\n        fitness = self._evaluate_population(func)\n\n        while self.eval_count < self.budget:\n            self.learning_rate = self.learning_rate_initial * (1 - self.eval_count / self.budget) + self.learning_rate_min\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F, CR = self._dynamic_parameters(fitness)\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                mutant = self._mutate(np.random.choice(indices, 3, replace=False), F)\n                trial = self._crossover(self.population[i], mutant, CR)\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                self._selection(fitness, trial, trial_fitness, i)\n\n            if self.eval_count < self.budget:\n                self._dynamic_population_replacement(func, fitness)\n\n            if self.eval_count % (self.budget // 10) == 0:\n                reinit_count = self.population_size // 5\n                self.population[:reinit_count] = np.random.uniform(self.lb, self.ub, (reinit_count, self.dim))\n\n            if self.eval_count < self.budget:\n                self._niching_strategy(func, fitness)\n\n        return self.best\n\n    def _dynamic_population_replacement(self, func, fitness):\n        dynamic_sigma = 0.1 + 0.5 * (1 - self.eval_count / self.budget)\n        new_population = self.best + np.random.normal(0, dynamic_sigma, (self.population_size, self.dim))\n        dynamic_perturbation = 0.7 * (1 - self.eval_count / self.budget)\n        perturbed_population = new_population + dynamic_perturbation * np.random.uniform(-1, 1, new_population.shape)\n        perturbed_population = np.clip(perturbed_population, self.lb, self.ub)\n        new_fitness = np.apply_along_axis(func, 1, perturbed_population)\n        self.eval_count += self.population_size\n\n        combined_population = np.vstack((self.population, perturbed_population))\n        combined_fitness = np.hstack((fitness, new_fitness))\n        best_indices = np.argsort(combined_fitness)[:self.population_size]\n        self.population = combined_population[best_indices]\n        fitness[:] = combined_fitness[best_indices]\n        self.best = self.population[np.argmin(fitness)]\n\n    def _niching_strategy(self, func, fitness):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            cluster_pop = self.population[cluster_indices]\n            cluster_fitness = fitness[cluster_indices]\n\n            if len(cluster_pop) > 1:\n                best_cluster_index = np.argmin(cluster_fitness)\n                if cluster_fitness[best_cluster_index] > np.min(fitness):\n                    cluster_pop[best_cluster_index] = np.clip(self.best + np.random.normal(0, 0.1, self.dim), self.lb, self.ub)\n                    fitness[cluster_indices[best_cluster_index]] = func(cluster_pop[best_cluster_index])\n                    self.eval_count += 1\n\n    def _chaotic_map(self, x):\n        # Use a logistic map for chaos: x_(n+1) = r * x_n * (1 - x_n)\n        r = 3.99  # parameter for chaotic behavior\n        return r * x * (1 - x)", "configspace": "", "generation": 99, "feedback": "The algorithm EnhancedAdaptiveDifferentialMigrationWithChaoticNiching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04262 with standard deviation 0.08077.", "error": "", "parent_ids": ["20f864ce-d7ed-4a46-9ea7-0df9478a094e"], "operator": null, "metadata": {"aucs": [0.08651595035363258, 0.08333501920072395, 0.12815791038630975, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020132046628436928, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.7772732006327508, 0.05326824029141619, 0.7265412280365338, 0.027634035558873227, 0.03369921806511389, 0.020000000000000018, 0.038488189263308104, 0.09282742375407504, 0.06780937252335462, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.023247102030956035, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.020000000000000018, 0.1249046422588529, 0.12909707461941167, 0.14376829965251292, 0.020895011162074817, 0.020000000000000018, 0.020000000000000018, 0.09857294095494029, 0.10159018258712615, 0.09592604507574687, 0.139314121872813, 0.14322470487215255, 0.13383593104002756, 0.06799539179188718, 0.08017675363673615, 0.07417424632606184, 0.12678372093048296, 0.096619094468119, 0.14612482958415796, 0.020000000000000018, 0.020000000000000018, 0.09471633776233768, 0.18312474413869817, 0.08303062902417002, 0.10851461854443412, 0.0903591228603764, 0.06540980021679044, 0.05907827455994319, 0.15169389695059887, 0.15615812512833827, 0.15475448018179316, 0.05151908562632168, 0.03392426887360889, 0.0292802906550671, 0.025819860179809973, 0.03435661850729954, 0.04181066333151273, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.027088170959974622, 0.029892432021599857, 0.03730440399033341, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.08035991792494646, 0.07775571295033135, 0.06959336017305862, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.06230999177390928, 0.06393018834262354, 0.05861813890417045, 0.12066694323872496, 0.09855641517082314, 0.10364735879221654, 0.06480941779026406, 0.038826464832413476, 0.05759783763709192, 0.08559544915626827, 0.08083425944873401, 0.09586627775383816, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.02953528718711107, 0.03146931286774579, 0.027087998956335846, 0.025402951026821885, 0.024025489854748283, 0.025302061381791807, 0.13562821414138937, 0.13146643906922773, 0.13802144197877597, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03779577982572657, 0.030163622104217414, 0.029890571760543994, 0.0, 0.0, 0.0, 0.032576856471912774, 0.04053289674381633, 0.03619160084956552, 0.08072778292416571, 0.07965841969894782, 0.08430029781132409, 0.020806169371402228, 0.024192448856156457, 0.02590229237562558, 0.06501016208547095, 0.05775643297460331, 0.05966882349753466, 0.0, 0.0, 0.0, 0.010123842630744284, 0.011227022168228906, 0.009980632960592595, 0.007414913450769278, 0.010051284513262426, 0.0076892738595363275, 0.11299881674022583, 0.13063520168041376, 0.1142190228355412, 0.0, 0.0, 0.0]}}
