{"id": "73055906-e00e-4643-adf4-22c72f85615b", "fitness": 0.1845442762746529, "name": "HybridPSOSA", "description": "A hybrid metaheuristic algorithm combining Particle Swarm Optimization (PSO) and Simulated Annealing (SA) to balance exploration and exploitation in optimizing black box functions.", "code": "import numpy as np\n\nclass HybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)  # Dynamic adjustment based on budget\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100  # Initial temperature for Simulated Annealing\n        self.alpha = 0.99  # Cooling rate\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            # PSO Update\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            self.velocities = 0.5 * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            # Evaluate particles\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            # Update personal and global best\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            # Simulated Annealing local search\n            for i in range(self.num_particles):\n                candidate = self.particles[i] + np.random.normal(0, 0.1, self.dim)\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            # Cool down temperature\n            self.temp *= self.alpha\n\n        return self.global_best", "configspace": "", "generation": 0, "feedback": "The algorithm HybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18454 with standard deviation 0.00985.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.172749948066503, 0.1968626039040311, 0.1840202768534246]}}
{"id": "17587001-d62c-4009-9bdc-4ea3e1d314a6", "fitness": 0.17974376065429434, "name": "HybridPSOSADE", "description": "A novel hybrid optimization algorithm combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) to enhance convergence speed and robustness in solving black box optimization problems.", "code": "import numpy as np\n\nclass HybridPSOSADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)  # Dynamic adjustment based on budget\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100  # Initial temperature for Simulated Annealing\n        self.alpha = 0.99  # Cooling rate\n        self.cross_prob = 0.7  # Crossover probability for DE\n        self.f_weight = 0.5  # Differential weight for DE\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            # PSO Update\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            self.velocities = 0.5 * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            # Evaluate particles\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            # Update personal and global best\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            # Simulated Annealing local search\n            for i in range(self.num_particles):\n                candidate = self.particles[i] + np.random.normal(0, 0.1, self.dim)\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            # Differential Evolution Mutation and Crossover\n            for i in range(self.num_particles):\n                idxs = [idx for idx in range(self.num_particles) if idx != i]\n                a, b, c = self.particles[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.f_weight * (b - c)\n                mutant = np.clip(mutant, bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < self.cross_prob\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.particles[i])\n                trial_fitness = func(trial)\n                eval_count += 1\n                \n                if trial_fitness < fitness[i]:\n                    self.particles[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = trial\n                        pbest_fitness[i] = trial_fitness\n                        if trial_fitness < gbest_fitness:\n                            gbest_fitness = trial_fitness\n                            self.global_best = trial\n\n            # Cool down temperature\n            self.temp *= self.alpha\n\n        return self.global_best", "configspace": "", "generation": 1, "feedback": "The algorithm HybridPSOSADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17974 with standard deviation 0.00841.", "error": "", "parent_ids": ["73055906-e00e-4643-adf4-22c72f85615b"], "operator": null, "metadata": {"aucs": [0.17436395603595778, 0.19162224114037818, 0.1732450847865471]}}
{"id": "db9b31ca-8341-473b-9b9b-af5bf8c6913d", "fitness": 0.18833664174688639, "name": "EnhancedHybridPSOSA", "description": "Introduce a dynamic inertia weight and adaptive cooling schedule to enhance the balance between exploration and exploitation in the HybridPSOSA, aiming for improved convergence efficiency in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100  \n        self.alpha = 0.99  \n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            # Dynamic inertia weight\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            # PSO Update with dynamic inertia weight\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            # Evaluate particles\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            # Update personal and global best\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            # Adaptive Simulated Annealing local search\n            for i in range(self.num_particles):\n                candidate = self.particles[i] + np.random.normal(0, 0.1, self.dim)\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            # Adaptive cooling schedule\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 2, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18834 with standard deviation 0.01135.", "error": "", "parent_ids": ["73055906-e00e-4643-adf4-22c72f85615b"], "operator": null, "metadata": {"aucs": [0.17406284951932283, 0.2018264438668882, 0.18912063185444816]}}
{"id": "91c6fa0c-f79a-47ae-987c-7b2dfec1cf83", "fitness": 0.19280766318695905, "name": "EnhancedHybridPSOSA", "description": "Integrate a local search mechanism based on Levy Flights to enhance exploration capabilities in the EnhancedHybridPSOSA, aiming for better handling of complex landscapes.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100  \n        self.alpha = 0.99  \n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            # Dynamic inertia weight\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            # PSO Update with dynamic inertia weight\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            # Evaluate particles\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            # Update personal and global best\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            # Adaptive Simulated Annealing local search with Levy Flight\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            # Adaptive cooling schedule\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19281 with standard deviation 0.01513.", "error": "", "parent_ids": ["db9b31ca-8341-473b-9b9b-af5bf8c6913d"], "operator": null, "metadata": {"aucs": [0.17142274474476515, 0.20299216463539238, 0.2040080801807196]}}
{"id": "f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0", "fitness": 0.19561043299738864, "name": "EnhancedHybridPSOSA", "description": "Introduce an adaptive mutation mechanism alongside an improved local search strategy to enhance convergence in EnhancedHybridPSOSA.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation = np.random.normal(0, 0.01, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19561 with standard deviation 0.00758.", "error": "", "parent_ids": ["91c6fa0c-f79a-47ae-987c-7b2dfec1cf83"], "operator": null, "metadata": {"aucs": [0.18496351639177178, 0.20206076493274727, 0.19980701766764686]}}
{"id": "565d017f-d652-4c30-94e6-a165dd6ac8be", "fitness": -Infinity, "name": "EnhancedHybridPSOSA", "description": "Implement a dynamic inertia weight adjustment and leverage Levy flight for enhanced exploration in EnhancedHybridPSOSA.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.inertia_weight_max = 0.9\n        self.convergence_threshold = 0.1\n\n    def levy_flight(self, lam=1.5):\n        sigma = (np.gamma(1 + lam) * np.sin(np.pi * lam / 2) / (np.gamma((1 + lam) / 2) * lam * 2 ** ((lam - 1) / 2))) ** (1 / lam)\n        u = np.random.normal(0, sigma, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = self.inertia_weight_min + (self.inertia_weight_max - self.inertia_weight_min) * np.exp(-self.convergence_threshold * eval_count / self.budget)\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = self.levy_flight()\n                candidate = self.particles[i] + step_size\n                mutation = np.random.normal(0, 0.01, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n\n        return self.global_best", "configspace": "", "generation": 5, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {}}
{"id": "3cab7f24-3202-47fe-851b-77b85fc05245", "fitness": 0.18774883553669167, "name": "EnhancedHybridPSOSA", "description": "Integrate a differential evolution crossover mechanism and adaptive learning rate to improve local exploration and convergence in EnhancedHybridPSOSA.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.learning_rate = 0.5  # Added adaptive learning rate\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + self.learning_rate * r2 * (self.global_best - self.particles)  # Modified\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n\n                # Differential Evolution Crossover\n                idxs = np.random.choice(self.num_particles, 3, replace=False)\n                donor = self.personal_best[idxs[0]] + 0.8 * (self.personal_best[idxs[1]] - self.personal_best[idxs[2]])\n                crossover = np.where(np.random.rand(self.dim) < 0.9, donor, candidate)\n                crossover = np.clip(crossover, bounds[0], bounds[1])\n\n                mutation = np.random.normal(0, 0.01, self.dim)\n                candidate = crossover + mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n            self.learning_rate *= 0.99  # Adaptive learning rate decay\n\n        return self.global_best", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18775 with standard deviation 0.00383.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.18516589430650976, 0.18491649775628383, 0.19316411454728144]}}
{"id": "b1d86f0f-4acd-42f1-8921-1a0bc4c8ae1e", "fitness": 0.18881158535679884, "name": "EnhancedHybridPSOSA", "description": "Integrate a dynamic and adaptive parameter control alongside a diversity preservation mechanism to improve convergence and solution quality in EnhancedHybridPSOSA.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.cognitive_param = 2.0\n        self.social_param = 2.0\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            self.cognitive_param = 2.5 - 2.0 * (eval_count / self.budget)\n            self.social_param = 0.5 + 2.0 * (eval_count / self.budget)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.cognitive_param * r1 * (self.personal_best - self.particles) \\\n                + self.social_param * r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            diversity_factor = np.std(self.particles, axis=0)\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * diversity_factor * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation = np.random.normal(0, 0.01, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18881 with standard deviation 0.01059.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.17396447091994305, 0.19451348420085302, 0.19795680094960044]}}
{"id": "53e03de8-d7e2-439e-a5ee-a551554c2875", "fitness": 0.1922123340563255, "name": "DynamicLearningHybridPSOSA", "description": "Introduce a dynamic learning factor and a multi-phase local search strategy to further enhance convergence in DynamicLearningHybridPSOSA.", "code": "import numpy as np\n\nclass DynamicLearningHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n            \n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation = np.random.normal(0, 0.01, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n            # Dynamic learning factor adjustment\n            self.c1 = max(1.0, self.c1 * 0.99)\n            self.c2 = min(2.5, self.c2 * 1.01)\n\n        return self.global_best", "configspace": "", "generation": 8, "feedback": "The algorithm DynamicLearningHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19221 with standard deviation 0.00698.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.20065412059993737, 0.18356373853812025, 0.1924191430309189]}}
{"id": "f4d135c9-1be6-45aa-863e-8225a14827e7", "fitness": 0.1792069630183617, "name": "RefinedEnhancedHybridPSO", "description": "Exploit a dynamic inertia and exploration-exploitation balance mechanism to enhance convergence in EnhancedHybridPSO.", "code": "import numpy as np\n\nclass RefinedEnhancedHybridPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.exploration_exploitation_factor = 0.5  # Added factor for dynamic balance\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n        \n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * self.alpha)\n            self.exploration_exploitation_factor = 1 - (eval_count / self.budget)  # Dynamic adjustment\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + (r1 * self.exploration_exploitation_factor) * (self.personal_best - self.particles) \\\n                + (r2 * (1 - self.exploration_exploitation_factor)) * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation = np.random.normal(0, 0.01, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n\n        return self.global_best", "configspace": "", "generation": 9, "feedback": "The algorithm RefinedEnhancedHybridPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17921 with standard deviation 0.00311.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.1748418350327392, 0.18182824550659515, 0.18095080851575074]}}
{"id": "a46b5624-007b-4936-842d-b8724d1e2fb7", "fitness": 0.18776691437588164, "name": "EnhancedHybridPSOSA", "description": "Integrate a cooperative learning mechanism with adaptive velocity update in EnhancedHybridPSOSA to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        \n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            social_component = np.sum(self.particles - self.global_best, axis=0) / self.num_particles\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles - social_component)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation = np.random.normal(0, 0.01, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 10, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18777 with standard deviation 0.01120.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.17397440334732228, 0.20139507903241205, 0.18793126074791056]}}
{"id": "21a1531b-fb4b-4e8e-82e9-b52151a238fa", "fitness": 0.1943547491224922, "name": "EnhancedHybridPSOSA", "description": "Enhance convergence by integrating an adaptive inertia mechanism and leveraging self-adaptive mutation scaling in EnhancedHybridPSOSA.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            # Adaptive inertia mechanism\n            self.inertia_weight = self.inertia_weight_min + (0.5 * np.std(fitness) / (1 + np.mean(fitness)))\n            self.inertia_weight = np.clip(self.inertia_weight, self.inertia_weight_min, 0.9)\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                # Self-adaptive mutation scaling\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                mutation_scale = np.random.uniform(0.01, 0.1)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate = self.particles[i] + step_size + mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                \n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 11, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19435 with standard deviation 0.01037.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.1801925548535741, 0.1981324476819244, 0.20473924483197814]}}
{"id": "f063a0cd-00ff-44d2-b44f-f734e04f3ccd", "fitness": -Infinity, "name": "EnhancedHybridPSOSA", "description": "Apply a dynamic learning factor to enhance convergence speed by adjusting the cognitive component of the particle swarm.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * 2.5 * (self.personal_best - self.particles) \\  # Changed line\n                + r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation = np.random.normal(0, 0.01, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 12, "feedback": "An exception occurred: SyntaxError('unexpected character after line continuation character', ('<string>', 34, 155, '            self.velocities = self.inertia_weight * self.velocities \\\\\\n                + r1 * 2.5 * (self.personal_best - self.particles) \\\\  # Changed line\\n')).", "error": "SyntaxError('unexpected character after line continuation character', ('<string>', 34, 155, '            self.velocities = self.inertia_weight * self.velocities \\\\\\n                + r1 * 2.5 * (self.personal_best - self.particles) \\\\  # Changed line\\n'))", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {}}
{"id": "debd8bbb-9de2-44d0-8af5-a1b1feff013b", "fitness": 0.1903583734147302, "name": "EnhancedHybridPSOSA", "description": "Enhance convergence by incorporating Lvy flight-based mutation and dynamic parameter adaptation in EnhancedHybridPSOSA.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            self.c1 = max(0.5, self.c1 * 0.99)\n            self.c2 = max(0.5, self.c2 * 0.99)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.standard_cauchy(self.dim)\n                candidate = self.particles[i] + step_size\n                mutation = np.random.normal(0, 0.01, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 13, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19036 with standard deviation 0.01150.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.17429916541662205, 0.19619075573937728, 0.20058519908819128]}}
{"id": "6dc07781-b06a-42c2-b590-b76dc5e2d4f0", "fitness": 0.1929866892176527, "name": "EnhancedHybridPSOSA", "description": "Augment EnhancedHybridPSOSA with a dynamic learning factor adjustment and multi-swarm strategy for enhanced exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.num_swarms = 2\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n        \n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            dynamic_factor = (self.budget - eval_count) / self.budget\n            self.c1 = 1.5 + 0.5 * dynamic_factor\n            self.c2 = 1.5 + 0.5 * (1 - dynamic_factor)\n            \n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n\n            for swarm in range(self.num_swarms):\n                offset = swarm * (self.num_particles // self.num_swarms)\n                r1 = np.random.rand(self.num_particles // self.num_swarms, self.dim)\n                r2 = np.random.rand(self.num_particles // self.num_swarms, self.dim)\n                for i in range(self.num_particles // self.num_swarms):\n                    idx = offset + i\n                    self.velocities[idx] = self.inertia_weight * self.velocities[idx] \\\n                        + self.c1 * r1[i] * (self.personal_best[idx] - self.particles[idx]) \\\n                        + self.c2 * r2[i] * (self.global_best - self.particles[idx])\n                    self.particles[idx] += self.velocities[idx]\n                    self.particles[idx] = np.clip(self.particles[idx], bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation = np.random.normal(0, 0.01, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 14, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19299 with standard deviation 0.01241.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.17616149440819862, 0.20572184421679185, 0.19707672902796758]}}
{"id": "190cec4a-563b-4a8d-a651-31c1d7fd7bc3", "fitness": 0.17657820234741228, "name": "EnhancedHybridPSOSA", "description": "EnhancedHybridPSOSA with Adaptive Diversity Preservation and Chaotic Perturbation for better exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            diversity = np.std(self.particles, axis=0).mean()  # Calculate diversity\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size * (1 + diversity)  # Adjust step size by diversity\n                chaotic_factor = np.random.uniform(-1, 1, self.dim) * (1 - eval_count/self.budget)  # Chaotic perturbation\n                candidate += chaotic_factor\n                mutation = np.random.normal(0, 0.01, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 15, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17658 with standard deviation 0.00396.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.17591998020700417, 0.17209024978571397, 0.18172437704951871]}}
{"id": "cbe87423-da71-4774-aa30-9c2d26405591", "fitness": -Infinity, "name": "EnhancedHybridPSOSA", "description": "Introduce a dynamic population adjustment and adaptive velocity update to enhance exploration and exploitation balance in EnhancedHybridPSOSA.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            # Dynamic population adjustment\n            if eval_count < self.budget / 2:\n                new_particles = np.random.rand(self.num_particles // 2, self.dim)\n                new_particles = bounds[0] + new_particles * (bounds[1] - bounds[0])\n                new_fitness = np.apply_along_axis(func, 1, new_particles)\n                eval_count += self.num_particles // 2\n                \n                self.particles = np.vstack((self.particles, new_particles))\n                fitness = np.concatenate((fitness, new_fitness))\n                self.velocities = np.vstack((\n                    self.velocities, \n                    np.random.rand(self.num_particles // 2, self.dim) * 0.1\n                ))\n                self.personal_best = np.vstack((self.personal_best, new_particles))\n                pbest_fitness = np.concatenate((pbest_fitness, new_fitness))\n\n            for i in range(len(self.particles)):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation = np.random.normal(0, 0.01, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 16, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (50,45) (75,45) ').", "error": "ValueError('operands could not be broadcast together with shapes (50,45) (75,45) ')", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {}}
{"id": "71f89e96-1400-4fc2-8f29-2014fb0ccc3d", "fitness": 0.19033261782328692, "name": "EnhancedHybridPSOSA", "description": "Introduce dynamic velocity adjustment based on fitness improvement to enhance convergence in EnhancedHybridPSOSA.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation = np.random.normal(0, 0.01, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n            self.velocities[improved] *= 0.9  # Dynamic velocity adjustment\n\n        return self.global_best", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19033 with standard deviation 0.00445.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.18427057884496778, 0.1918836054558174, 0.19484366916907558]}}
{"id": "1f738abc-b9dd-4c24-8569-74b4f37996a7", "fitness": 0.1909447447827571, "name": "EnhancedHybridPSOSA", "description": "Enhance convergence by modifying mutation strategy and adjusting parameter decay.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim)  # Adjusted mutation\n                candidate = self.particles[i] + step_size\n                mutation = np.random.normal(0, 0.05, self.dim)  # Adjusted mutation\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= (self.alpha * 0.99)  # Adjusted parameter decay\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19094 with standard deviation 0.00515.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.18409005790334032, 0.19650936824414977, 0.1922348082007812]}}
{"id": "e03162f9-80bf-478c-8c35-fb40c2ceec2f", "fitness": 0.19053325310925026, "name": "EnhancedHybridPSOSA", "description": "Enhance convergence in EnhancedHybridPSOSA by introducing a nonlinear dynamic inertia weight and an adaptive differential mutation strategy inspired by DE.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight_max = 0.9\n        self.inertia_weight_min = 0.4\n        self.inertia_weight = self.inertia_weight_max\n        self.learning_factor = 2.0\n        self.F = 0.5  # Differential weight\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = self.inertia_weight_min + (self.inertia_weight_max - self.inertia_weight_min) * (\n                        1 - eval_count / self.budget) ** 2\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.learning_factor * r1 * (self.personal_best - self.particles) \\\n                + self.learning_factor * r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                indices = np.random.choice(range(self.num_particles), 3, replace=False)\n                x1, x2, x3 = self.particles[indices]\n                mutant_vector = x1 + self.F * (x2 - x3)\n                mutant_vector = np.clip(mutant_vector, bounds[0], bounds[1])\n                crossover = np.random.rand(self.dim) < 0.9\n                trial_vector = np.where(crossover, mutant_vector, self.particles[i])\n                \n                mutation = np.random.normal(0, 0.01, self.dim)\n                trial_vector += mutation\n                trial_vector = np.clip(trial_vector, bounds[0], bounds[1])\n                \n                trial_fitness = func(trial_vector)\n                eval_count += 1\n\n                if trial_fitness < fitness[i]:\n                    self.particles[i] = trial_vector\n                    fitness[i] = trial_fitness\n                    if trial_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = trial_vector\n                        pbest_fitness[i] = trial_fitness\n                        if trial_fitness < gbest_fitness:\n                            gbest_fitness = trial_fitness\n                            self.global_best = trial_vector\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 19, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19053 with standard deviation 0.00847.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.17858782463907308, 0.19572886993337968, 0.197283064755298]}}
{"id": "31fbb8d6-0df1-499e-a987-839e53ef9d63", "fitness": 0.18549661774192897, "name": "EnhancedHybridPSOSA", "description": "Introduce a dynamic learning factor to improve local search efficiency in EnhancedHybridPSOSA.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.cognitive_learning_factor = 2.0  # Added\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.cognitive_learning_factor * r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles)  # Modified line\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation = np.random.normal(0, 0.01, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 20, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18550 with standard deviation 0.00854.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.17671339314604795, 0.1827037121435452, 0.19707274793619378]}}
{"id": "05b957a0-aab5-47fe-a5d9-37b8d891a13e", "fitness": 0.18985758186890034, "name": "EnhancedHybridPSOSA", "description": "Strengthen the local search with a dynamic neighborhood approach and fine-tune inertia to improve convergence stability in EnhancedHybridPSOSA.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.3  # Adjusted from 0.4\n        self.local_search_radius = 0.1  # New parameter for dynamic neighborhood\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.99)  # Adjusted factor\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                adaptive_radius = self.local_search_radius * (1 - (eval_count / self.budget))\n                step_size = np.random.normal(0, adaptive_radius, self.dim)  # Adjusted step size\n                candidate = self.particles[i] + step_size\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n\n        return self.global_best", "configspace": "", "generation": 21, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18986 with standard deviation 0.00551.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.1820750357451547, 0.19352419905256568, 0.19397351080898062]}}
{"id": "24dac6b5-6d2c-4b5f-9a6c-00b66beb557b", "fitness": 0.19561043299738864, "name": "EnhancedHybridPSOSA", "description": "Integrate a stochastic perturbation mechanism and dynamic selection intensity in EnhancedHybridPSOSA for improved exploration and faster convergence.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.selection_intensity = 1.0  # New parameter for selection intensity\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                mutation = np.random.normal(0, 0.01, self.dim) * self.selection_intensity\n                candidate = self.particles[i] + step_size + mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n            self.selection_intensity = 0.9 * self.selection_intensity + 0.1  # Dynamic adjustment\n\n        return self.global_best", "configspace": "", "generation": 22, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19561 with standard deviation 0.00758.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.18496351639177178, 0.20206076493274727, 0.19980701766764686]}}
{"id": "9077be59-f3ab-4eaa-95ee-85bc3d635a78", "fitness": 0.18405469769353952, "name": "EnhancedHybridPSOSA", "description": "Introduce an adaptive learning rate and dynamic velocity clamping to further enhance convergence in EnhancedHybridPSOSA.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.learning_rate = 0.5\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            self.learning_rate = min(1.0, self.learning_rate * 1.01)\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.learning_rate * (r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles))\n            self.velocities = np.clip(self.velocities, -0.1, 0.1)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation = np.random.normal(0, 0.01, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 23, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18405 with standard deviation 0.00596.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.17618123887561843, 0.19058603136082275, 0.18539682284417736]}}
{"id": "5e962cb2-de23-4923-8dd9-86bcf4d3839b", "fitness": 0.18880972564033027, "name": "EnhancedHybridPSOSA", "description": "Implement an adaptive learning rate schedule for velocity updates and integrate a multi-elitism strategy to preserve diverse solutions in the EnhancedHybridPSOSA.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.learning_rate_schedule = np.linspace(0.9, 0.4, self.budget)\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(\n                self.inertia_weight_min, \n                self.learning_rate_schedule[min(eval_count, self.budget - 1)]\n            )\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            multi_elite_indices = np.argsort(fitness)[:5]  # Keep top 5 elites\n            for i in multi_elite_indices:\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation = np.random.normal(0, 0.01, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 24, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18881 with standard deviation 0.00351.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.1843497498230391, 0.19293461640636222, 0.1891448106915895]}}
{"id": "d7dd087f-781e-4e8a-baf2-711e7440b088", "fitness": 0.18750056811684554, "name": "EnhancedHybridPSOSA", "description": "Incorporate an adaptive learning rate for velocity and integrate differential mutation for enhanced exploration in EnhancedHybridPSOSA.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 3)  # Adjusted to balance exploration\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.98  # Slightly modified cooling rate\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.learning_rate = 1.5  # Added adaptive learning rate\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            self.learning_rate = 0.5 + 0.5 * (1 - eval_count / self.budget)  # Adaptive learning rate\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.learning_rate * r1 * (self.personal_best - self.particles) \\\n                + self.learning_rate * r2 * (self.global_best - self.particles)  # Applied learning rate\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                f = 0.8  # Differential mutation parameters\n                a, b, c = self.particles[np.random.choice(self.num_particles, 3, replace=False)]\n                mutant = a + f * (b - c)\n                candidate = self.particles[i] + 0.5 * (mutant - self.particles[i])  # Integrated differential mutation\n                mutation = np.random.normal(0, 0.01, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 25, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18750 with standard deviation 0.01255.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.17070445982881544, 0.20087041224256652, 0.19092683227915463]}}
{"id": "4b2d827d-a603-41b0-8987-c52a6dc981e1", "fitness": 0.20073872335743773, "name": "EnhancedHybridPSOSA", "description": "Enhance convergence by dynamically adjusting mutation scale and leveraging a stochastic component in velocity updates.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01  # Modified line\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component  # Modified line\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)  # Modified line\n                mutation = np.random.normal(0, mutation_scale, self.dim)  # Modified line\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 26, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20074 with standard deviation 0.01047.", "error": "", "parent_ids": ["f87fd26d-11fa-4ec3-a4cb-e0290cfeabd0"], "operator": null, "metadata": {"aucs": [0.18595223046925446, 0.2087962084417696, 0.20746773116128914]}}
{"id": "66843f2f-80dc-4db9-b892-478257786af9", "fitness": 0.19718622886932236, "name": "EnhancedHybridPSOSA", "description": "Improve exploration by introducing adaptive noise scaling and enhanced inertia weight decay for optimized convergence in stochastic optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.99)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01 * (1 - eval_count / self.budget)  # Adaptive noise scaling\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 - eval_count / self.budget)  # Adaptive mutation scaling\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 27, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19719 with standard deviation 0.00171.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.1961268121287446, 0.19582857286870958, 0.19960330161051287]}}
{"id": "4384f2d7-b857-46d0-a9e2-fc333be9382f", "fitness": 0.17720172452797942, "name": "AdaptiveHybridPSOSA", "description": "Introduce adaptive exploration-exploitation balance by dynamically adjusting inertia and mutation based on fitness variance to enhance convergence.", "code": "import numpy as np\n\nclass AdaptiveHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.exploration_factor = 0.1\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            fitness_variance = np.var(fitness)\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * (1 - fitness_variance / (1 + fitness_variance)))\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * self.exploration_factor\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget) * (1 - fitness_variance / (1 + fitness_variance))\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 28, "feedback": "The algorithm AdaptiveHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17720 with standard deviation 0.00678.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.17328203775355444, 0.18674156766368688, 0.1715815681666969]}}
{"id": "d89e1fa5-a2a4-4a78-8679-8ed684c77bb9", "fitness": -Infinity, "name": "EnhancedHybridPSOSA", "description": "Introducing adaptive learning rates for personal and global best updates to enhance convergence and exploration balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.05  # Added line\n        self.c2 = 2.05  # Added line\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\  # Modified line\n                + self.c2 * r2 * (self.global_best - self.particles) \\  # Modified line\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 29, "feedback": "An exception occurred: SyntaxError('unexpected character after line continuation character', ('<string>', 37, 160, '            self.velocities = self.inertia_weight * self.velocities \\\\\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\\  # Modified line\\n')).", "error": "SyntaxError('unexpected character after line continuation character', ('<string>', 37, 160, '            self.velocities = self.inertia_weight * self.velocities \\\\\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\\  # Modified line\\n'))", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {}}
{"id": "44081c27-dfd0-4548-9ccf-fced3ca3ebb9", "fitness": -Infinity, "name": "EnhancedHybridPSOSA", "description": "Improve convergence by introducing adaptive learning factors and an exponentially decreasing temperature schedule.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.05  # Cognitive component\n        self.c2 = 2.05  # Social component\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\  # Modified line\n                + self.c2 * r2 * (self.global_best - self.particles) \\  # Modified line\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= 0.99  # Modified line\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 30, "feedback": "An exception occurred: SyntaxError('unexpected character after line continuation character', ('<string>', 37, 160, '            self.velocities = self.inertia_weight * self.velocities \\\\\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\\  # Modified line\\n')).", "error": "SyntaxError('unexpected character after line continuation character', ('<string>', 37, 160, '            self.velocities = self.inertia_weight * self.velocities \\\\\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\\  # Modified line\\n'))", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {}}
{"id": "43d965ff-d87a-4912-9070-915c8b922109", "fitness": 0.19006141389844192, "name": "AdaptiveDiversityPSOSA", "description": "Introduce adaptive learning rates and dynamic diversity mechanisms to accelerate convergence and escape local optima in diverse search spaces.", "code": "import numpy as np\n\nclass AdaptiveDiversityPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.learning_rate_min = 0.1\n        self.learning_rate_max = 2.0\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            learning_rate = self.learning_rate_min + (self.learning_rate_max - self.learning_rate_min) * np.random.rand(self.num_particles, self.dim)\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + learning_rate * r1 * (self.personal_best - self.particles) \\\n                + learning_rate * r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                diversity_factor = 1 + 0.5 * np.std(self.particles - self.global_best)\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim) * diversity_factor\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 31, "feedback": "The algorithm AdaptiveDiversityPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19006 with standard deviation 0.00589.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.18506223789302068, 0.1983330499368593, 0.1867889538654458]}}
{"id": "93d3f005-6427-4727-9a25-1ed39e43fd0a", "fitness": -Infinity, "name": "EnhancedHybridPSOSA", "description": "Incorporate adaptive learning rates for velocity updating and mutation to enhance convergence speed and robustness.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01 \n            adaptive_lr = 0.5 + 0.5 * (1 - eval_count / self.budget)  # Modified line\n            self.velocities = self.inertia_weight * self.velocities \\\n                + adaptive_lr * r1 * (self.personal_best - self.particles) \\  # Modified line\n                + adaptive_lr * r2 * (self.global_best - self.particles) \\  # Modified line\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 32, "feedback": "An exception occurred: SyntaxError('unexpected character after line continuation character', ('<string>', 36, 164, '            self.velocities = self.inertia_weight * self.velocities \\\\\\n                + adaptive_lr * r1 * (self.personal_best - self.particles) \\\\  # Modified line\\n')).", "error": "SyntaxError('unexpected character after line continuation character', ('<string>', 36, 164, '            self.velocities = self.inertia_weight * self.velocities \\\\\\n                + adaptive_lr * r1 * (self.personal_best - self.particles) \\\\  # Modified line\\n'))", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {}}
{"id": "9c2b2590-d5a9-43b4-b9bb-1a9838961303", "fitness": 0.18891511444245945, "name": "RefinedHybridPSOSA", "description": "Integrate adaptive inertia weight and crossover operator to fine-tune exploration and exploitation balance in particle swarm optimization.", "code": "import numpy as np\n\nclass RefinedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, 0.9 - 0.5 * eval_count / self.budget)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            # Crossover step\n            if np.random.rand() < 0.3:\n                parents = np.random.choice(range(self.num_particles), 2, replace=False)\n                crossover_point = np.random.randint(1, self.dim)\n                offspring = np.concatenate((self.particles[parents[0], :crossover_point],\n                                            self.particles[parents[1], crossover_point:]))\n                offspring = np.clip(offspring, bounds[0], bounds[1])\n                offspring_fitness = func(offspring)\n                eval_count += 1\n                if offspring_fitness < gbest_fitness:\n                    gbest_fitness = offspring_fitness\n                    self.global_best = offspring\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 33, "feedback": "The algorithm RefinedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18892 with standard deviation 0.00793.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.17771774154628373, 0.19406581075134932, 0.1949617910297453]}}
{"id": "fcc2e390-9bc2-4de7-8394-50d239fcb90f", "fitness": 0.18580000368479152, "name": "EnhancedHybridPSOSA", "description": "Introduce adaptive inertia and dynamic mutation scaling based on convergence progress to balance exploration and exploitation for improved optimization performance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.05\n        self.c2 = 2.05\n        self.kappa = 1.0\n        self.phi = self.c1 + self.c2\n        self.chi = 2 * self.kappa / abs(2 - self.phi - np.sqrt(self.phi**2 - 4 * self.phi))\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        initial_global_best = gbest_fitness\n\n        while eval_count < self.budget:\n            convergence_progress = 1 - (gbest_fitness / initial_global_best)\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * (1 - convergence_progress))\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.chi * (self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (self.global_best - self.particles) \\\n                + stochastic_component)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget) * (1 - convergence_progress)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 34, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18580 with standard deviation 0.01062.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.17087081678972782, 0.1918173447697903, 0.19471184949485643]}}
{"id": "73d484da-5d58-4657-b348-af5bbcadafa7", "fitness": 0.20073872335743773, "name": "AdaptiveHybridPSOSA", "description": "Adaptive convergence leveraging diversity preservation and dynamic hierarchical learning in particle updates.", "code": "import numpy as np\n\nclass AdaptiveHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.diversity_threshold = 0.1  # New line\n        self.diversity_factor = 1.0  # New line\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01 * self.diversity_factor  # Modified line\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n            # Diversity control mechanism\n            diversity = np.mean(np.std(self.particles, axis=0))\n            if diversity < self.diversity_threshold:\n                self.diversity_factor = 1.5\n            else:\n                self.diversity_factor = 1.0\n\n        return self.global_best", "configspace": "", "generation": 35, "feedback": "The algorithm AdaptiveHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20074 with standard deviation 0.01047.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.18595223046925446, 0.2087962084417696, 0.20746773116128914]}}
{"id": "7278e614-0bc8-42b8-aa9e-030bd5af25e9", "fitness": 0.19128268317695676, "name": "EnhancedHybridPSOSA", "description": "Further enhance convergence by introducing a dynamic exploration factor and improving the mutation scale update to be more reactive to current evaluations.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            exploration_factor = np.sin(eval_count / self.budget * np.pi)  # Modified line\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + exploration_factor * stochastic_component  # Modified line\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + (eval_count / self.budget)**2)  # Modified line\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 36, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19128 with standard deviation 0.00626.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.18313598580112145, 0.19836271839689146, 0.1923493453328574]}}
{"id": "95fd5b59-5e7c-474f-8c0a-02c094c4c970", "fitness": 0.1942094392902034, "name": "RefinedEnhancedHybridPSOSA", "description": "Enhance convergence by introducing adaptive inertia and mutation strategies based on evaluation progress, leveraging dynamic learning factors for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass RefinedEnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.3\n        self.c1, self.c2 = 2.0, 2.0  # Cognitive and social components\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            progress = eval_count / self.budget\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * (1 - progress))\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01 * (1 - progress)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 - progress)  \n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 37, "feedback": "The algorithm RefinedEnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19421 with standard deviation 0.01577.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.1719652139218295, 0.20670700350406046, 0.20395610044472023]}}
{"id": "fac0a8aa-a488-493d-b6d5-2f87f3077e23", "fitness": 0.1980784618606917, "name": "EnhancedHybridPSO_Refined", "description": "Enhance optimization by dynamically adjusting mutation scale, leveraging stochastic components in velocity and local search, and incorporating adaptive temperature control.", "code": "import numpy as np\n\nclass EnhancedHybridPSO_Refined:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.05  # Modified line\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.02 * (1 - eval_count / self.budget)  # Modified line\n                mutation = np.random.normal(0, mutation_scale, self.dim)  # Modified line\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                acceptance_prob = np.exp((fitness[i] - candidate_fitness) / max(self.temp, 1e-9))  # Modified line\n                if candidate_fitness < fitness[i] or acceptance_prob > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 38, "feedback": "The algorithm EnhancedHybridPSO_Refined got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19808 with standard deviation 0.00616.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.18936798780612074, 0.20272899694260305, 0.20213840083335133]}}
{"id": "63445a7c-1953-4742-bbdf-b4660e0b17f4", "fitness": 0.19957742202261944, "name": "EnhancedHybridPSOSA", "description": "Further enhance convergence by modifying inertia weight adaptation and mutation scale dynamics.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.993)  # Modified line\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.005 * (1 + eval_count / self.budget)  # Modified line\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 39, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19958 with standard deviation 0.00238.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.19636656037299038, 0.20032844834247632, 0.20203725735239164]}}
{"id": "b568843b-dd49-47a2-b989-940fd72a832f", "fitness": 0.19796105438728975, "name": "EnhancedHybridPSOSA_Improved", "description": "Improve convergence and diversity by integrating adaptive inertia weight with dynamic mutation scaling based on fitness variance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.inertia_weight_factor = 0.995  # New line\n        self.mutation_scale_base = 0.01  # New line\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * self.inertia_weight_factor)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            fitness_variance = np.var(fitness)  # New line\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = self.mutation_scale_base * (1 + eval_count / self.budget) + fitness_variance * 0.01  # Modified line\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, 0.1)\n\n        return self.global_best", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedHybridPSOSA_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19796 with standard deviation 0.00972.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.18421447873905739, 0.20498814174152002, 0.20468054268129188]}}
{"id": "56dd6fb5-a857-406d-ae40-0ee580557190", "fitness": 0.19957742202261944, "name": "EnhancedHybridPSOSA", "description": "Refine inertia weight decay and mutation mechanism for improved convergence stability.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.993)  # Modified line\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.005 * (1 + eval_count / self.budget)  # Modified line\n                mutation = np.random.normal(0, mutation_scale, self.dim)  # Modified line\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 41, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19958 with standard deviation 0.00238.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.19636656037299038, 0.20032844834247632, 0.20203725735239164]}}
{"id": "19296dde-019f-4d3b-8dc7-28269b663543", "fitness": -Infinity, "name": "EnhancedHybridPSOSA", "description": "Refine convergence by introducing adaptive learning factors that adjust based on performance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1, self.c2 = 2.0, 2.0  # Initial learning factors\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\  # Modified line\n                + self.c2 * r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 42, "feedback": "An exception occurred: SyntaxError('unexpected character after line continuation character', ('<string>', 36, 160, '            self.velocities = self.inertia_weight * self.velocities \\\\\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\\  # Modified line\\n')).", "error": "SyntaxError('unexpected character after line continuation character', ('<string>', 36, 160, '            self.velocities = self.inertia_weight * self.velocities \\\\\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\\  # Modified line\\n'))", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {}}
{"id": "61c9b789-b47e-4db5-bfa5-dc4beef60f47", "fitness": -Infinity, "name": "AdaptiveHybridPSOSA", "description": "Improve convergence by introducing adaptive learning rates for personal and global best influences, along with a diversity preservation mechanism through a mutation step based on population entropy.", "code": "import numpy as np\n\nclass AdaptiveHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 1.5  # Cognitive constant\n        self.c2 = 1.5  # Social constant\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            diversity = np.log(np.std(self.particles, axis=0) + 1e-10)\n            mutation_scale = 0.01 * (1 + eval_count / self.budget)\n            \n            self.velocities = (self.inertia_weight * self.velocities\n                               + self.c1 * r1 * (self.personal_best - self.particles)\n                               + self.c2 * r2 * (self.global_best - self.particles)\n                               + stochastic_component)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1 * (1 + diversity), self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 43, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {}}
{"id": "e25b94e3-9e86-48a4-9542-200ed7c427cf", "fitness": 0.1986071553303159, "name": "EnhancedHybridPSOSA", "description": "Refine convergence by modifying the inertia weight decay rate to enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.993)  # Modified line\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01  # Modified line\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component  # Modified line\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)  # Modified line\n                mutation = np.random.normal(0, mutation_scale, self.dim)  # Modified line\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19861 with standard deviation 0.00929.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.1866170872164301, 0.19995530113018023, 0.2092490776443373]}}
{"id": "0157b08d-fb8b-4db8-98de-b02be2d0b489", "fitness": 0.1840674049628076, "name": "EnhancedHybridPSOSA", "description": "Improve convergence by introducing adaptive inertia weight decay based on evaluation count.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995) * (1 - eval_count / self.budget)\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 45, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18407 with standard deviation 0.00561.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.18276848613947994, 0.17793792657852536, 0.1914958021704175]}}
{"id": "dd50aed5-1813-4c48-883f-795d241ff7c4", "fitness": 0.19146522019816437, "name": "EnhancedHybridPSOSA", "description": "Enhance global exploration by introducing a controlled random walk component to particle updates.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles += self.velocities + np.random.normal(0, 0.05, self.particles.shape)  # Modified line\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 46, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19147 with standard deviation 0.00517.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.1876674025193601, 0.19876910264668202, 0.18795915542845099]}}
{"id": "f8a5ec00-2f54-4094-8bd5-4c4726e99429", "fitness": 0.19686542057731518, "name": "EnhancedHybridPSOSA", "description": "Introduce adaptive control of exploration and exploitation phases using diversity-aware adaptive inertia weights and neighborhood-based global best enhancement.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight_max = 0.9\n        self.inertia_weight_min = 0.4\n        self.inertia_weight = self.inertia_weight_max\n    \n    def diversity(self):\n        mean_pos = np.mean(self.particles, axis=0)\n        diversity = np.mean(np.linalg.norm(self.particles - mean_pos, axis=1))\n        return diversity\n    \n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n        \n        eval_count = self.num_particles\n        while eval_count < self.budget:\n            current_diversity = self.diversity()\n            self.inertia_weight = self.inertia_weight_min + (self.inertia_weight_max - self.inertia_weight_min) * (1 - current_diversity / self.dim)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n            \n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n            \n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n            \n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n                \n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n                \n                neighborhood_best_idx = np.argmin(fitness)\n                if fitness[neighborhood_best_idx] < gbest_fitness:\n                    gbest_fitness = fitness[neighborhood_best_idx]\n                    self.global_best = self.particles[neighborhood_best_idx]\n            \n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n        \n        return self.global_best", "configspace": "", "generation": 47, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19687 with standard deviation 0.01147.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.18877423185552789, 0.18873790124886092, 0.21308412862755677]}}
{"id": "5ccaed30-7075-46ed-b708-5f8763682897", "fitness": 0.19355940715676975, "name": "RefinedHybridPSOSA", "description": "Refine convergence by incorporating adaptive inertia, dynamic learning coefficients, and a diversity-enhancing random drift.", "code": "import numpy as np\n\nclass RefinedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.c1_min = 1.5\n        self.c2_min = 1.5\n        self.diversity_drift_scale = 0.01\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            self.c1 = max(self.c1_min, self.c1 * 0.995)\n            self.c2 = max(self.c2_min, self.c2 * 0.995)\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * self.diversity_drift_scale\n            self.velocities = (self.inertia_weight * self.velocities\n                               + r1 * self.c1 * (self.personal_best - self.particles)\n                               + r2 * self.c2 * (self.global_best - self.particles)\n                               + stochastic_component)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 48, "feedback": "The algorithm RefinedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19356 with standard deviation 0.01448.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.17311317671457382, 0.2028490992907025, 0.2047159454650329]}}
{"id": "e320fde6-da15-4ce1-b385-4375240e0262", "fitness": 0.20073872335743773, "name": "EnhancedHybridPSO_Diversity", "description": "Improve convergence by introducing a dynamic diversity preservation strategy and adaptively adjusting exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSO_Diversity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n            # Implementing diversity preservation strategy\n            diversity = np.std(self.particles, axis=0).mean()\n            if diversity < self.diversity_threshold:\n                perturbation = np.random.normal(0, 0.1, (self.num_particles, self.dim))\n                self.particles += perturbation\n                self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n        return self.global_best", "configspace": "", "generation": 49, "feedback": "The algorithm EnhancedHybridPSO_Diversity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20074 with standard deviation 0.01047.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.18595223046925446, 0.2087962084417696, 0.20746773116128914]}}
{"id": "59de6b54-a2ea-4bfc-8d7e-0edada37221d", "fitness": 0.19468334154692757, "name": "EnhancedHybridPSOSA", "description": "Enhance exploration by dynamically adjusting the stochastic component scale in velocity updates.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component_scale = 0.01 * (1 + eval_count / self.budget)  # Modified line\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * stochastic_component_scale  # Modified line\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 50, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19468 with standard deviation 0.01094.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.18194615267791325, 0.20865334721514894, 0.1934505247477205]}}
{"id": "f88fb949-6896-4e1d-97f6-c802e6c67781", "fitness": 0.18979971560325282, "name": "EnhancedAdaptivePSOSA", "description": "Introduce adaptive local search and dynamic social learning to enhance convergence and global exploration by refining particle update rules based on particle diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptivePSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            diversity = np.std(self.particles, axis=0).mean()  # Calculate diversity\n            \n            r1, r2, r3 = np.random.rand(3, self.num_particles, self.dim)\n            social_component = r2 * (self.global_best - self.particles)\n            personal_component = r1 * (self.personal_best - self.particles)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n\n            if diversity < 0.1:  # Adaptive local search if diversity is low\n                local_search = np.random.normal(0, 0.1, (self.num_particles, self.dim))\n            else:\n                local_search = 0\n\n            self.velocities = (self.inertia_weight * self.velocities \n                               + personal_component + social_component + stochastic_component + local_search)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                candidate = self.particles[i] + np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                mutation_scale = 0.01 * (1 + eval_count / self.budget) \n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 51, "feedback": "The algorithm EnhancedAdaptivePSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18980 with standard deviation 0.00800.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.17851179144892682, 0.19471957444861066, 0.19616778091222098]}}
{"id": "0801ab04-f130-4ae3-9cab-d29c5e822a9b", "fitness": -Infinity, "name": "EnhancedHybridPSOSA", "description": "Improve convergence by introducing adaptive learning rates and enhanced diversity in candidate solutions.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + (r1 + 0.5) * (self.personal_best - self.particles) \\  # Modified line\n                + (r2 + 0.5) * (self.global_best - self.particles) \\  # Modified line\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 52, "feedback": "An exception occurred: SyntaxError('unexpected character after line continuation character', ('<string>', 35, 158, '            self.velocities = self.inertia_weight * self.velocities \\\\\\n                + (r1 + 0.5) * (self.personal_best - self.particles) \\\\  # Modified line\\n')).", "error": "SyntaxError('unexpected character after line continuation character', ('<string>', 35, 158, '            self.velocities = self.inertia_weight * self.velocities \\\\\\n                + (r1 + 0.5) * (self.personal_best - self.particles) \\\\  # Modified line\\n'))", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {}}
{"id": "bb8c5d1e-8337-4974-9d70-10e30186f6b8", "fitness": -Infinity, "name": "EnhancedHybridPSOSA", "description": "Introduce adaptive learning rates for velocity updates to improve convergence by adjusting based on particle performance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.learning_rate = np.ones(self.num_particles)  # New line\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * self.learning_rate[:, np.newaxis] * (self.personal_best - self.particles) \\  # Modified line\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.learning_rate[improved] *= 1.1  # New line\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 53, "feedback": "An exception occurred: SyntaxError('unexpected character after line continuation character', ('<string>', 36, 186, '            self.velocities = self.inertia_weight * self.velocities \\\\\\n                + r1 * self.learning_rate[:, np.newaxis] * (self.personal_best - self.particles) \\\\  # Modified line\\n')).", "error": "SyntaxError('unexpected character after line continuation character', ('<string>', 36, 186, '            self.velocities = self.inertia_weight * self.velocities \\\\\\n                + r1 * self.learning_rate[:, np.newaxis] * (self.personal_best - self.particles) \\\\  # Modified line\\n'))", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {}}
{"id": "2fc6ee61-d963-4e90-8cb5-e8b31a5081d9", "fitness": 0.19177666998510967, "name": "EnhancedHybridPSO_DE", "description": "Enhance convergence and diversity management by incorporating adaptive learning rates and a differential evolution-inspired perturbation mechanism.", "code": "import numpy as np\n\nclass EnhancedHybridPSO_DE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.05  # cognitive component\n        self.c2 = 2.05  # social component\n        self.F = 0.8  # differential evolution scaling factor\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                a, b, c = np.random.choice(self.num_particles, 3, replace=False)\n                mutant = self.particles[a] + self.F * (self.particles[b] - self.particles[c])\n                mutant = np.clip(mutant, bounds[0], bounds[1])\n                candidate = np.where(np.random.rand(self.dim) < 0.5, mutant, self.particles[i])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 54, "feedback": "The algorithm EnhancedHybridPSO_DE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19178 with standard deviation 0.01226.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.1787207437939915, 0.20818393463729612, 0.1884253315240414]}}
{"id": "2cf64aab-a493-4376-999f-c495b84a24f9", "fitness": 0.18879999265610714, "name": "EnhancedHybridPSOSA", "description": "Improve exploration by introducing a Lvy flight-inspired mutation step and a dynamically adaptive velocity component.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.standard_cauchy(self.dim) * 0.1  # Modified line\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.standard_cauchy(self.dim) * mutation_scale  # Modified line\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 55, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18880 with standard deviation 0.01431.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.16868768485249463, 0.19689223511964082, 0.20082005799618596]}}
{"id": "894c7ee2-b760-4a69-b3f2-2c05e0048c93", "fitness": 0.1908057475812918, "name": "EnhancedHybridPSOSA", "description": "Improve convergence by introducing adaptive acceleration coefficients and periodic reinitialization of velocities to enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.5  # cognitive coefficient\n        self.c2 = 2.5  # social coefficient\n        self.c1_min, self.c1_max = 1.5, 2.5\n        self.c2_min, self.c2_max = 1.5, 2.5\n        self.reinit_interval = max(1, budget // 10)  # Periodic reinitialization\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            self.c1 = self.c1_max - (self.c1_max - self.c1_min) * (eval_count / self.budget)\n            self.c2 = self.c2_min + (self.c2_max - self.c2_min) * (eval_count / self.budget)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            if eval_count % self.reinit_interval == 0:\n                self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 56, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19081 with standard deviation 0.01210.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.17383774408814723, 0.20119182239802624, 0.19738767625770193]}}
{"id": "58fca8d4-d207-4a6c-9839-b6feebe8aef8", "fitness": 0.1802489636633823, "name": "EnhancedHybridPSOSA", "description": "Improve convergence by introducing adaptive inertia weight and differential evolution-inspired crossover to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.crossover_prob = 0.9\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995 + 0.05 * np.random.rand())\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                indices = np.random.choice(np.delete(np.arange(self.num_particles), i), 3, replace=False)\n                a, b, c = self.particles[indices]\n                donor = a + 0.8 * (b - c)\n                crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n                candidate = np.where(crossover_mask, donor, self.particles[i])\n                \n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 57, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18025 with standard deviation 0.00139.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.1798513486548854, 0.1787837810596603, 0.18211176127560125]}}
{"id": "a6cb6220-f758-4996-90e9-266e9559835f", "fitness": 0.18994107595754292, "name": "EnhancedHybridPSOSA", "description": "Incorporate adaptive inertia weight adjustment for improved exploration and convergence balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * (0.995 + 0.005 * (1 - eval_count / self.budget)))  # Modified line\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 58, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18994 with standard deviation 0.01082.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.1753689440036399, 0.20126352647491352, 0.19319075739407532]}}
{"id": "29a92151-6021-411c-be71-e0eca1fc84cd", "fitness": 0.18527972285228342, "name": "EnhancedHybridPSOSA_v2", "description": "Improve convergence by introducing adaptive learning factors and a dynamic mutation adaptation strategy based on the progress of optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA_v2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0  # Cognitive component\n        self.c2 = 2.0  # Social component\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n\n            # Adaptive learning factors\n            c1_dynamic = self.c1 * (1 - eval_count / self.budget)\n            c2_dynamic = self.c2 * (eval_count / self.budget)\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + c1_dynamic * r1 * (self.personal_best - self.particles) \\\n                + c2_dynamic * r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget / 2)  # Further refined mutation scale\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 59, "feedback": "The algorithm EnhancedHybridPSOSA_v2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18528 with standard deviation 0.01090.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.1978028855525491, 0.1712252859904334, 0.18681099701386772]}}
{"id": "1ceab72c-0e1c-48f6-aa7c-c3ecc7ba7ffa", "fitness": 0.20036879303859487, "name": "EnhancedHybridPSOSA", "description": "Introduce adaptive stochastic scaling to further balance exploration and exploitation dynamics.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01 * (1 - eval_count / self.budget)  # Modified line\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 60, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20037 with standard deviation 0.00902.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.18789865096989633, 0.20892880493149057, 0.20427892321439767]}}
{"id": "30f0310b-21d2-456d-b576-f28fa13ef37f", "fitness": 0.17465897152618748, "name": "EnhancedHybridPSOSA", "description": "Integrate adaptive learning rate and diversity preservation to enhance exploration and exploitation balance in the optimization process.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.learning_rate = 0.1\n        self.learning_rate_min = 0.01\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            self.learning_rate = max(self.learning_rate_min, self.learning_rate * 0.995)\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.02  # Increased noise\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.learning_rate * (r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles)) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            # Diversity preservation: random reinitialization for stagnated particles\n            if eval_count % (self.num_particles * 10) == 0:\n                indices_to_reinit = np.random.rand(self.num_particles) < 0.1\n                self.particles[indices_to_reinit] = np.random.rand(np.sum(indices_to_reinit), self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17466 with standard deviation 0.00508.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.17032132719188964, 0.17186442245213973, 0.18179116493453307]}}
{"id": "121cc0e6-a13e-42b0-9297-d775cd616f3c", "fitness": 0.18795360336574832, "name": "EnhancedHybridPSOSA", "description": "Incorporate dynamic inertia weight adjustment and introduce adaptive mutation scaling to enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.02 * (1 + eval_count / self.budget)  # Modified line\n                mutation = np.random.normal(0, mutation_scale, self.dim)  # Modified line\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 62, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18795 with standard deviation 0.01024.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.17374929789738014, 0.19751392670785572, 0.19259758549200912]}}
{"id": "229dcdbc-dd6c-4269-9e0b-b9327336b4ce", "fitness": 0.18488946011407736, "name": "EnhancedHybridPSOSA", "description": "Enhance exploration by scaling the stochastic component based on particle distance from the global best.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            distance_to_gbest = np.linalg.norm(self.particles - self.global_best, axis=1, keepdims=True)  # Added line\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01 * (1 + distance_to_gbest)  # Modified line\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 63, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18489 with standard deviation 0.00358.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.181529740459503, 0.1898554028242081, 0.183283237058521]}}
{"id": "a65002b2-2e6d-4d30-907e-8bfcc518a20c", "fitness": 0.18714986928107014, "name": "EnhancedHybridPSOSA_v2", "description": "Enhance convergence by incorporating adaptive mutation based on diversity and accelerating exploitation with a dynamic inertia weight.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA_v2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            # Update inertia weight based on diversity\n            diversity = np.mean(np.std(self.particles, axis=0))\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * (1 + 0.1 * diversity))\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 / (1 + np.log1p(eval_count / self.budget))  # Adjusted mutation scaling\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 64, "feedback": "The algorithm EnhancedHybridPSOSA_v2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18715 with standard deviation 0.00754.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.1787874201577947, 0.19705307984342668, 0.18560910784198903]}}
{"id": "7f583392-5061-44cf-bf01-dac03d91d8c9", "fitness": 0.1956489194848761, "name": "RefinedHybridPSOSA", "description": "Introduce adaptive learning factors and individual stochastic perturbations to refine personal and global exploration phases for improved convergence.", "code": "import numpy as np\n\nclass RefinedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0  # Cognitive component\n        self.c2 = 2.0  # Social component\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            individual_perturbation = np.random.randn(self.num_particles, self.dim) * 0.01 * (1 - eval_count / self.budget)\n            self.velocities = (self.inertia_weight * self.velocities\n                               + self.c1 * r1 * (self.personal_best - self.particles)\n                               + self.c2 * r2 * (self.global_best - self.particles)\n                               + individual_perturbation)\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 65, "feedback": "The algorithm RefinedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19565 with standard deviation 0.00338.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.20041852619823552, 0.19300215040797164, 0.19352608184842113]}}
{"id": "47d77d6d-0ced-423e-ace5-a8770ddb446e", "fitness": 0.19461008998578502, "name": "EnhancedHybridPSOSA", "description": "Introduce adaptive component mixing based on convergence state to improve global exploration balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget) * np.abs(gbest_fitness - fitness[i]) / (gbest_fitness + 1e-9)  # Modified line\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 66, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19461 with standard deviation 0.01731.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.1713078909146536, 0.21274617382792305, 0.19977620521477846]}}
{"id": "8ea7b552-a56e-4609-bfaf-118eb60f02fc", "fitness": 0.18563771819507277, "name": "EnhancedHybridPSOSALearningRate", "description": "Introduce a learning rate adaptation mechanism and enhance stochastic components for improved convergence in varying landscapes.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSALearningRate:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.learning_rate = 0.5  # New learning rate parameter\n        self.learning_decay = 0.995\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * self.learning_decay)\n            self.learning_rate *= self.learning_decay  # Decay learning rate\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.05  # Enhanced from 0.01\n            self.velocities = (self.inertia_weight * self.velocities \n                               + self.learning_rate * (r1 * (self.personal_best - self.particles)\n                               + r2 * (self.global_best - self.particles) \n                               + stochastic_component))\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.2, self.dim) * np.random.pareto(1.5, self.dim)  # Enhanced from 0.1\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget) * self.learning_rate  # Adjusted line\n                mutation = np.random.normal(0, mutation_scale, self.dim)  # Adjusted line\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedHybridPSOSALearningRate got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18564 with standard deviation 0.00618.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.17697142062600213, 0.1909262503245197, 0.18901548363469645]}}
{"id": "89cc7a4c-23d1-4941-b301-11e1887b341c", "fitness": 0.1953723015345228, "name": "EnhancedHybridPSOSA", "description": "Enhance convergence by adjusting particle influence through a dynamic learning factor.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            learning_factor = 1.5 + 2.5 * (eval_count / self.budget)  # Modified line\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + learning_factor * r1 * (self.personal_best - self.particles) \\\n                + learning_factor * r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 68, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19537 with standard deviation 0.00730.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.1856779920771372, 0.19714195623583597, 0.2032969562905953]}}
{"id": "148c0d45-c9ab-46f2-8139-f4ea61c9fb0f", "fitness": 0.19349990962139738, "name": "EnhancedHybridPSOSA", "description": "Refine inertia weight decay strategy for better exploration-exploitation balance in particle swarm optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, (0.995 ** (eval_count / self.budget)) * 0.9)  # Modified line\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 69, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19350 with standard deviation 0.01434.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.17336746809854042, 0.20147279348568548, 0.2056594672799662]}}
{"id": "36b9ce64-89e4-47be-9843-ccf75782d929", "fitness": 0.19223194210198027, "name": "EnhancedDifferentialPSOSA", "description": "Improve convergence by integrating adaptive inertia weight decay and differential evolution-inspired mutation for enhanced exploration.", "code": "import numpy as np\n\nclass EnhancedDifferentialPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.alpha = 0.99\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.99)\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            if eval_count < self.budget:\n                for i in range(self.num_particles):\n                    r1 = np.random.randint(0, self.num_particles)\n                    r2 = np.random.randint(0, self.num_particles)\n                    while r1 == i: r1 = np.random.randint(0, self.num_particles)\n                    while r2 == i or r2 == r1: r2 = np.random.randint(0, self.num_particles)\n                    \n                    step_size = 0.8 * (self.personal_best[r1] - self.personal_best[r2])\n                    candidate = self.particles[i] + step_size\n                    candidate = np.clip(candidate, bounds[0], bounds[1])\n                    \n                    mutation_scale = 0.1 * (1.0 - eval_count / self.budget)\n                    mutation = np.random.normal(0, mutation_scale, self.dim)\n                    candidate += mutation\n                    candidate = np.clip(candidate, bounds[0], bounds[1])\n\n                    candidate_fitness = func(candidate)\n                    eval_count += 1\n                    if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness)) > np.random.rand():\n                        self.particles[i] = candidate\n                        fitness[i] = candidate_fitness\n                        if candidate_fitness < pbest_fitness[i]:\n                            self.personal_best[i] = candidate\n                            pbest_fitness[i] = candidate_fitness\n                            if candidate_fitness < gbest_fitness:\n                                gbest_fitness = candidate_fitness\n                                self.global_best = candidate\n\n        return self.global_best", "configspace": "", "generation": 70, "feedback": "The algorithm EnhancedDifferentialPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19223 with standard deviation 0.01187.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.1769483813256738, 0.20588854109747678, 0.19385890388279026]}}
{"id": "4e77a0c5-a3ab-4451-a124-ef0ed7cfc37f", "fitness": 0.17915067464615952, "name": "EnhancedHybridPSOSA", "description": "Improve exploration by introducing adaptive inertia weight decay and dynamically altering mutation variance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.993)  # Modified line\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.02 * (1 - eval_count / self.budget)  # Modified line\n                mutation = np.random.normal(0, mutation_scale, self.dim)  # Modified line\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 71, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17915 with standard deviation 0.00831.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.17129721169810042, 0.1755126595734665, 0.19064215266691165]}}
{"id": "2fe283e4-2368-49e2-94a9-707924ac55ec", "fitness": 0.19160564335973218, "name": "EnhancedHybridPSOSA", "description": "Improve convergence by enhancing exploration with an adaptive step size adjustment based on fitness diversity.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                step_size *= 1 + np.std(fitness) / (np.mean(fitness) + 1e-9)  # Modified line\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 72, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19161 with standard deviation 0.00601.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.18348154080866252, 0.19785034010974112, 0.1934850491607929]}}
{"id": "879f8832-5d03-47eb-823b-7876da759afa", "fitness": 0.19004063661702708, "name": "EnhancedHybridPSOSA", "description": "Improve exploitation by adjusting personal learning factor to enhance convergence towards personal best.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            personal_learning_factor = 2.05  # Modified line\n            self.velocities = self.inertia_weight * self.velocities \\\n                + personal_learning_factor * r1 * (self.personal_best - self.particles) \\\n                + r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 73, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19004 with standard deviation 0.00643.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.18243506853561153, 0.19815409427432384, 0.18953274704114587]}}
{"id": "ce6b8a93-2c3b-4390-af9e-27c00d7c3bf5", "fitness": 0.2007723694666903, "name": "EnhancedHybridPSOSA", "description": "Introduce adaptive learning rates for velocity and mutation based on feedback from fitness improvements to enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20077 with standard deviation 0.00714.", "error": "", "parent_ids": ["4b2d827d-a603-41b0-8987-c52a6dc981e1"], "operator": null, "metadata": {"aucs": [0.20983482211293825, 0.20010810662978584, 0.19237417965734682]}}
{"id": "a36292ca-e7d4-4197-a022-0d4de2e75bae", "fitness": 0.18997996361151592, "name": "EnhancedHybridPSOSA", "description": "Enhanced adaptive mutation through dynamic scaling for improved exploration-exploitation synergy.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget) * np.random.uniform(0.5, 1.5)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 75, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18998 with standard deviation 0.00984.", "error": "", "parent_ids": ["ce6b8a93-2c3b-4390-af9e-27c00d7c3bf5"], "operator": null, "metadata": {"aucs": [0.1781535520547861, 0.20224285979990853, 0.18954347897985313]}}
{"id": "ab131206-2d4d-4f88-9523-3c6f697d3dad", "fitness": 0.19219743215053284, "name": "EnhancedHybridPSOSA", "description": "Incorporate dynamic topology adaptation and multi-phase mutation strategies to improve search efficiency and convergence in EnhancedHybridPSOSA.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.topology_switch_interval = 100  # Interval to switch topology\n        self.topology = np.random.choice(['global', 'local'], p=[0.5, 0.5])\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            if eval_count % self.topology_switch_interval == 0:\n                self.topology = 'local' if self.topology == 'global' else 'global'\n\n            if self.topology == 'global':\n                r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n                self.velocities = self.inertia_weight * self.velocities \\\n                    + self.c1 * r1 * (self.personal_best - self.particles) \\\n                    + self.c2 * r2 * (self.global_best - self.particles)\n            else:\n                neighbors_best = self.get_local_best(fitness)\n                r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n                self.velocities = self.inertia_weight * self.velocities \\\n                    + self.c1 * r1 * (self.personal_best - self.particles) \\\n                    + self.c2 * r2 * (neighbors_best - self.particles)\n\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                if eval_count < self.budget / 2:\n                    step_size = np.random.normal(0, 0.1, self.dim)\n                else:\n                    step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                mutate_vector = np.random.normal(0, mutation_scale, self.dim)\n                candidate = self.particles[i] + step_size + mutate_vector\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best\n\n    def get_local_best(self, fitness):\n        neighbors_best = np.zeros_like(self.particles)\n        for i in range(self.num_particles):\n            local_indices = [(i-1) % self.num_particles, i, (i+1) % self.num_particles]\n            local_fitness = fitness[local_indices]\n            best_local_index = local_indices[np.argmin(local_fitness)]\n            neighbors_best[i] = self.particles[best_local_index]\n        return neighbors_best", "configspace": "", "generation": 76, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19220 with standard deviation 0.00867.", "error": "", "parent_ids": ["ce6b8a93-2c3b-4390-af9e-27c00d7c3bf5"], "operator": null, "metadata": {"aucs": [0.20003734497378378, 0.19644618904216926, 0.18010876243564544]}}
{"id": "c6f20046-eb92-48d8-968c-29ba3dc6b758", "fitness": 0.19072523799662722, "name": "EnhancedHybridPSOSA", "description": "Enhance the convergence by adjusting the random component in stochastic component to improve stability during exploration.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.005  # Adjusted component\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 77, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19073 with standard deviation 0.01516.", "error": "", "parent_ids": ["ce6b8a93-2c3b-4390-af9e-27c00d7c3bf5"], "operator": null, "metadata": {"aucs": [0.171433170393125, 0.20847867478626025, 0.1922638688104964]}}
{"id": "5af48f63-b474-4cb8-a4ad-98d28c76bc03", "fitness": 0.1947344613122982, "name": "EnhancedHybridPSOSA", "description": "Refined velocity update by introducing a nonlinear inertia weight decay to further balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, 0.9 * (0.5 + 0.5 * np.cos(np.pi * eval_count / self.budget)))\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19473 with standard deviation 0.01398.", "error": "", "parent_ids": ["ce6b8a93-2c3b-4390-af9e-27c00d7c3bf5"], "operator": null, "metadata": {"aucs": [0.1752838509576623, 0.20140207277013433, 0.207517460209098]}}
{"id": "89519ccd-2adb-432f-b8d4-8b525ad5bfcc", "fitness": 0.1881375987917886, "name": "AdaptiveDifferentialPSO", "description": "Introduce adaptive differential strategy with a dynamic exploration-exploitation balance guided by particle diversity and fitness improvement feedback.", "code": "import numpy as np\n\nclass AdaptiveDifferentialPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.mutation_factor = 0.5\n        self.crossover_prob = 0.9\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (self.global_best - self.particles)\n\n            # Adaptation based on diversity\n            population_std = np.std(self.particles, axis=0)\n            diversity_factor = np.mean(population_std) / np.max([1e-10, np.linalg.norm(self.global_best)])\n            self.mutation_factor = 0.5 + 0.5 * diversity_factor\n\n            # Differential mutation\n            for i in range(self.num_particles):\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                a, b, c = self.particles[indices[0]], self.particles[indices[1]], self.particles[indices[2]]\n                mutant_vector = a + self.mutation_factor * (b - c)\n                mutant_vector = np.clip(mutant_vector, bounds[0], bounds[1])\n                \n                cross_points = np.random.rand(self.dim) < self.crossover_prob\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial_vector = np.where(cross_points, mutant_vector, self.particles[i])\n                trial_fitness = func(trial_vector)\n                eval_count += 1\n\n                if trial_fitness < fitness[i]:\n                    self.particles[i] = trial_vector\n                    fitness[i] = trial_fitness\n                    if trial_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = trial_vector\n                        pbest_fitness[i] = trial_fitness\n                        if trial_fitness < gbest_fitness:\n                            gbest_fitness = trial_fitness\n                            self.global_best = trial_vector\n\n            self.particles = np.clip(self.particles + self.velocities, bounds[0], bounds[1])\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n        return self.global_best", "configspace": "", "generation": 79, "feedback": "The algorithm AdaptiveDifferentialPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18814 with standard deviation 0.00999.", "error": "", "parent_ids": ["ce6b8a93-2c3b-4390-af9e-27c00d7c3bf5"], "operator": null, "metadata": {"aucs": [0.17435885087775904, 0.19771580718450965, 0.1923381383130971]}}
{"id": "fdaa1b87-0b9d-4927-85b3-cc8016df8e3c", "fitness": 0.19914467425636403, "name": "EnhancedHybridPSOSA", "description": "Introduce an adaptive inertia weight factor that increases when the global best does not improve for several iterations, to enhance exploration.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.inertia_weight_increase = 1.05  # New line: adaptive inertia weight increase\n        self.c1 = 2.0\n        self.c2 = 2.0\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        no_improvement_count = 0  # Initialize count for no improvement\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n                no_improvement_count = 0  # Reset on improvement\n            else:\n                no_improvement_count += 1\n\n            if no_improvement_count > 10:  # Increase inertia weight if no improvement\n                self.inertia_weight *= self.inertia_weight_increase\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 80, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19914 with standard deviation 0.00767.", "error": "", "parent_ids": ["ce6b8a93-2c3b-4390-af9e-27c00d7c3bf5"], "operator": null, "metadata": {"aucs": [0.20949311552835015, 0.1967990725481531, 0.19114183469258883]}}
{"id": "bab7fbb3-d054-4646-b406-7404944b3f60", "fitness": 0.1896008434246418, "name": "EnhancedHybridPSOSA", "description": "Introduce a self-adaptive parameter tuning mechanism to dynamically adjust inertia weight and cognitive-social coefficients based on convergence feedback.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (self.global_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n                self.inertia_weight *= 1.02  # Increase inertia if improved\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n\n        return self.global_best", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18960 with standard deviation 0.01237.", "error": "", "parent_ids": ["ce6b8a93-2c3b-4390-af9e-27c00d7c3bf5"], "operator": null, "metadata": {"aucs": [0.17223717782033487, 0.20009436217372467, 0.19647099027986592]}}
{"id": "5217c7c1-ab1f-4e14-91e2-b5a0904e899e", "fitness": 0.20478189942231284, "name": "EnhancedDynamicTopologyPSO", "description": "Integrate a dynamic topology modification using ring and star formations to enhance information flow and convergence rate.", "code": "import numpy as np\n\nclass EnhancedDynamicTopologyPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.topology_switch_threshold = 0.5\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        topology_switch_counter = 0\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            if topology_switch_counter % 20 < 10:\n                # Star topology\n                local_best = self.global_best\n            else:\n                # Ring topology\n                local_best = np.array([self.particles[(i + 1) % self.num_particles] for i in range(self.num_particles)])\n                local_best = local_best[np.argmin(fitness[(np.arange(self.num_particles) + 1) % self.num_particles])]\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (local_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n            topology_switch_counter += 1\n\n        return self.global_best", "configspace": "", "generation": 82, "feedback": "The algorithm EnhancedDynamicTopologyPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20478 with standard deviation 0.00975.", "error": "", "parent_ids": ["ce6b8a93-2c3b-4390-af9e-27c00d7c3bf5"], "operator": null, "metadata": {"aucs": [0.21011763082654067, 0.21312455444578293, 0.19110351299461492]}}
{"id": "134193dd-ef16-419b-9277-88b21e811b3e", "fitness": 0.1865908900177988, "name": "EnhancedAdaptivePSO", "description": "Combine adaptive inertia weight and neighborhood-based local search to balance exploration and exploitation dynamically.", "code": "import numpy as np\n\nclass EnhancedAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.inertia_weight_max = 1.2\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.topology_switch_threshold = 0.5\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        topology_switch_counter = 0\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(\n                self.inertia_weight_min,\n                self.inertia_weight_max - (eval_count / self.budget) * (self.inertia_weight_max - self.inertia_weight_min)\n            )\n            \n            if topology_switch_counter % 20 < 10:\n                local_best = self.global_best\n            else:\n                local_best_indices = np.roll(np.arange(self.num_particles), -1)\n                local_best = self.particles[local_best_indices[np.argmin(fitness[local_best_indices])]]\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (local_best - self.particles)\n            self.particles = np.clip(self.particles + self.velocities, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = np.clip(self.particles[i] + step_size, bounds[0], bounds[1])\n                mutation = np.random.normal(0, 0.01, self.dim)\n                candidate = np.clip(candidate + mutation, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i]:\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            topology_switch_counter += 1\n\n        return self.global_best", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18659 with standard deviation 0.00956.", "error": "", "parent_ids": ["5217c7c1-ab1f-4e14-91e2-b5a0904e899e"], "operator": null, "metadata": {"aucs": [0.17327440478349287, 0.1952538530762833, 0.1912444121936202]}}
{"id": "4de76d64-4b40-4f71-9db1-35ae0d4463d8", "fitness": 0.1878362609994628, "name": "QuantumAdaptivePSO", "description": "Introduce adaptive learning coefficients and quantum-inspired position updates to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1_initial = 2.0\n        self.c2_initial = 2.0\n        self.c1, self.c2 = self.c1_initial, self.c2_initial\n        self.topology_switch_threshold = 0.5\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        topology_switch_counter = 0\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            if topology_switch_counter % 20 < 10:\n                local_best = self.global_best\n            else:\n                local_best = np.array([self.particles[(i + 1) % self.num_particles] for i in range(self.num_particles)])\n                local_best = local_best[np.argmin(fitness[(np.arange(self.num_particles) + 1) % self.num_particles])]\n\n            self.c1 = self.c1_initial * (0.5 + 0.5 * eval_count / self.budget)\n            self.c2 = self.c2_initial * (1 - eval_count / (2 * self.budget))\n            \n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (local_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                phi = np.random.uniform(-np.pi, np.pi, self.dim)\n                step_size = np.random.normal(0, 0.1, self.dim)\n                quantum_position = self.particles[i] + step_size * np.cos(phi)\n                quantum_position = np.clip(quantum_position, bounds[0], bounds[1])\n                candidate_fitness = func(quantum_position)\n                eval_count += 1\n                if candidate_fitness < fitness[i]:\n                    self.particles[i] = quantum_position\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = quantum_position\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = quantum_position\n\n            topology_switch_counter += 1\n\n        return self.global_best", "configspace": "", "generation": 84, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18784 with standard deviation 0.00652.", "error": "", "parent_ids": ["5217c7c1-ab1f-4e14-91e2-b5a0904e899e"], "operator": null, "metadata": {"aucs": [0.18047289048431336, 0.19632017527051393, 0.18671571724356106]}}
{"id": "16b46256-f4fc-45c2-a144-b05400692a9f", "fitness": 0.19817775859821832, "name": "EnhancedDynamicTopologyPSO", "description": "Introduce adaptive inertia to dynamically balance exploration and exploitation throughout iterations.", "code": "import numpy as np\n\nclass EnhancedDynamicTopologyPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.topology_switch_threshold = 0.5\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        topology_switch_counter = 0\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n\n            # Adaptive inertia weight adjustment\n            self.inertia_weight = 0.4 + 0.5 * (eval_count / self.budget)\n\n            if topology_switch_counter % 20 < 10:\n                # Star topology\n                local_best = self.global_best\n            else:\n                # Ring topology\n                local_best = np.array([self.particles[(i + 1) % self.num_particles] for i in range(self.num_particles)])\n                local_best = local_best[np.argmin(fitness[(np.arange(self.num_particles) + 1) % self.num_particles])]\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (local_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n            topology_switch_counter += 1\n\n        return self.global_best", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedDynamicTopologyPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19818 with standard deviation 0.00971.", "error": "", "parent_ids": ["5217c7c1-ab1f-4e14-91e2-b5a0904e899e"], "operator": null, "metadata": {"aucs": [0.1849314196902705, 0.20168436915038357, 0.20791748695400092]}}
{"id": "712211ae-81e5-415e-be2b-c83349dfd9e7", "fitness": -Infinity, "name": "ImprovedDynamicTopologyPSO", "description": "Enhance DynamicTopologyPSO by introducing adaptive mutation scaling and a convergence-based inertia weight adjustment for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass ImprovedDynamicTopologyPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.topology_switch_threshold = 0.5\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        topology_switch_counter = 0\n\n        while eval_count < self.budget:\n            # Convergence-based inertia weight adjustment\n            global_best_distance = np.linalg.norm(self.global_best - self.particles, axis=1)\n            average_distance = np.mean(global_best_distance)\n            self.inertia_weight = max(self.inertia_weight_min, 0.9 - 0.5 * (1 - average_distance))\n\n            if topology_switch_counter % 20 < 10:\n                # Star topology\n                local_best = self.global_best\n            else:\n                # Ring topology\n                local_best = np.array([self.particles[(i + 1) % self.num_particles] for i in range(self.num_particles)])\n                local_best = local_best[np.argmin(fitness[(np.arange(self.num_particles) + 1) % self.num_particles])]\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (local_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                # Adaptive mutation scale\n                progress_ratio = eval_count / self.budget\n                mutation_scale = 0.01 * (1 + progress_ratio) * (1 - pbest_fitness[i] / gbest_fitness)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            topology_switch_counter += 1\n\n        return self.global_best", "configspace": "", "generation": 86, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_ids": ["5217c7c1-ab1f-4e14-91e2-b5a0904e899e"], "operator": null, "metadata": {}}
{"id": "7b98d9d2-0ae3-47a8-bf1f-89cbcd4697bf", "fitness": 0.1920512233648863, "name": "EnhancedDynamicTopologyPSO", "description": "Introduce adaptive learning rates and dynamic topology adaptation based on convergence behavior to improve exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedDynamicTopologyPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.topology_switch_threshold = 0.5\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        topology_switch_counter = 0\n        stagnation_counter = 0\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            if topology_switch_counter % 20 < 10:\n                local_best = self.global_best\n            else:\n                local_best = np.array([self.particles[(i + 1) % self.num_particles] for i in range(self.num_particles)])\n                local_best = local_best[np.argmin(fitness[(np.arange(self.num_particles) + 1) % self.num_particles])]\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01 * (1 + (stagnation_counter / 10))\n            learning_rate = np.random.uniform(1.5, 2.5, (self.num_particles, self.dim))\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (local_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities * learning_rate\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n            topology_switch_counter += 1\n\n        return self.global_best", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedDynamicTopologyPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19205 with standard deviation 0.01201.", "error": "", "parent_ids": ["5217c7c1-ab1f-4e14-91e2-b5a0904e899e"], "operator": null, "metadata": {"aucs": [0.1753919597694027, 0.20323929272033936, 0.19752241760491684]}}
{"id": "5491d0a9-5093-4886-bd5e-6e27ab0c343f", "fitness": 0.18349244520302535, "name": "EnhancedDynamicTopologyPSO", "description": "Introduce adaptive velocity scaling and chaotic perturbations to boost convergence speed while maintaining diversity.", "code": "import numpy as np\n\nclass EnhancedDynamicTopologyPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.topology_switch_threshold = 0.5\n        self.velocity_scaling_factor = 0.1\n\n    def logistic_map(self, x):  # Chaotic perturbation\n        return 4 * x * (1 - x)\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        topology_switch_counter = 0\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            if topology_switch_counter % 20 < 10:\n                local_best = self.global_best\n            else:\n                local_best = np.array([self.particles[(i + 1) % self.num_particles] for i in range(self.num_particles)])\n                local_best = local_best[np.argmin(fitness[(np.arange(self.num_particles) + 1) % self.num_particles])]\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (local_best - self.particles) \\\n                + stochastic_component\n            self.particles += self.velocities * self.velocity_scaling_factor\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                chaotic_value = self.logistic_map(np.random.rand())\n                step_size = np.random.normal(0, 0.1, self.dim) * chaotic_value\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i]:\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n            topology_switch_counter += 1\n\n        return self.global_best", "configspace": "", "generation": 88, "feedback": "The algorithm EnhancedDynamicTopologyPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18349 with standard deviation 0.01238.", "error": "", "parent_ids": ["5217c7c1-ab1f-4e14-91e2-b5a0904e899e"], "operator": null, "metadata": {"aucs": [0.17156644289002276, 0.20056023994257, 0.17835065277648332]}}
{"id": "479dd330-a4eb-4442-bd10-767e13c4c1e5", "fitness": 0.19116555940505833, "name": "MultiSwarmEnhancedPSO", "description": "Introduce a multi-swarm approach with elite particles exchange for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass MultiSwarmEnhancedPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.num_swarms = 3\n        self.swarm_size = self.num_particles // self.num_swarms\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = np.inf * np.ones(self.dim)\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.topology_switch_threshold = 0.5\n        self.exchange_rate = 5\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        topology_switch_counter = 0\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            for swarm in range(self.num_swarms):\n                swarm_start = swarm * self.swarm_size\n                swarm_end = (swarm + 1) * self.swarm_size\n                local_particles = self.particles[swarm_start:swarm_end]\n                local_velocities = self.velocities[swarm_start:swarm_end]\n                local_best_idx = np.argmin(pbest_fitness[swarm_start:swarm_end])\n                local_best = self.personal_best[swarm_start + local_best_idx]\n\n                r1, r2 = np.random.rand(self.swarm_size, self.dim), np.random.rand(self.swarm_size, self.dim)\n                stochastic_component = np.random.randn(self.swarm_size, self.dim) * 0.01\n                local_velocities = self.inertia_weight * local_velocities \\\n                    + self.c1 * r1 * (self.personal_best[swarm_start:swarm_end] - local_particles) \\\n                    + self.c2 * r2 * (local_best - local_particles) \\\n                    + stochastic_component\n                local_particles += local_velocities\n                local_particles = np.clip(local_particles, bounds[0], bounds[1])\n\n                local_fitness = np.apply_along_axis(func, 1, local_particles)\n                eval_count += self.swarm_size\n\n                improved = local_fitness < pbest_fitness[swarm_start:swarm_end]\n                self.personal_best[swarm_start:swarm_end][improved] = local_particles[improved]\n                pbest_fitness[swarm_start:swarm_end][improved] = local_fitness[improved]\n\n                if np.min(local_fitness) < gbest_fitness:\n                    gbest_fitness = np.min(local_fitness)\n                    self.global_best = local_particles[np.argmin(local_fitness)]\n\n                if eval_count % (self.exchange_rate * self.swarm_size) < self.swarm_size:\n                    elite_indices = np.argpartition(pbest_fitness[swarm_start:swarm_end], 2)[:2]\n                    for i in elite_indices:\n                        step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                        candidate = self.personal_best[swarm_start + i] + step_size\n                        mutation = np.random.normal(0, 0.01, self.dim)\n                        candidate += mutation\n                        candidate = np.clip(candidate, bounds[0], bounds[1])\n                        candidate_fitness = func(candidate)\n                        eval_count += 1\n                        if candidate_fitness < pbest_fitness[swarm_start + i]:\n                            self.personal_best[swarm_start + i] = candidate\n                            pbest_fitness[swarm_start + i] = candidate_fitness\n                            if candidate_fitness < gbest_fitness:\n                                gbest_fitness = candidate_fitness\n                                self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n            topology_switch_counter += 1\n\n        return self.global_best", "configspace": "", "generation": 89, "feedback": "The algorithm MultiSwarmEnhancedPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19117 with standard deviation 0.00232.", "error": "", "parent_ids": ["5217c7c1-ab1f-4e14-91e2-b5a0904e899e"], "operator": null, "metadata": {"aucs": [0.19240869182747977, 0.1931673481945877, 0.18792063819310756]}}
{"id": "79362dbc-a8c5-4fbb-9448-6fc270711c53", "fitness": 0.1911318977016767, "name": "EnhancedDynamicTopologyPSO", "description": "Adjust inertia weight decay to stabilize convergence in EnhancedDynamicTopologyPSO.", "code": "import numpy as np\n\nclass EnhancedDynamicTopologyPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.topology_switch_threshold = 0.5\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        topology_switch_counter = 0\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.998)  # Adjusted decay rate from 0.995 to 0.998\n            \n            if topology_switch_counter % 20 < 10:\n                # Star topology\n                local_best = self.global_best\n            else:\n                # Ring topology\n                local_best = np.array([self.particles[(i + 1) % self.num_particles] for i in range(self.num_particles)])\n                local_best = local_best[np.argmin(fitness[(np.arange(self.num_particles) + 1) % self.num_particles])]\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (local_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n            topology_switch_counter += 1\n\n        return self.global_best", "configspace": "", "generation": 90, "feedback": "The algorithm EnhancedDynamicTopologyPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19113 with standard deviation 0.00442.", "error": "", "parent_ids": ["5217c7c1-ab1f-4e14-91e2-b5a0904e899e"], "operator": null, "metadata": {"aucs": [0.18488107087208017, 0.19441114118009506, 0.1941034810528549]}}
{"id": "d8664b55-089b-46a7-a83c-764fd387018d", "fitness": 0.20172275141010101, "name": "EnhancedDynamicTopologyPSO", "description": "Incorporate adaptive mutation rates and a dynamic inertia weight adjustment based on diversity measures to enhance both exploration and exploitation phases. ", "code": "import numpy as np\n\nclass EnhancedDynamicTopologyPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.topology_switch_threshold = 0.5\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        topology_switch_counter = 0\n\n        while eval_count < self.budget:\n            # Calculate diversity\n            diversity = np.mean(np.std(self.particles, axis=0))\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * (1 - diversity / self.dim))\n            \n            if topology_switch_counter % 20 < 10:\n                # Star topology\n                local_best = self.global_best\n            else:\n                # Ring topology\n                local_best = np.array([self.particles[(i + 1) % self.num_particles] for i in range(self.num_particles)])\n                local_best = local_best[np.argmin(fitness[(np.arange(self.num_particles) + 1) % self.num_particles])]\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (local_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 - diversity / self.dim)  # Adaptive mutation based on diversity\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n            topology_switch_counter += 1\n\n        return self.global_best", "configspace": "", "generation": 91, "feedback": "The algorithm EnhancedDynamicTopologyPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20172 with standard deviation 0.00255.", "error": "", "parent_ids": ["5217c7c1-ab1f-4e14-91e2-b5a0904e899e"], "operator": null, "metadata": {"aucs": [0.20450535284131166, 0.2023132720093952, 0.19834962937959622]}}
{"id": "3f3df838-50ae-412c-bb59-60bc56b2c2d8", "fitness": 0.19246478852426177, "name": "EnhancedDynamicTopologyPSO", "description": "Introduce a small random scaling factor to inertia weight for enhancing exploration.", "code": "import numpy as np\n\nclass EnhancedDynamicTopologyPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.topology_switch_threshold = 0.5\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        topology_switch_counter = 0\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995) * np.random.uniform(0.95, 1.05)\n            \n            if topology_switch_counter % 20 < 10:\n                # Star topology\n                local_best = self.global_best\n            else:\n                # Ring topology\n                local_best = np.array([self.particles[(i + 1) % self.num_particles] for i in range(self.num_particles)])\n                local_best = local_best[np.argmin(fitness[(np.arange(self.num_particles) + 1) % self.num_particles])]\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (local_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n            topology_switch_counter += 1\n\n        return self.global_best", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedDynamicTopologyPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19246 with standard deviation 0.00430.", "error": "", "parent_ids": ["5217c7c1-ab1f-4e14-91e2-b5a0904e899e"], "operator": null, "metadata": {"aucs": [0.19063136819008752, 0.18835435604337303, 0.1984086413393248]}}
{"id": "62fa32c1-65f0-4018-9063-963c590a2c8e", "fitness": 0.19244600247998067, "name": "RefinedDynamicTopologyPSO", "description": "Incorporate adaptive neighborhood selection with dynamic inertia and stochastic perturbation to balance exploration and exploitation in particle swarm optimization.", "code": "import numpy as np\n\nclass RefinedDynamicTopologyPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.topology_switch_threshold = 0.5\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        topology_switch_counter = 0\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.99)\n            \n            if topology_switch_counter % 20 < 10:\n                # Star topology\n                local_best = self.global_best\n            else:\n                # Adaptively select neighbors\n                neighbors = [(i + np.random.randint(1, self.num_particles - 1)) % self.num_particles for i in range(self.num_particles)]\n                local_best = self.particles[neighbors]\n                local_best = local_best[np.argmin(fitness[neighbors])]\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (local_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n            topology_switch_counter += 1\n\n        return self.global_best", "configspace": "", "generation": 93, "feedback": "The algorithm RefinedDynamicTopologyPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19245 with standard deviation 0.01420.", "error": "", "parent_ids": ["5217c7c1-ab1f-4e14-91e2-b5a0904e899e"], "operator": null, "metadata": {"aucs": [0.17236158975779214, 0.2021438399519141, 0.20283257773023577]}}
{"id": "9c2d9008-8e0b-4387-b5ca-078ecaa6cfed", "fitness": 0.19035598820197797, "name": "EnhancedDynamicTopologyPSO", "description": "Integrates adaptive learning rates and diverse mutation strategies to enhance exploration-exploitation balance and convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicTopologyPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.topology_switch_threshold = 0.5\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        topology_switch_counter = 0\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            if topology_switch_counter % 20 < 10:\n                # Star topology\n                local_best = self.global_best\n            else:\n                # Ring topology\n                local_best = np.array([self.particles[(i + 1) % self.num_particles] for i in range(self.num_particles)])\n                local_best = local_best[np.argmin(fitness[(np.arange(self.num_particles) + 1) % self.num_particles])]\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (local_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget) * np.random.choice([0.5, 1.5])\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n            topology_switch_counter += 1\n\n        return self.global_best", "configspace": "", "generation": 94, "feedback": "The algorithm EnhancedDynamicTopologyPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19036 with standard deviation 0.00645.", "error": "", "parent_ids": ["5217c7c1-ab1f-4e14-91e2-b5a0904e899e"], "operator": null, "metadata": {"aucs": [0.18159296044088313, 0.19255084614375417, 0.1969241580212966]}}
{"id": "53b39c9a-a628-4ed9-8f5b-eeb6a104a344", "fitness": 0.19661742847512284, "name": "EnhancedDynamicTopologyPSO", "description": "Fine-tune inertia weight decay rate for EnhancedDynamicTopologyPSO to potentially enhance convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicTopologyPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.topology_switch_threshold = 0.5\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        topology_switch_counter = 0\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.997)  # Change rate here\n            \n            if topology_switch_counter % 20 < 10:\n                # Star topology\n                local_best = self.global_best\n            else:\n                # Ring topology\n                local_best = np.array([self.particles[(i + 1) % self.num_particles] for i in range(self.num_particles)])\n                local_best = local_best[np.argmin(fitness[(np.arange(self.num_particles) + 1) % self.num_particles])]\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (local_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n            topology_switch_counter += 1\n\n        return self.global_best", "configspace": "", "generation": 95, "feedback": "The algorithm EnhancedDynamicTopologyPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19662 with standard deviation 0.00664.", "error": "", "parent_ids": ["5217c7c1-ab1f-4e14-91e2-b5a0904e899e"], "operator": null, "metadata": {"aucs": [0.18979950837676807, 0.2056262952544451, 0.19442648179415534]}}
{"id": "cbeaad20-d845-45de-85ed-63612c61f102", "fitness": 0.1489606196835156, "name": "EnhancedDynamicTopologyPSO", "description": "Introduce adaptive learning factors and chaos theory for velocity update to enhance convergence and escape local optima.", "code": "import numpy as np\n\nclass EnhancedDynamicTopologyPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.topology_switch_threshold = 0.5\n        self.chaos_factor = 0.7\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        topology_switch_counter = 0\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            if topology_switch_counter % 20 < 10:\n                # Star topology\n                local_best = self.global_best\n            else:\n                # Ring topology\n                local_best = np.array([self.particles[(i + 1) % self.num_particles] for i in range(self.num_particles)])\n                local_best = local_best[np.argmin(fitness[(np.arange(self.num_particles) + 1) % self.num_particles])]\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            # Adaptive learning factors\n            c1 = self.c1 * (1 - eval_count / self.budget)\n            c2 = self.c2 * (eval_count / self.budget)\n\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            chaos_component = np.sin(np.pi * eval_count / self.budget) * self.chaos_factor\n            self.velocities = self.inertia_weight * self.velocities \\\n                + c1 * r1 * (self.personal_best - self.particles) \\\n                + c2 * r2 * (local_best - self.particles) \\\n                + stochastic_component + chaos_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n            topology_switch_counter += 1\n\n        return self.global_best", "configspace": "", "generation": 96, "feedback": "The algorithm EnhancedDynamicTopologyPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.14896 with standard deviation 0.01631.", "error": "", "parent_ids": ["5217c7c1-ab1f-4e14-91e2-b5a0904e899e"], "operator": null, "metadata": {"aucs": [0.16779511476360465, 0.1510803747076791, 0.12800636957926304]}}
{"id": "8f931075-5e0e-452e-ac67-0d358e23cac0", "fitness": 0.19129217262378953, "name": "EnhancedDynamicTopologyPSO", "description": "Introduce adaptive c1 and c2 coefficients based on fitness improvement to balance exploration and exploitation more effectively.", "code": "import numpy as np\n\nclass EnhancedDynamicTopologyPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.topology_switch_threshold = 0.5\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        topology_switch_counter = 0\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            if topology_switch_counter % 20 < 10:\n                # Star topology\n                local_best = self.global_best\n            else:\n                # Ring topology\n                local_best = np.array([self.particles[(i + 1) % self.num_particles] for i in range(self.num_particles)])\n                local_best = local_best[np.argmin(fitness[(np.arange(self.num_particles) + 1) % self.num_particles])]\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (local_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n            topology_switch_counter += 1\n\n            # Adaptive c1 and c2 based on improvement\n            if improved.any():\n                self.c1, self.c2 = 2.5, 1.5\n            else:\n                self.c1, self.c2 = 2.0, 2.0\n\n        return self.global_best", "configspace": "", "generation": 97, "feedback": "The algorithm EnhancedDynamicTopologyPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19129 with standard deviation 0.01099.", "error": "", "parent_ids": ["5217c7c1-ab1f-4e14-91e2-b5a0904e899e"], "operator": null, "metadata": {"aucs": [0.17575280890684752, 0.19865090421952736, 0.19947280474499374]}}
{"id": "7beabdcf-07b5-4e7e-a553-ad1d70522147", "fitness": 0.19471642888361287, "name": "RefinedAdaptivePSO", "description": "Introduce adaptive inertia and adaptive learning coefficients based on swarm diversity to refine convergence and exploration balance.", "code": "import numpy as np\n\nclass RefinedAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1_initial = 2.0\n        self.c2_initial = 2.0\n        self.c1 = self.c1_initial\n        self.c2 = self.c2_initial\n        self.topology_switch_threshold = 0.5\n\n    def calculate_diversity(self):\n        centroid = np.mean(self.particles, axis=0)\n        distances = np.linalg.norm(self.particles - centroid, axis=1)\n        return np.mean(distances)\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        topology_switch_counter = 0\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            diversity = self.calculate_diversity()\n            self.c1 = max(1.5, self.c1_initial * (1 + diversity))\n            self.c2 = max(1.5, self.c2_initial * (1 - diversity))\n            \n            if topology_switch_counter % 20 < 10:\n                local_best = self.global_best\n            else:\n                local_best = np.array([self.particles[(i + 1) % self.num_particles] for i in range(self.num_particles)])\n                local_best = local_best[np.argmin(fitness[(np.arange(self.num_particles) + 1) % self.num_particles])]\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (local_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                mutation_scale = 0.01 * (1 + eval_count / self.budget)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i]:\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            topology_switch_counter += 1\n\n        return self.global_best", "configspace": "", "generation": 98, "feedback": "The algorithm RefinedAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19472 with standard deviation 0.00839.", "error": "", "parent_ids": ["5217c7c1-ab1f-4e14-91e2-b5a0904e899e"], "operator": null, "metadata": {"aucs": [0.18288443885339578, 0.20133383624669443, 0.19993101155074844]}}
{"id": "3675794f-dec8-4776-9505-3295f2d6cb69", "fitness": 0.19333044354659665, "name": "EnhancedDynamicTopologyPSO", "description": "Introduce adaptive mutation scale based on the improvement rate to balance exploration and exploitation phases.", "code": "import numpy as np\n\nclass EnhancedDynamicTopologyPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.particles = np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best = self.particles.copy()\n        self.global_best = self.particles[np.random.choice(range(self.num_particles))]\n        self.temp = 100\n        self.alpha = 0.99\n        self.initial_alpha = self.alpha\n        self.inertia_weight = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.topology_switch_threshold = 0.5\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        self.particles = bounds[0] + self.particles * (bounds[1] - bounds[0])\n        self.personal_best = self.particles.copy()\n        fitness = np.apply_along_axis(func, 1, self.particles)\n        pbest_fitness = fitness.copy()\n        gbest_fitness = np.min(fitness)\n        self.global_best = self.particles[np.argmin(fitness)]\n\n        eval_count = self.num_particles\n        topology_switch_counter = 0\n\n        while eval_count < self.budget:\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * 0.995)\n            \n            if topology_switch_counter % 20 < 10:\n                # Star topology\n                local_best = self.global_best\n            else:\n                # Ring topology\n                local_best = np.array([self.particles[(i + 1) % self.num_particles] for i in range(self.num_particles)])\n                local_best = local_best[np.argmin(fitness[(np.arange(self.num_particles) + 1) % self.num_particles])]\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            stochastic_component = np.random.randn(self.num_particles, self.dim) * 0.01\n            self.velocities = self.inertia_weight * self.velocities \\\n                + self.c1 * r1 * (self.personal_best - self.particles) \\\n                + self.c2 * r2 * (local_best - self.particles) \\\n                + stochastic_component\n            self.particles = self.particles + self.velocities\n            self.particles = np.clip(self.particles, bounds[0], bounds[1])\n\n            fitness = np.apply_along_axis(func, 1, self.particles)\n            eval_count += self.num_particles\n\n            improved = fitness < pbest_fitness\n            self.personal_best[improved] = self.particles[improved]\n            pbest_fitness[improved] = fitness[improved]\n\n            if np.min(fitness) < gbest_fitness:\n                gbest_fitness = np.min(fitness)\n                self.global_best = self.particles[np.argmin(fitness)]\n\n            for i in range(self.num_particles):\n                step_size = np.random.normal(0, 0.1, self.dim) * np.random.pareto(1.5, self.dim)\n                candidate = self.particles[i] + step_size\n                improvement_rate = np.mean(improved)\n                mutation_scale = 0.01 * (1 + eval_count / self.budget) / (1 + improvement_rate)\n                mutation = np.random.normal(0, mutation_scale, self.dim)\n                candidate += mutation\n                candidate = np.clip(candidate, bounds[0], bounds[1])\n                candidate_fitness = func(candidate)\n                eval_count += 1\n                if candidate_fitness < fitness[i] or np.exp((fitness[i] - candidate_fitness) / self.temp) > np.random.rand():\n                    self.particles[i] = candidate\n                    fitness[i] = candidate_fitness\n                    if candidate_fitness < pbest_fitness[i]:\n                        self.personal_best[i] = candidate\n                        pbest_fitness[i] = candidate_fitness\n                        if candidate_fitness < gbest_fitness:\n                            gbest_fitness = candidate_fitness\n                            self.global_best = candidate\n\n            self.temp *= self.alpha\n            self.alpha = max(0.9 * self.alpha, self.initial_alpha * 0.1)\n            topology_switch_counter += 1\n\n        return self.global_best", "configspace": "", "generation": 99, "feedback": "The algorithm EnhancedDynamicTopologyPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19333 with standard deviation 0.01377.", "error": "", "parent_ids": ["5217c7c1-ab1f-4e14-91e2-b5a0904e899e"], "operator": null, "metadata": {"aucs": [0.17704654140296083, 0.21072686782526728, 0.1922179214115618]}}
