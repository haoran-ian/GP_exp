{"id": "b8a3c91e-0319-428e-9038-78c812cc8ce6", "fitness": 0.055417073283749736, "name": "AdaptiveDifferentialEvolution", "description": "An adaptive differential evolution algorithm with a dynamic crossover rate and mutation strategy to efficiently explore and exploit the search space.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n\n    def adapt_parameters(self):\n        self.crossover_rate = np.random.rand() * 0.5 + 0.25\n        self.mutation_factor = np.random.rand() * 0.4 + 0.6\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness)\n                if evaluations >= self.budget:\n                    break\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05542 with standard deviation 0.05303.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1294528786101916, 0.1304249999510395, 0.12072921634607858, 0.040252069851881345, 0.03957381372628899, 0.03765401440160088, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "28285b93-0b10-40fe-860e-9932f51b41cb", "fitness": 0.055374883772856966, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with Dynamic Population Sizing and Adaptive Mutation for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.adaptive_mutation_factor() * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def adaptive_mutation_factor(self):\n        return self.mutation_factor + np.random.rand() * 0.2\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n\n    def adapt_parameters(self):\n        self.crossover_rate = np.random.rand() * 0.5 + 0.25\n        self.mutation_factor = np.random.rand() * 0.4 + 0.6\n        if self.generations % 10 == 0:\n            self.population_size = max(4, int(0.9 * self.population_size))\n            self.reinitialize_population()\n\n    def reinitialize_population(self):\n        lb, ub = self.bounds.lb, self.bounds.ub\n        self.population = np.vstack((self.population[:self.population_size],\n                                     lb + (ub - lb) * np.random.rand(self.initial_population_size - self.population_size, self.dim)))\n        self.fitness = np.hstack((self.fitness[:self.population_size],\n                                  np.full(self.initial_population_size - self.population_size, np.inf)))\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness)\n                if evaluations >= self.budget:\n                    break\n            self.generations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 1, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05537 with standard deviation 0.05319.", "error": "", "parent_ids": ["b8a3c91e-0319-428e-9038-78c812cc8ce6"], "operator": null, "metadata": {"aucs": [0.12722236573074708, 0.13113643951495135, 0.12315544889779828, 0.04031188856304346, 0.03677758013620258, 0.03910356444630325, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "c239a7d4-edda-4dd8-9247-2cb0457a9821", "fitness": 0.05601720477868361, "name": "AdaptiveDifferentialEvolution", "description": "Improved Adaptive Differential Evolution by fine-tuning the mutation factor adaptation for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n\n    def adapt_parameters(self):\n        self.crossover_rate = np.random.rand() * 0.5 + 0.25\n        self.mutation_factor = np.random.rand() * 0.5 + 0.5  # changed from 0.4 to 0.5 for better exploration\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness)\n                if evaluations >= self.budget:\n                    break\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05602 with standard deviation 0.05389.", "error": "", "parent_ids": ["b8a3c91e-0319-428e-9038-78c812cc8ce6"], "operator": null, "metadata": {"aucs": [0.12790694848439377, 0.13077372556281863, 0.12787593432234234, 0.040252069851881345, 0.03803215051593134, 0.038647347604118365, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "fbc02682-5ca2-49d0-a745-d859330c98e7", "fitness": 0.05709381258567429, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with dynamic population control and improved parameter self-adaptation for robust optimization.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True  # Enable dynamic control\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n\n    def adapt_parameters(self):\n        self.crossover_rate = 0.1 + np.random.rand() * 0.9  # Broader range for crossover\n        self.mutation_factor = 0.6 + np.random.rand() * 0.4  # Broader range for mutation factor\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()  # Adjust population size if needed\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness)\n                if evaluations >= self.budget:\n                    break\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05709 with standard deviation 0.05483.", "error": "", "parent_ids": ["c239a7d4-edda-4dd8-9247-2cb0457a9821"], "operator": null, "metadata": {"aucs": [0.13008324844319885, 0.13027346675282414, 0.13305887820756412, 0.042615680434316405, 0.03893575978298358, 0.0382106129835148, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "33e3022b-e068-4bdd-b310-54ebc55e2f31", "fitness": 0.05644764489744755, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Optimized Differential Evolution with diversity preservation through crowding distance and enhanced mutation for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True  # Enable dynamic control\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n\n    def adapt_parameters(self):\n        self.crossover_rate = 0.1 + np.random.rand() * 0.9\n        self.mutation_factor = 0.6 + np.random.rand() * 0.4\n        # Introduce diversity preservation\n        self.crossover_rate += 0.1 * (np.std(self.fitness) / np.mean(self.fitness))\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n            # Enhance mutation for population control\n            self.mutation_factor *= 1.2\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness)\n                if evaluations >= self.budget:\n                    break\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05645 with standard deviation 0.05435.", "error": "", "parent_ids": ["fbc02682-5ca2-49d0-a745-d859330c98e7"], "operator": null, "metadata": {"aucs": [0.12698400157147693, 0.1325049467471312, 0.13025266717852135, 0.04105717751067839, 0.037534168887360386, 0.039029175515193026, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "0fbc4952-1fc8-4e14-90ac-3c41fde4af6c", "fitness": 0.05709381258567429, "name": "AdaptiveDiversityGuidedDifferentialEvolution", "description": "Adaptive Differential Evolution with Dynamic Population and Diversity-guided Parameter Adaptation for Enhanced Robustness and Efficiency.", "code": "import numpy as np\n\nclass AdaptiveDiversityGuidedDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.diversity_threshold = 0.1  # New: threshold to monitor diversity\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n\n    def adapt_parameters(self):\n        # New: Adapt parameters based on diversity\n        diversity = np.mean(np.std(self.population, axis=0))\n        if diversity < self.diversity_threshold:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.9  # Increase mutation factor if diversity is low\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness)\n                if evaluations >= self.budget:\n                    break\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDiversityGuidedDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05709 with standard deviation 0.05483.", "error": "", "parent_ids": ["fbc02682-5ca2-49d0-a745-d859330c98e7"], "operator": null, "metadata": {"aucs": [0.13008324844319885, 0.13027346675282414, 0.13305887820756412, 0.042615680434316405, 0.03893575978298358, 0.0382106129835148, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "9847e599-4ce4-4709-87fe-82409626147b", "fitness": 0.055707156585826416, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with fitness-based dynamic mutation and crossover strategies for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.base_crossover_rate = 0.5\n        self.base_mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n\n    def adapt_parameters(self):\n        best_fitness = np.min(self.fitness)\n        worst_fitness = np.max(self.fitness)\n        fitness_range = worst_fitness - best_fitness\n\n        self.crossover_rate = self.base_crossover_rate + (1 - self.base_crossover_rate) * (1 - (best_fitness / (fitness_range + 1e-9)))\n        self.mutation_factor = self.base_mutation_factor + (1 - self.base_mutation_factor) * (1 - (best_fitness / (fitness_range + 1e-9)))\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness)\n                if evaluations >= self.budget:\n                    break\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05571 with standard deviation 0.05399.", "error": "", "parent_ids": ["fbc02682-5ca2-49d0-a745-d859330c98e7"], "operator": null, "metadata": {"aucs": [0.12854248895331633, 0.12995297034881015, 0.1279877116794782, 0.04160842429199607, 0.037930363844013826, 0.034675783488156475, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "e9e1a582-794c-4dab-bab9-05e13f78aa27", "fitness": 0.054394848009701485, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with self-adaptive crossover rate and mutation factor with elitism for enhanced convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n\n    def adapt_parameters(self):\n        self.crossover_rate = 0.1 + np.random.rand() * 0.9\n        self.mutation_factor = 0.5 + np.random.rand() * 0.5  # Adjusted mutation factor range\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness)\n                if evaluations >= self.budget:\n                    break\n\n            # Elitism: always keep the best solution\n            best_idx = np.argmin(self.fitness)\n            best_solution = self.population[best_idx]\n            for i in range(self.population_size):\n                if func(self.population[i]) > func(best_solution):\n                    self.population[i] = best_solution\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05439 with standard deviation 0.05174.", "error": "", "parent_ids": ["fbc02682-5ca2-49d0-a745-d859330c98e7"], "operator": null, "metadata": {"aucs": [0.12273297353359613, 0.1303081503219975, 0.11871608172385706, 0.041959699892743174, 0.037865845503296414, 0.037304214445156414, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "b382aadc-71c1-47fc-b506-b3d592e4ec3d", "fitness": 0.05616541814916996, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with adaptive local search integration for improved convergence in multimodal landscapes.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.local_search_prob = 0.1  # Probability of applying local search\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n\n    def adapt_parameters(self):\n        self.crossover_rate = 0.1 + np.random.rand() * 0.9\n        self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def local_search(self, vector, func):\n        step_size = (self.bounds.ub - self.bounds.lb) * 0.05\n        candidate = vector + np.random.uniform(-step_size, step_size)\n        candidate = np.clip(candidate, self.bounds.lb, self.bounds.ub)\n        candidate_fitness = func(candidate)\n        return candidate, candidate_fitness\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                if np.random.rand() < self.local_search_prob:\n                    trial, trial_fitness = self.local_search(self.population[i], func)\n                else:\n                    mutant = self.mutate(i)\n                    trial = self.crossover(self.population[i], mutant)\n                    trial_fitness = func(trial)\n                \n                evaluations += 1\n                self.select(i, trial, trial_fitness)\n                if evaluations >= self.budget:\n                    break\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05617 with standard deviation 0.05400.", "error": "", "parent_ids": ["fbc02682-5ca2-49d0-a745-d859330c98e7"], "operator": null, "metadata": {"aucs": [0.12522831181280603, 0.13286196012834384, 0.12920680730722378, 0.04025255323086818, 0.03826496622471032, 0.03900749797191083, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "71039ad3-7cc0-44fd-881f-ae9baf73c874", "fitness": 0.05638873396781093, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Hybrid Enhanced Adaptive Differential Evolution with local search exploitation and improved parameter adaptation for more efficient convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True  # Enable dynamic control\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n\n    def adapt_parameters(self):\n        self.crossover_rate = 0.2 + np.random.rand() * 0.8  # Adjusted range for crossover\n        self.mutation_factor = 0.5 + np.random.rand() * 0.5  # Adjusted range for mutation factor\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def local_search(self, best_vector, func):\n        perturbation = 0.1 * (self.bounds.ub - self.bounds.lb)\n        trial_vector = best_vector + perturbation * np.random.randn(self.dim)\n        trial_vector = np.clip(trial_vector, self.bounds.lb, self.bounds.ub)\n        trial_fitness = func(trial_vector)\n        return trial_vector, trial_fitness\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()  # Adjust population size if needed\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness)\n                if evaluations >= self.budget:\n                    break\n\n            # Local search for exploitation\n            best_idx = np.argmin(self.fitness)\n            best_vector = self.population[best_idx]\n            local_best_vector, local_best_fitness = self.local_search(best_vector, func)\n            if local_best_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_best_vector\n                self.fitness[best_idx] = local_best_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 9, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05639 with standard deviation 0.05350.", "error": "", "parent_ids": ["fbc02682-5ca2-49d0-a745-d859330c98e7"], "operator": null, "metadata": {"aucs": [0.12699888335713483, 0.13192341241849348, 0.12601513863366587, 0.04305179992137187, 0.03834634493472733, 0.04049635977823829, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "9c221614-2963-480e-b82e-701e865dc981", "fitness": -Infinity, "name": "EnhancedDynamicEvolutionaryStrategy", "description": "Enhanced Dynamic Evolutionary Strategy with adaptive parameter tuning and surrogate modeling for efficient black-box optimization.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\n\nclass EnhancedDynamicEvolutionaryStrategy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.surrogate_model = GaussianProcessRegressor(kernel=RBF(length_scale=1.0), alpha=1e-6)\n        self.use_surrogate = True  # Enable surrogate model\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n\n    def adapt_parameters(self):\n        self.crossover_rate = 0.1 + np.random.rand() * 0.9\n        self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def surrogate_evaluate(self, trial_vector):\n        predicted_fitness, _ = self.surrogate_model.predict(trial_vector.reshape(1, -1), return_std=True)\n        return predicted_fitness[0]\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        all_data_X = self.population.copy()\n        all_data_y = self.fitness.copy()\n\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                \n                if self.use_surrogate and evaluations >= self.population_size:\n                    trial_fitness = self.surrogate_evaluate(trial)\n                else:\n                    trial_fitness = func(trial)\n                    evaluations += 1\n                    if self.use_surrogate and evaluations % (self.population_size // 2) == 0:\n                        self.surrogate_model.fit(all_data_X, all_data_y)\n                    all_data_X = np.vstack((all_data_X, trial))\n                    all_data_y = np.append(all_data_y, trial_fitness)\n\n                self.select(i, trial, trial_fitness)\n                if evaluations >= self.budget:\n                    break\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 10, "feedback": "An exception occurred: IndexError('too many indices for array: array is 0-dimensional, but 1 were indexed').", "error": "IndexError('too many indices for array: array is 0-dimensional, but 1 were indexed')", "parent_ids": ["fbc02682-5ca2-49d0-a745-d859330c98e7"], "operator": null, "metadata": {}}
{"id": "a0a16324-cf97-4818-9256-f980a902424a", "fitness": 0.06674871374818007, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with Self-Adjusting Learning Rates and Memory Mechanism for Improved Search Efficiency.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True  # Enable dynamic control\n        self.memory = []  # Memory for storing past successes\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))  # Store successful parameters\n\n    def adapt_parameters(self):\n        if self.memory:\n            # Use memory to adjust parameters, rewarding successful rates\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()  # Adjust population size if needed\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness)\n                if evaluations >= self.budget:\n                    break\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 11, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06675 with standard deviation 0.07106.", "error": "", "parent_ids": ["fbc02682-5ca2-49d0-a745-d859330c98e7"], "operator": null, "metadata": {"aucs": [0.21101686981192458, 0.1361026730865702, 0.13305887820756412, 0.040788592853447825, 0.03996103292179476, 0.039143710185652436, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "840c8324-fee5-49c8-a6b6-eae9535a47c4", "fitness": 0.05697381849058261, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Improved Adaptive Differential Evolution with Enhanced Memory and Dynamic Strategy Selection for Superior Optimization Performance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = []  # Memory for strategy success\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory.append((True, self.mutation_factor))\n        else:\n            self.strategy_memory.append((False, self.mutation_factor))\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n        \n        if self.strategy_memory:\n            success_rate = sum(1 for success, _ in self.strategy_memory if success) / len(self.strategy_memory)\n            if success_rate < 0.2:\n                self.mutation_factor = np.clip(self.mutation_factor + 0.1, 0.5, 1.0)\n            elif success_rate > 0.8:\n                self.mutation_factor = np.clip(self.mutation_factor - 0.1, 0.5, 1.0)\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness)\n                if evaluations >= self.budget:\n                    break\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05697 with standard deviation 0.05472.", "error": "", "parent_ids": ["a0a16324-cf97-4818-9256-f980a902424a"], "operator": null, "metadata": {"aucs": [0.12304281249354743, 0.1361026730865702, 0.13305887820756412, 0.040788592853447825, 0.03996103292179476, 0.039143710185652436, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "bd7f8509-7804-4f58-a879-d4c17acc29cc", "fitness": 0.05697381849058261, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Hybrid Differential Evolution with Adaptive Population and Adaptive Strategies for Enhanced Global Search.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.9  # Adjusted crossover rate for exploration\n        self.mutation_factor = 0.6 # Adjusted mutation factor for diversity\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = []  # Memory for storing successful strategies\n    \n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory.append(\"DE/rand/1/bin\")  # Store successful strategy\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n        elif best_fitness > 1e-2:\n            self.population_size = min(int(self.population_size * 1.1), self.initial_population_size)  # Increase pop size\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness)\n                if evaluations >= self.budget:\n                    break\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 13, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05697 with standard deviation 0.05472.", "error": "", "parent_ids": ["a0a16324-cf97-4818-9256-f980a902424a"], "operator": null, "metadata": {"aucs": [0.12304281249354743, 0.1361026730865702, 0.13305887820756412, 0.040788592853447825, 0.03996103292179476, 0.039143710185652436, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "c1d65e20-bb0d-4dba-9f22-cb83c9650d84", "fitness": 0.05622091761388884, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Introduction of a dynamic scaling factor and improved memory adaptation for enhanced search robustness.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.scaling_factor = 0.5 + np.random.rand() * 0.5  # Dynamic scaling factor\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.randn() * 0.1, 0.1, 0.9)  # Gaussian noise for diversification\n            self.mutation_factor = np.clip(average_f + np.random.randn() * 0.1, 0.5, 1.0)\n            self.scaling_factor = np.clip(np.mean(self.fitness) / (1 + self.generations), 0.5, 1.0)  # New scaling factor\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * self.scaling_factor), self.dim * 2)  # Apply scaling factor\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness)\n                if evaluations >= self.budget:\n                    break\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 14, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05622 with standard deviation 0.05308.", "error": "", "parent_ids": ["a0a16324-cf97-4818-9256-f980a902424a"], "operator": null, "metadata": {"aucs": [0.12748580023172862, 0.1257039363178164, 0.1291101208424279, 0.03648539576873289, 0.04233906523373743, 0.044197273463889575, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "a0028203-f470-4700-8541-661dcf9ca348", "fitness": 0.05697381849058261, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Differential Evolution with Adaptive Mutation and Crossover Using Multi-Strategy Population Diversification.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.85), self.dim * 2)  # Change factor\n\n    def multi_strategy_population_diversification(self):  # New method\n        if np.std(self.fitness) < 1e-3:  # Check stagnation\n            diversification_factor = 0.2\n            for i in range(self.population_size):\n                if np.random.rand() < diversification_factor:\n                    self.population[i] = self.bounds.lb + (self.bounds.ub - self.bounds.lb) * np.random.rand(self.dim)\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            self.multi_strategy_population_diversification()  # Invoke diversification\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness)\n                if evaluations >= self.budget:\n                    break\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 15, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05697 with standard deviation 0.05472.", "error": "", "parent_ids": ["a0a16324-cf97-4818-9256-f980a902424a"], "operator": null, "metadata": {"aucs": [0.12304281249354743, 0.1361026730865702, 0.13305887820756412, 0.040788592853447825, 0.03996103292179476, 0.039143710185652436, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "6ee04f6f-ab9e-4592-b893-c04a01c3a313", "fitness": 0.05740977848216608, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with Dynamic Parameter Tuning and Adaptive Population Control for Efficient Search.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True  # Enable dynamic control\n        self.memory = []  # Memory for storing past successes\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.randn() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.randn() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.85), self.dim * 2)\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness)\n                if evaluations >= self.budget:\n                    break\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 16, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05741 with standard deviation 0.05571.", "error": "", "parent_ids": ["a0a16324-cf97-4818-9256-f980a902424a"], "operator": null, "metadata": {"aucs": [0.13467921175016007, 0.13100202479998335, 0.13305887820756412, 0.040951162283006215, 0.03936589741581542, 0.03696416521629886, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "73cfba60-c8f2-428b-aab0-06236de54fd7", "fitness": -Infinity, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with Self-Adjusting and Dynamic Population Control augmented by Adaptive Mutation Factor Reduction for accelerated convergence and robustness.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.base_mutation_factor = 0.8\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.normal(0, 0.1), 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f * (1 - (evaluations / self.budget)), 0.1, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness)\n                if evaluations >= self.budget:\n                    break\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 17, "feedback": "An exception occurred: NameError(\"name 'evaluations' is not defined\").", "error": "NameError(\"name 'evaluations' is not defined\")", "parent_ids": ["a0a16324-cf97-4818-9256-f980a902424a"], "operator": null, "metadata": {}}
{"id": "d6af1740-63c1-4a96-ab8a-9b0c641a140d", "fitness": 0.07653715340505403, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Adaptive Memetic Differential Evolution with Dynamic Strategy Selection and Local Search for Enhanced Convergence.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 18, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07654 with standard deviation 0.09360.", "error": "", "parent_ids": ["a0a16324-cf97-4818-9256-f980a902424a"], "operator": null, "metadata": {"aucs": [0.30284183263495335, 0.1321136590817955, 0.13233369168324427, 0.040495551940109675, 0.04301701065491925, 0.03736596798379754, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "b1212170-bdb2-41d2-893c-ea48a937d564", "fitness": 0.055280814836794825, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Introduce a bias towards the best strategy using heuristic success rate for improved convergence.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total_success = sum(self.strategy_memory.values())\n        if total_success > 0:\n            probabilities = {k: v / total_success for k, v in self.strategy_memory.items()}\n            strategies, weights = zip(*probabilities.items())\n            strategy = np.random.choice(strategies, p=weights)\n        else:\n            strategy = np.random.choice(['DE/rand/1/bin', 'DE/best/1/bin'])\n        return strategy\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 19, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05528 with standard deviation 0.05307.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12683540090993406, 0.12995297034881015, 0.1239907390757502, 0.040252069851881345, 0.03640098384764823, 0.03942850283046273, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "5ac7f152-f963-4fb1-bf4f-2015ebc1ae53", "fitness": -Infinity, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with Advanced Self-Adaptive Parameter Control and Hybrid Strategy Integration for Robust Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1': 0}  # Added strategy\n        self.successful_strategies = {'DE/rand/1/bin': [], 'DE/best/1/bin': [], 'DE/current-to-best/1': []}\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/current-to-best/1\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx]) + \\\n                     self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n            self.successful_strategies[strategy].append((self.crossover_rate, self.mutation_factor))\n\n    def adapt_parameters(self):\n        if self.memory:\n            successful_cr = [cr for strategy in self.successful_strategies for cr, _ in self.successful_strategies[strategy]]\n            successful_mf = [mf for strategy in self.successful_strategies for _, mf in self.successful_strategies[strategy]]\n            if successful_cr and successful_mf:\n                self.crossover_rate = np.mean(successful_cr) + np.random.rand() * 0.1\n                self.mutation_factor = np.mean(successful_mf) + np.random.rand() * 0.1\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total = sum(self.strategy_memory.values())\n        probabilities = [self.strategy_memory[strategy]/total for strategy in self.strategy_memory]\n        return np.random.choice(list(self.strategy_memory.keys()), p=probabilities)\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 20, "feedback": "An exception occurred: ZeroDivisionError('division by zero').", "error": "ZeroDivisionError('division by zero')", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {}}
{"id": "540251a9-aa51-4ee8-8282-692686671248", "fitness": 0.0662710793184621, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Adaptive Memetic Differential Evolution enhanced with Self-Adaptive Parameter Control and Hierarchical Local Search for Improved Convergence Efficiency.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n        self.success_history = []  # New: track successful strategies\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n            self.success_history.append((strategy, trial_fitness))  # New: track success\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if self.success_history:\n            latest_strategy = self.success_history[-1][0]\n            if np.random.rand() < 0.7:\n                return latest_strategy\n        return 'DE/rand/1/bin' if np.random.rand() < 0.5 else 'DE/best/1/bin'\n\n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.05 * (self.bounds.ub - self.bounds.lb)  # New: fine perturbation\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n\n    def hierarchical_local_search(self, func):\n        best_idx = np.argmin(self.fitness)\n        local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n        if local_fitness < self.fitness[best_idx]:\n            self.population[best_idx] = local_candidate\n            self.fitness[best_idx] = local_fitness\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            self.hierarchical_local_search(func)  # New: hierarchical approach\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 21, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06627 with standard deviation 0.07206.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.21850356481454547, 0.13342029429663782, 0.1271944702680936, 0.040252069851881345, 0.038714479541132385, 0.037688168427201574, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "ac6f9036-6d99-4c1a-86e0-44d58c2c4169", "fitness": -Infinity, "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced Memetic Differential Evolution with Adaptive Parameter Tuning and Adaptive Population Control for Improved Convergence and Efficiency.", "code": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr * (1 + np.random.randn() * 0.1), 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f * (1 + np.random.randn() * 0.1), 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        strategy_use_probability = {k: v / sum(self.strategy_memory.values()) for k, v in self.strategy_memory.items()}\n        return np.random.choice(list(strategy_use_probability.keys()), p=list(strategy_use_probability.values()))\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 22, "feedback": "An exception occurred: ZeroDivisionError('division by zero').", "error": "ZeroDivisionError('division by zero')", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {}}
{"id": "dc3bda2a-28a5-4444-abe1-1b5d862cb6a6", "fitness": -Infinity, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Dynamic Strategy Selection and Improved Local Search for Faster Convergence.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.7  # Adjusted for better exploration-exploitation balance\n        self.mutation_factor = 0.9  # Adjusted for larger mutation steps\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/rand/2/bin': 0}  # Added new strategy\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/rand/2/bin':  # New mutation strategy\n            a, b, c, d, e = np.random.choice(indices, 5, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c] + self.population[d] - self.population[e])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            self.crossover_rate, self.mutation_factor = self._calculate_adaptive_parameters(self.memory)\n        else:\n            self.crossover_rate = 0.2 + np.random.rand() * 0.8\n            self.mutation_factor = 0.7 + np.random.rand() * 0.3\n\n    def _calculate_adaptive_parameters(self, memory):\n        average_cr = np.clip(np.mean([cr for cr, _ in memory]) + np.random.normal(0, 0.05), 0.1, 0.9)\n        average_f = np.clip(np.mean([mf for _, mf in memory]) + np.random.normal(0, 0.05), 0.5, 1.0)\n        return average_cr, average_f\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.85), self.dim * 2)  # Adjusted shrink factor\n\n    def select_strategy(self):\n        weights = [self.strategy_memory[key] for key in self.strategy_memory]\n        weights = np.array(weights) / np.sum(weights)\n        return np.random.choice(list(self.strategy_memory.keys()), p=weights)\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.randn(self.dim) - 0.5) * 0.05 * (self.bounds.ub - self.bounds.lb)  # Adjusted for smaller steps\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 23, "feedback": "An exception occurred: ValueError('probabilities contain NaN').", "error": "ValueError('probabilities contain NaN')", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {}}
{"id": "08cafbd6-7f0f-4639-867f-e8a1d0ace9bb", "fitness": 0.07542734686617146, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Stochastic Parameter Adaptation and Novel Dynamic Strategy Choice for Improved Optimization Performance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0} \n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        memory_len = len(self.memory)\n        if memory_len:\n            average_cr = np.mean([cr for cr, _ in self.memory[-memory_len//2:]])\n            average_f = np.mean([mf for _, mf in self.memory[-memory_len//2:]])\n            self.crossover_rate = np.clip(average_cr + (np.random.rand() - 0.5) * 0.2, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + (np.random.rand() - 0.5) * 0.2, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total_success = sum(self.strategy_memory.values())\n        if total_success > 0:\n            prob_rand = self.strategy_memory['DE/rand/1/bin'] / total_success\n        else:\n            prob_rand = 0.5\n        return 'DE/rand/1/bin' if np.random.rand() < prob_rand else 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 24, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07543 with standard deviation 0.09362.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.30394489866705887, 0.12995297034881015, 0.1275098435753783, 0.04085929473484651, 0.03929740356315048, 0.03661504423963213, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "0b144669-98f6-41b7-aa87-92683783fd7b", "fitness": 0.056390547515912534, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Enhance strategy selection by giving a higher probability to the strategy that resulted in better fitness improvement in recent generations.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total = sum(self.strategy_memory.values())  # Calculate the total number of successes for both strategies\n        probs = {k: (v / total) if total > 0 else 0.5 for k, v in self.strategy_memory.items()}  # Compute selection probabilities\n        return np.random.choice(list(self.strategy_memory.keys()), p=[probs['DE/rand/1/bin'], probs['DE/best/1/bin']])\n\n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 25, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05639 with standard deviation 0.05353.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12683540090993406, 0.1302198317133666, 0.12811370520316656, 0.040252069851881345, 0.04348567009750881, 0.037941583200688744, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "e22ae6d1-9e47-42dc-9c8c-8e0dc276ecb2", "fitness": 0.05562970409930853, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Dynamic Strategy Selection and Adaptive Local Search for Improved Convergence and Exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1/bin': 0}\n        self.local_search_probability = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/current-to-best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx] + self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total = sum(self.strategy_memory.values())\n        if total == 0 or np.random.rand() < 0.2:\n            return np.random.choice(list(self.strategy_memory.keys()))\n        probabilities = [self.strategy_memory[strat] / total for strat in self.strategy_memory]\n        return np.random.choice(list(self.strategy_memory.keys()), p=probabilities)\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def adaptive_local_search(self, func):\n        for i in range(self.population_size):\n            if np.random.rand() < self.local_search_probability:\n                local_candidate, local_fitness = self.local_search(self.population[i], func)\n                if local_fitness < self.fitness[i]:\n                    self.population[i] = local_candidate\n                    self.fitness[i] = local_fitness\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Adaptive local search\n            self.adaptive_local_search(func)\n            \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 26, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05563 with standard deviation 0.05282.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12437452538054972, 0.1321264971201016, 0.12337497813931075, 0.040252069851881345, 0.03988541745820895, 0.039987182277057665, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "dee58870-c2b0-4274-9744-5c88f0827a30", "fitness": 0.056620407419996494, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Adaptive Strategy Selection Using Success Rate for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1': 0}\n        self.strategy_success_counter = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1': 0}\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        elif strategy == 'DE/current-to-best/1':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n            self.strategy_success_counter[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total_success = sum(self.strategy_success_counter.values())\n        if total_success == 0:\n            return np.random.choice(['DE/rand/1/bin', 'DE/best/1/bin', 'DE/current-to-best/1'])\n        \n        success_rate = {k: v / total_success for k, v in self.strategy_success_counter.items()}\n        return max(success_rate, key=success_rate.get)\n\n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 27, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05662 with standard deviation 0.05455.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12648137945344717, 0.13143558985261172, 0.13305887820756412, 0.04121277860040773, 0.04066120332936385, 0.03606717066990717, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "c30cb810-af99-477d-af60-dfc4126723b5", "fitness": 0.05668401638194455, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Fitness-Based Strategy Selection and Adaptive Mutation Factor for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n\n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total_success = sum(self.strategy_memory.values())\n        probability_rand = (self.strategy_memory['DE/rand/1/bin'] / total_success) if total_success > 0 else 0.5\n        return 'DE/rand/1/bin' if np.random.rand() < probability_rand else 'DE/best/1/bin'\n\n    def adaptive_mutation_factor(self, current_fitness):\n        fitness_range = np.max(self.fitness) - np.min(self.fitness)\n        if fitness_range > 0:\n            normalized_fitness = (current_fitness - np.min(self.fitness)) / fitness_range\n            return 0.5 + 0.5 * (1 - normalized_fitness)\n        return 0.8\n\n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                self.mutation_factor = self.adaptive_mutation_factor(self.fitness[i])\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 28, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05668 with standard deviation 0.05507.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12466186096218623, 0.13437171420967775, 0.13463127645461304, 0.04201218814104202, 0.03611696167302636, 0.03769547933028883, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "b362053c-5dbd-404e-8705-de89276d6ff8", "fitness": 0.057173629874217896, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Adaptive Memetic Differential Evolution with Enhanced Dynamic Strategy Selection and Learning Mechanism to Improve Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1/bin': 0}  # Added strategy\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/current-to-best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        probabilities = np.array(list(self.strategy_memory.values()))\n        probabilities = probabilities / probabilities.sum() if probabilities.sum() > 0 else np.ones(len(probabilities)) / len(probabilities)\n        return np.random.choice(list(self.strategy_memory.keys()), p=probabilities)\n    \n    def local_search(self, candidate, func):\n        perturbation_magnitude = 0.1 * (self.bounds.ub - self.bounds.lb) / (1 + self.generations)\n        perturbation = (np.random.rand(self.dim) - 0.5) * perturbation_magnitude\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n            self.generations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 29, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05717 with standard deviation 0.05366.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.128936825683457, 0.13246081994951342, 0.12559390930868208, 0.043044492759108266, 0.0450246969976712, 0.03883525750286243, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "3f1c13a7-ff0d-4e0f-abac-2621dcf20b70", "fitness": 0.056655812256127445, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Adaptive Memetic Differential Evolution with enhanced strategy selection probability for balanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.6:  # Changed probability of selecting strategies\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 30, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05666 with standard deviation 0.05413.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12697704946296695, 0.1322329884879928, 0.1295932352846656, 0.04028685567743118, 0.04301701065491925, 0.0371285040705045, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "1982c054-aa41-401f-9c1f-b32ef6715c28", "fitness": -Infinity, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Adaptive Memetic Differential Evolution with Dynamic Strategy Selection and Local Search for Enhanced Convergence.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 19, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": "\n    Refine the strategy of the selected solution to improve it. Make sure you only change 0.9% of the code, which means if the code has 100 lines, you can only change 0.9090909090909091 lines, and the rest of the lines should remain unchanged. This input code has 110 lines, so you can only change 1 lines, the rest 109 lines should remain unchanged. This changing rate 0.9% is the mandatory requirement, you cannot change more or less than this rate.\n    ", "metadata": {"aucs": [0.30284183263495335, 0.1321136590817955, 0.13233369168324427, 0.040495551940109675, 0.04301701065491925, 0.03736596798379754, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "8bdefecd-6ada-4645-8a63-098adc7c82d7", "fitness": 0.056009465397711095, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Historical Success Learning and Adaptive Mutation for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n        self.success_memory = []\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n            self.success_memory.append(1)\n        else:\n            self.success_memory.append(0)\n\n    def adapt_parameters(self):\n        if self.memory:\n            recent_success = np.mean(self.success_memory[-self.population_size:])\n            self.crossover_rate = 0.9 * recent_success + 0.1\n            self.mutation_factor = 0.6 + 0.3 * recent_success\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        weights = np.array([self.strategy_memory['DE/rand/1/bin'], self.strategy_memory['DE/best/1/bin']])\n        weights = (weights + 1) / (np.sum(weights) + 2)\n        return np.random.choice(['DE/rand/1/bin', 'DE/best/1/bin'], p=weights)\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05601 with standard deviation 0.05362.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.1274589096246178, 0.12995297034881015, 0.12769494126780057, 0.040252069851881345, 0.03867335150911866, 0.039386279310504646, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "781f4d7f-69af-41fa-89f3-a8d744417fc0", "fitness": 0.05654778164743282, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Introduced Gaussian noise perturbation in the local search for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = np.random.normal(0, 0.1, self.dim) * (self.bounds.ub - self.bounds.lb)  # Changed to Gaussian noise\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 33, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05655 with standard deviation 0.05362.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12455727854853449, 0.1321136590817955, 0.12907608770563117, 0.040252069851881345, 0.04419264854535099, 0.038071624427035156, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "0a2b7737-fe5d-4cad-ac17-48048a9f6112", "fitness": 0.05742655684348633, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Introduced a small random perturbation during mutation for better exploration.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        # Introducing small random perturbation\n        mutant += np.random.normal(0, 0.01, size=self.dim)\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 34, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05743 with standard deviation 0.05427.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12442808735395117, 0.1371933881979105, 0.12889757447972028, 0.040951576309436244, 0.0415893510731431, 0.04311236751054903, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "584fc106-3f40-434a-b073-76b3d171574c", "fitness": 0.05529726970507357, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Contextual Parameter Tuning and Intelligent Local Search for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n        self.local_search_intensity = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.normal(0, 0.1), 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.normal(0, 0.1), 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total_success = sum(self.strategy_memory.values())\n        if total_success == 0 or np.random.rand() < 0.2:\n            return np.random.choice(list(self.strategy_memory.keys()))\n        else:\n            success_rates = {k: v / total_success for k, v in self.strategy_memory.items()}\n            return max(success_rates, key=success_rates.get)\n\n    def local_search(self, candidate, func):\n        for _ in range(5):  # Iteratively improve local search by limiting local perturbations\n            perturbation = (np.random.rand(self.dim) - 0.5) * self.local_search_intensity * (self.bounds.ub - self.bounds.lb)\n            neighbor = candidate + perturbation\n            neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n            neighbor_fitness = func(neighbor)\n            if neighbor_fitness < func(candidate):\n                return neighbor, neighbor_fitness\n        return candidate, func(candidate)\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply enhanced local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 35, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05530 with standard deviation 0.05294.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12446830897371608, 0.13074168936573694, 0.1249125608405246, 0.040252069851881345, 0.038586065970328653, 0.03804806567680785, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "d3e72443-298d-4e56-b25e-ee46b4ecc0eb", "fitness": 0.056763029862855446, "name": "ImprovedAdaptiveMemeticDifferentialEvolution", "description": "Improved Adaptive Memetic Differential Evolution with Enhanced Parameter Adaptation and Strategy Selection for More Robust Convergence.", "code": "import numpy as np\n\nclass ImprovedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1/bin': 0}  # Added new strategy\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/current-to-best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.normal(0, 0.1), 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.normal(0, 0.1), 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        probabilities = np.array(list(self.strategy_memory.values()))\n        if sum(probabilities) == 0:\n            probabilities = np.ones(len(probabilities)) / len(probabilities) \n        else:\n            probabilities = probabilities / probabilities.sum()\n        strategy = np.random.choice(list(self.strategy_memory.keys()), p=probabilities)\n        return strategy\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 36, "feedback": "The algorithm ImprovedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05676 with standard deviation 0.05445.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13303259970862047, 0.13246081994951342, 0.12512442804114798, 0.043044492759108266, 0.03697171535253119, 0.03956654628811096, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "c73bbed0-8c7e-41bc-b931-777a26b9caf8", "fitness": 0.057124485237682245, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Multi-Strategy Adaptation and Learning-assisted Local Search.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1/bin': 0}  # Added new strategy\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/current-to-best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        probabilities = np.array(list(self.strategy_memory.values()))\n        probabilities = probabilities / probabilities.sum() if probabilities.sum() > 0 else [1/len(probabilities)]*len(probabilities)\n        return np.random.choice(list(self.strategy_memory.keys()), p=probabilities)\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def learning_local_search(self, best_candidate, func):\n        best_fitness = func(best_candidate)\n        for _ in range(5):  # Try a few local variations\n            candidate, candidate_fitness = self.local_search(best_candidate, func)\n            if candidate_fitness < best_fitness:\n                best_candidate, best_fitness = candidate, candidate_fitness\n        return best_candidate, best_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply enhanced local search using learning behaviour\n            best_idx = np.argmin(self.fitness)\n            improved_candidate, improved_fitness = self.learning_local_search(self.population[best_idx], func)\n            if improved_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = improved_candidate\n                self.fitness[best_idx] = improved_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 37, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05712 with standard deviation 0.05418.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12810825167490492, 0.13250445789032705, 0.12927904610024432, 0.043044492759108266, 0.0430412063248059, 0.03747624572308306, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "1d5f2a92-76aa-417e-8354-45d9c7c6e325", "fitness": 0.05637390136106603, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Dynamic Strategy Selection, Local Search Intensification, and Adaptive Memory-based Parameter Tuning for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1': 0}  # Added strategy\n        self.memory_size = 5  # Limit memory size\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/current-to-best/1\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n        \n        if len(self.memory) > self.memory_size:  # Trim memory\n            self.memory.pop(0)\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.normal(0, 0.1), 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.normal(0, 0.1), 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        probabilities = np.array(list(self.strategy_memory.values()))\n        probabilities = probabilities / np.sum(probabilities) if np.sum(probabilities) > 0 else np.ones(len(probabilities)) / len(probabilities)\n        return np.random.choice(list(self.strategy_memory.keys()), p=probabilities)\n\n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def intensify_local_search(self, func):\n        for _ in range(2):  # Double the local search attempts\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Intensify local search\n            self.intensify_local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 38, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05637 with standard deviation 0.05365.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12750177679895647, 0.13246081994951342, 0.12572573583520052, 0.043044492759108266, 0.03735810091646896, 0.0406075193236799, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "e1dbe3b3-850f-4118-8942-92df38ba4ad3", "fitness": 0.05747973379490731, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Incorporate adaptive learning of successful mutation strategies and population rescaling based on convergence rate to improve efficiency and performance.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n        self.strategy_success_rate = {'DE/rand/1/bin': 0.5, 'DE/best/1/bin': 0.5}\n        self.convergence_threshold = 1e-5\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n\n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n            self.strategy_success_rate[strategy] += 1\n        else:\n            self.strategy_success_rate[strategy] *= 0.9\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self, convergence_rate):\n        if self.dynamic_population_control and convergence_rate < self.convergence_threshold:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total_success = sum(self.strategy_success_rate.values())\n        probabilities = {strategy: rate / total_success for strategy, rate in self.strategy_success_rate.items()}\n        return np.random.choice(list(probabilities.keys()), p=list(probabilities.values()))\n\n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        previous_best_fitness = np.inf\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            current_best_fitness = np.min(self.fitness)\n            self.control_population_size(abs(current_best_fitness - previous_best_fitness))\n            previous_best_fitness = current_best_fitness\n\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 39, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05748 with standard deviation 0.05387.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.1272129067504758, 0.1330254102030306, 0.1283585441099041, 0.04693904429671403, 0.0418703430006252, 0.03924468912674939, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "a86844c3-e67a-4b70-ad06-0426e7c80c55", "fitness": 0.056390547515912534, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Improved Adaptive Memetic Differential Evolution with Enhanced Strategy Memory and Diversification Mechanism.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n        self.diversification_factor = 0.1  # Added diversification factor\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total_success = sum(self.strategy_memory.values())\n        if total_success > 0:\n            probabilities = {k: v / total_success for k, v in self.strategy_memory.items()}\n            return np.random.choice(list(probabilities.keys()), p=list(probabilities.values()))\n        else:\n            return 'DE/rand/1/bin' if np.random.rand() < 0.5 else 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * self.diversification_factor * (self.bounds.ub - self.bounds.lb)  # Diversification factor applied here\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 40, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05639 with standard deviation 0.05353.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12683540090993406, 0.1302198317133666, 0.12811370520316656, 0.040252069851881345, 0.04348567009750881, 0.037941583200688744, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "82894849-dea2-41c1-95e3-5c9a1b6f0099", "fitness": -Infinity, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Integrated Adaptive Covariance Matrix Adaptation for Robust Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/cma-es': 0}\n        self.cma_covariance = np.eye(dim)\n        self.cma_mean = None\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n        self.cma_mean = np.mean(self.population, axis=0)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n\n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/cma-es\n            deviation = np.random.multivariate_normal(np.zeros(self.dim), self.cma_covariance)\n            mutant = self.cma_mean + deviation\n\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def update_cma_parameters(self):\n        weights = np.exp(-np.arange(1, self.population_size + 1) / (0.2 * self.population_size))\n        weights /= np.sum(weights)\n        ranked_indices = np.argsort(self.fitness)\n        self.cma_mean = np.dot(weights, self.population[ranked_indices[:self.population_size]])\n        cov_update = sum(\n            w * np.outer(self.population[i] - self.cma_mean, self.population[i] - self.cma_mean)\n            for i, w in zip(ranked_indices, weights)\n        )\n        self.cma_covariance = 0.9 * self.cma_covariance + 0.1 * cov_update\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        probabilities = np.array([self.strategy_memory['DE/rand/1/bin'], self.strategy_memory['DE/best/1/bin'], self.strategy_memory['DE/cma-es']])\n        probabilities = probabilities / probabilities.sum()\n        return np.random.choice(['DE/rand/1/bin', 'DE/best/1/bin', 'DE/cma-es'], p=probabilities)\n\n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            self.update_cma_parameters()\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 41, "feedback": "An exception occurred: ValueError('probabilities contain NaN').", "error": "ValueError('probabilities contain NaN')", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {}}
{"id": "a2d81692-165d-45ca-b2bb-54f3d5491603", "fitness": 0.0662372933506206, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Adaptive Learning Rates and Improved Diversity Preservation for Superior Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1/bin': 0}\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/current-to-best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        self.crossover_rate = max(0.1, min(0.9, self.crossover_rate + np.random.normal(0, 0.1)))\n        self.mutation_factor = max(0.5, min(1.0, self.mutation_factor + np.random.normal(0, 0.1)))\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n            if self.population_size < len(self.population):\n                self.population = self.population[:self.population_size]\n                self.fitness = self.fitness[:self.population_size]\n\n    def select_strategy(self):\n        total = sum(self.strategy_memory.values())\n        if total == 0:\n            return 'DE/rand/1/bin'\n        probabilities = {k: v / total for k, v in self.strategy_memory.items()}\n        return np.random.choice(list(probabilities.keys()), p=list(probabilities.values()))\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06624 with standard deviation 0.07017.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.2127946562270775, 0.13338622529398914, 0.12457369069917856, 0.044859623926687076, 0.041009145758927446, 0.03884563158305898, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "a5d96fe3-6042-43e1-a9db-ac1e9500e14c", "fitness": 0.056390547515912534, "name": "EnhancedAdaptiveMemeticDE", "description": "Enhanced Adaptive Memetic Differential Evolution with Adaptive Strategy Selection and Fitness-Based Mutation Adjustment for Improved Exploration and Exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adaptive_mutation_factor(self):\n        avg_fitness = np.mean(self.fitness)\n        min_fitness = np.min(self.fitness)\n        self.mutation_factor = 0.8 + 0.2 * (1 - (min_fitness / avg_fitness))\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        de_rand_score = self.strategy_memory['DE/rand/1/bin']\n        de_best_score = self.strategy_memory['DE/best/1/bin']\n        if de_rand_score + de_best_score > 0:\n            de_rand_prob = de_rand_score / (de_rand_score + de_best_score)\n            return 'DE/rand/1/bin' if np.random.rand() < de_rand_prob else 'DE/best/1/bin'\n        else:\n            return 'DE/rand/1/bin' if np.random.rand() < 0.5 else 'DE/best/1/bin'\n\n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adaptive_mutation_factor()\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 43, "feedback": "The algorithm EnhancedAdaptiveMemeticDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05639 with standard deviation 0.05353.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12683540090993406, 0.1302198317133666, 0.12811370520316656, 0.040252069851881345, 0.04348567009750881, 0.037941583200688744, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "7cd41e3b-58fa-4082-97b4-7e003b307ab8", "fitness": 0.057525087236596786, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Enhanced crossover strategy for improved exploration in Adaptive Memetic Differential Evolution.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < (0.7 if np.random.rand() < 0.5 else self.crossover_rate)\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 44, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05753 with standard deviation 0.05409.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13323291473252596, 0.12995297034881015, 0.1268086167094462, 0.04523471797652423, 0.03995409051122911, 0.041875808184168695, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "4aa8d52b-3436-4197-a772-a8d902d27116", "fitness": 0.05666301824982932, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Adaptive Memetic Differential Evolution with Enhanced Diversity Preservation through Novel Mutation and Crossover Strategies.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n\n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n\n        # Enhanced diversity preservation using a novel mutation strategy\n        random_vector = np.random.rand(self.dim)\n        mutant = np.clip(mutant + 0.1 * (random_vector - 0.5), self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n\n        # Additional exploration through uniform crossover enhancement\n        if np.random.rand() < 0.1:\n            trial = np.mean([target, mutant], axis=0)\n\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n\n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 45, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05666 with standard deviation 0.05449.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.1340915816552436, 0.1315273442725795, 0.12511788865305762, 0.040252069851881345, 0.03986900397938853, 0.03844260916964659, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "869e8670-7a16-4bf2-a33b-df077b06f0d8", "fitness": 0.056763029862855446, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Multi-strategy Framework and Adaptive Parameter Control for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/rand-to-best/1/bin': 0}\n        \n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        best_idx = np.argmin(self.fitness)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/rand-to-best/1/bin\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[target_idx] + self.mutation_factor * (\n                self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (\n                    self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.normal(0, 0.1), 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.normal(0, 0.1), 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        strategy_probabilities = np.array(list(self.strategy_memory.values()), dtype=np.float32)\n        if strategy_probabilities.sum() == 0:\n            strategy_probabilities.fill(1.0)\n        strategy_probabilities /= strategy_probabilities.sum()\n        strategy = np.random.choice(list(self.strategy_memory.keys()), p=strategy_probabilities)\n        return strategy\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 46, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05676 with standard deviation 0.05445.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13303259970862047, 0.13246081994951342, 0.12512442804114798, 0.043044492759108266, 0.03697171535253119, 0.03956654628811096, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "af556841-716e-4041-b896-fe735b554e3c", "fitness": 0.05611748322592761, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Self-Adaptive Parameter Control and Multi-Strategy Fusion for Robust Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1': 0}  # Added new strategy\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/current-to-best/1\n            best_idx = np.argmin(self.fitness)\n            a = np.random.choice(indices, 1)[0]\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (self.population[a] - self.population[target_idx])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.normal(0, 0.1), 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.normal(0, 0.1), 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n    def select_strategy(self):\n        total_successes = sum(self.strategy_memory.values())\n        if total_successes == 0:\n            return np.random.choice(['DE/rand/1/bin', 'DE/best/1/bin', 'DE/current-to-best/1'])\n        probabilities = [self.strategy_memory[strategy] / total_successes for strategy in self.strategy_memory]\n        return np.random.choice(['DE/rand/1/bin', 'DE/best/1/bin', 'DE/current-to-best/1'], p=probabilities)\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 47, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05612 with standard deviation 0.05410.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13113172187220756, 0.13010615238569523, 0.12657352999336746, 0.040252069851881345, 0.03697171535253119, 0.039355492910999046, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "55365bae-03b6-4415-b5d6-ec449b3f5e27", "fitness": 0.05672006334220862, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Introduced an additional strategy 'DE/current-to-best/1' to further diversify exploration and exploitation dynamics.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1': 0}  # Added new strategy\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/current-to-best/1\n            best_idx = np.argmin(self.fitness)\n            a = np.random.choice(indices, 1, replace=False)[0]\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (self.population[a] - self.population[target_idx])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        elif np.random.rand() < 0.75:\n            return 'DE/best/1/bin'\n        else:\n            return 'DE/current-to-best/1'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 48, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05672 with standard deviation 0.05402.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.1289788042078478, 0.1344554423693045, 0.12469955959287549, 0.0438119054088858, 0.03820194812916733, 0.039666243705129944, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "dd9134c8-d89b-4bf8-8256-a3b2bc8e9047", "fitness": 0.05783297463578438, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Anomaly Detection-Based Strategy Switching for Robust Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n        self.anomaly_threshold = 0.1  # Threshold to detect stagnation\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.std(self.fitness) < self.anomaly_threshold:  # Detect stagnation\n            return 'DE/best/1/bin'\n        return 'DE/rand/1/bin'\n\n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 49, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05783 with standard deviation 0.05662.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13838888711703723, 0.13143558985261172, 0.13438245546198802, 0.040426507578953164, 0.03991514802095475, 0.03528151702384785, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "556d7826-a6dc-4378-b79f-bfca06ed9c59", "fitness": -Infinity, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Stochastic Strategy Adjustment and Adaptive Crossover for Improved Exploration-Exploitation Balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/rand/2/bin': 0}\n        self.weighted_strategy_choice_prob = [0.3, 0.3, 0.4]  # Initial probabilities for strategies\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/rand/2/bin\n            a, b, c, d = np.random.choice(indices, 4, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c] + self.population[d] - self.population[a])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        self.crossover_rate = np.clip(0.5 + 0.3 * (np.random.rand() - 0.5), 0.1, 0.9)\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total_strategy_success = sum(self.strategy_memory.values())\n        if total_strategy_success > 0:\n            self.weighted_strategy_choice_prob = [\n                0.3 + 0.7 * (self.strategy_memory['DE/rand/1/bin'] / total_strategy_success),\n                0.3 + 0.7 * (self.strategy_memory['DE/best/1/bin'] / total_strategy_success),\n                0.4 + 0.7 * (self.strategy_memory['DE/rand/2/bin'] / total_strategy_success),\n            ]\n        return np.random.choice(list(self.strategy_memory.keys()), p=self.weighted_strategy_choice_prob)\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 50, "feedback": "An exception occurred: ValueError('probabilities do not sum to 1').", "error": "ValueError('probabilities do not sum to 1')", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {}}
{"id": "dd125d62-cabf-46e9-a308-a7d6fc71322d", "fitness": 0.06631493182383541, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Introduced a dynamic crossover rate adjustment based on population diversity to enhance convergence.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n        \n        # New line: Adjust crossover rate based on population diversity\n        diversity = np.std(self.population, axis=0).mean()\n        self.crossover_rate = np.clip(self.crossover_rate * diversity, 0.1, 0.9)\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 51, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06631 with standard deviation 0.07095.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.21305601261960894, 0.1330686695721237, 0.1303849949504865, 0.041920490417479694, 0.03827756753286948, 0.039459984655283664, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "48c06bd6-263b-42c1-9611-33a70a4d5f7e", "fitness": 0.057126355885796315, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Dynamic Strategy Selection, Learning-based Parameter Adaptation, and Adaptive Local Search for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n        self.successful_strategies = []\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n            if strategy not in self.successful_strategies:\n                self.successful_strategies.append(strategy)\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if self.successful_strategies and np.random.rand() < 0.7:\n            return np.random.choice(self.successful_strategies)\n        else:\n            return 'DE/rand/1/bin' if np.random.rand() < 0.5 else 'DE/best/1/bin'\n    \n    def adaptive_local_search(self, candidate, func, improvement_ratio):\n        perturbation_scale = 0.1 * (1.0 - improvement_ratio)\n        perturbation = (np.random.rand(self.dim) - 0.5) * perturbation_scale * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            initial_best_fitness = np.min(self.fitness)\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n            \n            improvement_ratio = (initial_best_fitness - np.min(self.fitness)) / np.abs(initial_best_fitness) if initial_best_fitness != 0 else 0\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.adaptive_local_search(self.population[best_idx], func, improvement_ratio)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 52, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05713 with standard deviation 0.05491.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13377755791223833, 0.1325161542604174, 0.12761655851219544, 0.040252069851881345, 0.04017985691331405, 0.039128338855453526, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "cf103512-78e2-4b6a-b8c2-fa545932c93b", "fitness": -Infinity, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Adaptive Memetic Differential Evolution with Dynamic Strategy Selection and Local Search for Enhanced Convergence.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 19, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": "\n    Refine the strategy of the selected solution to improve it. Make sure you only change 0.9% of the code, which means if the code has 100 lines, you can only change 0.9090909090909091 lines, and the rest of the lines should remain unchanged. This input code has 110 lines, so you can only change 1 lines, the rest 109 lines should remain unchanged. This changing rate 0.9% is the mandatory requirement, you cannot change more or less than this rate.\n    ", "metadata": {"aucs": [0.30284183263495335, 0.1321136590817955, 0.13233369168324427, 0.040495551940109675, 0.04301701065491925, 0.03736596798379754, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "727906ba-3d9a-4bb0-890a-687012921d66", "fitness": 0.0563398805496137, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Adaptive Hyperparameter Tuning and Dynamic Strategy Balancing for Robust Black Box Optimization.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n        self.strategy_success = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  \n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n            self.strategy_success[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.normal(0, 0.1), 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.normal(0, 0.1), 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total_success = sum(self.strategy_success.values())\n        if total_success == 0:\n            return np.random.choice(list(self.strategy_memory.keys()))\n        else:\n            probabilities = [self.strategy_success[s] / total_success for s in self.strategy_memory.keys()]\n            return np.random.choice(list(self.strategy_memory.keys()), p=probabilities)\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 54, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05634 with standard deviation 0.05402.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13113172187220756, 0.13010615238569523, 0.12657352999336746, 0.040252069851881345, 0.03897329126570592, 0.039355492910999046, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "7f168440-65c0-466c-99cb-bf8a45d4eb6c", "fitness": 0.05736697091111796, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Slightly tweak the mutation factor to potentially enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.85  # Changed from 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 55, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05737 with standard deviation 0.05499.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13031019018952872, 0.1321136590817955, 0.13233369168324427, 0.040495551940109675, 0.04301701065491925, 0.03736596798379754, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "f9768767-a59f-4d15-b6f8-5d2dc904d969", "fitness": -Infinity, "name": "HybridCooperativeDifferentialEvolution", "description": "Hybrid Cooperative Differential Evolution with Adaptive Learning for Enhanced Global and Local Search.", "code": "import numpy as np\n\nclass HybridCooperativeDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1': 0}\n        self.learning_period = 5  # New learning period parameter\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/current-to-best/1\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory and len(self.memory) >= self.learning_period:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n            self.memory = []  # Reset memory after adaptation\n\n    def select_strategy(self):\n        weights = list(self.strategy_memory.values())\n        total = sum(weights)\n        probabilities = [w / total for w in weights]\n        strategies = list(self.strategy_memory.keys())\n        return np.random.choice(strategies, p=probabilities)\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 56, "feedback": "An exception occurred: ZeroDivisionError('division by zero').", "error": "ZeroDivisionError('division by zero')", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {}}
{"id": "9ee32a65-d8fc-45ef-8fec-dbf72272aa9c", "fitness": 0.06540100176871039, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Improved mutation strategy by dynamically adjusting the mutation factor and implementing a more focused local search.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            if self.fitness[a] < self.fitness[b] and self.fitness[a] < self.fitness[c]:\n                self.mutation_factor = 0.9  # Adjust mutation factor based on fitness comparison\n            else:\n                self.mutation_factor = 0.8\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.randn(self.dim) * 0.05) * (self.bounds.ub - self.bounds.lb)  # Focused search with normal distribution\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 57, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06540 with standard deviation 0.07073.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.21549419192436925, 0.12995297034881015, 0.12474608031567169, 0.04129443131770538, 0.03711382841565336, 0.03934084692951689, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "189a8f4d-d486-4eab-be00-ecb62d745510", "fitness": 0.056548054549804, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Refine the strategy by adding a weight to strategy selection based on historical success rates to enhance convergence.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 1, 'DE/best/1/bin': 1}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total = sum(self.strategy_memory.values())\n        rand_strategy = np.random.rand() * total\n        cumulative = 0\n        for strategy, count in self.strategy_memory.items():\n            cumulative += count\n            if rand_strategy < cumulative:\n                return strategy\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 58, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05655 with standard deviation 0.05396.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12790575566257534, 0.1321136590817955, 0.127788722430472, 0.040252069851881345, 0.041902794197695936, 0.03830282305714916, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "618c6dcd-1a28-4433-887b-02674521e568", "fitness": 0.05656277449439991, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Adaptive Mutation and Crossover for Improved Convergence Stability.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.normal(0, 0.1), 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.normal(0, 0.1), 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.95), self.dim * 2)\n\n    def select_strategy(self):\n        strategy_probabilities = np.array(list(self.strategy_memory.values())) + 1\n        strategy_probabilities = strategy_probabilities / strategy_probabilities.sum()\n        return np.random.choice(['DE/rand/1/bin', 'DE/best/1/bin'], p=strategy_probabilities)\n\n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.05 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 59, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05656 with standard deviation 0.05390.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12494063885231288, 0.1321136590817955, 0.1303951544264319, 0.04104319348864416, 0.03930677588745579, 0.04059888204629225, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "8041f678-558f-4942-8886-ffb2d6fe370e", "fitness": 0.05675203332086071, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Enhanced adaptive parameter adjustment for improved convergence in differential evolution.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.randn() * 0.05, 0.1, 0.9)  # Change: smaller adjustment\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 60, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05675 with standard deviation 0.05375.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12577062521022975, 0.1321136590817955, 0.12907608770563117, 0.040252069851881345, 0.04301701065491925, 0.039872180716622685, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "e186e7c1-b8c6-4c89-aab5-79d7e0e48526", "fitness": 0.056727753341373845, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Reinforced Strategy Selection for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1/bin': 0}  # Added new strategy\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n\n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/current-to-best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (self.population[a] - self.population[b])\n\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        strategy_weights = np.array(list(self.strategy_memory.values()))\n        strategy_probs = strategy_weights / np.sum(strategy_weights) if np.sum(strategy_weights) > 0 else [1/3] * 3\n        return np.random.choice(['DE/rand/1/bin', 'DE/best/1/bin', 'DE/current-to-best/1/bin'], p=strategy_probs)\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05673 with standard deviation 0.05380.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.128936825683457, 0.13246081994951342, 0.12559390930868208, 0.043044492759108266, 0.04348567009750881, 0.03636139560742835, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "13bb2c2e-32e8-4cab-8336-2b8910daaab7", "fitness": 0.05688329546414946, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Introduced a random perturbation to crossover rate to enhance exploration.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < (self.crossover_rate + np.random.rand() * 0.1 - 0.05)  # Add random perturbation\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 62, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05688 with standard deviation 0.05422.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13100524500640642, 0.13000890489497208, 0.1287880282143672, 0.04252640597538371, 0.04111184709103788, 0.03784256132851116, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "81ba7798-51cc-4464-8f3f-3428d9673e3c", "fitness": 0.0583958479064089, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Introduced a refined mutation factor adaptation mechanism to enhance convergence stability.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            # Refined mutation factor adaptation mechanism\n            self.mutation_factor = np.clip(average_f + np.random.normal(loc=0, scale=0.05), 0.5, 1.0)\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 63, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05840 with standard deviation 0.05518.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13623863945134762, 0.1321136590817955, 0.12907608770563117, 0.040411175066058735, 0.04301701065491925, 0.0440393925312611, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "3e59ec9c-d7e2-4253-8574-714882145baf", "fitness": 0.056390547515912534, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Minor enhancement to increase the probability of using the more successful strategy.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total = sum(self.strategy_memory.values())\n        if total > 0:\n            probabilities = [self.strategy_memory[strategy] / total for strategy in self.strategy_memory]\n            return np.random.choice(['DE/rand/1/bin', 'DE/best/1/bin'], p=probabilities)\n        else:\n            return 'DE/rand/1/bin' if np.random.rand() < 0.5 else 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 64, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05639 with standard deviation 0.05353.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12683540090993406, 0.1302198317133666, 0.12811370520316656, 0.040252069851881345, 0.04348567009750881, 0.037941583200688744, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "a6894bde-fbe0-4f62-b243-12430e8228ff", "fitness": 0.05611748322592761, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Hybrid Strategy Incorporating Self-Adaptive Parameter Adjustments and Dynamic Population Scaling for Improved Optimization Performance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1/bin': 0}\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        best_idx = np.argmin(self.fitness)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/current-to-best/1/bin\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.normal(0, 0.1), 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.normal(0, 0.1), 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total_successes = sum(self.strategy_memory.values())\n        if total_successes == 0:\n            return np.random.choice(list(self.strategy_memory.keys()))\n        \n        probabilities = [self.strategy_memory[strategy] / total_successes for strategy in self.strategy_memory]\n        return np.random.choice(list(self.strategy_memory.keys()), p=probabilities)\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 65, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05612 with standard deviation 0.05410.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13113172187220756, 0.13010615238569523, 0.12657352999336746, 0.040252069851881345, 0.03697171535253119, 0.039355492910999046, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "0a720691-58b3-4819-854c-5ac1e366f179", "fitness": 0.05759216873196512, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Adaptive Neighborhood Search and Improved Parameter Adaptation for Better Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n        self.adaptive_search_intensity = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.normal(0, 0.1), 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.normal(0, 0.1), 0.5, 1.2)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5 and self.population_size > self.dim * 2:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * self.adaptive_search_intensity * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 66, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05759 with standard deviation 0.05486.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13256605717969772, 0.1321136590817955, 0.1297950658596566, 0.04059839513503738, 0.04301701065491925, 0.03957266400991288, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "3f7b0065-ebde-4333-90d9-17561ed322b0", "fitness": -Infinity, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Adaptive Memetic Differential Evolution with Dynamic Strategy Selection and Local Search for Enhanced Convergence.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 19, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": "\n    Refine the strategy of the selected solution to improve it. Make sure you only change 0.9% of the code, which means if the code has 100 lines, you can only change 0.9090909090909091 lines, and the rest of the lines should remain unchanged. This input code has 110 lines, so you can only change 1 lines, the rest 109 lines should remain unchanged. This changing rate 0.9% is the mandatory requirement, you cannot change more or less than this rate.\n    ", "metadata": {"aucs": [0.30284183263495335, 0.1321136590817955, 0.13233369168324427, 0.040495551940109675, 0.04301701065491925, 0.03736596798379754, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "66b2f9ae-fc92-4447-86c5-c93a8a1acbe9", "fitness": 0.056548054549804, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution using Dynamic Learning Automata for Strategy Selection to improve convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 1, 'DE/best/1/bin': 1}  # Using initial weight for exploration\n        self.strategy_probabilities = {'DE/rand/1/bin': 0.5, 'DE/best/1/bin': 0.5}\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total = sum(self.strategy_memory.values())\n        for strategy in self.strategy_probabilities.keys():\n            self.strategy_probabilities[strategy] = self.strategy_memory[strategy] / total\n        return np.random.choice(list(self.strategy_probabilities.keys()), p=list(self.strategy_probabilities.values()))\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 68, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05655 with standard deviation 0.05396.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12790575566257534, 0.1321136590817955, 0.127788722430472, 0.040252069851881345, 0.041902794197695936, 0.03830282305714916, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "e751780d-92d1-4914-a68d-c69369557ac3", "fitness": 0.057162430467010764, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Hybrid Mutation Strategy and Stochastic Population Reduction for Faster Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1': 0}\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/current-to-best/1\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        if self.dynamic_population_control and np.random.rand() < 0.1:\n            self.population_size = max(int(self.population_size * 0.95), self.dim * 2)\n\n    def select_strategy(self):\n        strategy_weights = np.array(list(self.strategy_memory.values()))\n        total = np.sum(strategy_weights)\n        if total > 0:\n            strategy_probs = strategy_weights / total\n            return np.random.choice(list(self.strategy_memory.keys()), p=strategy_probs)\n        else:\n            return np.random.choice(list(self.strategy_memory.keys()))\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 69, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05716 with standard deviation 0.05428.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12878634283221235, 0.12995297034881015, 0.13184115871216373, 0.040252069851881345, 0.04348567009750881, 0.03947699569385377, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "fad40b0f-9ffa-421e-ab65-5ef9afbb295b", "fitness": 0.056727753341373845, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Multi-Strategy Learning and Adaptive Control for Superior Optimization.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1/bin': 0}  # Added a new strategy\n        self.strategy_probabilities = {'DE/rand/1/bin': 0.33, 'DE/best/1/bin': 0.33, 'DE/current-to-best/1/bin': 0.34}\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/current-to-best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total_counts = sum(self.strategy_memory.values())\n        if total_counts > 0:\n            for strategy in self.strategy_probabilities:\n                self.strategy_probabilities[strategy] = self.strategy_memory[strategy] / total_counts\n        chosen_strategy = np.random.choice(list(self.strategy_probabilities.keys()), p=list(self.strategy_probabilities.values()))\n        return chosen_strategy\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 70, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05673 with standard deviation 0.05380.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.128936825683457, 0.13246081994951342, 0.12559390930868208, 0.043044492759108266, 0.04348567009750881, 0.03636139560742835, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "c5a139a4-b0f8-42a3-a026-84d2845c3ad5", "fitness": 0.05659647469848127, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Time-Varying Parameters and Stochastic Local Search for Improved Global Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n        self.time_varying_rate = 0.9 / budget  # Reduce mutation factor over time\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n\n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n        # Decrease mutation factor over time\n        self.mutation_factor *= (1 - self.time_varying_rate)\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.normal(0, 0.1, self.dim)) * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply stochastic local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 71, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05660 with standard deviation 0.05376.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.1265089714786466, 0.13170662389186527, 0.12847200313519325, 0.04026131438515035, 0.04443256526500472, 0.03732012746380453, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "650cacd4-da62-42ef-917c-9ef8987703db", "fitness": 0.056343447199109255, "name": "EnhancedMemeticDifferentialEvolution", "description": "Adaptive Memetic Differential Evolution with Enhanced Strategy Adaptation and Local Search Intensity Control for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1/bin': 0}\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/current-to-best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        strategy_probabilities = np.array(list(self.strategy_memory.values())) + 1\n        strategy_probabilities = strategy_probabilities / strategy_probabilities.sum()\n        strategy = np.random.choice(list(self.strategy_memory.keys()), p=strategy_probabilities)\n        return strategy\n\n    def local_search(self, candidate, func, intensity=0.1):\n        perturbation = (np.random.rand(self.dim) - 0.5) * intensity * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func, intensity=0.05)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 72, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05634 with standard deviation 0.05351.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13127843162184027, 0.12995297034881015, 0.12362001155760372, 0.042179386637245964, 0.04068052770877817, 0.03871303025103834, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "655158c3-79db-4e0f-a57d-bb2120fd31f0", "fitness": 0.057422230829974455, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Progressive Population Reduction and Adaptive Learning Rate for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.7\n        self.mutation_factor = 0.9\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n        self.learning_rate = 0.1  # New parameter for adaptive adjustments\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.normal(0, self.learning_rate), 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.normal(0, self.learning_rate), 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.7\n            self.mutation_factor = 0.9\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * (0.8 + 0.2 * np.random.rand())), self.dim * 2)\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n\n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 73, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05742 with standard deviation 0.05412.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13080389845115226, 0.12995297034881015, 0.12935495513649908, 0.042722224359119876, 0.04366458625698555, 0.03963477625053646, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "6b923e09-a8db-4360-bb9b-f302885c16d6", "fitness": 0.056735263551589615, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Dynamic Population and Strategy Reinforcement Learning for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n        self.strategy_adaption_rate = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n        else:\n            self.strategy_memory[strategy] -= self.strategy_adaption_rate\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total = sum(self.strategy_memory.values())\n        if total == 0:\n            return 'DE/rand/1/bin' if np.random.rand() < 0.5 else 'DE/best/1/bin'\n        rand_value = np.random.rand() * total\n        cumulative = 0.0\n        for strategy, weight in self.strategy_memory.items():\n            cumulative += weight\n            if rand_value < cumulative:\n                return strategy\n        return 'DE/rand/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05674 with standard deviation 0.05337.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12683540090993406, 0.12995297034881015, 0.12811370520316656, 0.040252069851881345, 0.04348567009750881, 0.04131088888633894, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "44b2b654-9f38-42e3-94e9-e5ee5b8b07b2", "fitness": 0.05604700708816509, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Introduced a third strategy 'DE/current-to-best/1/bin' to improve convergence rates.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1/bin': 0}  # Added new strategy\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/current-to-best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        strategy_probabilities = np.array(list(self.strategy_memory.values())) + 1\n        strategy_probabilities = strategy_probabilities / np.sum(strategy_probabilities)\n        return np.random.choice(list(self.strategy_memory.keys()), p=strategy_probabilities)\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 75, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05605 with standard deviation 0.05360.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13127843162184027, 0.12995297034881015, 0.12362001155760372, 0.042179386637245964, 0.0397802322026235, 0.036945364758695476, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "ea76dbec-3a17-4f58-ab24-83f26bd9b058", "fitness": 0.055649718282349006, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Enhanced selection strategy for choosing between `DE/rand/1/bin` and `DE/best/1/bin`.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        success_prob_rand = self.strategy_memory['DE/rand/1/bin'] / (self.strategy_memory['DE/rand/1/bin'] + self.strategy_memory['DE/best/1/bin'] + 1e-9)\n        if np.random.rand() < success_prob_rand:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 76, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05565 with standard deviation 0.05384.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12683540090993406, 0.13068542572807007, 0.12811370520316656, 0.040252069851881345, 0.036352612980733556, 0.037941583200688744, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "fba06724-01f5-43b2-b8b8-4151a91deeca", "fitness": 0.05736697091111796, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Enhanced selection mechanism by incorporating elitism to maintain the best solution, ensuring improved convergence.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        best_idx = np.argmin(self.fitness)\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n        if trial_fitness < self.fitness[best_idx]:\n            best_idx = target_idx\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 77, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05737 with standard deviation 0.05499.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13031019018952872, 0.1321136590817955, 0.13233369168324427, 0.040495551940109675, 0.04301701065491925, 0.03736596798379754, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "2b086ba5-63c0-4785-84bd-c0eec7ef9778", "fitness": 0.056390547515912534, "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced Memetic Differential Evolution with Adaptive Strategy Probability and Dynamic Parameter Scaling for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n        self.strategy_probabilities = {'DE/rand/1/bin': 0.5, 'DE/best/1/bin': 0.5}\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def update_strategy_probabilities(self):\n        total_success = sum(self.strategy_memory.values())\n        if total_success > 0:\n            for strategy in self.strategy_memory:\n                self.strategy_probabilities[strategy] = self.strategy_memory[strategy] / total_success\n\n    def select_strategy(self):\n        self.update_strategy_probabilities()\n        strategies = list(self.strategy_probabilities.keys())\n        probabilities = list(self.strategy_probabilities.values())\n        return np.random.choice(strategies, p=probabilities)\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05639 with standard deviation 0.05353.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12683540090993406, 0.1302198317133666, 0.12811370520316656, 0.040252069851881345, 0.04348567009750881, 0.037941583200688744, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "d46127bc-2f8b-4ff0-8edb-1d0924b2571f", "fitness": 0.0560198478804704, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Introduce a bias towards 'DE/best/1/bin' strategy to increase convergence speed.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.3:  # Bias toward 'DE/best/1/bin' strategy\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 79, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05602 with standard deviation 0.05383.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12645933957600475, 0.1321136590817955, 0.1276289951237426, 0.04075532154698014, 0.03832219187829733, 0.03823245705074663, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "7916b892-1fc6-486e-83aa-311e803146d7", "fitness": 0.05660871835492812, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with Improved Parameter Adaptation and Selective Local Search for Efficient Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.05, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.05, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.3 + np.random.rand() * 0.7\n            self.mutation_factor = 0.7 + np.random.rand() * 0.3\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            new_population_size = max(int(self.population_size * 0.9), self.dim * 2)\n            self.population = self.population[:new_population_size]\n            self.fitness = self.fitness[:new_population_size]\n            self.population_size = new_population_size\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.05 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                best_idx = np.argmin(self.fitness)\n                local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n                if local_fitness < self.fitness[best_idx]:\n                    self.population[best_idx] = local_candidate\n                    self.fitness[best_idx] = local_fitness\n                    evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 80, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05661 with standard deviation 0.05478.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.1372589396371766, 0.12995297034881015, 0.12476087415333581, 0.04056828627729392, 0.038496554899395075, 0.037774173211674755, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "9c2c9761-a33d-4a8f-b396-d31a1cb50b3d", "fitness": 0.056665469285929825, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Entropy-Based Strategy Selection and Self-Adaptive Control for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1/bin': 0}  # Added new strategy\n        self.entropy_threshold = 0.2  # Entropy threshold for strategy selection\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/current-to-best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a = np.random.choice(indices)\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (self.population[a] - self.population[target_idx])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def calculate_strategy_entropy(self):\n        total = sum(self.strategy_memory.values())\n        if total == 0:\n            return 0\n        probabilities = [count / total for count in self.strategy_memory.values()]\n        entropy = -sum(p * np.log(p) if p > 0 else 0 for p in probabilities)\n        return entropy\n\n    def select_strategy(self):\n        entropy = self.calculate_strategy_entropy()\n        if entropy < self.entropy_threshold:\n            # Choose strategy based on success rates\n            strategies = list(self.strategy_memory.keys())\n            weights = [self.strategy_memory[strategy] + 1 for strategy in strategies]  # Add 1 to avoid zero probability\n            return np.random.choice(strategies, p=np.array(weights) / sum(weights))\n        else:\n            # Random selection to encourage exploration\n            return np.random.choice(list(self.strategy_memory.keys()))\n\n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05667 with standard deviation 0.05465.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.1260242862502403, 0.12995297034881015, 0.13559407497187204, 0.040252069851881345, 0.0384871616752086, 0.03901199380868925, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "fb8cd535-f068-4715-a302-c2c5e3ebb609", "fitness": 0.056217536863405346, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Improved adaptive strategy selection and parameter tuning to enhance convergence rate by refining randomness and strategy success tracking.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.normal(0, 0.05), 0.1, 0.9)  # Changed line\n            self.mutation_factor = np.clip(average_f + np.random.normal(0, 0.05), 0.5, 1.0)  # Changed line\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):  # Modified strategy choice probabilities\n        total = sum(self.strategy_memory.values()) + 1e-9  # Prevent division by zero\n        rand_prob = np.random.rand()\n        if (rand_prob < self.strategy_memory['DE/rand/1/bin'] / total):  # Changed line\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 82, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05622 with standard deviation 0.05374.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12797619275926964, 0.12995297034881015, 0.12817789213160125, 0.04053788779080969, 0.0399524862505678, 0.0386937358229229, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "14c7ac14-a3c7-4e5d-86d7-db4d34f2bcdd", "fitness": 0.056390547515912534, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Hybrid Strategy Selection and Population Diversity Preservation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n        self.diversity_threshold = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        diversity = np.std(self.population, axis=0).mean()\n        if self.dynamic_population_control and diversity < self.diversity_threshold:\n            self.population_size = min(self.initial_population_size, int(self.population_size * 1.1))\n        best_fitness = np.min(self.fitness)\n        if best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        success_rates = np.array(list(self.strategy_memory.values()))\n        total_success = success_rates.sum()\n        if total_success > 0:\n            probabilities = success_rates / total_success\n        else:\n            probabilities = [0.5, 0.5]\n        return np.random.choice(list(self.strategy_memory.keys()), p=probabilities)\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05639 with standard deviation 0.05353.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12683540090993406, 0.1302198317133666, 0.12811370520316656, 0.040252069851881345, 0.04348567009750881, 0.037941583200688744, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "0d6240a5-0e3b-4241-8cc0-66cd7c36bb23", "fitness": -Infinity, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Adaptive Learning Rate and Adaptive Mutation Strategies for Robust Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/rand-to-best/1/bin': 0}\n        self.adaptive_learning_rate = 0.5\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/rand-to-best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[target_idx] + self.adaptive_learning_rate * (self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor, self.adaptive_learning_rate))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _, _ in self.memory])\n            average_f = np.mean([mf for _, mf, _ in self.memory])\n            average_lr = np.mean([lr for _, _, lr in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n            self.adaptive_learning_rate = np.clip(average_lr + np.random.rand() * 0.1, 0.1, 0.7)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n            self.adaptive_learning_rate = 0.2 + np.random.rand() * 0.5\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total = sum(self.strategy_memory.values())\n        probabilities = [self.strategy_memory[s] / total + 0.1 for s in self.strategy_memory]\n        return np.random.choice(list(self.strategy_memory.keys()), p=probabilities)\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.01 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 84, "feedback": "An exception occurred: ZeroDivisionError('division by zero').", "error": "ZeroDivisionError('division by zero')", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {}}
{"id": "63d0ab88-6afd-4c6a-8fd2-2a4c2b07fbe5", "fitness": 0.06698236723579223, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Adaptive Memetic Differential Evolution with Dynamic Strategy Selection, Local Search, and Diversity Enhancement through Orthogonal Design for Improved Convergence.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n        self.diversity_threshold = 1e-5\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < self.diversity_threshold:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def enhance_diversity(self):\n        mean = np.mean(self.population, axis=0)\n        for i in range(self.population_size):\n            if np.linalg.norm(self.population[i] - mean) < self.diversity_threshold:\n                self.population[i] = mean + (np.random.rand(self.dim) - 0.5) * (self.bounds.ub - self.bounds.lb)\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            self.enhance_diversity()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 85, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06698 with standard deviation 0.07188.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.21684875711159712, 0.1321136590817955, 0.13233369168324427, 0.040495551940109675, 0.04301701065491925, 0.03736596798379754, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "c3c10c63-c87f-4d53-b77a-715f9eb03d09", "fitness": -Infinity, "name": "ImprovedMemeticDifferentialEvolution", "description": "Improved Memetic Differential Evolution with Adaptive Learning Rate and Enhanced Exploration for Superior Convergence.", "code": "import numpy as np\n\nclass ImprovedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.learning_rate = 0.1  # New adaptive learning rate\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0} \n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n            self.learning_rate = np.clip(average_f * 0.05 + np.random.rand() * 0.05, 0.01, 0.2)  # Adjust the learning rate\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n            self.learning_rate = 0.05 + np.random.rand() * 0.05\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        weights = np.array(list(self.strategy_memory.values()), dtype=float)\n        weights /= (weights.sum() + 1e-9)  # Normalize, avoiding zero division\n        return np.random.choice(['DE/rand/1/bin', 'DE/best/1/bin'], p=weights)\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * self.learning_rate * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 86, "feedback": "An exception occurred: ValueError('probabilities do not sum to 1').", "error": "ValueError('probabilities do not sum to 1')", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {}}
{"id": "049b02ce-493d-4ffa-989d-f73969d63233", "fitness": 0.06698236723579223, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Slightly adjusted mutation factor starting range to potentially enhance convergence speed.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.65, 1.0)  # Changed line\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 87, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06698 with standard deviation 0.07188.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.21684875711159712, 0.1321136590817955, 0.13233369168324427, 0.040495551940109675, 0.04301701065491925, 0.03736596798379754, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "06084d41-f602-4197-9e0b-a2f351f6b6e7", "fitness": 0.056390547515912534, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Enhanced DE strategy selection by dynamically adjusting probabilities based on success rates.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total_success = sum(self.strategy_memory.values())\n        if total_success > 0:\n            prob_rand = self.strategy_memory['DE/rand/1/bin'] / total_success\n            prob_best = self.strategy_memory['DE/best/1/bin'] / total_success\n        else:\n            prob_rand, prob_best = 0.5, 0.5\n        return 'DE/rand/1/bin' if np.random.rand() < prob_rand else 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 88, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05639 with standard deviation 0.05353.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12683540090993406, 0.1302198317133666, 0.12811370520316656, 0.040252069851881345, 0.04348567009750881, 0.037941583200688744, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "c2405365-fd1a-44aa-8215-56a1a016f1ce", "fitness": 0.04824160654946113, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Sine-Cosine Initialization and Adaptive Learning Strategies for Improved Diversity and Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        # Sine-Cosine initialization to ensure diverse initial population\n        self.population = np.zeros((self.population_size, self.dim))\n        for i in range(self.population_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            self.population[i] = lb + (ub - lb) * (r1 * np.sin(2 * np.pi * r2) + (1 - r1) * np.cos(2 * np.pi * r2))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        # Adaptive changes based on strategy success rate\n        total_success = sum(self.strategy_memory.values())\n        if total_success > 0:\n            cr_weight = self.strategy_memory['DE/best/1/bin'] / total_success\n            mf_weight = self.strategy_memory['DE/rand/1/bin'] / total_success\n            self.crossover_rate = np.clip(0.5 + 0.3 * (cr_weight - mf_weight), 0.1, 0.9)\n            self.mutation_factor = np.clip(0.8 + 0.2 * (mf_weight - cr_weight), 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.5\n            self.mutation_factor = 0.8\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 89, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04824 with standard deviation 0.04785.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.11158852555182808, 0.11910121640186855, 0.10960529530592145, 0.029647487991648735, 0.03118882728092065, 0.032376439746296004, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "8a1e8bc9-26d6-494f-890d-3f015a6399d4", "fitness": 0.056727753341373845, "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced Memetic Differential Evolution with Dynamic Strategy Adaptation and Adaptive Local Search for Superior Convergence.", "code": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/current-to-best/1/bin': 0} \n        self.strategy_probabilities = np.array([0.33, 0.33, 0.34])\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n\n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/current-to-best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[target_idx] + self.mutation_factor * (self.population[best_idx] - self.population[target_idx]) + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total_success = sum(self.strategy_memory.values())\n        if total_success > 0:\n            self.strategy_probabilities = np.array([self.strategy_memory[k] for k in self.strategy_memory.keys()]) / total_success\n        strategy_idx = np.random.choice(len(self.strategy_probabilities), p=self.strategy_probabilities)\n        return list(self.strategy_memory.keys())[strategy_idx]\n\n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 90, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05673 with standard deviation 0.05380.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.128936825683457, 0.13246081994951342, 0.12559390930868208, 0.043044492759108266, 0.04348567009750881, 0.03636139560742835, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "fdf351a9-a760-4263-89fb-a6f057ed66d2", "fitness": 0.05723956339397418, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Minor refinement of local search perturbation for enhanced local exploitation.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.05 * (self.bounds.ub - self.bounds.lb)  # refined perturbation\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 91, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05724 with standard deviation 0.05502.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13031019018952872, 0.1321136590817955, 0.13233369168324427, 0.040495551940109675, 0.0418703430006252, 0.03736596798379754, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "7f618952-2241-4de8-9610-106658786a2c", "fitness": 0.05680533179330267, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Learning-Based Strategy Selection and Adaptive Local Search for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n        self.strategy_probabilities = np.array([0.5, 0.5])\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor, strategy))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _, _ in self.memory])\n            average_f = np.mean([mf for _, mf, _ in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        strategy = np.random.choice(['DE/rand/1/bin', 'DE/best/1/bin'], p=self.strategy_probabilities)\n        success_counts = np.array(list(self.strategy_memory.values()))\n        total_success = np.sum(success_counts)\n        if total_success > 0:\n            self.strategy_probabilities = success_counts / total_success\n        else:\n            self.strategy_probabilities = np.array([0.5, 0.5])\n        return strategy\n\n    def local_search(self, candidate, func):\n        perturbation_scale = 0.1 / (1 + np.min(self.fitness))  # Adaptive perturbation scale\n        perturbation = (np.random.rand(self.dim) - 0.5) * perturbation_scale * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05681 with standard deviation 0.05376.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12683540090993406, 0.1321136590817955, 0.12811370520316656, 0.04111246278856906, 0.043542466109111055, 0.03886362538048105, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "8a9e9308-b595-4c42-aa29-5b3303526dfd", "fitness": 0.05567759024999197, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Adaptive Crossover and Mutation Strategies for Improved Convergence on Black Box Functions.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rates = np.full(self.initial_population_size, 0.5)\n        self.mutation_factors = np.full(self.initial_population_size, 0.8)\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factors[target_idx] * (self.population[b] - self.population[c])\n        else:\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factors[target_idx] * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant, target_idx):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rates[target_idx]\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rates[target_idx], self.mutation_factors[target_idx]))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            for i in range(self.population_size):\n                self.crossover_rates[i] = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n                self.mutation_factors[i] = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rates = 0.1 + np.random.rand(self.population_size) * 0.9\n            self.mutation_factors = 0.6 + np.random.rand(self.population_size) * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            new_population_size = max(int(self.population_size * 0.9), self.dim * 2)\n            if new_population_size < self.population_size:\n                self.population = self.population[:new_population_size]\n                self.fitness = self.fitness[:new_population_size]\n            self.population_size = new_population_size\n\n    def select_strategy(self):\n        return 'DE/rand/1/bin' if np.random.rand() < 0.5 else 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant, i)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 93, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05568 with standard deviation 0.05349.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.1274567166919507, 0.12995297034881015, 0.12642635059122098, 0.04059558221000348, 0.03894040713106717, 0.03705961861020857, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "d32f5ce7-f87f-4300-ae95-b6fea976357f", "fitness": 0.05677690036932838, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution using Multi-Strategy Pool and Adaptive Memory for Dynamic Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0,\n                                'DE/rand-to-best/1/bin': 0, 'DE/current-to-best/1/bin': 0}\n        self.alpha = 0.1  # Memory influence factor\n        self.dynamic_population_control = True\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        best_idx = np.argmin(self.fitness)\n        target = self.population[target_idx]\n\n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        elif strategy == 'DE/rand-to-best/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * ((self.population[best_idx] - target) +\n                                                                  (self.population[b] - self.population[c]))\n        else:  # DE/current-to-best/1/bin\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = target + self.mutation_factor * ((self.population[best_idx] - target) +\n                                                      (self.population[a] - self.population[b]))\n\n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor, strategy))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            recent_memory = self.memory[-int(self.alpha * len(self.memory)):]\n            average_cr = np.mean([cr for cr, _, _ in recent_memory])\n            average_f = np.mean([mf for _, mf, _ in recent_memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total_success = sum(self.strategy_memory.values())\n        if total_success > 0:\n            probabilities = {k: v / total_success for k, v in self.strategy_memory.items()}\n            strategies, probs = zip(*probabilities.items())\n            return np.random.choice(strategies, p=probs)\n        else:\n            return np.random.choice(list(self.strategy_memory.keys()))\n\n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 94, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05678 with standard deviation 0.05413.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12518720675799133, 0.1377735621013102, 0.12558284693844923, 0.04189394566897098, 0.04222065514492368, 0.03766722004564327, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "c9c63c7b-c627-4588-af52-d83067027feb", "fitness": 0.056339067624179034, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Periodic Population Renewal and Strategy Rebalancing for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n        self.last_reset = 0\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total = sum(self.strategy_memory.values())\n        if total == 0 or np.random.rand() < 0.1:\n            return np.random.choice(['DE/rand/1/bin', 'DE/best/1/bin'])\n        else:\n            probabilities = [self.strategy_memory['DE/rand/1/bin']/(total+1e-8), self.strategy_memory['DE/best/1/bin']/(total+1e-8)]\n            return np.random.choice(['DE/rand/1/bin', 'DE/best/1/bin'], p=probabilities)\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n\n    def periodic_population_reset(self, func, evaluations):\n        if evaluations - self.last_reset > self.budget / 4:\n            self.last_reset = evaluations\n            worst_indices = np.argsort(self.fitness)[-int(0.2 * self.population_size):]\n            for idx in worst_indices:\n                self.population[idx] = self.bounds.lb + (self.bounds.ub - self.bounds.lb) * np.random.rand(self.dim)\n                self.fitness[idx] = func(self.population[idx])\n                evaluations += 1\n            return evaluations\n        return evaluations\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            evaluations = self.periodic_population_reset(func, evaluations)\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 95, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05634 with standard deviation 0.05414.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13078155966195237, 0.13044205588246227, 0.12717167884502267, 0.040850987719696086, 0.040993615041852505, 0.036145044799958725, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "2a686f18-a583-4efa-a253-f65c0fe032ee", "fitness": 0.0568242021903369, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution using Dynamic Parameter Scaling and Enhanced Local Search for Improved Convergence and Exploration.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n        self.exploration_factor = 1.0\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.3, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.4, 1.2)\n        else:\n            self.crossover_rate = 0.3 + np.random.rand() * 0.7\n            self.mutation_factor = 0.5 + np.random.rand() * 0.7\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.85), self.dim * 2)\n            self.exploration_factor *= 0.9\n\n    def select_strategy(self):\n        exploration_bias = self.exploration_factor / (self.exploration_factor + 1.0)\n        if np.random.rand() < exploration_bias:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def enhanced_local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.2 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply enhanced local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.enhanced_local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 96, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05682 with standard deviation 0.05490.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13644561653418452, 0.13184312483866512, 0.12475791176739037, 0.040354383001878324, 0.03790136132066679, 0.0394487555835803, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "c5b905c2-6911-4534-b9a2-09797622d3ce", "fitness": 0.05611748322592761, "name": "EnhancedAdaptiveMemeticDE", "description": "Enhanced Adaptive Memetic Differential Evolution with Strategy Adaptation and Local Search for Better Convergence Control.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0, 'DE/rand-to-best/1/bin': 0}\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        elif strategy == 'DE/best/1/bin':\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        else:  # DE/rand-to-best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * ((self.population[best_idx] - self.population[a]) + (self.population[b] - self.population[c]))\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.randn() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.randn() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        strategy_scores = [self.strategy_memory[strategy] for strategy in self.strategy_memory]\n        total = sum(strategy_scores)\n        if total > 0:\n            probabilities = [score / total for score in strategy_scores]\n            return np.random.choice(list(self.strategy_memory.keys()), p=probabilities)\n        else:\n            return np.random.choice(list(self.strategy_memory.keys()))\n\n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 97, "feedback": "The algorithm EnhancedAdaptiveMemeticDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05612 with standard deviation 0.05410.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.13113172187220756, 0.13010615238569523, 0.12657352999336746, 0.040252069851881345, 0.03697171535253119, 0.039355492910999046, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "0b756256-5f3a-42ee-8ee2-a306c72cda25", "fitness": 0.05642992904775111, "name": "EnhancedAdaptiveMemeticDifferentialEvolution", "description": "Enhanced Adaptive Memetic Differential Evolution with Improved Local Search and Dynamic Strategy Weighting for Accelerated Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 1, 'DE/best/1/bin': 1}  # Strategy success tracking\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            average_f = np.mean([mf for _, mf in self.memory])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        total_success = sum(self.strategy_memory.values())\n        probabilities = [self.strategy_memory[strategy] / total_success for strategy in self.strategy_memory]\n        return np.random.choice(list(self.strategy_memory.keys()), p=probabilities)\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.randn(self.dim)) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 98, "feedback": "The algorithm EnhancedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05643 with standard deviation 0.05431.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12665358889498668, 0.133282265357345, 0.12945526197964308, 0.04139924843285292, 0.04036390106975585, 0.0360484290285098, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "d243db2a-4ada-4d66-ac65-0001b0e38992", "fitness": 0.05693216496760657, "name": "AdaptiveMemeticDifferentialEvolution", "description": "Introduced a strategic adjustment to the mutation factor to improve convergence in diverse scenarios.", "code": "import numpy as np\n\nclass AdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.bounds = None\n        self.fitness = None\n        self.crossover_rate = 0.5\n        self.mutation_factor = 0.8\n        self.generations = 0\n        self.dynamic_population_control = True\n        self.memory = []\n        self.strategy_memory = {'DE/rand/1/bin': 0, 'DE/best/1/bin': 0}\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_fitness(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def mutate(self, target_idx, strategy):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        \n        if strategy == 'DE/rand/1/bin':\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        else:  # DE/best/1/bin\n            best_idx = np.argmin(self.fitness)\n            a, b = np.random.choice(indices, 2, replace=False)\n            mutant = self.population[best_idx] + self.mutation_factor * (self.population[a] - self.population[b])\n        \n        mutant = np.clip(mutant, self.bounds.lb, self.bounds.ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def select(self, target_idx, trial_vector, trial_fitness, strategy):\n        if trial_fitness < self.fitness[target_idx]:\n            self.population[target_idx] = trial_vector\n            self.fitness[target_idx] = trial_fitness\n            self.memory.append((self.crossover_rate, self.mutation_factor))\n            self.strategy_memory[strategy] += 1\n\n    def adapt_parameters(self):\n        if self.memory:\n            average_cr = np.mean([cr for cr, _ in self.memory])\n            # Modified line: Added consideration of strategy success for mutation factor adjustment\n            average_f = np.mean([mf for _, mf in self.memory]) + 0.1 * (self.strategy_memory['DE/best/1/bin'] - self.strategy_memory['DE/rand/1/bin'])\n            self.crossover_rate = np.clip(average_cr + np.random.rand() * 0.1, 0.1, 0.9)\n            self.mutation_factor = np.clip(average_f + np.random.rand() * 0.1, 0.5, 1.0)\n        else:\n            self.crossover_rate = 0.1 + np.random.rand() * 0.9\n            self.mutation_factor = 0.6 + np.random.rand() * 0.4\n\n    def control_population_size(self):\n        best_fitness = np.min(self.fitness)\n        if self.dynamic_population_control and best_fitness < 1e-5:\n            self.population_size = max(int(self.population_size * 0.9), self.dim * 2)\n\n    def select_strategy(self):\n        if np.random.rand() < 0.5:\n            return 'DE/rand/1/bin'\n        else:\n            return 'DE/best/1/bin'\n    \n    def local_search(self, candidate, func):\n        perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (self.bounds.ub - self.bounds.lb)\n        neighbor = candidate + perturbation\n        neighbor = np.clip(neighbor, self.bounds.lb, self.bounds.ub)\n        neighbor_fitness = func(neighbor)\n        return neighbor, neighbor_fitness\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        self.evaluate_fitness(func)\n\n        evaluations = self.population_size\n        while evaluations < self.budget:\n            self.adapt_parameters()\n            self.control_population_size()\n            for i in range(self.population_size):\n                strategy = self.select_strategy()\n                mutant = self.mutate(i, strategy)\n                trial = self.crossover(self.population[i], mutant)\n                trial_fitness = func(trial)\n                evaluations += 1\n                self.select(i, trial, trial_fitness, strategy)\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to the best candidate\n            best_idx = np.argmin(self.fitness)\n            local_candidate, local_fitness = self.local_search(self.population[best_idx], func)\n            if local_fitness < self.fitness[best_idx]:\n                self.population[best_idx] = local_candidate\n                self.fitness[best_idx] = local_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 99, "feedback": "The algorithm AdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05693 with standard deviation 0.05372.", "error": "", "parent_ids": ["d6af1740-63c1-4a96-ab8a-9b0c641a140d"], "operator": null, "metadata": {"aucs": [0.12588920912495105, 0.1321136590817955, 0.12907608770563117, 0.040252069851881345, 0.04354561968729087, 0.04084617259024248, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
