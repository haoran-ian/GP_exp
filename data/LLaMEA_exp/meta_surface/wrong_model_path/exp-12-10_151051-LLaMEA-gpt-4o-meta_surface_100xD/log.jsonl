{"id": "7c312d25-3bc4-4d80-90fb-1934bd33d0d9", "fitness": 0.673129186062608, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with Dynamic Parameter Control to efficiently explore and exploit the search space for black box optimization.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        F = 0.5  # differential weight\n        CR = 0.9  # crossover probability\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                mutant = np.clip(x0 + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.7, 1.0)\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.67313 with standard deviation 0.10068.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.7962003519753238, 0.5495795373961827, 0.6736076688163177]}}
{"id": "3a73ded6-c9ca-40d8-854d-2b62c432e053", "fitness": 0.7775283870677353, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution using dynamic crossover and mutation strategies for improved convergence in black box optimization.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 1, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.77753 with standard deviation 0.09370.", "error": "", "parent_ids": ["7c312d25-3bc4-4d80-90fb-1934bd33d0d9"], "operator": null, "metadata": {"aucs": [0.7794307648574108, 0.6618292517553119, 0.891325144590483]}}
{"id": "01169226-1427-465a-a7a3-a94409d36577", "fitness": 0.6520944213875817, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with adaptive population size adjustment based on convergence rate for improved efficiency in black box optimization.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptive population size based on convergence rate\n            if eval_count % (10 * self.dim) == 0:  # Checkpoint every 10 * dim evaluations\n                pop_size = max(4, pop_size - 1)  # Reduce population size to the minimum of 4 if stagnation is detected\n            \n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 2, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.65209 with standard deviation 0.06482.", "error": "", "parent_ids": ["3a73ded6-c9ca-40d8-854d-2b62c432e053"], "operator": null, "metadata": {"aucs": [0.7198417097183606, 0.6717069182271691, 0.5647346362172153]}}
{"id": "9dac118a-856f-4f58-ab8a-5917fd372c53", "fitness": 0.7775283870677353, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Introduced memory-based mutation strategy by leveraging historical best solutions to refine convergence dynamics.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        historical_best = population[np.argmin(fitness)].copy()  # Maintain historical best solution\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use historical best for mutation strategy\n                    mutant = np.clip(x0 + F * (historical_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_index]:\n                        historical_best = trial.copy()  # Update historical best if improved\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.77753 with standard deviation 0.09370.", "error": "", "parent_ids": ["3a73ded6-c9ca-40d8-854d-2b62c432e053"], "operator": null, "metadata": {"aucs": [0.7794307648574108, 0.6618292517553119, 0.891325144590483]}}
{"id": "2c79b240-2200-4e1d-9b13-3f06469e1ed5", "fitness": 0.8183943629289775, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with improved mutation strategy, leveraging the second-best individual for additional diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.81839 with standard deviation 0.03205.", "error": "", "parent_ids": ["3a73ded6-c9ca-40d8-854d-2b62c432e053"], "operator": null, "metadata": {"aucs": [0.7939144291084111, 0.7976034708199061, 0.8636651888586153]}}
{"id": "6c27a7af-cd1c-48b2-9297-e3710b232d7a", "fitness": 0.6182344537998036, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with modified crossover probability adjustment to dynamically increase diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0) + (0.5 - np.mean(fitness)/max(fitness)) # Modified CR\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.61823 with standard deviation 0.05926.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7017143610761257, 0.57011144013498, 0.5828775601883052]}}
{"id": "e840b086-88a5-4d15-892b-e5328a87f2c2", "fitness": 0.76056596388749, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with stochastic crossover probability for improved exploration.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.1, 1.0)  # Modified line: Changed crossover probability range for exploration\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.76057 with standard deviation 0.07371.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7334364437687961, 0.8613009809502987, 0.6869604669433752]}}
{"id": "4e238c1f-d1fd-498d-9d58-67fa94b27bc8", "fitness": 0.6754187724295603, "name": "DynamicDifferentialEvolution", "description": "Dynamic Differential Evolution with adaptive parameter control using historical success rates for enhanced convergence.", "code": "import numpy as np\n\nclass DynamicDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Tracking success history for parameter adaptation\n        success_history = []\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters based on historical success\n            if success_history:\n                F = np.clip(0.5 + 0.3 * np.mean(success_history), 0.4, 0.9)\n                CR = np.clip(0.6 + 0.2 * np.mean(success_history), 0.5, 1.0)\n            else:\n                F = np.random.uniform(0.4, 0.9)\n                CR = np.random.uniform(0.5, 1.0)\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1])  # second-best individual\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    success_history.append(1)\n                    population[i] = trial\n                    fitness[i] = f_trial\n                else:\n                    success_history.append(0)\n\n                # Maintain a fixed size for success history\n                if len(success_history) > 100:\n                    success_history.pop(0)\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 7, "feedback": "The algorithm DynamicDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.67542 with standard deviation 0.22116.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.3649876576101596, 0.7976034708199061, 0.8636651888586153]}}
{"id": "b07e57de-b681-4e2c-9ce6-e896a0085885", "fitness": 0.7271563774071913, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced parameter adaptation by dynamically adjusting the mutation factor (F) based on population diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9) * (np.std(fitness) / np.mean(fitness))  # Adjust mutation factor based on diversity\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.72716 with standard deviation 0.01136.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7193416995091598, 0.7189018926658048, 0.7432255400466092]}}
{"id": "bb27ca2d-6cc7-475e-8875-ed7cbefd1e14", "fitness": 0.635638512482024, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with adaptive crossover probability based on fitness variance for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            \n            # New line: Adaptive CR based on fitness variance\n            CR = np.random.uniform(0.3, 1.0 - (np.std(fitness) / np.mean(fitness)))\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 9, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.63564 with standard deviation 0.18795.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7474090797825712, 0.7886042443201258, 0.3709022133433749]}}
{"id": "f0df626f-caa6-4287-a481-b9d39400eb1c", "fitness": 0.753949901500695, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Improved diversity by adding a random scaling factor to the difference vector in the mutation strategy.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                random_scale = np.random.uniform(0.5, 1.5)  # New line: Add random scaling factor\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + random_scale * F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + random_scale * F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 10, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.75395 with standard deviation 0.01971.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7534669224646167, 0.77832992155936, 0.7300528604781082]}}
{"id": "e8161050-b9a5-4631-bad3-2ff17a529931", "fitness": 0.741442601801961, "name": "DynamicHierarchicalAdaptiveDifferentialEvolution", "description": "Dynamic Hierarchical Adaptive Differential Evolution (DHADE) introduces hierarchical mutation selection based on dynamic fitness improvement to enhance diversity and convergence.", "code": "import numpy as np\n\nclass DynamicHierarchicalAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on hierarchical fitness ranking\n                sorted_indices = fitness.argsort()\n                best_index = sorted_indices[0]\n                second_best_index = sorted_indices[1]\n                random_index = np.random.choice(sorted_indices[:pop_size//2])  # Select from top half for diversity\n                \n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                x_random = population[random_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                \n                choice = np.random.rand()\n                if choice < 0.5:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                elif choice < 0.8:\n                    # Use second best for added diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use random from top half for hierarchical strategy\n                    mutant = np.clip(x0 + F * (x_random - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 11, "feedback": "The algorithm DynamicHierarchicalAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.74144 with standard deviation 0.02078.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7473525722765529, 0.7634227961465478, 0.7135524369827826]}}
{"id": "f22c70d6-34e5-419c-9407-4935a2af8505", "fitness": 0.7065626817409253, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with a refined crossover strategy for increased diversity using even distribution.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                # Updated line: Increased diversity by using uniform distribution in crossover\n                cross_points = np.random.rand(self.dim) < np.random.uniform(0.3, 1.0)\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.70656 with standard deviation 0.02179.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7205802689404761, 0.6757915548463631, 0.7233162214359365]}}
{"id": "f27847c1-daed-47fb-8351-dce6576a6e6d", "fitness": -Infinity, "name": "AdaptiveHybridDifferentialEvolution", "description": "Hybrid Differential Evolution with Adaptive Population Resizing and Memory-Enhanced Mutation Strategy for Improved Convergence.", "code": "import numpy as np\n\nclass AdaptiveHybridDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        memory = np.zeros((5, self.dim))  # Memory to store successful mutations\n        memory_idx = 0\n        \n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.5, 0.9)\n            CR = np.random.uniform(0.2, 0.9)\n            \n            # Dynamic population resizing\n            new_pop_size = max(5, int(pop_size * (1 - eval_count / self.budget)))\n            population = population[:new_pop_size]\n            fitness = fitness[:new_pop_size]\n            \n            for i in range(new_pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with memory-enhanced strategy\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                \n                if eval_count > pop_size:\n                    memory_indices = np.random.choice(min(len(memory), eval_count-pop_size), 3, replace=False)\n                    x_mem = memory[memory_indices]\n                    x0, x1, x2 = x_mem\n                else:\n                    indices = np.random.choice(new_pop_size, 3, replace=False)\n                    x0, x1, x2 = [population[idx] for idx in indices]\n                \n                if np.random.rand() < 0.8:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    memory[memory_idx % len(memory)] = mutant\n                    memory_idx += 1\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 13, "feedback": "An exception occurred: ValueError(\"Cannot take a larger sample than population when 'replace=False'\").", "error": "ValueError(\"Cannot take a larger sample than population when 'replace=False'\")", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {}}
{"id": "a5c1a7a7-c654-4005-8a55-eccc00ce1733", "fitness": 0.7416000855870916, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with adaptive crossover rate for improved diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 1.0)  # Changed line: Adjusted crossover rate range for diversity\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 14, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.74160 with standard deviation 0.01442.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7403671627319014, 0.7598424996351589, 0.7245905943942144]}}
{"id": "696e829f-ebc3-47ec-8b33-18e454c0b8c8", "fitness": 0.8119323367098561, "name": "EnhancedAdaptiveDifferentialEvolutionV2", "description": "Enhanced Adaptive Differential Evolution with dynamic population size and adaptive crossover for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolutionV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        init_pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(init_pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = init_pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Dynamic Population Size\n            pop_size = max(4, int(init_pop_size * (1 - eval_count / self.budget)))\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            \n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR_min, CR_max = 0.1, 1.0\n            CR = CR_min + (CR_max - CR_min) * ((self.budget - eval_count) / self.budget)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) if pop_size > 1 else best_index\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 15, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolutionV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.81193 with standard deviation 0.07197.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.8967231430815462, 0.7207877629357644, 0.8182861041122576]}}
{"id": "30dae98f-2ddd-443a-8a5c-f9cbaf26d2d6", "fitness": 0.4446927416839796, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Improved mutation strategy by incorporating population diversity measure to balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                diversity_factor = np.std(population) / (np.mean(population) + 1e-8) # New line: Calculate diversity factor\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * diversity_factor * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1]) # Modified line\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * diversity_factor * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1]) # Modified line\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 16, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.44469 with standard deviation 0.32337.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.3819769678613678, 0.8683497951503633, 0.08375146204020789]}}
{"id": "45536678-575d-4547-af37-05530e925be1", "fitness": 0.5449141098843236, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with dynamic crossover probability adjustment based on fitness improvement.  ", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover with dynamic adjustment\n                cross_points = np.random.rand(self.dim) < CR * (fitness[i] / (np.max(fitness) + 1e-8))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.54491 with standard deviation 0.25873.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.18037770932399966, 0.6997864110961084, 0.754578209232863]}}
{"id": "050b7259-e482-4484-af66-a3db4acc23e3", "fitness": 0.8160915022304486, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Improved Enhanced Adaptive Differential Evolution by integrating a dynamic mutation scaling factor influenced by population diversity metrics.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            # Calculate diversity metric\n            diversity = np.mean([np.linalg.norm(population[i] - population[j]) for i in range(pop_size) for j in range(i+1, pop_size)])\n            F = 0.4 + (0.9 - 0.4) * (diversity / np.mean(bounds[:, 1] - bounds[:, 0]))  # Dynamic scaling factor\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.81609 with standard deviation 0.01042.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.8296833255548495, 0.8043785411045965, 0.8142126400318997]}}
{"id": "21a212ff-9e28-49bf-b8f4-c621eccc6e01", "fitness": 0.7981110620369597, "name": "DynamicNeighborhoodDifferentialEvolution", "description": "Dynamic Neighborhood Differential Evolution with Self-Adaptive Control Parameters, integrating a dynamic neighborhood search to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass DynamicNeighborhoodDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            local_neighborhood_size = max(2, int(pop_size * 0.1))  # Dynamic neighborhood size\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic neighborhood strategy\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1])\n                x_best = population[best_index]\n\n                # Get indices for neighborhood search\n                neighbors = np.random.choice(pop_size, local_neighborhood_size, replace=False)\n                neighbors = neighbors[neighbors != i]\n\n                # Select three random solutions from neighborhood\n                indices = np.random.choice(neighbors, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (population[second_best_index] - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 19, "feedback": "The algorithm DynamicNeighborhoodDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.79811 with standard deviation 0.03958.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7568952395529631, 0.8515193463673061, 0.7859186001906099]}}
{"id": "7cb39903-c810-4f6d-a740-d5c473667fcf", "fitness": 0.7697377622817966, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with adaptive crossover rate and population size adjustment for better exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptive population size\n            pop_size = max(4, int(10 * self.dim * ((self.budget - eval_count) / self.budget)))\n            # Adaptation of parameters\n            F = np.random.uniform(0.5, 0.9)\n            CR = 0.9 - (0.8 * (eval_count / self.budget))  # Adaptive crossover rate decreases over time\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best and worst fitness\n                best_index = np.argmin(fitness)\n                worst_index = np.argmax(fitness)\n                x_best = population[best_index]\n                x_worst = population[worst_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use worst for additional exploration\n                    mutant = np.clip(x0 + F * (x_worst - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 20, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.76974 with standard deviation 0.01671.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7906946017202079, 0.7687217796344439, 0.7497969054907381]}}
{"id": "0eccbef7-65a6-48e2-9f4c-549bac521c40", "fitness": 0.7689278324233647, "name": "EnhancedAdaptiveDifferentialEvolutionV2", "description": "Enhanced Adaptive Differential Evolution with dual mutation strategies leveraging both best and second-best individuals for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolutionV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dual dynamic strategy based on best and second-best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1]\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                \n                # Dual mutation strategy\n                if np.random.rand() < 0.5:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 21, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolutionV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.76893 with standard deviation 0.04504.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7488567625462581, 0.8313184580836496, 0.7266082766401865]}}
{"id": "30ab89fd-077d-492f-a674-5874a6835e9a", "fitness": 0.7857326822817914, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced mutation strategy using fitness-proportional weighting between current best and second best individuals to improve convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) \n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                \n                # New line: Fitness-proportional weighting for mutation strategy\n                weight = fitness[second_best_index] / (fitness[best_index] + fitness[second_best_index])\n                if np.random.rand() < weight:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 22, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.78573 with standard deviation 0.04765.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.718831244406588, 0.8261842573930134, 0.8121825450457727]}}
{"id": "37ca0470-9957-44d3-ae6c-fdcace9c0d28", "fitness": 0.746911878593482, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with a novel balanced exploration-exploitation approach, integrating adaptive parameter tuning based on population diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            diversity_measure = np.std(population, axis=0).mean()  # Diversity of the population\n            adaptive_F = np.clip(0.5 + (0.5 * np.exp(-diversity_measure)), 0.4, 0.9)  # Adaptive scaling factor\n            adaptive_CR = 0.5 + (0.5 * (1 - np.exp(-diversity_measure)))  # Adaptive crossover rate\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1])  # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + adaptive_F * (x_best - x0) + adaptive_F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + adaptive_F * (x_second_best - x0) + adaptive_F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 23, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.74691 with standard deviation 0.06835.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.8366074184164236, 0.7332699262387457, 0.6708582911252767]}}
{"id": "3aa15274-13a0-4566-9188-b0165eeef131", "fitness": 0.5419380324835122, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with improved crossover strategy to increase diversity and exploration.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Introduce small mutation to trial vector for exploration\n                trial = trial + np.random.normal(0, 0.1, self.dim) * cross_points\n\n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 24, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.54194 with standard deviation 0.27609.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.8339738910838003, 0.6203621503310591, 0.17147805603567712]}}
{"id": "d8f04041-4c11-4f0f-aa17-6fe7b7578cc0", "fitness": 0.7570550432228051, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with adaptive crossover probability influenced by population diversity to improve exploration.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0) * (1 - np.std(fitness) / np.mean(fitness))  # Modified line: CR adapts based on fitness diversity\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 25, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.75706 with standard deviation 0.05665.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.8253051862858081, 0.7592731669420444, 0.6865867764405629]}}
{"id": "6e207b93-1cbf-4a8e-b2fa-8567d7e27c01", "fitness": 0.7745848554633189, "name": "AdvancedAdaptiveDifferentialEvolution", "description": "Advanced Adaptive Differential Evolution with dual-strategy mutation and self-adaptive parameter tuning for enhanced exploration-exploitation balance.", "code": "import numpy as np\n\nclass AdvancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            mean_fitness = np.mean(fitness)\n            stddev_fitness = np.std(fitness)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Adaptation of parameters based on current population diversity\n                F = np.random.uniform(0.5, 0.9) if stddev_fitness > 0.1 * mean_fitness else np.random.uniform(0.3, 0.7)\n                CR = np.random.uniform(0.6, 1.0) if stddev_fitness > 0.1 * mean_fitness else np.random.uniform(0.4, 0.7)\n                \n                # Mutation with dual-strategy\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1]\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.6:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                elif np.random.rand() < 0.9:\n                    # Use second best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Introduce random reinitialization for enhanced exploration\n                    mutant = np.random.rand(self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 26, "feedback": "The algorithm AdvancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.77458 with standard deviation 0.00844.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7758918123591843, 0.7636513048929869, 0.7842114491377857]}}
{"id": "859e4001-13c1-414c-9331-6709114a1bb4", "fitness": 0.7590167512129179, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with an adaptive population size and adaptive crossover rate, incorporating a dynamic diversity preservation mechanism for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = initial_pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0) * (1 - eval_count / self.budget)\n            \n            pop_size = max(4, int(initial_pop_size * (1 - eval_count / self.budget)))\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 27, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.75902 with standard deviation 0.08283.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7809214609352415, 0.8477233874135792, 0.6484054052899328]}}
{"id": "069cd678-bdd2-4484-bd6d-c4dfd8f2bcb4", "fitness": 0.7570550432228051, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with adaptive crossover rate based on population diversity for improved exploration.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0) * (1 - np.std(fitness) / np.mean(fitness)) # Change: adaptive CR based on diversity\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 28, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.75706 with standard deviation 0.05665.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.8253051862858081, 0.7592731669420444, 0.6865867764405629]}}
{"id": "199c2b9d-e415-4c71-aa03-8af23f241392", "fitness": 0.7116928490611724, "name": "EnhancedDynamicDifferentialEvolution", "description": "Enhanced Dynamic Differential Evolution with adaptive control parameters and fitness-weighted mutation to improve convergence speed and solution diversity.", "code": "import numpy as np\n\nclass EnhancedDynamicDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptive control parameters based on diversity and convergence rate\n            F = 0.5 + (0.5 * np.std(fitness) / np.mean(fitness))\n            CR = 0.5 + (0.5 * np.std(fitness) / np.mean(fitness))\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with fitness-weighted strategy\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                indices = np.random.choice(pop_size, 4, replace=False)\n                x0, x1, x2, x3 = population[indices]\n                \n                # Use fitness to weight the mutation strategy\n                fitness_weight = fitness[indices] / np.sum(fitness[indices])\n                mutant = np.clip(\n                    x0 + F * (fitness_weight[0] * (x_best - x0) + fitness_weight[1] * (x1 - x2) + fitness_weight[2] * (x2 - x3)),\n                    bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 29, "feedback": "The algorithm EnhancedDynamicDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.71169 with standard deviation 0.07467.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.6121977817110944, 0.7307864547777946, 0.7920943106946285]}}
{"id": "985b11cb-67ac-4c45-8610-fc371aab0f81", "fitness": 0.7184557820328837, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Improved Enhanced Adaptive Differential Evolution with adaptive learning rate and targeted local search for faster convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 12 * self.dim  # Increased population size for diversity\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.3, 0.9)  # Widened range for F\n            CR = np.random.uniform(0.6, 1.0)  # Increased lower bound of CR\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Targeted local search for further refinement\n                trial = np.clip(trial + np.random.normal(0, 0.1, self.dim), bounds[:, 0], bounds[:, 1])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 30, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.71846 with standard deviation 0.22204.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.4063352074685258, 0.8447252269598796, 0.9043069116702458]}}
{"id": "62022b3b-05aa-4df5-a5e1-1643aec527d2", "fitness": 0.7038065223948706, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with adaptive population size and archive-based mutation for increased exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Archive initialization\n        archive = np.empty((0, self.dim))\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            # Dynamic population adaptation\n            if eval_count % (self.budget // 10) == 0:\n                pop_size = max(4, int(pop_size * 0.9))\n                population = population[:pop_size]\n                fitness = fitness[:pop_size]\n\n            new_population = []\n            new_fitness = []\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1])\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                \n                if np.random.rand() < 0.7:\n                    mutant = x0 + F * (x_best - x0) + F * (x1 - x2)\n                elif np.random.rand() < 0.85 and len(archive) > 0:\n                    rand_idx = np.random.choice(len(archive))\n                    x_archive = archive[rand_idx]\n                    mutant = x0 + F * (x_archive - x0) + F * (x1 - x2)\n                else:\n                    mutant = x0 + F * (x_second_best - x0) + F * (x1 - x2)\n                \n                mutant = np.clip(mutant, bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    new_population.append(trial)\n                    new_fitness.append(f_trial)\n                    archive = np.vstack([archive, population[i]])\n                else:\n                    new_population.append(population[i])\n                    new_fitness.append(fitness[i])\n                \n            population = np.array(new_population)\n            fitness = np.array(new_fitness)\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 31, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.70381 with standard deviation 0.09080.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7838576806433779, 0.5768244136603464, 0.7507374728808874]}}
{"id": "4691f027-de2a-4a9f-9ffb-4235c3736ce4", "fitness": 0.8183943629289775, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with dynamic population sizing and adaptive parameter control for improved convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop with dynamic population sizing\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            # Dynamic population resizing based on convergence speed\n            if eval_count > 0.5 * self.budget:\n                pop_size = max(5 * self.dim, int(pop_size * 0.5))\n                population = population[np.argsort(fitness)[:pop_size]]\n                fitness = fitness[np.argsort(fitness)[:pop_size]]\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.81839 with standard deviation 0.03205.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7939144291084111, 0.7976034708199061, 0.8636651888586153]}}
{"id": "847a3c20-abd4-42e5-9808-f39f71c25cb0", "fitness": 0.6681876283326256, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with dynamic population size adjustment for better exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            # Dynamically adjust population size\n            current_best = np.min(fitness)\n            if eval_count % (self.budget // 10) == 0:  # Change line\n                pop_size = int(pop_size * (0.9 if current_best >= np.min(fitness) else 1.1))  # Change line\n                pop_size = max(4, min(10 * self.dim, pop_size))  # Change line\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) \n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 33, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.66819 with standard deviation 0.17127.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.4269155569331472, 0.8073371524599204, 0.7703101756048094]}}
{"id": "835d57a9-edc8-4187-bfe4-672b9479f5dc", "fitness": 0.8183943629289775, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with cyclic restart strategy for maintaining diversity and improving convergence.  ", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop with cyclic restart every quarter budget\n        cycle_budget = self.budget // 4\n        while eval_count < self.budget:\n            if eval_count % cycle_budget == 0:  # Check if it's time for restart\n                # Reset half of the population\n                indices = np.random.choice(pop_size, pop_size // 2, replace=False)\n                for i in indices:\n                    population[i] = bounds[:, 0] + np.random.rand(self.dim) * (bounds[:, 1] - bounds[:, 0])\n                    fitness[i] = func(population[i])\n                eval_count += len(indices)\n\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 34, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.81839 with standard deviation 0.03205.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7939144291084111, 0.7976034708199061, 0.8636651888586153]}}
{"id": "da98947e-88e2-44c3-bcef-9fa1816b0991", "fitness": 0.7175300730340467, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Introducing Smart Learning in Enhanced Adaptive Differential Evolution, leveraging successful trial vectors to guide future mutation strategies.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        successful_trials = []  # New line: Store successful trial vectors\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1])\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                if successful_trials and np.random.rand() < 0.3:  # New line: Use successful trial for mutation\n                    successful_index = np.random.randint(len(successful_trials))\n                    successful_trial = successful_trials[successful_index]\n                    mutant = np.clip(mutant + F * (successful_trial - mutant), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    successful_trials.append(trial)  # New line: Record successful trial vector\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 35, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.71753 with standard deviation 0.11602.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7673555035536586, 0.8279957907195183, 0.5572389248289631]}}
{"id": "913b7a52-bae2-4729-a05f-61bfd8d986f2", "fitness": 0.7753792874081826, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with dynamic weighting factors and diversity enhancement through archival of inferior solutions for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        \n        # Initialize archive for diversity\n        archive = []\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters with dynamic strategy\n            F = 0.5 + np.random.rand() * 0.5  # Dynamic weighting factor\n            CR = 0.5 + np.random.rand() * 0.5  # Dynamic crossover probability\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n                \n                # Mutation strategy using best and archived solutions\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                if archive:\n                    archive_choice = archive[np.random.randint(len(archive))]\n                    mutation_factor = np.random.rand()\n                    mutant = np.clip(population[i] + mutation_factor * (x_best - archive_choice), bounds[:, 0], bounds[:, 1])\n                else:\n                    indices = np.random.choice(pop_size, 3, replace=False)\n                    x0, x1, x2 = population[indices]\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                else:\n                    # Archive inferior solutions for potential future use\n                    if len(archive) < pop_size:\n                        archive.append(population[i])\n                    else:\n                        # Randomly replace an archived solution if the archive is full\n                        archive[np.random.randint(pop_size)] = population[i]\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 36, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.77538 with standard deviation 0.10947.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.6207309546660109, 0.8465971834502715, 0.8588097241082653]}}
{"id": "2cda9511-7369-4c98-b7bf-996b0222b7a8", "fitness": 0.7350176777131066, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with dynamic scaling factors and self-adaptive crossover rates, emphasizing exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Dynamic adaptation of scale factor and crossover rate\n            F = np.random.uniform(0.5, 0.9)\n            CR = np.random.uniform(0.3, 0.9)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation strategy with emphasis on exploration and exploitation\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1])\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Self-adaptive crossover mechanism\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 37, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.73502 with standard deviation 0.02684.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7726011220946671, 0.7208323590481177, 0.7116195519965349]}}
{"id": "eb76fcb8-33f1-4b1c-907e-7644361937ba", "fitness": -Infinity, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with dynamic population resizing and individual learning to improve convergence rates and diversity maintenance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n\n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = initial_pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Dynamic population resizing\n            pop_size = self._dynamic_resize(eval_count, self.budget, initial_pop_size)\n            \n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1])\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                \n                if np.random.rand() < 0.7:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Individual learning enhancement\n                trial = self._individual_learning(trial, bounds)\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        best_index = np.argmin(fitness)\n        return population[best_index]\n\n    def _dynamic_resize(self, eval_count, budget, initial_pop_size):\n        # Dynamic resizing strategy\n        return int(initial_pop_size * (1 - eval_count / budget))\n\n    def _individual_learning(self, trial, bounds):\n        # Individual learning strategy to refine solutions\n        new_trial = trial + np.random.normal(0, 0.01, size=self.dim)\n        return np.clip(new_trial, bounds[:, 0], bounds[:, 1])", "configspace": "", "generation": 38, "feedback": "An exception occurred: ValueError(\"Cannot take a larger sample than population when 'replace=False'\").", "error": "ValueError(\"Cannot take a larger sample than population when 'replace=False'\")", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {}}
{"id": "f5ee4692-05ac-45b8-9ae0-79c9494c7543", "fitness": 0.7313010344940709, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Refined Enhanced Adaptive Differential Evolution with dynamic crossover rate and population resampling technique for improved exploration.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.4, 0.9)  # Adjusted crossover rate range\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                else:\n                    # Resample population to increase exploration\n                    population[i] = bounds[:, 0] + np.random.rand(self.dim) * (bounds[:, 1] - bounds[:, 0]) \n                    fitness[i] = func(population[i])\n                    eval_count += 1\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 39, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.73130 with standard deviation 0.04970.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.6774274631820796, 0.7191483745780511, 0.7973272657220822]}}
{"id": "e4510f18-5ba0-4709-b09a-7bb6601e9a49", "fitness": 0.5705462682524444, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with dynamic crossover control and elitism to improve convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1])\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Dynamic Crossover Control\n                if fitness[i] < np.median(fitness):\n                    CR = 0.9  # Increase crossover rate for better solutions\n                else:\n                    CR = 0.5  # Decrease crossover rate for worse solutions\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection with elitism\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                elif f_trial < fitness[best_index]: # Elitism\n                    population[best_index] = trial\n                    fitness[best_index] = f_trial\n\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.57055 with standard deviation 0.06946.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.6332686230360582, 0.6046568830598015, 0.4737132986614736]}}
{"id": "10b71f09-ab54-4eed-973f-89823c9c836e", "fitness": 0.7945412948099281, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Introduce a dynamic adaptation mechanism for the mutation factor F to enhance convergence and diversity balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9) * (1 - (eval_count / self.budget)) # Line changed for dynamic adaptation\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 41, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.79454 with standard deviation 0.05354.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7188872821189484, 0.8296979535122526, 0.8350386487985832]}}
{"id": "449498d7-82de-488d-b24e-1c6eb7654479", "fitness": 0.8177370013549566, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with improved adaptive crossover strategy for increased exploration.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0 + np.max(fitness)/np.sum(fitness)) # Updated line for adaptive crossover\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.81774 with standard deviation 0.03109.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.793950652608527, 0.7976034708199061, 0.8616568806364369]}}
{"id": "4e237e73-830e-49a7-98e6-ad6e9916f216", "fitness": 0.09015696749626352, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with dynamic crossover and mutation, integrating population entropy to guide exploitation and exploration balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Calculate population diversity (entropy)\n            norm_pop = (population - bounds[:, 0]) / (bounds[:, 1] - bounds[:, 0])\n            entropy = -np.sum(norm_pop * np.log(np.clip(norm_pop, 1e-8, 1)), axis=1).mean()\n\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9) * (1 + entropy)  # dynamic scaling factor\n            CR = np.random.uniform(0.5, 1.0) * (1 - entropy) # dynamic crossover rate\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 43, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09016 with standard deviation 0.01622.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.11058305802609647, 0.08898250584489387, 0.07090533861780024]}}
{"id": "db77479a-56d3-415e-afda-54ed964b87f3", "fitness": 0.7312079944909083, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with dynamic crossover probability based on generation progress.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5 * (self.budget - eval_count) / self.budget, 1.0)  # Dynamic crossover probability\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.73121 with standard deviation 0.05513.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7333963292638233, 0.7976034708199061, 0.6626241833889959]}}
{"id": "d17c3518-c738-409e-874b-cdb07cc5a149", "fitness": 0.5924709266688275, "name": "EnhancedDifferentialEvolutionWithMemory", "description": "Enhanced Differential Evolution with Adaptive Memory and Dynamic Population for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedDifferentialEvolutionWithMemory:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        memory_F = [0.5]\n        memory_CR = [0.9]\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters with memory\n            F = np.clip(np.random.normal(memory_F[-1], 0.1), 0.1, 1.0)\n            CR = np.clip(np.random.normal(memory_CR[-1], 0.1), 0.1, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with adaptive memory strategy\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    memory_F.append(F)\n                    memory_CR.append(CR)\n                    if len(memory_F) > 10:\n                        memory_F.pop(0)\n                        memory_CR.pop(0)\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 45, "feedback": "The algorithm EnhancedDifferentialEvolutionWithMemory got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.59247 with standard deviation 0.28549.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7980349803607348, 0.7906217252732379, 0.18875607437250974]}}
{"id": "15315865-683a-48fc-aa9e-56b854bbac42", "fitness": 0.848469570561493, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Improved Enhanced Adaptive Differential Evolution using dynamic crossover rates and a greedy local search for enhanced convergence.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Attempt slight perturbation for fine-tuning\n                    local_perturb = np.random.normal(0, 0.01, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 46, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.84847 with standard deviation 0.01101.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.8418159122806186, 0.8639816354733921, 0.8396111639304684]}}
{"id": "363fe453-8039-4009-99d3-36f3a5b48567", "fitness": 0.7929346929327671, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with improved local perturbation and adaptive population resizing for better exploration and exploitation balance.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Attempt slight perturbation for fine-tuning\n                    local_perturb = np.random.normal(0, 0.005, self.dim)  # Reduced standard deviation for finer tuning\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n            # Adaptive Population Resizing\n            if eval_count % (self.budget // 10) == 0:  # Resize population periodically\n                pop_size = max(5, int(0.9 * pop_size))\n                population = population[:pop_size]\n                fitness = fitness[:pop_size]\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 47, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.79293 with standard deviation 0.05515.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.8418159122806186, 0.8211319212957359, 0.7158562452219468]}}
{"id": "dd6ab23d-92d2-4429-a883-678f7e6a70da", "fitness": 0.7278141949123601, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with Adaptive Population Size and Dynamic Scaling Factor for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = initial_pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adjust population size based on remaining budget\n            pop_size = max(4, int(initial_pop_size * (1 - eval_count / self.budget)))\n\n            # Adaptation of parameters\n            F = np.random.uniform(0.5, 1.0 - eval_count / self.budget)  # Decrease F dynamically\n            CR = np.random.uniform(0.3, 0.9)\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Attempt slight perturbation for fine-tuning\n                    local_perturb = np.random.normal(0, 0.01, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n        \n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 48, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.72781 with standard deviation 0.11946.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.7748057701363861, 0.8448520023832027, 0.5637848122174918]}}
{"id": "d0d054db-0d46-4c3f-bf5f-ff14dcfc8792", "fitness": 0.7559047712933973, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution using elite group strategy and adaptive mutation scaling for improved diversity and convergence.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)\n\n            # Identify elite group\n            elite_indices = fitness.argsort()[:5]\n            elite_population = population[elite_indices]\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Dynamic mutation strategy\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                \n                if np.random.rand() < 0.7:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use elite group for enhanced diversity\n                    elite_choice = elite_population[np.random.randint(0, 5)]\n                    mutant = np.clip(x0 + F * (elite_choice - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    local_perturb = np.random.normal(0, 0.01, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 49, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.75590 with standard deviation 0.07977.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.8175503173134484, 0.6432586000837965, 0.8069053964829469]}}
{"id": "0d3eaa2d-4b0d-4bef-afc3-bd473e8e342a", "fitness": 0.7116601962963737, "name": "EnhancedDifferentialEvolutionWithElitePreservation", "description": "Enhanced Differential Evolution with Adaptive Mutation and Crossover, integrating an elite preservation strategy for improved convergence and solution quality.", "code": "import numpy as np\n\nclass EnhancedDifferentialEvolutionWithElitePreservation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        elite = None\n        elite_fitness = np.inf\n\n        while eval_count < self.budget:\n            F = np.random.uniform(0.5, 0.9)\n            CR = np.random.uniform(0.2, 0.8)\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n\n                if np.random.rand() < 0.6:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(elite + F * (x1 - x2), bounds[:, 0], bounds[:, 1]) if elite is not None else x0\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                if f_trial < elite_fitness:\n                    elite = trial\n                    elite_fitness = f_trial\n\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 50, "feedback": "The algorithm EnhancedDifferentialEvolutionWithElitePreservation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.71166 with standard deviation 0.12301.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.6553162906879839, 0.88236880393365, 0.5972954942674871]}}
{"id": "edc7304e-e93d-42c8-9823-5689885d26c9", "fitness": 0.7268632815559745, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with a refined local search strategy for improved exploitation.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement with slightly increased perturbation\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Attempt slight perturbation for fine-tuning\n                    local_perturb = np.random.normal(0, 0.02, self.dim)  # Slightly increased perturbation\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 51, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.72686 with standard deviation 0.06656.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.6631658234664486, 0.818731852767933, 0.6986921684335419]}}
{"id": "24071bea-f2a5-438c-b4dc-a7cba03243be", "fitness": -Infinity, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Improved Enhanced Adaptive Differential Evolution with adaptive population size and self-adaptive control parameters for efficient convergence.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)\n            \n            # Adaptive Population Size Reduction\n            reduced_size = max(4, int(pop_size * (1 - eval_count / self.budget)))\n            indices_to_select = np.argpartition(fitness, reduced_size)[:reduced_size]\n            population, fitness = population[indices_to_select], fitness[indices_to_select]\n            pop_size = len(population)\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1]\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Attempt slight perturbation for fine-tuning\n                    local_perturb = np.random.normal(0, 0.01, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 52, "feedback": "An exception occurred: ValueError('kth(=4) out of bounds (4)').", "error": "ValueError('kth(=4) out of bounds (4)')", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {}}
{"id": "2a47bc3e-49fa-4936-9542-11a47ac0194d", "fitness": 0.6569164787383169, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Refined Improved Enhanced Adaptive Differential Evolution by introducing adaptive scaling factor and dynamic mutation strategy to enhance convergence speed and solution quality.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = 0.5 + 0.3 * np.random.rand()  # Changed for adaptive scaling factor\n            CR = np.random.uniform(0.3, 0.9)\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1]\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.5:  # Changed for dynamic mutation probability\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Attempt slight perturbation for fine-tuning\n                    local_perturb = np.random.normal(0, 0.01, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 53, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.65692 with standard deviation 0.09006.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.7590770487871269, 0.5399732396105139, 0.67169914781731]}}
{"id": "75bf590d-fc52-4b17-af16-d1956b4459b7", "fitness": 0.7018598232933942, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with dynamic recruitment and redundant exploration for improved robustness and faster convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9) \n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic recruitment\n                best_index = np.argmin(fitness)\n                elite_indices = fitness.argsort()[:3] # Use top 3 for enhancing exploration\n                x_best = population[best_index]\n                indices = np.random.choice(pop_size, 3, replace=False)\n\n                # Select dynamic strategy for diversity\n                if np.random.rand() < 0.5:\n                    x0, x1, x2 = population[elite_indices]\n                else:\n                    x0, x1, x2 = population[indices]\n\n                # Mutation and crossover with redundant exploration\n                mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Attempt slight perturbation for fine-tuning\n                    local_perturb = np.random.normal(0, 0.01, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 54, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.70186 with standard deviation 0.11875.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.6578236673707678, 0.5835275596857815, 0.8642282428236334]}}
{"id": "f14d88c9-d92b-4a75-a99a-ac27aea917ef", "fitness": 0.848469570561493, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhanced convergence with random elitism selection for improved diversity.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Attempt slight perturbation for fine-tuning\n                    local_perturb = np.random.normal(0, 0.01, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution with random elitism\n        best_index = np.random.choice(np.where(fitness == np.min(fitness))[0])\n        return population[best_index]", "configspace": "", "generation": 55, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.84847 with standard deviation 0.01101.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.8418159122806186, 0.8639816354733921, 0.8396111639304684]}}
{"id": "f5f10773-f7c1-4c86-82f9-6b2097213d83", "fitness": 0.7957239656502147, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Improved Enhanced Adaptive Differential Evolution with adaptive local search perturbation based on fitness variance for enhanced robustness.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Attempt slight perturbation for fine-tuning\n                    local_perturb_scale = np.std(fitness) * 0.01  # Adaptive perturbation based on fitness variance\n                    local_perturb = np.random.normal(0, local_perturb_scale, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 56, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.79572 with standard deviation 0.05670.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.8418159122806186, 0.8294997394480789, 0.7158562452219468]}}
{"id": "23118970-b12a-4a10-9b8e-5ffd0f6bde18", "fitness": 0.7092055224569404, "name": "AdvancedDifferentialEvolutionWithAdaptiveLearning", "description": "Advanced Differential Evolution with Adaptive Learning, incorporating a self-adaptive crossover and mutation strategy based on individual performance to enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass AdvancedDifferentialEvolutionWithAdaptiveLearning:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Initialize fitness and strategy adaptation parameters\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        F_memory = np.full(pop_size, 0.5)  # Memory of mutation factors\n        CR_memory = np.full(pop_size, 0.5) # Memory of crossover rates\n\n        # Optimization loop\n        while eval_count < self.budget:\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Dynamic parameter adaptation\n                F = np.clip(F_memory[i] + 0.1 * np.random.standard_normal(), 0.4, 0.9)\n                CR = np.clip(CR_memory[i] + 0.1 * np.random.standard_normal(), 0.3, 0.9)\n                \n                # Mutation strategy\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                \n                indices = np.random.choice([idx for idx in range(pop_size) if idx != i], 3, replace=False)\n                x0, x1, x2 = population[indices]\n                mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    F_memory[i] = F  # Update memory with successful mutation factor\n                    CR_memory[i] = CR # Update memory with successful crossover rate\n\n                    # Adaptive local search improvement\n                    local_perturb = np.random.normal(0, 0.01, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 57, "feedback": "The algorithm AdvancedDifferentialEvolutionWithAdaptiveLearning got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.70921 with standard deviation 0.04180.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.6707972505007217, 0.7673223197797611, 0.6894969970903382]}}
{"id": "a9be821c-28b2-4767-b4f8-89db309aece2", "fitness": 0.42761855825839984, "name": "AdaptiveDifferentialEvolutionWithLocalSearch", "description": "Adaptive Differential Evolution with enhanced local search uses hierarchical mutation strategies and adaptive parameter tuning for improved convergence.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionWithLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptive parameter tuning based on evaluation budget utilization\n            F = np.random.uniform(0.5, 0.9 - 0.4 * (eval_count / self.budget))\n            CR = np.random.uniform(0.4, 0.9 - 0.5 * (eval_count / self.budget))\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Hierarchical mutation strategy\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                \n                if np.random.rand() < 0.5:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Randomized mutation for diversity\n                    mutant = np.clip(x0 + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Enhanced local search\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    for _ in range(3):  # Conduct local search with multiple perturbations\n                        local_perturb = np.random.normal(0, 0.01, self.dim)\n                        perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                        f_perturbed_trial = func(perturbed_trial)\n                        eval_count += 1\n                        if f_perturbed_trial < fitness[i]:\n                            population[i] = perturbed_trial\n                            fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 58, "feedback": "The algorithm AdaptiveDifferentialEvolutionWithLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42762 with standard deviation 0.10615.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.3077666707812481, 0.4092595754996986, 0.5658294284942529]}}
{"id": "1b18aa7c-9907-40cf-a37e-472a93992610", "fitness": 0.4710379264797489, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhanced crossover strategy by introducing a dynamic probability threshold for crossover based on current generation performance.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < (CR * (1 - (eval_count / self.budget))) # Introduce dynamic threshold\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Attempt slight perturbation for fine-tuning\n                    local_perturb = np.random.normal(0, 0.01, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 59, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.47104 with standard deviation 0.25334.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.43694142047347273, 0.17922072751087992, 0.7969516314548941]}}
{"id": "752b7e98-d489-431d-afd7-338447f631ba", "fitness": 0.6173483952110641, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhanced mutation strategy and refined local search for optimized convergence.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Attempt slight perturbation for fine-tuning\n                    local_perturb = np.random.normal(0, 0.01, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb * np.random.uniform(0.5, 1.5), bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 60, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.61735 with standard deviation 0.18139.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.41865848493160807, 0.8572037415632373, 0.5761829591383469]}}
{"id": "3f5348d8-9e17-48cf-b75e-9c75feffc04e", "fitness": 0.5207078524470928, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Refined Enhanced Adaptive DE with Improved Mutation Strategy using Dual Mutation Vectors for Increased Diversity.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 5, replace=False)  # Changed to 5 for dual mutation vectors\n                x0, x1, x2, x3, x4 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2) + F * (x3 - x4), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2) + F * (x3 - x4), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Attempt slight perturbation for fine-tuning\n                    local_perturb = np.random.normal(0, 0.01, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 61, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.52071 with standard deviation 0.16961.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.4381511531854597, 0.7570240976779667, 0.36694830647785226]}}
{"id": "a10baf0b-e7d5-4049-991a-95e09396b401", "fitness": 0.848469570561493, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhanced convergence by introducing feedback-based parameter adaptation and improving local search.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Attempt slight perturbation for fine-tuning\n                    local_perturb = np.random.normal(0, 0.01, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n            # Adjust CR based on success rate of local search\n            successful_attempts = sum(fitness < f_trial)\n            CR = CR + 0.1 * (successful_attempts / pop_size - 0.2)  # Feedback-based adaptation\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 62, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.84847 with standard deviation 0.01101.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.8418159122806186, 0.8639816354733921, 0.8396111639304684]}}
{"id": "48a3788f-0fec-44cf-9798-59ed48a9bdec", "fitness": 0.5407963612179798, "name": "OptimizedEnhancedAdaptiveDifferentialEvolution", "description": "Optimized Enhanced Adaptive Differential Evolution with adaptive population resizing and multi-phase mutation strategies to improve convergence speed and solution quality.", "code": "import numpy as np\n\nclass OptimizedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = initial_pop_size\n        \n        # Adaptive population resizing parameters\n        max_pop_size = 15 * self.dim\n        min_pop_size = 5 * self.dim\n        resize_threshold = 0.1 * self.budget\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Dynamic control of crossover rate\n\n            # Resize population based on progress\n            if eval_count % resize_threshold == 0 and eval_count < self.budget:\n                # Simple heuristic: halve the population size if progress stalls\n                current_pop_size = len(population)\n                if current_pop_size > min_pop_size:\n                    population = population[:current_pop_size // 2]\n                    fitness = fitness[:current_pop_size // 2]\n\n            for i in range(len(population)):\n                if eval_count >= self.budget:\n                    break\n\n                # Multi-phase mutation strategy\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1]\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(len(population), 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    local_perturb = np.random.normal(0, 0.01, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 63, "feedback": "The algorithm OptimizedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.54080 with standard deviation 0.28910.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.5896410326590036, 0.8679109251530069, 0.1648371258419289]}}
{"id": "9f583cd4-f2f6-4b89-afb5-4ce2ff4fca2a", "fitness": 0.6576253232324515, "name": "EnhancedDifferentialEvolution", "description": "Enhanced Differential Evolution with Diversity Preservation and Adaptive Mutation for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        while eval_count < self.budget:\n            F = np.random.uniform(0.5, 1.0)\n            CR = np.random.uniform(0.3, 0.9)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n\n                # Adaptive mutation strategy based on diversity\n                diversity = np.mean(np.std(population, axis=0))\n                F_adaptive = F * (1 + 0.1 * (1 - diversity))\n                mutant = np.clip(x0 + F_adaptive * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    local_perturb = np.random.normal(0, 0.01, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 64, "feedback": "The algorithm EnhancedDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.65763 with standard deviation 0.15722.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.6428527970239919, 0.47288659141548495, 0.8571365812578777]}}
{"id": "5f3fbd7c-737d-442e-93eb-0e50e39a607f", "fitness": 0.6836208672784087, "name": "EnhancedHybridDifferentialEvolution", "description": "Enhanced Hybrid Differential Evolution incorporating adaptive selection and crossover strategies with local search for improved convergence and diversity.", "code": "import numpy as np\n\nclass EnhancedHybridDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptive parameters\n            F = np.random.uniform(0.5, 0.9)\n            CR = np.random.uniform(0.4, 0.9)\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation strategy using adaptive selection\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.5:\n                    # Hybrid mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Adaptive crossover strategy\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Local search improvement\n                    local_perturb = np.random.normal(0, 0.01, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 65, "feedback": "The algorithm EnhancedHybridDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.68362 with standard deviation 0.06643.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.5953569899890119, 0.7556039344641292, 0.699901677382085]}}
{"id": "b314c928-11b7-4630-b30d-e0b518b9b8ae", "fitness": 0.6176990337290243, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhanced strategy for mutation and crossover with adaptive local search for more robust convergence.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.5, 0.9)  # Adjusted to encourage exploration\n            CR = np.random.uniform(0.3, 0.9)\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1]\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Attempt slight perturbation for fine-tuning\n                    local_perturb = np.random.normal(0, 0.02, self.dim)  # Increased perturbation for exploration\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 66, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.61770 with standard deviation 0.33342.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.14912809511196712, 0.8976346634443332, 0.8063343426307727]}}
{"id": "8639f019-fb92-4dd4-b19e-36420ef22f10", "fitness": 0.612145174861088, "name": "HybridOppositionDifferentialEvolution", "description": "Hybridized Differential Evolution with Elite Opposition-based Learning and Adaptive Local Search for improved convergence and diversity.", "code": "import numpy as np\n\nclass HybridOppositionDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Initial fitness evaluation\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Parameter adaptation\n            F = np.random.uniform(0.5, 0.9)  # Slightly higher mutation factor for exploration\n            CR = np.random.uniform(0.3, 0.9)\n\n            # Elite opposition-based learning\n            best_index = np.argmin(fitness)\n            elite = population[best_index]\n\n            opposite_population = bounds[:, 0] + bounds[:, 1] - population\n            opposite_fitness = np.array([func(ind) for ind in opposite_population])\n            eval_count += pop_size\n            combined_population = np.vstack((population, opposite_population))\n            combined_fitness = np.hstack((fitness, opposite_fitness))\n\n            sorted_indices = np.argsort(combined_fitness)\n            population = combined_population[sorted_indices[:pop_size]]\n            fitness = combined_fitness[sorted_indices[:pop_size]]\n\n            # Differential Evolution with adaptive local search\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                mutant = np.clip(x0 + F * (elite - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                # Adaptive local search\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    local_perturb = np.random.normal(0, 0.01, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 67, "feedback": "The algorithm HybridOppositionDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.61215 with standard deviation 0.15681.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.7984431827503302, 0.41481001246479166, 0.6231823293681422]}}
{"id": "f9613f6b-b479-41f7-9ac6-9a5f6b3f3796", "fitness": 0.848470134604313, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhanced exploration by adjusting the perturbation scale based on convergence speed.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust perturbation scale based on budget consumption\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 68, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.84847 with standard deviation 0.01101.", "error": "", "parent_ids": ["15315865-683a-48fc-aa9e-56b854bbac42"], "operator": null, "metadata": {"aucs": [0.8418159122806186, 0.863983327601852, 0.8396111639304684]}}
{"id": "8afe47d3-28c4-49cc-8e12-3c92de23a68a", "fitness": 0.36337014235257853, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhanced diversity by introducing an adaptive mutation strategy based on population diversity.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n                \n                # New mutation strategy based on a diversity measure\n                diversity = np.std(population, axis=0).mean()\n                adaptive_F = F * np.tanh(diversity)\n\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                mutant = np.clip(x0 + adaptive_F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 69, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.36337 with standard deviation 0.16248.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.5645026582286944, 0.16658264086053765, 0.3590251279685036]}}
{"id": "fbfff9ad-728a-43f5-8c46-4f5211e783fb", "fitness": 0.5968145295269517, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Introduce neighborhood learning and adaptive mutation scaling based on population diversity to enhance exploration and convergence.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Helper function to calculate diversity\n        def population_diversity():\n            return np.std(population, axis=0).mean()\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9) * (1 - eval_count / self.budget)\n            CR = np.random.uniform(0.3, 0.9)\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on diversity\n                diversity_factor = population_diversity()\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                mutant = np.clip(x0 + F * diversity_factor * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement with neighborhood learning\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust perturbation scale based on neighborhood improvement\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 70, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.59681 with standard deviation 0.10176.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.45502663676402, 0.6890391526198039, 0.6463777991970312]}}
{"id": "c50ad325-f67f-4cbe-b680-505ee6394581", "fitness": 0.49192865242157596, "name": "RefinedAdaptiveDifferentialEvolution", "description": "Adaptive mutation strategy with self-adjusting control parameters to enhance diversity and convergence speed.", "code": "import numpy as np\n\nclass RefinedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Adaptive parameters\n        F_max, F_min = 0.9, 0.4\n        CR_max, CR_min = 0.9, 0.3\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters based on convergence\n            F = F_min + (F_max - F_min) * (1 - eval_count / self.budget)\n            CR = CR_min + (CR_max - CR_min) * np.random.rand()\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with adaptive strategy\n                best_index = np.argmin(fitness)\n                worst_index = np.argmax(fitness)\n                x_best = population[best_index]\n\n                indices = np.random.choice([idx for idx in range(pop_size) if idx != i], 3, replace=False)\n                x0, x1, x2 = population[indices]\n\n                if np.random.rand() < 0.5:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use random mix for diversity\n                    mutant = np.clip(x0 + F * (population[worst_index] - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust perturbation scale based on remaining budget\n                    local_perturb = np.random.normal(0, 0.01 * (eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 71, "feedback": "The algorithm RefinedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.49193 with standard deviation 0.23048.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.7500456362889875, 0.5352442959234867, 0.19049602505225383]}}
{"id": "59de9736-b407-418a-8d7d-1473dbd23304", "fitness": 0.7957239656502147, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhanced selection strategy using adaptive mutation scale based on fitness diversity.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust perturbation scale based on fitness diversity\n                    diversity_factor = np.std(fitness) / np.mean(fitness)  # Change here\n                    local_perturb = np.random.normal(0, 0.01 * diversity_factor, self.dim)  # Change here\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 72, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.79572 with standard deviation 0.05670.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.8418159122806186, 0.8294997394480789, 0.7158562452219468]}}
{"id": "28ae3db2-92fb-4acc-b0a4-5e237098e0dd", "fitness": 0.5745770287896962, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Refined strategy by introducing a probabilistic selection between best and average individuals for increased diversity.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on probabilistic choice\n                best_index = np.argmin(fitness)\n                avg_index = np.random.choice(np.arange(pop_size))\n                x_choice = population[best_index] if np.random.rand() < 0.5 else population[avg_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                mutant = np.clip(x0 + F * (x_choice - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 73, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.57458 with standard deviation 0.19239.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.30321765208047025, 0.7274780899996143, 0.6930353442890042]}}
{"id": "786e3e28-85f9-4003-b18d-2be9c092796a", "fitness": 0.5578455022972898, "name": "AdaptiveDynamicDifferentialEvolution", "description": "Adaptive Differential Evolution with Dynamic Parameter Adaptation and Fitness-Based Selection for Enhanced Exploration and Exploitation.", "code": "import numpy as np\n\nclass AdaptiveDynamicDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters with a dynamic approach\n            F = 0.4 + (0.9 - 0.4) * (1 - eval_count / self.budget)**2  # Quadratic decay\n            CR = 0.3 + (0.9 - 0.3) * np.random.rand()  # Stochastic variation\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with enhanced diversity approach\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1]\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                rand_selector = np.random.rand()\n                if rand_selector < 0.5:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover with self-adaptive mechanism\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search with adaptive step\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    local_perturb = np.random.normal(0, 0.02 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 74, "feedback": "The algorithm AdaptiveDynamicDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.55785 with standard deviation 0.12149.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.5406399627783356, 0.7144984273869389, 0.4183981167265951]}}
{"id": "c29c368b-70fe-4694-a3e0-f12988989e4a", "fitness": 0.6647114598405058, "name": "AdaptivePopulationDifferentialEvolution", "description": "Introduce an adaptive population size mechanism and fitness diversity measure to dynamically control exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptivePopulationDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        initial_pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(initial_pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = initial_pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)\n\n            # Dynamically adjust population size based on fitness diversity\n            if np.std(fitness) < 0.01:  # Small diversity, increase exploration\n                pop_size = min(initial_pop_size * 2, self.budget - eval_count)\n            else:  # High diversity, focus on exploitation\n                pop_size = initial_pop_size\n\n            # Evaluate using a subset of current population if necessary\n            current_indices = np.random.choice(len(population), size=pop_size, replace=False)\n            current_population = population[current_indices]\n            current_fitness = fitness[current_indices]\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(current_fitness)\n                x_best = current_population[best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = current_population[indices]\n                mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, current_population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < current_fitness[i]:\n                    current_population[i] = trial\n                    current_fitness[i] = f_trial\n                    # Adjust perturbation scale based on budget consumption\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        current_population[i] = perturbed_trial\n                        current_fitness[i] = f_perturbed_trial\n\n            # Update the original population with improved candidates\n            population[current_indices] = current_population\n            fitness[current_indices] = current_fitness\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 75, "feedback": "The algorithm AdaptivePopulationDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.66471 with standard deviation 0.09879.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.8023990191593948, 0.6164045404313857, 0.5753308199307371]}}
{"id": "43d3a8c7-15fc-478a-b47a-c8f9ccf88e70", "fitness": 0.848470134604313, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Introducing a diversity-preserving mechanism by periodically resetting part of the population to avoid premature convergence and enhance search space exploration.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        reset_rate = 0.1  # Rate at which the population will be reset\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1]\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n            \n            # Periodically reset a portion of the population for diversity\n            if eval_count % int(self.budget * reset_rate) == 0:\n                reset_indices = np.random.choice(pop_size, int(pop_size * reset_rate), replace=False)\n                for idx in reset_indices:\n                    population[idx] = np.random.rand(self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                    fitness[idx] = func(population[idx])\n                    eval_count += 1\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 76, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.84847 with standard deviation 0.01101.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.8418159122806186, 0.863983327601852, 0.8396111639304684]}}
{"id": "c24ef3cc-1fda-4f85-8fb1-3e57d250141b", "fitness": 0.7690655047695905, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Utilize a variable mutation scaling factor to enhance diversity and convergence by modulating the mutation strategy dynamically.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + np.random.uniform(0.5, 1.0) * F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust perturbation scale based on budget consumption\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 77, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.76907 with standard deviation 0.10127.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.9052375723606617, 0.7393940855719523, 0.6625648563761573]}}
{"id": "c0749a93-efd6-4399-94cd-c70143f44c5b", "fitness": 0.5376004515685247, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhanced exploration by dynamically adjusting the crossover rate based on current fitness spread.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9) * (1 - np.std(fitness) / np.mean(fitness))  # Dynamic CR based on fitness spread\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust perturbation scale based on budget consumption\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 78, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.53760 with standard deviation 0.26262.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.23471508072430547, 0.8751959081357051, 0.5028903658455635]}}
{"id": "edc0af6f-d147-4247-8c8b-213ec0a854c7", "fitness": 0.39316559290008996, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Improved exploration-exploitation balance by adaptive mutation strategy selection and fitness diversity maintenance.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with updated strategy based on fitness diversity\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1]\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.std(fitness) > 0.01:  # Use fitness diversity to choose strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 79, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.39317 with standard deviation 0.19583.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.6381881748501328, 0.3824513280545697, 0.15885727579556752]}}
{"id": "6cfd5f84-35b6-4cea-ba81-b6efeb6d9c6e", "fitness": 0.848470134604313, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Incorporate adaptive mutation scaling based on fitness variance to further enhance exploration.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n\n                fitness_variance = np.var(fitness)  # Adapted line: Calculate fitness variance for adaptive mutation scaling\n                \n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust perturbation scale based on budget consumption\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 80, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.84847 with standard deviation 0.01101.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.8418159122806186, 0.863983327601852, 0.8396111639304684]}}
{"id": "a563e92e-1b6e-47da-a23e-0f0c14e5f4b7", "fitness": 0.6845511628055151, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhanced exploration by introducing a secondary mutation strategy for increased population diversity.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust perturbation scale based on budget consumption\n                    local_perturb = np.random.normal(0, 0.02 * (1 - eval_count / self.budget), self.dim)  # Altered perturbation scale\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 81, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.68455 with standard deviation 0.11260.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.8014115002342, 0.7198057229182415, 0.5324362652641038]}}
{"id": "779ea42e-20d3-43fb-9338-b34dfd4eaa18", "fitness": 0.6692591134466879, "name": "ImprovedEnhancedAdaptiveDifferentialEvolutionV2", "description": "Adaptive perturbation scale dynamically shifts focus between exploration and exploitation using historical population diversity.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolutionV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Historical diversity tracking\n        historical_diversity = np.std(population, axis=0)\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on population diversity\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Prioritize mutation with diversity when historical diversity is low\n                    if np.mean(historical_diversity) < 0.1:\n                        mutant = np.clip(x0 + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                    else:\n                        mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Random target for mutation\n                    mutant = np.clip(x0 + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Local search improvement and diversity tracking\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Improve perturbation strategy based on diversity\n                    local_perturb = np.random.normal(0, 0.01 * np.mean(historical_diversity), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n            # Update historical diversity\n            historical_diversity = np.std(population, axis=0)\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 82, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolutionV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.66926 with standard deviation 0.07448.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.5686539357359743, 0.7465862245741133, 0.6925371800299762]}}
{"id": "bcec5b41-6caf-412f-a7b7-5c833338787c", "fitness": 0.49620480819778967, "name": "SelfAdaptiveDifferentialEvolution", "description": "Improved convergence by incorporating self-adaptive control of mutation and crossover rates with local search reinforcement.", "code": "import numpy as np\n\nclass SelfAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n\n        # Initialize population and strategy parameters\n        population = np.random.rand(pop_size, self.dim)\n        mutation_rates = np.random.uniform(0.4, 0.9, pop_size)\n        crossover_rates = np.random.uniform(0.3, 0.9, pop_size)\n        \n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n\n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Select mutation and crossover rates\n                F = mutation_rates[i]\n                CR = crossover_rates[i]\n\n                # Mutation strategy\n                indices = np.random.choice(pop_size, 5, replace=False)\n                x0, x1, x2, x3, x4 = population[indices]\n                # Dynamic mutation strategy selection\n                if np.random.rand() < 0.7:\n                    mutant = np.clip(x0 + F * (x1 - x2) + F * (x3 - x4), bounds[:, 0], bounds[:, 1])\n                else:\n                    best_index = np.argmin(fitness)\n                    x_best = population[best_index]\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    mutation_rates[i] = 0.9 * mutation_rates[i] + 0.1 * F  # Self-adaptive mutation rate\n                    crossover_rates[i] = 0.9 * crossover_rates[i] + 0.1 * CR  # Self-adaptive crossover rate\n\n                    # Local search reinforcement\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 83, "feedback": "The algorithm SelfAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.49620 with standard deviation 0.26368.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.44748368351791, 0.8407436829504394, 0.20038705812501945]}}
{"id": "f0ffb8c2-68b5-42e2-a4bc-72118abcd4e7", "fitness": 0.848470134604313, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Introduce a stochastic local search intensification step based on diversity to enhance convergence.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust perturbation scale based on budget consumption\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n                    # New intensification step\n                    if np.std(fitness) < 0.01:  # Simple diversity check\n                        random_index = np.random.choice(pop_size)\n                        perturb = np.random.normal(0, 0.1, self.dim)\n                        intensified_trial = np.clip(population[random_index] + perturb, bounds[:, 0], bounds[:, 1])\n                        f_intensified_trial = func(intensified_trial)\n                        eval_count += 1\n                        if f_intensified_trial < fitness[random_index]:\n                            population[random_index] = intensified_trial\n                            fitness[random_index] = f_intensified_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 84, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.84847 with standard deviation 0.01101.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.8418159122806186, 0.863983327601852, 0.8396111639304684]}}
{"id": "ee0c1ec9-60a1-4f38-a5f3-54b76e3806c7", "fitness": 0.5198231034114873, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhanced adaptive diversity by incorporating stochastic scaling factors for improved exploration.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2) * np.random.normal(1, 0.1), bounds[:, 0], bounds[:, 1])  # Stochastic scaling factor added\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2) * np.random.normal(1, 0.1), bounds[:, 0], bounds[:, 1])  # Stochastic scaling factor added\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust perturbation scale based on budget consumption\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 85, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.51982 with standard deviation 0.25751.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.1585183369509955, 0.6609730388900925, 0.7399779343933739]}}
{"id": "d53f305d-43ff-480d-bfe3-43c958847eee", "fitness": 0.7957239656502147, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Introduced adaptive scaling for perturbation based on fitness diversity to boost convergence.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust perturbation scale based on fitness diversity\n                    diversity = np.std(fitness) / (np.mean(fitness) + 1e-9)\n                    local_perturb = np.random.normal(0, 0.01 * diversity, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 86, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.79572 with standard deviation 0.05670.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.8418159122806186, 0.8294997394480789, 0.7158562452219468]}}
{"id": "b265adbc-6bfd-4a5d-8693-4ad46c394bc0", "fitness": 0.6755574946880017, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Introduced adaptive learning factors and enhanced local search to dynamically balance exploration and exploitation based on convergence progress.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptive parameters based on convergence\n            progress_ratio = eval_count / self.budget\n            F = 0.4 + 0.5 * (1 - progress_ratio)\n            CR = 0.3 + 0.6 * (1 - np.power(progress_ratio, 0.5))\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation strategy\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1]\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    local_perturb_scale = 0.01 * (1 - eval_count / self.budget)\n                    local_perturb = np.random.normal(0, local_perturb_scale, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.67556 with standard deviation 0.16692.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.46190684813935134, 0.8693272305080131, 0.6954384054166409]}}
{"id": "557c2f46-2077-4f31-b6b4-4e279b3a39df", "fitness": 0.6424910309102271, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhanced local search efficiency by adjusting perturbation scale based on iteration count and dynamic CR enhancement.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.4, 0.95)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust perturbation scale based on iteration count instead of budget consumption\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / (self.budget * 2)), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 88, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.64249 with standard deviation 0.09742.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.5777174733696384, 0.7801846480532092, 0.5695709713078339]}}
{"id": "7df2863d-83cf-4645-ad41-17760bd8e31d", "fitness": 0.6623436475705474, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhanced exploration through adaptive mutation scaling and diversity increase.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2) + np.random.normal(0, 0.05, self.dim), bounds[:, 0], bounds[:, 1])  # Added Gaussian noise for diversity\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust perturbation scale based on budget consumption\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 89, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.66234 with standard deviation 0.11814.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.6448381066957691, 0.5272067108639974, 0.8149861251518757]}}
{"id": "b6e04e97-d183-4391-b8cb-7f4c760336f6", "fitness": 0.6949249497432423, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhance local search by introducing adaptive neighborhood search to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1]\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adaptive local search neighborhood\n                    local_perturb = np.random.normal(0, 0.005 + 0.015 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 90, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.69492 with standard deviation 0.09868.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.8014115002342, 0.7198057229182415, 0.5635576260772853]}}
{"id": "57972941-5eb3-4274-99c2-a372006e89fd", "fitness": 0.68205726112526, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Introduce self-adaptive population size and adaptive crossover rate to enhance convergence and diversity.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial parameters\n        base_pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initial population\n        population = np.random.rand(base_pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = base_pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Self-adaptive population size\n            pop_size = int(base_pop_size * (1 - eval_count / self.budget) + 2)\n            F = np.random.uniform(0.4, 0.9)\n            CR = 0.1 + 0.8 * (1 - eval_count / self.budget)  # Adaptive crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1]\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(base_pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 91, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.68206 with standard deviation 0.08598.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.6329913955365222, 0.6102441572030182, 0.8029362306362396]}}
{"id": "a664b098-5848-4ae6-8f24-ae9964c2a0e6", "fitness": 0.5044583609591003, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhanced perturbation by scaling with adaptive noise to improve exploration.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust perturbation scale based on budget consumption\n                    local_perturb = np.random.normal(0, 0.02 * (1 - eval_count / self.budget), self.dim)  # Adjusted scale\n                    perturbed_trial = np.clip(trial + local_perturb * np.random.rand(self.dim), bounds[:, 0], bounds[:, 1])  # Added random scaling\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 92, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.50446 with standard deviation 0.24867.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.15735154567080822, 0.6290873182758258, 0.726936218930667]}}
{"id": "8445bf18-3e1d-4c80-9ab2-d34c207586ee", "fitness": 0.10078310092134261, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Improved convergence by dynamically adjusting the crossover rate based on the diversity of the population.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9) * (np.var(fitness) / np.max(fitness))  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust perturbation scale based on budget consumption\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 93, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10078 with standard deviation 0.00129.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.10175413952104206, 0.09895556482132406, 0.1016395984216617]}}
{"id": "f510a0ab-719e-474f-ae62-f012cb170813", "fitness": 0.7029614846190425, "name": "RefinedAdaptiveDifferentialEvolution", "description": "Utilize a self-adaptive learning parameter mechanism to dynamically adjust mutation strategies and crossover rates based on convergence trends.", "code": "import numpy as np\n\nclass RefinedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Self-adaptive learning parameters\n            F = 0.5 + 0.3 * np.tanh(-0.01 * eval_count + 5)\n            CR = 0.3 + 0.5 * np.sin(np.pi * eval_count / self.budget)\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with improved strategy\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1]\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 94, "feedback": "The algorithm RefinedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.70296 with standard deviation 0.14031.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.5048789000764966, 0.8121387191873033, 0.7918668345933279]}}
{"id": "b3484865-8efa-4e88-99f7-ddc17db125fd", "fitness": 0.47262120858585427, "name": "DynamicSelfAdaptiveDifferentialEvolution", "description": "Dynamic self-adaptive differential evolution using adaptive learning rates and diversity control for improved convergence.", "code": "import numpy as np\n\nclass DynamicSelfAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Dynamic selection of strategy based on population diversity\n                best_index = np.argmin(fitness)\n                diversity = np.std(population, axis=0).mean()\n                adaptive_threshold = 0.1 * (1 - eval_count / self.budget)\n                \n                if diversity < adaptive_threshold:\n                    F = 0.9\n                    CR = 0.9\n                else:\n                    F = 0.4 + 0.5 * (1 - eval_count / self.budget)\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                mutant = np.clip(x0 + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Local search with adaptive learning rate\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust learning rate based on improvement\n                    learning_rate = 0.01 * (1 - eval_count / self.budget)\n                    local_perturb = np.random.normal(0, learning_rate, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 95, "feedback": "The algorithm DynamicSelfAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.47262 with standard deviation 0.09207.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.5550659899801953, 0.34412332618418284, 0.5186743095931847]}}
{"id": "f5f25611-0b10-484d-80e4-d634d8d5ffd1", "fitness": 0.5701136760690125, "name": "EnhancedAdaptiveClusteredDifferentialEvolution", "description": "Integrate adaptive acceleration and clustering into differential evolution to enhance diversification and intensification dynamically.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass EnhancedAdaptiveClusteredDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)\n\n            # Cluster population for diversity management\n            num_clusters = max(2, pop_size // 5)\n            kmeans = KMeans(n_clusters=num_clusters).fit(population)\n            cluster_centers = kmeans.cluster_centers_\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                cluster_center = cluster_centers[kmeans.labels_[i]]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Mutate towards cluster center for exploration\n                    mutant = np.clip(x0 + F * (cluster_center - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Exploit the best individual\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Adaptive local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust perturbation scale based on convergence\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 96, "feedback": "The algorithm EnhancedAdaptiveClusteredDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.57011 with standard deviation 0.22486.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.8266199769774908, 0.6046292031740754, 0.27909184805547127]}}
{"id": "857e4c33-9a2c-4a8b-9e2b-dce26651730d", "fitness": 0.7957239656502147, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Improved local search by dynamically adjusting perturbation magnitude based on fitness variance.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust perturbation scale based on budget consumption\n                    fitness_std = np.std(fitness)  # Added code for dynamic adjustment based on fitness variance\n                    local_perturb = np.random.normal(0, fitness_std * 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 97, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.79572 with standard deviation 0.05670.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.8418159122806186, 0.8294997394480789, 0.7158562452219468]}}
{"id": "a8d74d82-8e9d-4e28-9e8d-f64950ba1124", "fitness": 0.7203937491918676, "name": "EnhancedAdaptiveDifferentialEvolutionWithMemory", "description": "Introduce adaptive local search intensity and memory-based mutation strategy to enhance diversity and convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolutionWithMemory:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        memory = np.zeros((pop_size, self.dim))  # Memory to store previous best individuals\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Dynamic control of crossover rate\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                \n                # Memory-based selection\n                if np.random.rand() < 0.5:\n                    x_memory = memory[np.random.randint(pop_size)]\n                else:\n                    x_memory = x_best\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                \n                # Memory-enhanced mutation strategy\n                mutant = np.clip(x0 + F * (x_memory - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Adaptive local search intensity\n                adaptive_intensity = 0.01 * (1 - eval_count / self.budget)\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Store the improved individual in memory\n                    memory[i] = trial\n                    # Apply adaptive local search\n                    local_perturb = np.random.normal(0, adaptive_intensity, self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n                        memory[i] = perturbed_trial  # Update memory with the perturbed trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 98, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolutionWithMemory got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.72039 with standard deviation 0.16525.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.4956945429739499, 0.8883623811446577, 0.7771243234569951]}}
{"id": "c809c6db-5a31-4935-b52f-e83281fc950a", "fitness": 0.5844283345922064, "name": "ImprovedEnhancedAdaptiveDifferentialEvolution", "description": "Enhanced strategy with dynamic adaptation of mutation factor and crossover rate based on fitness diversity.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.3, 0.9)  # Changed for dynamic control of crossover rate\n            fitness_std = np.std(fitness) / (np.mean(fitness) + 1e-9)  # Add dynamic adaptation based on fitness diversity\n            F *= (1 + 0.5 * fitness_std)\n            CR = max(0.1, CR * (1 + 0.5 * fitness_std))  # Enhance adaptation with fitness diversity\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = fitness.argsort()[1] # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Greedy local search improvement\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Adjust perturbation scale based on budget consumption\n                    local_perturb = np.random.normal(0, 0.01 * (1 - eval_count / self.budget), self.dim)\n                    perturbed_trial = np.clip(trial + local_perturb, bounds[:, 0], bounds[:, 1])\n                    f_perturbed_trial = func(perturbed_trial)\n                    eval_count += 1\n                    if f_perturbed_trial < f_trial:\n                        population[i] = perturbed_trial\n                        fitness[i] = f_perturbed_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 99, "feedback": "The algorithm ImprovedEnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.58443 with standard deviation 0.22192.", "error": "", "parent_ids": ["f9613f6b-b479-41f7-9ac6-9a5f6b3f3796"], "operator": null, "metadata": {"aucs": [0.2707799370372018, 0.7318563127690306, 0.7506487539703871]}}
