{"id": "b54dc4e8-f99b-439a-a239-6a72b80007f6", "fitness": 0.12292152349901711, "name": "HybridSwarmOptimizer", "description": "A Hybrid Swarm-based Optimization Algorithm that combines Particle Swarm Optimization (PSO) with Differential Evolution (DE) to efficiently explore and exploit the search space.", "code": "import numpy as np\n\nclass HybridSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))  # Population size based on dimension\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n\n    def __call__(self, func):\n        while self.fitness_function_calls < self.budget:\n            # Evaluate fitness of particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                # Update personal best\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                # Update global best\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            inertia_weight = 0.5 + np.random.rand() / 2  # Dynamic inertia weight\n            cognitive_constant = 1.5\n            social_constant = 1.5\n\n            # PSO Update\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] = (inertia_weight * self.velocities[i] +\n                                      cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i]) +\n                                      social_constant * r2 * (self.global_best_position - self.particles[i]))\n                self.particles[i] = self.particles[i] + self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Differential Evolution Mutation and Crossover\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant_vector = self.particles[a] + 0.8 * (self.particles[b] - self.particles[c])\n                mutant_vector = np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n                crossover_vector = np.where(np.random.rand(self.dim) < 0.9, mutant_vector, self.particles[i])\n\n                crossover_fitness = func(crossover_vector)\n                self.fitness_function_calls += 1\n\n                if crossover_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = crossover_fitness\n                    self.particles[i] = crossover_vector\n\n                    if crossover_fitness < self.global_best_score:\n                        self.global_best_score = crossover_fitness\n                        self.global_best_position = crossover_vector\n\n        return self.global_best_position", "configspace": "", "generation": 0, "feedback": "The algorithm HybridSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12292 with standard deviation 0.01730.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.132691363000775, 0.13266794281399485, 0.13339354496109512, 0.13700409525819035, 0.13697738499104395, 0.13780611308164725, 0.09850516182917146, 0.09849689932997718, 0.09875120622525857]}}
{"id": "443ac2fc-f5bc-47f5-83e5-6076faa027af", "fitness": 0.12291900961043008, "name": "HybridSwarmOptimizer", "description": "Enhanced Hybrid Swarm Optimizer with Adaptive Parameters for Improved Exploration and Exploitation.", "code": "import numpy as np\n\nclass HybridSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.cognitive_constant = 2.0  # Adaptive cognitive constant\n\n    def __call__(self, func):\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            inertia_weight = 0.5 + np.random.rand() / 2\n            self.cognitive_constant = 1.5 + 0.5 * np.random.rand()  # Adaptive cognitive constant\n            social_constant = 1.5 + 0.5 * np.random.rand()  # Adaptive social constant\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] = (inertia_weight * self.velocities[i] +\n                                      self.cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i]) +\n                                      social_constant * r2 * (self.global_best_position - self.particles[i]))\n                self.particles[i] = self.particles[i] + self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                F = 0.5 + np.random.rand() * 0.5  # Adaptive differential weight\n                mutant_vector = self.particles[a] + F * (self.particles[b] - self.particles[c])\n                mutant_vector = np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n                crossover_vector = np.where(np.random.rand(self.dim) < 0.9, mutant_vector, self.particles[i])\n\n                crossover_fitness = func(crossover_vector)\n                self.fitness_function_calls += 1\n\n                if crossover_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = crossover_fitness\n                    self.particles[i] = crossover_vector\n\n                    if crossover_fitness < self.global_best_score:\n                        self.global_best_score = crossover_fitness\n                        self.global_best_position = crossover_vector\n\n        return self.global_best_position", "configspace": "", "generation": 1, "feedback": "The algorithm HybridSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12292 with standard deviation 0.01730.", "error": "", "parent_ids": ["b54dc4e8-f99b-439a-a239-6a72b80007f6"], "operator": null, "metadata": {"aucs": [0.1326824026903144, 0.13266782861890636, 0.13339354496109512, 0.13699387573226907, 0.13697725466376276, 0.13780611308164725, 0.09850200134961129, 0.09849685917100581, 0.09875120622525857]}}
{"id": "5ab7812a-5015-4aad-9b22-862e846afae4", "fitness": 0.1230144946766254, "name": "HybridSwarmOptimizer", "description": "Enhance convergence by adapting the mutation factor dynamically based on the current iteration.  ", "code": "import numpy as np\n\nclass HybridSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))  # Population size based on dimension\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n\n    def __call__(self, func):\n        while self.fitness_function_calls < self.budget:\n            # Evaluate fitness of particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                # Update personal best\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                # Update global best\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            inertia_weight = 0.5 + np.random.rand() / 2  # Dynamic inertia weight\n            cognitive_constant = 1.5\n            social_constant = 1.5\n\n            # PSO Update\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] = (inertia_weight * self.velocities[i] +\n                                      cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i]) +\n                                      social_constant * r2 * (self.global_best_position - self.particles[i]))\n                self.particles[i] = self.particles[i] + self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Differential Evolution Mutation and Crossover\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutation_factor = 0.5 + (0.9 - 0.5) * (self.budget - self.fitness_function_calls) / self.budget\n                mutant_vector = self.particles[a] + mutation_factor * (self.particles[b] - self.particles[c])\n                mutant_vector = np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n                crossover_vector = np.where(np.random.rand(self.dim) < 0.9, mutant_vector, self.particles[i])\n\n                crossover_fitness = func(crossover_vector)\n                self.fitness_function_calls += 1\n\n                if crossover_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = crossover_fitness\n                    self.particles[i] = crossover_vector\n\n                    if crossover_fitness < self.global_best_score:\n                        self.global_best_score = crossover_fitness\n                        self.global_best_position = crossover_vector\n\n        return self.global_best_position", "configspace": "", "generation": 2, "feedback": "The algorithm HybridSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12301 with standard deviation 0.01734.", "error": "", "parent_ids": ["b54dc4e8-f99b-439a-a239-6a72b80007f6"], "operator": null, "metadata": {"aucs": [0.13298895003462052, 0.1327059971805643, 0.13339354496109512, 0.1373437385831996, 0.13702078705611642, 0.13780611308164725, 0.0986097928507833, 0.09851032211634347, 0.09875120622525857]}}
{"id": "409ed91e-a6a7-43ac-8a8c-9d5d6960f997", "fitness": 0.12356738708211701, "name": "RefinedHybridSwarmOptimizer", "description": "Improve convergence by incorporating adaptive learning rates and dynamic exploration-exploitation balance to refine solutions.", "code": "import numpy as np\n\nclass RefinedHybridSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))  # Population size based on dimension\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5  # Adaptive learning rates\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            inertia_weight = 0.3 + np.random.rand() / 2  # Adjusted dynamic inertia weight\n            progress_ratio = self.fitness_function_calls / self.budget\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] = (inertia_weight * self.velocities[i] +\n                                      cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i]) +\n                                      social_constant * r2 * (self.global_best_position - self.particles[i]))\n                self.particles[i] = self.particles[i] + self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutation_factor = 0.6 + (0.8 - 0.6) * (self.budget - self.fitness_function_calls) / self.budget\n                mutant_vector = self.particles[a] + mutation_factor * (self.particles[b] - self.particles[c])\n                mutant_vector = np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n                crossover_vector = np.where(np.random.rand(self.dim) < 0.8, mutant_vector, self.particles[i])\n\n                crossover_fitness = func(crossover_vector)\n                self.fitness_function_calls += 1\n\n                if crossover_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = crossover_fitness\n                    self.particles[i] = crossover_vector\n\n                    if crossover_fitness < self.global_best_score:\n                        self.global_best_score = crossover_fitness\n                        self.global_best_position = crossover_vector\n\n        return self.global_best_position", "configspace": "", "generation": 3, "feedback": "The algorithm RefinedHybridSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12357 with standard deviation 0.01759.", "error": "", "parent_ids": ["5ab7812a-5015-4aad-9b22-862e846afae4"], "operator": null, "metadata": {"aucs": [0.1341857615365485, 0.13265124356249913, 0.13425014005209934, 0.13871723383202228, 0.13695834374406413, 0.13879599455987057, 0.09902030424881447, 0.09849100233542263, 0.09903645986771203]}}
{"id": "c4584714-403e-446d-b91b-e276996e0758", "fitness": 0.1239231068963876, "name": "EnhancedSwarmOptimizer", "description": "Enhance convergence by incorporating dynamic scaling of velocities and adaptive mutation strategies based on convergence progress for improved solution refinement.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.3 * np.random.rand() * (1 - progress_ratio)  # Dynamic scaling of inertia\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutation_factor = 0.6 + 0.4 * (1 - progress_ratio)  # Adaptive mutation factor\n                mutant_vector = self.particles[a] + mutation_factor * (self.particles[b] - self.particles[c])\n                mutant_vector = np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n                crossover_vector = np.where(np.random.rand(self.dim) < 0.9, mutant_vector, self.particles[i])\n\n                crossover_fitness = func(crossover_vector)\n                self.fitness_function_calls += 1\n\n                if crossover_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = crossover_fitness\n                    self.particles[i] = crossover_vector\n\n                    if crossover_fitness < self.global_best_score:\n                        self.global_best_score = crossover_fitness\n                        self.global_best_position = crossover_vector\n\n        return self.global_best_position", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12392 with standard deviation 0.01784.", "error": "", "parent_ids": ["409ed91e-a6a7-43ac-8a8c-9d5d6960f997"], "operator": null, "metadata": {"aucs": [0.1327376646644548, 0.132657306645816, 0.13698027470355867, 0.13705698510058084, 0.13696525778039315, 0.14203149436896867, 0.09852137865373956, 0.09849314248438013, 0.09986445766559682]}}
{"id": "341e08ae-5185-4357-848e-de9c773f4d39", "fitness": 0.12293145614554864, "name": "EnhancedSwarmOptimizer", "description": "Improve convergence by implementing non-linear dynamic inertia and introducing a local search phase to refine solutions near convergence.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.9 - 0.5 * np.sin(np.pi * progress_ratio)  # Non-linear dynamic scaling of inertia\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutation_factor = 0.6 + 0.4 * (1 - progress_ratio)\n                mutant_vector = self.particles[a] + mutation_factor * (self.particles[b] - self.particles[c])\n                mutant_vector = np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n                crossover_vector = np.where(np.random.rand(self.dim) < 0.9, mutant_vector, self.particles[i])\n\n                crossover_fitness = func(crossover_vector)\n                self.fitness_function_calls += 1\n\n                if crossover_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = crossover_fitness\n                    self.particles[i] = crossover_vector\n\n                    if crossover_fitness < self.global_best_score:\n                        self.global_best_score = crossover_fitness\n                        self.global_best_position = crossover_vector\n\n            # Local search phase\n            if progress_ratio > 0.75:\n                for i in range(self.population_size):\n                    if self.fitness_function_calls >= self.budget:\n                        break\n                    # Small perturbation around the global best\n                    perturbation = np.random.normal(0, 0.1, self.dim)\n                    local_candidate = self.global_best_position + perturbation\n                    local_candidate = np.clip(local_candidate, self.lower_bound, self.upper_bound)\n                    local_fitness = func(local_candidate)\n                    self.fitness_function_calls += 1\n\n                    if local_fitness < self.global_best_score:\n                        self.global_best_score = local_fitness\n                        self.global_best_position = local_candidate\n\n        return self.global_best_position", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12293 with standard deviation 0.01731.", "error": "", "parent_ids": ["c4584714-403e-446d-b91b-e276996e0758"], "operator": null, "metadata": {"aucs": [0.13274434215264286, 0.13265084224227397, 0.13339354496109512, 0.13706466166381348, 0.13695788609444615, 0.13780611308164725, 0.09852364820580306, 0.0984908606829571, 0.09875120622525857]}}
{"id": "83da4ebe-bcce-4f1c-a008-41557dc6a2cc", "fitness": 0.12293290600542435, "name": "EnhancedSwarmOptimizer", "description": "Introduce a nonlinear inertia weight decay and adaptive mutation probability to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.9 * (1 - (progress_ratio ** 3))  # Nonlinear decay of inertia\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutation_factor = 0.6 + 0.4 * (1 - progress_ratio)  # Adaptive mutation factor\n                mutant_vector = self.particles[a] + mutation_factor * (self.particles[b] - self.particles[c])\n                mutant_vector = np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n                crossover_probability = 0.8 + 0.1 * progress_ratio  # Adaptive crossover probability\n                crossover_vector = np.where(np.random.rand(self.dim) < crossover_probability, mutant_vector, self.particles[i])\n\n                crossover_fitness = func(crossover_vector)\n                self.fitness_function_calls += 1\n\n                if crossover_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = crossover_fitness\n                    self.particles[i] = crossover_vector\n\n                    if crossover_fitness < self.global_best_score:\n                        self.global_best_score = crossover_fitness\n                        self.global_best_position = crossover_vector\n\n        return self.global_best_position", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12293 with standard deviation 0.01731.", "error": "", "parent_ids": ["c4584714-403e-446d-b91b-e276996e0758"], "operator": null, "metadata": {"aucs": [0.1326734846930402, 0.13272691823084537, 0.13339354496109512, 0.13698370534784843, 0.13704470213537256, 0.13780611308164725, 0.09849885453867191, 0.09851762483503967, 0.09875120622525857]}}
{"id": "73bb98d7-bfd6-4d72-b20c-4237d432b0ce", "fitness": 0.12309262079387327, "name": "HybridDynamicSwarmOptimizer", "description": "Improve convergence by introducing hybrid dynamic swarm strategies that combine adaptive velocity control, mutation schemes, and local search for enhanced exploration-exploitation balance.", "code": "import numpy as np\n\nclass HybridDynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        local_search_prob = 0.1  # Probability of performing local search\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.5 + 0.2 * np.random.rand() * (1 - progress_ratio)  # Enhanced dynamic scaling\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutation_factor = 0.6 + 0.4 * (1 - progress_ratio)  # Adaptive mutation factor\n                mutant_vector = self.particles[a] + mutation_factor * (self.particles[b] - self.particles[c])\n                mutant_vector = np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n                crossover_vector = np.where(np.random.rand(self.dim) < 0.9, mutant_vector, self.particles[i])\n                \n                # Local search strategy\n                if np.random.rand() < local_search_prob:\n                    local_perturbation = np.random.normal(0, 0.1, self.dim)\n                    crossover_vector += local_perturbation\n                    crossover_vector = np.clip(crossover_vector, self.lower_bound, self.upper_bound)\n\n                crossover_fitness = func(crossover_vector)\n                self.fitness_function_calls += 1\n\n                if crossover_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = crossover_fitness\n                    self.particles[i] = crossover_vector\n\n                    if crossover_fitness < self.global_best_score:\n                        self.global_best_score = crossover_fitness\n                        self.global_best_position = crossover_vector\n\n        return self.global_best_position", "configspace": "", "generation": 7, "feedback": "The algorithm HybridDynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12309 with standard deviation 0.01738.", "error": "", "parent_ids": ["c4584714-403e-446d-b91b-e276996e0758"], "operator": null, "metadata": {"aucs": [0.13267342176022112, 0.13263583733306528, 0.1340622962835758, 0.13698363492333565, 0.13694077713013542, 0.1385775290771556, 0.09849883041152119, 0.09848556155939958, 0.09897569866644984]}}
{"id": "13d0c03f-e8ac-46c2-b383-4894e2614616", "fitness": 0.1229152460146774, "name": "AdvancedSwarmOptimizer", "description": "Introduce multi-phase strategy with hybrid adaptive inertia and learning coefficients, and enhanced mutation for robust exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdvancedSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        inertia_weight_phase1 = 0.9\n        inertia_weight_phase2 = 0.4\n        mutation_phase1 = 0.8\n        mutation_phase2 = 0.2\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            phase = 1 if progress_ratio < 0.5 else 2\n\n            if phase == 1:\n                inertia_weight = inertia_weight_phase1 - (inertia_weight_phase1 - inertia_weight_phase2) * (progress_ratio * 2)\n                mutation_factor = mutation_phase1\n            else:\n                inertia_weight = inertia_weight_phase2\n                mutation_factor = mutation_phase2 - (mutation_phase2 * (progress_ratio - 0.5) * 2)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant_vector = self.particles[a] + mutation_factor * (self.particles[b] - self.particles[c])\n                mutant_vector = np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n                crossover_vector = np.where(np.random.rand(self.dim) < 0.9, mutant_vector, self.particles[i])\n\n                crossover_fitness = func(crossover_vector)\n                self.fitness_function_calls += 1\n\n                if crossover_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = crossover_fitness\n                    self.particles[i] = crossover_vector\n\n                    if crossover_fitness < self.global_best_score:\n                        self.global_best_score = crossover_fitness\n                        self.global_best_position = crossover_vector\n\n        return self.global_best_position", "configspace": "", "generation": 8, "feedback": "The algorithm AdvancedSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12292 with standard deviation 0.01730.", "error": "", "parent_ids": ["c4584714-403e-446d-b91b-e276996e0758"], "operator": null, "metadata": {"aucs": [0.13263765462963195, 0.13267503038215456, 0.1334175225965235, 0.1369428489876542, 0.13698547421211515, 0.13783354970660988, 0.09848620373789907, 0.0984993912458405, 0.09875953863366782]}}
{"id": "76acc2fb-c90b-4f18-ac1a-255ffd62464b", "fitness": 0.12343946451121773, "name": "EnhancedSwarmOptimizer", "description": "Refine convergence by incorporating dynamic boundary adjustments and progressive particle learning rates.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.boundary_shrink_rate = 0.9  # New parameter for dynamic boundary adjustment\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.3 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            learning_rate = 0.1 + 0.9 * progress_ratio  # Progressive learning rate\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.particles[i] += learning_rate * self.velocities[i]  # Apply learning rate\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            self.lower_bound *= self.boundary_shrink_rate  # Adjust boundaries dynamically\n            self.upper_bound *= self.boundary_shrink_rate\n\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutation_factor = 0.6 + 0.4 * (1 - progress_ratio)\n                mutant_vector = self.particles[a] + mutation_factor * (self.particles[b] - self.particles[c])\n                mutant_vector = np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n                crossover_vector = np.where(np.random.rand(self.dim) < 0.9, mutant_vector, self.particles[i])\n\n                crossover_fitness = func(crossover_vector)\n                self.fitness_function_calls += 1\n\n                if crossover_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = crossover_fitness\n                    self.particles[i] = crossover_vector\n\n                    if crossover_fitness < self.global_best_score:\n                        self.global_best_score = crossover_fitness\n                        self.global_best_position = crossover_vector\n\n        return self.global_best_position", "configspace": "", "generation": 9, "feedback": "The algorithm EnhancedSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12344 with standard deviation 0.01753.", "error": "", "parent_ids": ["c4584714-403e-446d-b91b-e276996e0758"], "operator": null, "metadata": {"aucs": [0.1343892393774836, 0.13267994278978712, 0.1335550657076079, 0.13895190761614284, 0.1369910701956274, 0.13799099160023842, 0.09908857178033026, 0.09850113360958235, 0.09880725792415979]}}
{"id": "cc4fe8a9-b033-4202-8c15-fc7027188eb8", "fitness": 0.12389048468581094, "name": "EnhancedSwarmOptimizer", "description": "Refine convergence by incorporating a non-linear inertia weight schedule and a diversified random search strategy to escape local optima.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.3 * (np.cos(progress_ratio * np.pi) + 1) / 2  # Non-linear inertia\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutation_factor = 0.6 + 0.4 * (1 - progress_ratio)  # Adaptive mutation factor\n                random_search_vector = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)  # Diversified search\n                mutant_vector = self.particles[a] + mutation_factor * (self.particles[b] - self.particles[c])\n                mutant_vector = np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n                if np.random.rand() < 0.1:\n                    crossover_vector = random_search_vector\n                else:\n                    crossover_vector = np.where(np.random.rand(self.dim) < 0.9, mutant_vector, self.particles[i])\n\n                crossover_fitness = func(crossover_vector)\n                self.fitness_function_calls += 1\n\n                if crossover_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = crossover_fitness\n                    self.particles[i] = crossover_vector\n\n                    if crossover_fitness < self.global_best_score:\n                        self.global_best_score = crossover_fitness\n                        self.global_best_position = crossover_vector\n\n        return self.global_best_position", "configspace": "", "generation": 10, "feedback": "The algorithm EnhancedSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12389 with standard deviation 0.01775.", "error": "", "parent_ids": ["c4584714-403e-446d-b91b-e276996e0758"], "operator": null, "metadata": {"aucs": [0.13598656546530052, 0.13275798565566865, 0.13351044965796255, 0.14079397238295732, 0.13708017687345841, 0.13794032344164564, 0.09962512352743202, 0.09852852460291028, 0.09879124056496313]}}
{"id": "4fedf576-c7ce-4755-94ff-e973329e71a2", "fitness": 0.12289955879589673, "name": "RefinedSwarmOptimizer", "description": "Refine convergence by employing a multi-phase strategy with adaptive velocity control, elitist selection, and region-focused perturbations to enhance solution search efficiency.", "code": "import numpy as np\n\nclass RefinedSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.elite_size = max(1, self.population_size // 10)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.5 + 0.4 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            elite_indices = np.argsort(self.personal_best_scores)[:self.elite_size]\n            for i in range(self.elite_size, self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                a, b, c = np.random.choice(elite_indices, 3, replace=False)\n                mutation_factor = 0.5 + 0.5 * (1 - progress_ratio)\n                mutant_vector = self.particles[a] + mutation_factor * (self.particles[b] - self.particles[c])\n                mutant_vector = np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n                crossover_vector = np.where(np.random.rand(self.dim) < 0.8, mutant_vector, self.particles[i])\n\n                crossover_fitness = func(crossover_vector)\n                self.fitness_function_calls += 1\n\n                if crossover_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = crossover_fitness\n                    self.particles[i] = crossover_vector\n\n                    if crossover_fitness < self.global_best_score:\n                        self.global_best_score = crossover_fitness\n                        self.global_best_position = crossover_vector\n\n        return self.global_best_position", "configspace": "", "generation": 11, "feedback": "The algorithm RefinedSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12290 with standard deviation 0.01729.", "error": "", "parent_ids": ["c4584714-403e-446d-b91b-e276996e0758"], "operator": null, "metadata": {"aucs": [0.13263444061637686, 0.13263190136307368, 0.13340723402516708, 0.13693918417437057, 0.13693628886821185, 0.13782177959127306, 0.09848506884596653, 0.09848417206684734, 0.09875595961178352]}}
{"id": "4937131e-02d7-4fb8-8edc-29c63f7b7958", "fitness": 0.12301005868257914, "name": "EnhancedSwarmOptimizer", "description": "Enhance swarm convergence by introducing adaptive learning rates and dynamic inertia adjustments based on search progress.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.9 - 0.5 * progress_ratio  # Dynamic scaling of inertia, adjusted\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutation_factor = 0.7 + 0.3 * (1 - progress_ratio)  # Adjusted adaptive mutation factor\n                mutant_vector = self.particles[a] + mutation_factor * (self.particles[b] - self.particles[c])\n                mutant_vector = np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n                crossover_vector = np.where(np.random.rand(self.dim) < 0.9, mutant_vector, self.particles[i])\n\n                crossover_fitness = func(crossover_vector)\n                self.fitness_function_calls += 1\n\n                if crossover_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = crossover_fitness\n                    self.particles[i] = crossover_vector\n\n                    if crossover_fitness < self.global_best_score:\n                        self.global_best_score = crossover_fitness\n                        self.global_best_position = crossover_vector\n\n        return self.global_best_position", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12301 with standard deviation 0.01734.", "error": "", "parent_ids": ["c4584714-403e-446d-b91b-e276996e0758"], "operator": null, "metadata": {"aucs": [0.1327042387059899, 0.13297470943916145, 0.13339354496109512, 0.13701879190318467, 0.1373275697215327, 0.13780611308164725, 0.09850968699156704, 0.09860466711377525, 0.09875120622525857]}}
{"id": "fc1b76f2-3851-41e9-8d3d-ead45cd6fe9f", "fitness": 0.1456295733187093, "name": "EnhancedSwarmOptimizerV2", "description": "Improve solution accuracy by integrating adaptive velocity clamping, diversified exploration with opposition-based learning, and elitist strategy to preserve the best solutions for enhanced convergence.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)  # Adaptive velocity clamping\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.3 * np.random.rand() * (1 - progress_ratio)  # Dynamic scaling of inertia\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Opposition-based learning for diversification\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            # Elitist strategy to preserve best solutions\n            elite_idxs = np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)]\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 13, "feedback": "The algorithm EnhancedSwarmOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.14563 with standard deviation 0.04683.", "error": "", "parent_ids": ["c4584714-403e-446d-b91b-e276996e0758"], "operator": null, "metadata": {"aucs": [0.13673021934090723, 0.22598766175066853, 0.1335561160129608, 0.1416945202453853, 0.23001624489796813, 0.13799238548409487, 0.09982455243895572, 0.10605709717382072, 0.09880736252362254]}}
{"id": "2ae6681e-5006-42e7-802d-e8da6164413b", "fitness": 0.123363324231512, "name": "EnhancedSwarmOptimizerV3", "description": "Amplify convergence and solution quality by employing a multi-swarm approach with adaptive learning rates and dynamic regrouping for enhanced synergy and exploration.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.num_swarms = 3  # Utilize multiple swarms for diversity\n        self.swarms = [np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim)) for _ in range(self.num_swarms)]\n        self.velocities = [np.random.uniform(-1, 1, (self.population_size, dim)) for _ in range(self.num_swarms)]\n        self.personal_best_positions = [np.copy(swarm) for swarm in self.swarms]\n        self.personal_best_scores = [np.full(self.population_size, np.inf) for _ in range(self.num_swarms)]\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)  # Adaptive velocity clamping\n        \n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for swarm_index in range(self.num_swarms):\n                swarm, velocities, p_best_positions, p_best_scores = (\n                    self.swarms[swarm_index],\n                    self.velocities[swarm_index],\n                    self.personal_best_positions[swarm_index],\n                    self.personal_best_scores[swarm_index]\n                )\n\n                for i in range(self.population_size):\n                    if self.fitness_function_calls >= self.budget:\n                        break\n\n                    fitness = func(swarm[i])\n                    self.fitness_function_calls += 1\n\n                    if fitness < p_best_scores[i]:\n                        p_best_scores[i] = fitness\n                        p_best_positions[i] = swarm[i]\n\n                    if fitness < self.global_best_score:\n                        self.global_best_score = fitness\n                        self.global_best_position = swarm[i]\n\n                progress_ratio = self.fitness_function_calls / self.budget\n                inertia_weight = 0.4 + 0.3 * np.random.rand() * (1 - progress_ratio)  # Dynamic scaling of inertia\n\n                cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n                social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n                for i in range(self.population_size):\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n                    velocities[i] *= inertia_weight\n                    velocities[i] += cognitive_constant * r1 * (p_best_positions[i] - swarm[i])\n                    velocities[i] += social_constant * r2 * (self.global_best_position - swarm[i])\n                    velocities[i] = np.clip(velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                    swarm[i] += velocities[i]\n                    swarm[i] = np.clip(swarm[i], self.lower_bound, self.upper_bound)\n\n                # Opposition-based learning for diversification\n                opposition_particles = self.lower_bound + self.upper_bound - swarm\n                for i in range(self.population_size):\n                    if self.fitness_function_calls >= self.budget:\n                        break\n\n                    opp_fitness = func(opposition_particles[i])\n                    self.fitness_function_calls += 1\n\n                    if opp_fitness < p_best_scores[i]:\n                        p_best_scores[i] = opp_fitness\n                        p_best_positions[i] = opposition_particles[i]\n\n                        if opp_fitness < self.global_best_score:\n                            self.global_best_score = opp_fitness\n                            self.global_best_position = opposition_particles[i]\n\n            # Periodically regroup particles to promote synergy\n            if self.fitness_function_calls % (self.budget // 10) == 0:\n                best_swarm_index = np.argmin([np.min(p_best_scores) for p_best_scores in self.personal_best_scores])\n                self.global_best_position = self.personal_best_positions[best_swarm_index][np.argmin(self.personal_best_scores[best_swarm_index])]\n                self.global_best_score = np.min(self.personal_best_scores[best_swarm_index])\n\n        return self.global_best_position", "configspace": "", "generation": 14, "feedback": "The algorithm EnhancedSwarmOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12336 with standard deviation 0.01748.", "error": "", "parent_ids": ["fc1b76f2-3851-41e9-8d3d-ead45cd6fe9f"], "operator": null, "metadata": {"aucs": [0.1336632341637659, 0.133291844412895, 0.13339354496109512, 0.13811679948637812, 0.13769109896215004, 0.13780611308164725, 0.09884207030198333, 0.09871400648843465, 0.09875120622525857]}}
{"id": "57e046dd-1103-4d57-aa90-0308597a052f", "fitness": 0.12619985428475117, "name": "EnhancedSwarmOptimizerV2", "description": "Refine convergence by reducing cognitive and social constants only at later stages to maintain exploration initially.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)  # Adaptive velocity clamping\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.3 * np.random.rand() * (1 - progress_ratio)  # Dynamic scaling of inertia\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * (progress_ratio ** 2)\n            social_constant = c2_initial - (c2_initial - c2_final) * (progress_ratio ** 2)\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Opposition-based learning for diversification\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            # Elitist strategy to preserve best solutions\n            elite_idxs = np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)]\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 15, "feedback": "The algorithm EnhancedSwarmOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12620 with standard deviation 0.01896.", "error": "", "parent_ids": ["fc1b76f2-3851-41e9-8d3d-ead45cd6fe9f"], "operator": null, "metadata": {"aucs": [0.1346840705529171, 0.14016901789103586, 0.13570071755814228, 0.13930559203285997, 0.14623901094280578, 0.1404683032750873, 0.09917085317943641, 0.100536820721562, 0.09952430240891352]}}
{"id": "efa746a7-2328-4ba1-902b-7a5331844212", "fitness": 0.12325172925228466, "name": "EnhancedSwarmOptimizerV2", "description": "Enhance solution accuracy by incorporating adaptive learning rates for cognitive and social components to balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)  # Adaptive velocity clamping\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.3 * np.random.rand() * (1 - progress_ratio)  # Dynamic scaling of inertia\n\n            # Updated line: Adaptive learning rate for cognitive and social components\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio * np.random.rand()\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio * np.random.rand()\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Opposition-based learning for diversification\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            # Elitist strategy to preserve best solutions\n            elite_idxs = np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)]\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 16, "feedback": "The algorithm EnhancedSwarmOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12325 with standard deviation 0.01744.", "error": "", "parent_ids": ["fc1b76f2-3851-41e9-8d3d-ead45cd6fe9f"], "operator": null, "metadata": {"aucs": [0.133176323228558, 0.1333755571598283, 0.13339354496109512, 0.13755869382693542, 0.13778688058198885, 0.13780611308164725, 0.09867414828386056, 0.09874309592138963, 0.09875120622525857]}}
{"id": "aa65d2dd-4992-44e5-a1ab-1289bb29ee1b", "fitness": 0.29586270925351993, "name": "EnhancedSwarmOptimizerV2", "description": "Enhanced exploration by integrating stochastic inertia adjustment and improved elitist strategy to refine convergence.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)  # Adaptive velocity clamping\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)  # Adjusted inertia with stochastic element\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Opposition-based learning for diversification\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedSwarmOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29586 with standard deviation 0.23999.", "error": "", "parent_ids": ["fc1b76f2-3851-41e9-8d3d-ead45cd6fe9f"], "operator": null, "metadata": {"aucs": [0.13646232497857602, 0.6386674947093739, 0.13802792806766873, 0.1415575034162967, 0.6404827428638988, 0.14367052797757762, 0.09960162919858895, 0.6243380930452596, 0.09995613902443856]}}
{"id": "c9d42e98-0466-416b-b70f-66720aba3e2d", "fitness": 0.16181831232053184, "name": "EnhancedSwarmOptimizerV3", "description": "Introduced adaptive mutation strategy and enhanced opposition-based exploration to improve diversity and convergence rate.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        mutation_rate = 0.1  # Added mutation rate\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n                # Adaptive mutation\n                if np.random.rand() < mutation_rate:\n                    mutation_vector = np.random.uniform(-0.5, 0.5, self.dim)\n                    self.particles[i] += mutation_vector\n                    self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Enhanced opposition-based learning\n            opposition_particles = (self.lower_bound + self.upper_bound - self.particles) + np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedSwarmOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.16182 with standard deviation 0.06152.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.2651048109160722, 0.13286364492744762, 0.13412719954278596, 0.2734345064135003, 0.13720070742580093, 0.138646872042539, 0.1774168735029722, 0.09856576036726805, 0.0990044357464005]}}
{"id": "b9ea3e0f-a161-444f-8685-e102e9c3cd78", "fitness": 0.29586270925351993, "name": "AdaptiveOppositionSwarmOptimizer", "description": "Adaptive Opposition-based Swarm Optimizer enhances exploration and convergence by integrating dynamic boundary opposition and adaptive learning rates.", "code": "import numpy as np\n\nclass AdaptiveOppositionSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Dynamic boundary opposition learning\n            a = progress_ratio * self.lower_bound + (1 - progress_ratio) * self.upper_bound\n            b = (1 - progress_ratio) * self.lower_bound + progress_ratio * self.upper_bound\n            opposition_particles = a + b - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            # Improved elitist strategy\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 19, "feedback": "The algorithm AdaptiveOppositionSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29586 with standard deviation 0.23999.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.13646232497857602, 0.6386674947093739, 0.13802792806766873, 0.1415575034162967, 0.6404827428638988, 0.14367052797757762, 0.09960162919858895, 0.6243380930452596, 0.09995613902443856]}}
{"id": "ad0ffb94-c3b1-4919-b6e0-5434027a1841", "fitness": 0.12364991957459212, "name": "EnhancedSwarmOptimizerV3", "description": "Integrate adaptive chaotic search for enhanced exploration and dynamic swarm topology adjustment to boost convergence.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.neighborhood_size = max(2, self.population_size // 10)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                # Dynamic topology adjustment\n                neighbors = np.random.choice(self.population_size, self.neighborhood_size, replace=False)\n                local_best = min(neighbors, key=lambda x: self.personal_best_scores[x])\n                \n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.personal_best_positions[local_best] - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Adaptive chaotic search\n            chaos_factor = 0.7 * (1 - progress_ratio)\n            chaotic_particles = self.particles + chaos_factor * np.random.uniform(-1, 1, (self.population_size, self.dim))\n            chaotic_particles = np.clip(chaotic_particles, self.lower_bound, self.upper_bound)\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                chaotic_fitness = func(chaotic_particles[i])\n                self.fitness_function_calls += 1\n\n                if chaotic_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = chaotic_fitness\n                    self.personal_best_positions[i] = chaotic_particles[i]\n\n                if chaotic_fitness < self.global_best_score:\n                    self.global_best_score = chaotic_fitness\n                    self.global_best_position = chaotic_particles[i]\n\n        return self.global_best_position", "configspace": "", "generation": 20, "feedback": "The algorithm EnhancedSwarmOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12365 with standard deviation 0.01761.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.13452249466562827, 0.13336126763030587, 0.13350007015225374, 0.1391013331784111, 0.13776975977378403, 0.13792802495391288, 0.09913893201946766, 0.09873919594662173, 0.09878819785094384]}}
{"id": "cb11fd99-eb92-48c8-a8d0-3d28b6e2d8d5", "fitness": 0.1235419400253348, "name": "EnhancedSwarmOptimizerV3", "description": "Introduced adaptive mutation and diversity preservation strategies in EnhancedSwarmOptimizer to invigorate exploration and convergence balance.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        mutation_probability = 0.1\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n                \n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n            \n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n                # Adaptive mutation strategy\n                if np.random.rand() < mutation_probability:\n                    mutation_vector = np.random.normal(0, 0.1, self.dim)\n                    self.particles[i] += mutation_vector\n                    self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 21, "feedback": "The algorithm EnhancedSwarmOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12354 with standard deviation 0.01757.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.1345576284940353, 0.13281022232723594, 0.13362662834841454, 0.13914577405460082, 0.1371397259811773, 0.13807304446722757, 0.09914554739577863, 0.09854698966561348, 0.09883189949392979]}}
{"id": "742ea30e-4fe7-45b4-abe2-11f0975fc55c", "fitness": 0.14133797156178585, "name": "EnhancedSwarmOptimizerV2", "description": "Refine exploration by introducing random reinitialization for a subset of particles to escape local optima.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)  # Adaptive velocity clamping\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)  # Adjusted inertia with stochastic element\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Opposition-based learning for diversification\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n            if np.random.rand() < 0.1:  # Random reinitialization of some particles\n                random_idxs = np.random.choice(self.population_size, size=max(1, self.population_size // 20), replace=False)\n                self.particles[random_idxs] = np.random.uniform(self.lower_bound, self.upper_bound, (len(random_idxs), self.dim))\n\n        return self.global_best_position", "configspace": "", "generation": 22, "feedback": "The algorithm EnhancedSwarmOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.14134 with standard deviation 0.03964.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.13476655531944082, 0.13543004750698084, 0.20713781880971405, 0.13943651968812376, 0.14022310346672973, 0.21126028223049487, 0.09916311193571725, 0.09936596164620626, 0.10525834345266516]}}
{"id": "2c755f0a-9788-449e-a9eb-56466ba86883", "fitness": 0.12314607060309696, "name": "HybridSwarmOptimizer", "description": "Hybrid Particle-Swarm Optimization with Chaotic Perturbation and Adaptive Learning to Enhance Global Search and Convergence.", "code": "import numpy as np\n\nclass HybridSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)  # Adaptive velocity clamping\n        self.chaotic_sequence = np.array([np.sin(i) for i in range(self.budget)])  # Simple chaotic sequence\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        inertia_weight_min, inertia_weight_max = 0.4, 0.9\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * progress_ratio\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                chaotic_factor = self.chaotic_sequence[self.fitness_function_calls % len(self.chaotic_sequence)]\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i]) * chaotic_factor\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i]) * chaotic_factor\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Adaptive learning for diversification\n            if progress_ratio > 0.5:  # Introduce additional perturbation in later stages\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                self.particles += perturbation * (1 - progress_ratio)\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 23, "feedback": "The algorithm HybridSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12315 with standard deviation 0.01740.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.13349091484741327, 0.13267930163114172, 0.13339354496109512, 0.137918223373945, 0.13699033813709138, 0.13780611308164725, 0.09878408457331866, 0.0985009085969617, 0.09875120622525857]}}
{"id": "38c08903-b22f-4683-8b88-1b2144276cde", "fitness": 0.12370302257751215, "name": "EnhancedSwarmOptimizerV2", "description": "Introduced a dynamic velocity update strategy with adaptive inertia bounds and improved elitist selection criteria to enhance convergence.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)  # Adaptive velocity clamping\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.5 + 0.1 * np.random.rand() * (1 - progress_ratio)  # Adjusted inertia with stochastic element\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Opposition-based learning for diversification\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 8)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 24, "feedback": "The algorithm EnhancedSwarmOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12370 with standard deviation 0.01765.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.1334057031070487, 0.13461229035743938, 0.1335535569485189, 0.13782139442717212, 0.13928487002220025, 0.13798952831590716, 0.0987535551133204, 0.09909992570253878, 0.09880637920346358]}}
{"id": "966eac99-21b3-4b70-aec3-3b9460e36967", "fitness": 0.12369770728805093, "name": "EnhancedSwarmOptimizerV2", "description": "Optimized inertia weight adjustment for better balance between exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)  # Adaptive velocity clamping\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.5 + 0.3 * np.random.rand() * (1 - progress_ratio)  # Adjusted inertia with stochastic element\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Opposition-based learning for diversification\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 25, "feedback": "The algorithm EnhancedSwarmOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12370 with standard deviation 0.01768.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.1327751088051793, 0.13324856457232281, 0.13553572029078031, 0.13709963938285186, 0.13764071909784426, 0.14033647059865872, 0.09853465902409919, 0.09870013307766035, 0.09940835074306165]}}
{"id": "ce94e362-aab9-4fc5-b226-fa09be21ebb7", "fitness": 0.12338770017931827, "name": "EnhancedSwarmOptimizerV3", "description": "Enhance global exploration by introducing adaptive boundary control and hybrid differential evolution inspired updates.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.0, 2.0  # Adjusted coefficients for better balance\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.5 + 0.4 * np.random.rand() * (1 - progress_ratio)  # Adjusted inertia\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Adaptive boundary control\n            boundary_correction = np.where((self.particles < self.lower_bound) | (self.particles > self.upper_bound), \n                                           self.lower_bound + np.random.rand(self.population_size, self.dim) * (self.upper_bound - self.lower_bound), \n                                           self.particles)\n            self.particles = np.where(np.random.rand(self.population_size, self.dim) < 0.1, boundary_correction, self.particles)\n\n            # Hybrid updates inspired by DE\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n                if np.random.rand() < 0.2:\n                    a, b, c = np.random.choice(self.population_size, 3, replace=False)\n                    mutant_vector = self.personal_best_positions[a] + 0.5 * (self.personal_best_positions[b] - self.personal_best_positions[c])\n                    trial_vector = np.where(np.random.rand(self.dim) < 0.5, mutant_vector, self.particles[i])\n                    trial_vector = np.clip(trial_vector, self.lower_bound, self.upper_bound)\n                    trial_fitness = func(trial_vector)\n                    self.fitness_function_calls += 1\n\n                    if trial_fitness < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = trial_fitness\n                        self.personal_best_positions[i] = trial_vector\n\n                        if trial_fitness < self.global_best_score:\n                            self.global_best_score = trial_fitness\n                            self.global_best_position = trial_vector\n\n        return self.global_best_position", "configspace": "", "generation": 26, "feedback": "The algorithm EnhancedSwarmOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12339 with standard deviation 0.01751.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.1342167957119239, 0.13282752986153845, 0.13339354496109512, 0.13876154983555244, 0.13715951840231888, 0.13780611308164725, 0.09902002360366402, 0.09855301993086552, 0.09875120622525857]}}
{"id": "a40802f8-9823-48f9-bb48-a0d1e45f2bba", "fitness": 0.126619794312106, "name": "EnhancedSwarmOptimizerV2", "description": "Introduced a dynamic velocity clamping factor and a time-varying opposition mechanism to enhance exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)  # Adaptive velocity clamping\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)  # Adjusted inertia with stochastic element\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp * (1 - progress_ratio), self.velocity_clamp * (1 - progress_ratio))  # Dynamic clamping\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Time-varying opposition-based learning for diversification\n            opposition_particles = progress_ratio * (self.lower_bound + self.upper_bound) - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 27, "feedback": "The algorithm EnhancedSwarmOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12662 with standard deviation 0.02014.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.14668369870911036, 0.1330924010234109, 0.13437549276977567, 0.15146983841942196, 0.13746248743803602, 0.13895448164282642, 0.09982990664244029, 0.09864522370128503, 0.09906461846264725]}}
{"id": "927d3251-cc6f-444b-b8e4-675893855895", "fitness": 0.12642548218249405, "name": "EnhancedSwarmOptimizerV3", "description": "Integration of adaptive mutation and enhanced opposition-based learning to improve exploration and convergence in swarm optimization.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        adaptive_mutation_rate = 0.1  # Added mutation rate\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n                # Adaptive mutation to enhance exploration\n                if np.random.rand() < adaptive_mutation_rate:\n                    mutation = np.random.uniform(-0.1, 0.1, self.dim)\n                    self.particles[i] += mutation\n                    self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Enhanced opposition-based learning for diversification\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 28, "feedback": "The algorithm EnhancedSwarmOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12643 with standard deviation 0.01906.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.14158405558074683, 0.13569562738850005, 0.13414857204525577, 0.14741755351445907, 0.14047538119781666, 0.13868465970870636, 0.1013207689182235, 0.09950668486454906, 0.0989960364241892]}}
{"id": "091d656d-e4f3-4349-90af-4254cdce1a20", "fitness": 0.29586270925351993, "name": "EnhancedSwarmOptimizerV2", "description": "Enhanced exploration using chaotic map-based velocity initialization to increase diversity in the search space.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)  # Adaptive velocity clamping\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)  # Adjusted inertia with stochastic element\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Opposition-based learning for diversification\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 29, "feedback": "The algorithm EnhancedSwarmOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29586 with standard deviation 0.23999.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.13646232497857602, 0.6386674947093739, 0.13802792806766873, 0.1415575034162967, 0.6404827428638988, 0.14367052797757762, 0.09960162919858895, 0.6243380930452596, 0.09995613902443856]}}
{"id": "2bdcc822-7a38-421d-8a98-ee9d6ff32b66", "fitness": 0.16739265302048123, "name": "AdaptiveMultiSwarmOptimizer", "description": "Adaptive multi-swarm strategy with competition and cooperation dynamics to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.num_swarms = 3\n        self.swarm_size = self.population_size // self.num_swarms\n        self.particles = [np.random.uniform(self.lower_bound, self.upper_bound, (self.swarm_size, dim)) for _ in range(self.num_swarms)]\n        self.velocities = [np.random.uniform(-1, 1, (self.swarm_size, dim)) for _ in range(self.num_swarms)]\n        self.personal_best_positions = [np.copy(particles) for particles in self.particles]\n        self.personal_best_scores = [np.full(self.swarm_size, np.inf) for _ in range(self.num_swarms)]\n        self.global_best_position = np.random.uniform(self.lower_bound, self.upper_bound, dim)\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for s in range(self.num_swarms):\n                for i in range(self.swarm_size):\n                    if self.fitness_function_calls >= self.budget:\n                        break\n\n                    fitness = func(self.particles[s][i])\n                    self.fitness_function_calls += 1\n\n                    if fitness < self.personal_best_scores[s][i]:\n                        self.personal_best_scores[s][i] = fitness\n                        self.personal_best_positions[s][i] = self.particles[s][i]\n\n                    if fitness < self.global_best_score:\n                        self.global_best_score = fitness\n                        self.global_best_position = self.particles[s][i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n\n            for s in range(self.num_swarms):\n                cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n                social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n                for i in range(self.swarm_size):\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n                    self.velocities[s][i] *= inertia_weight\n                    self.velocities[s][i] += cognitive_constant * r1 * (self.personal_best_positions[s][i] - self.particles[s][i])\n                    self.velocities[s][i] += social_constant * r2 * (self.global_best_position - self.particles[s][i])\n                    self.velocities[s][i] = np.clip(self.velocities[s][i], -self.velocity_clamp, self.velocity_clamp)\n                    self.particles[s][i] += self.velocities[s][i]\n                    self.particles[s][i] = np.clip(self.particles[s][i], self.lower_bound, self.upper_bound)\n\n                # Perform competition and cooperation between swarms\n                if s > 0:\n                    for i in range(self.swarm_size):\n                        if np.random.rand() < 0.1:\n                            self.particles[s][i] = np.copy(self.particles[s-1][np.random.randint(self.swarm_size)])\n\n            # Update global best from all swarms\n            for s in range(self.num_swarms):\n                best_idx = np.argmin(self.personal_best_scores[s])\n                if self.personal_best_scores[s][best_idx] < self.global_best_score:\n                    self.global_best_score = self.personal_best_scores[s][best_idx]\n                    self.global_best_position = np.copy(self.personal_best_positions[s][best_idx])\n\n        return self.global_best_position", "configspace": "", "generation": 30, "feedback": "The algorithm AdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.16739 with standard deviation 0.06195.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.26095523268121823, 0.13524696301448913, 0.13664024831280852, 0.26488063859186195, 0.13995193570306452, 0.141597388136128, 0.2281095040014094, 0.0993632320064648, 0.09978873473688665]}}
{"id": "66b10268-3cc9-40f1-998b-a3912d68f619", "fitness": 0.12554070538453554, "name": "AdvancedSwarmOptimizer", "description": "Integrates adaptive learning rate and diversity preservation using a hybrid mutation strategy to intensify search and evade premature convergence.", "code": "import numpy as np\n\nclass AdvancedSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.learning_rate = 0.5\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.learning_rate * self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Hybrid mutation for diversity\n            if np.random.rand() < 0.1:\n                mutation_idx = np.random.choice(self.population_size, 1, replace=False)\n                self.particles[mutation_idx] = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n\n            # Opposition-based learning for diversification\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 31, "feedback": "The algorithm AdvancedSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12554 with standard deviation 0.01899.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.1340485644471774, 0.13278301945568172, 0.1409699056994591, 0.13859446262907127, 0.13710877866057403, 0.14832062656555856, 0.09893760487735292, 0.0985372843897312, 0.10056610173621372]}}
{"id": "ec61f8eb-3305-426a-b4d7-d60ba3f3ac08", "fitness": -Infinity, "name": "EnhancedSwarmOptimizerV3", "description": "Introduce adaptive neighborhood topology and chaotic map initialization to improve exploration and convergence robustness.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = self._initialize_particles_chaotically()\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)  # Adaptive velocity clamping\n\n    def _initialize_particles_chaotically(self):\n        x = np.linspace(0, 1, self.population_size)\n        logistic_map = 4.0 * x * (1 - x)  # Chaotic sequence\n        return self.lower_bound + logistic_map[:, None] * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        neighborhood_size = max(3, self.population_size // 5)\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                neighbors_idxs = np.random.choice(self.population_size, neighborhood_size, replace=False)\n                local_best_idx = neighbors_idxs[np.argmin(self.personal_best_scores[neighbors_idxs])]\n                local_best_position = self.personal_best_positions[local_best_idx]\n\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (local_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 32, "feedback": "An exception occurred: ValueError(\"non-broadcastable output operand with shape (1,) doesn't match the broadcast shape (45,)\").", "error": "ValueError(\"non-broadcastable output operand with shape (1,) doesn't match the broadcast shape (45,)\")", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {}}
{"id": "4d52a935-af83-4922-9e02-8e416252a463", "fitness": 0.16955115170769924, "name": "EnhancedSwarmOptimizerV3", "description": "Incorporating dynamic neighborhood topology with adaptive mutation enhances exploration and convergence in complex search spaces.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        mutation_probability = 0.1\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.1 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            # Dynamic neighborhood topology with adaptive mutation\n            neighborhood_size = max(3, self.population_size // 10)\n            for i in range(self.population_size):\n                neighbors = np.random.choice(self.population_size, neighborhood_size, replace=False)\n                local_best_idx = neighbors[np.argmin(self.personal_best_scores[neighbors])]\n                local_best_position = self.personal_best_positions[local_best_idx]\n\n                if np.random.rand() < mutation_probability:\n                    mutation_vector = np.random.normal(0, 0.1, self.dim)\n                    self.particles[i] = np.clip(local_best_position + mutation_vector, self.lower_bound, self.upper_bound)\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 33, "feedback": "The algorithm EnhancedSwarmOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.16955 with standard deviation 0.12238.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.1327265602224349, 0.17234347973612585, 0.13388695089685387, 0.13704423943651478, 0.509888938960544, 0.13837180907527635, 0.09851757573428288, 0.1042596466129998, 0.09892116469426093]}}
{"id": "2bc7e2ea-daa1-446d-86ec-0c7b9c068c7a", "fitness": 0.12302440229041152, "name": "EnhancedSwarmOptimizerV2", "description": "Enhanced exploitation through adaptive velocity scaling and selective mutation strategy for improved local search.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)  # Adaptive velocity clamping\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)  # Adjusted inertia with stochastic element\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp * (1 - progress_ratio))  # Adaptive velocity scaling\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Opposition-based learning for diversification\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                if np.random.rand() < 0.1:  # Selective mutation strategy\n                    elite_positions[i] += np.random.normal(0, 0.1, self.dim)\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 34, "feedback": "The algorithm EnhancedSwarmOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12302 with standard deviation 0.01735.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.13271604957601557, 0.13276939212835515, 0.1336389685238636, 0.13703225987811618, 0.13709319396539854, 0.13808714052985005, 0.09851385678632518, 0.09853253855040633, 0.09883622067537301]}}
{"id": "f8bf5bbf-4177-4dd7-b309-418426df0d8c", "fitness": 0.1237527563371141, "name": "EnhancedSwarmOptimizerV2", "description": "Enhanced exploration and dynamic adaptation by incorporating a stochastic factor in the elitist strategy to further refine solution diversity.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)  # Adaptive velocity clamping\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)  # Adjusted inertia with stochastic element\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Opposition-based learning for diversification\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                perturbation = np.random.normal(0, 0.1, self.dim)  # Stochastic perturbation added\n                self.particles[elite_idxs[i]] = elite_positions[i] + perturbation\n\n        return self.global_best_position", "configspace": "", "generation": 35, "feedback": "The algorithm EnhancedSwarmOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12375 with standard deviation 0.01766.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.13349274352628726, 0.13486953703059545, 0.13339354496109512, 0.1379200768821105, 0.13950288613794426, 0.13780611308164725, 0.09878505142586747, 0.09925364776322099, 0.09875120622525857]}}
{"id": "c20df80a-ee14-4fed-9537-7843a85fa485", "fitness": 0.12316761089043456, "name": "DynamicAdaptationSwarmOptimizer", "description": "Introduce dynamic swarm adaptation by employing environment-driven learning rates and adaptive neighborhood influence for enhanced global optimization.", "code": "import numpy as np\n\nclass DynamicAdaptationSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        adaptive_factor = lambda: 0.5 + 0.5 * np.random.rand()\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n            \n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n            \n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                neighbor_idx = np.random.choice(self.population_size)\n                self.particles[elite_idxs[i]] = adaptive_factor() * elite_positions[i] + (1 - adaptive_factor()) * self.particles[neighbor_idx]\n\n        return self.global_best_position", "configspace": "", "generation": 36, "feedback": "The algorithm DynamicAdaptationSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12317 with standard deviation 0.01740.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.13320794291896865, 0.13303988025244695, 0.13339354496109512, 0.13759422868209958, 0.13740188685425236, 0.13780611308164725, 0.09868602389806291, 0.09862767114007942, 0.09875120622525857]}}
{"id": "7fe668f0-0600-48f5-b9f6-2d8fe3bd7704", "fitness": 0.12329202537572442, "name": "AdaptiveMemorySwarmOptimizer", "description": "Introducing adaptive memory and dynamic neighborhood adjustment to enhance convergence rate and robustness in diverse scenarios.", "code": "import numpy as np\n\nclass AdaptiveMemorySwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.memory = []  # Memory to store historically best positions\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            # Dynamic neighborhood adjustment based on proximity to the global best\n            neighborhood_radius = np.max([0.5 * (1 - progress_ratio), 0.1])\n            \n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                \n                # Identify neighborhood of particles\n                neighbors = np.where(np.linalg.norm(self.particles - self.particles[i], axis=1) < neighborhood_radius)[0]\n                neighborhood_best = np.min(self.personal_best_scores[neighbors])\n                \n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                if neighborhood_best < self.global_best_score:\n                    self.velocities[i] += social_constant * r2 * (self.personal_best_positions[neighbors[np.argmin(self.personal_best_scores[neighbors])]] - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Store best positions in memory and use memory for diversification\n            if len(self.memory) < self.population_size:\n                self.memory.append(self.global_best_position)\n            else:\n                self.memory[np.random.randint(self.population_size)] = self.global_best_position\n\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            # Use memory positions to update elite particles\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            for idx in elite_idxs:\n                if np.random.rand() < 0.1:  # Small chance to diversify using memory\n                    self.particles[idx] = self.memory[np.random.choice(len(self.memory))]\n                else:\n                    self.particles[idx] = self.personal_best_positions[idx]\n\n        return self.global_best_position", "configspace": "", "generation": 37, "feedback": "The algorithm AdaptiveMemorySwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12329 with standard deviation 0.01746.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.13305042657462962, 0.1329356412156728, 0.13410516861686628, 0.1374141386554375, 0.13728334199299574, 0.13862307452352018, 0.09863107804904081, 0.09859043085562114, 0.09899492789773567]}}
{"id": "1f044f70-a498-427d-803b-0e058fd7195f", "fitness": 0.1268662407910616, "name": "AdvancedSwarmOptimizer", "description": "Integrating adaptive swarm intelligence with dynamic regrouping and chaos-inspired perturbations to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdvancedSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        chaos_strength = 0.05  # for chaos-based perturbations\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Chaos-inspired perturbation for enhanced exploration\n            if np.random.rand() < chaos_strength:\n                perturbation = np.random.normal(0, 0.1, self.dim)\n                self.particles += perturbation\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n\n            # Dynamic regrouping for enhanced exploitation\n            if self.fitness_function_calls % (self.budget // 5) == 0:\n                sorted_idxs = np.argsort(self.personal_best_scores)\n                regroup_count = self.population_size // 4\n                for idx in range(regroup_count):\n                    self.particles[sorted_idxs[idx]] = np.copy(self.global_best_position + np.random.normal(0, 0.1, self.dim))\n                    self.particles[sorted_idxs[idx]] = np.clip(self.particles[sorted_idxs[idx]], self.lower_bound, self.upper_bound)\n\n        return self.global_best_position", "configspace": "", "generation": 38, "feedback": "The algorithm AdvancedSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12687 with standard deviation 0.01947.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.13302344444670822, 0.14310966771345945, 0.13687279313492273, 0.13738381594170335, 0.14934289311458604, 0.1419632822504877, 0.09862094798109589, 0.10167688748444448, 0.09980243505214681]}}
{"id": "45955040-7a04-43fc-99a1-57cc26a414d6", "fitness": 0.12377590330838222, "name": "AdaptiveMomentumSwarmOptimizer", "description": "Adaptive Momentum Swarm Optimizer leverages dynamic momentum and adaptive elitism to enhance convergence speed and solution quality.", "code": "import numpy as np\n\nclass AdaptiveMomentumSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)  \n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        momentum_min, momentum_max = 0.4, 0.9\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            momentum = momentum_max - (momentum_max - momentum_min) * progress_ratio  \n            inertia_weight = momentum * (0.5 + 0.5 * np.random.rand())\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            elite_idxs = np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)]\n            for idx in elite_idxs:\n                self.particles[idx] = self.personal_best_positions[idx]\n\n        return self.global_best_position", "configspace": "", "generation": 39, "feedback": "The algorithm AdaptiveMomentumSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12378 with standard deviation 0.01769.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.1351853975571914, 0.13316001323055338, 0.13349617511195078, 0.1398978207323449, 0.13754009582099413, 0.1379236429294628, 0.09932479847046238, 0.09866844347853165, 0.0987867424439487]}}
{"id": "3b747b20-d13a-4f30-81f9-327b35048cc3", "fitness": 0.12326173438998245, "name": "EnhancedSwarmOptimizerV2", "description": "Enhanced exploitation capability by incorporating dynamic inertia weight tuned to converge faster in later stages of the optimization process.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)  # Adaptive velocity clamping\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.6 * (1 - progress_ratio)  # Adjusted inertia to enhance late-stage convergence\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Opposition-based learning for diversification\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedSwarmOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12326 with standard deviation 0.01744.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.13311551611720474, 0.1333641836913101, 0.1335018461813775, 0.13748881984828853, 0.137774143322405, 0.13793006041799782, 0.09865341763000046, 0.09873881168398713, 0.09878881061727063]}}
{"id": "eaf5fab9-9f14-4cb9-8ff1-5348ecd79cfe", "fitness": 0.12324122630440731, "name": "AdaptiveDynamicSwarmOptimizer", "description": "Adaptive Dynamic Swarm Optimization (ADSO): Integrates an adaptive dynamic swarm strategy with non-linear inertia and diversity-driven selection to enhance exploration and convergence.", "code": "import numpy as np\n\nclass AdaptiveDynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.9 - 0.5 * (progress_ratio ** 2)  # Non-linear inertia adjustment\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Diversity-driven selection strategy\n            diversity_measure = np.std(self.particles, axis=0).mean()\n            if diversity_measure < 0.1:  # Threshold for diversity\n                mutation_strength = (self.upper_bound - self.lower_bound) * 0.05 * (1 - progress_ratio)\n                mutation = np.random.uniform(-mutation_strength, mutation_strength, self.particles.shape)\n                self.particles += mutation\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n\n            # Improved elitist strategy to preserve better solutions\n            elite_size = max(1, self.population_size // 10)\n            elite_idxs = np.argsort(self.personal_best_scores)[:elite_size]\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(elite_size):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 41, "feedback": "The algorithm AdaptiveDynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12324 with standard deviation 0.01744.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.13319717950633003, 0.13306892557084526, 0.13364155960703317, 0.13758298111685308, 0.13743506659441973, 0.13809239944578333, 0.09868086849739721, 0.09863784202771764, 0.09883421437328621]}}
{"id": "fc2e7537-1d5a-42c7-a41c-6c876b6caa4a", "fitness": 0.20646457672535973, "name": "AdaptiveOppositionBasedPSO", "description": "Adaptive Opposition-Based Particle Swarm Optimization (AOB-PSO) improves convergence by dynamically adjusting exploration based on fitness trends and employing adaptive inertia for optimal balance between exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveOppositionBasedPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        progress_ratio_prev = 0\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.3 * np.random.rand() * (1 - progress_ratio) if progress_ratio > progress_ratio_prev else 0.4  # Adaptive inertia\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n            progress_ratio_prev = progress_ratio\n\n        return self.global_best_position", "configspace": "", "generation": 42, "feedback": "The algorithm AdaptiveOppositionBasedPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20646 with standard deviation 0.11880.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.13340307941015528, 0.1331688789603349, 0.3802320884422905, 0.13783741872705746, 0.1375493712441852, 0.3834669024191033, 0.0987321787635771, 0.09867266987354939, 0.35511860268798423]}}
{"id": "10d00e23-291e-45d1-abf7-c784fac7cfca", "fitness": 0.1456295733187093, "name": "HybridSwarmOptimizer", "description": "Hybrid swarm optimizer integrating opposition-based learning with adaptive elitism and dynamic velocity control for enhanced convergence and diversity.", "code": "import numpy as np\n\nclass HybridSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.3 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            elite_size = max(1, self.population_size // 10)\n            elite_idxs = np.argsort(self.personal_best_scores)[:elite_size]\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(elite_size):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 43, "feedback": "The algorithm HybridSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.14563 with standard deviation 0.04683.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.13673021934090723, 0.22598766175066853, 0.1335561160129608, 0.1416945202453853, 0.23001624489796813, 0.13799238548409487, 0.09982455243895572, 0.10605709717382072, 0.09880736252362254]}}
{"id": "bce11476-9b8a-414f-a2c3-944a075071c6", "fitness": 0.2647456505507109, "name": "EnhancedSwarmOptimizerV3", "description": "Enhanced exploration and exploitation using adaptive inertia and dynamic opposition learning to improve convergence.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.3 + 0.7 * np.random.rand() * (1 - progress_ratio)  # Increased variability in inertia\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Dynamic opposition learning for better exploration\n            random_factors = np.random.rand(self.population_size, self.dim)\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles + random_factors\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            # Improved elitist strategy with diversity preservation\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 5)], max(1, self.population_size // 10), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedSwarmOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26475 with standard deviation 0.20080.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.13329025954884266, 0.13311647916902203, 0.5531152297729589, 0.13769049577146808, 0.13748950986813002, 0.5553904194044424, 0.09871182763420272, 0.09865432629990578, 0.535252307487426]}}
{"id": "ba51cd99-f973-4c87-b405-32a061e2f7de", "fitness": 0.12480496250140977, "name": "AdaptiveSwarmOptimizer", "description": "Adaptive swarm optimizer utilizing multi-phase inertia adjustment and diversity-driven opposition to enhance convergence efficiency.", "code": "import numpy as np\n\nclass AdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n                self.fitness_function_calls += 1\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.2 + 0.6 * (1 - (progress_ratio ** 2))  # Multi-phase inertia adjustment\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Diversity-driven opposition learning\n            if np.random.rand() < 0.5:\n                opposition_particles = self.lower_bound + self.upper_bound - self.particles\n                for i in range(self.population_size):\n                    if self.fitness_function_calls >= self.budget:\n                        break\n\n                    opp_fitness = func(opposition_particles[i])\n                    if opp_fitness < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = opp_fitness\n                        self.personal_best_positions[i] = opposition_particles[i]\n\n                        if opp_fitness < self.global_best_score:\n                            self.global_best_score = opp_fitness\n                            self.global_best_position = opposition_particles[i]\n\n                    self.fitness_function_calls += 1\n\n            # Improved elitist strategy\n            elite_idxs = np.argsort(self.personal_best_scores)[:max(1, self.population_size // 5)]\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                if np.random.rand() < 0.1:\n                    self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 45, "feedback": "The algorithm AdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12480 with standard deviation 0.01823.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.13578612567989312, 0.13362676979203525, 0.13588746815661412, 0.14059846961596367, 0.1380747053495538, 0.14159832252279259, 0.09951622733517484, 0.09882993093731163, 0.09932664312334893]}}
{"id": "969d50d7-81a9-4b98-bf61-25b41029a913", "fitness": -Infinity, "name": "EnhancedSwarmOptimizerV2", "description": "Introduce dynamic population size adjustment to foster adaptability and enhance convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)  # Adaptive velocity clamping\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)  # Adjusted inertia with stochastic element\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            # Dynamic population size adjustment\n            self.population_size = int(min(50, max(10, dim * 2)) * (1 + 0.1 * np.sin(progress_ratio * np.pi)))\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            # Opposition-based learning for diversification\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 46, "feedback": "An exception occurred: IndexError('index 50 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 50 is out of bounds for axis 0 with size 50')", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {}}
{"id": "10961946-fe53-4aa5-b144-2a489641b4a8", "fitness": 0.12307473735748446, "name": "AdaptiveNeighborhoodSwarmOptimizer", "description": "Introducing an adaptive neighborhood-based exploration strategy and dynamic opposition learning to enhance convergence speed and solution quality.", "code": "import numpy as np\n\nclass AdaptiveNeighborhoodSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        neighborhood_size = max(1, self.population_size // 10)\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                neighbors = np.random.choice(self.population_size, neighborhood_size, replace=False)\n                local_best_position = min(neighbors, key=lambda j: self.personal_best_scores[j])\n                \n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.personal_best_positions[local_best_position] - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            opposition_factor = 1 - progress_ratio\n            opposition_particles = self.lower_bound + self.upper_bound - self.particles + opposition_factor * (self.global_best_position - self.particles)\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                opp_fitness = func(opposition_particles[i])\n                self.fitness_function_calls += 1\n\n                if opp_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = opp_fitness\n                    self.personal_best_positions[i] = opposition_particles[i]\n\n                    if opp_fitness < self.global_best_score:\n                        self.global_best_score = opp_fitness\n                        self.global_best_position = opposition_particles[i]\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:neighborhood_size], max(1, neighborhood_size // 2), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 47, "feedback": "The algorithm AdaptiveNeighborhoodSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12307 with standard deviation 0.01737.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.13273041888834336, 0.1331797574073018, 0.13339601658819078, 0.1370486653367914, 0.13756289973481217, 0.13780894048239378, 0.09851890128707763, 0.09867497028437566, 0.09875206620807353]}}
{"id": "c0e0019d-f3e4-441b-afdb-50b8bb800566", "fitness": 0.4120812810975648, "name": "EnhancedSwarmOptimizerV3", "description": "Incorporating self-adaptive learning mechanism and diversity control to enhance convergence and avoid premature stagnation.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                # Reinitialize some particles to improve diversity\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 48, "feedback": "The algorithm EnhancedSwarmOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41208 with standard deviation 0.11174.", "error": "", "parent_ids": ["aa65d2dd-4992-44e5-a1ab-1289bb29ee1b"], "operator": null, "metadata": {"aucs": [0.30324695554649617, 0.5641843530781332, 0.4355296680450461, 0.3070989390627049, 0.5664805359103944, 0.4391321979906784, 0.21623098071916325, 0.46593956510057666, 0.4108883344248905]}}
{"id": "f8838918-11c6-4243-8f9e-6a64fcbc6e77", "fitness": 0.3125894357228025, "name": "EnhancedSwarmOptimizerV3", "description": "Enhanced particle reinitialization strategy to dynamically increase exploration and improve convergence.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                # Reinitialize some particles to improve diversity\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 3, replace=False)  # Modified line\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 49, "feedback": "The algorithm EnhancedSwarmOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31259 with standard deviation 0.10274.", "error": "", "parent_ids": ["c0e0019d-f3e4-441b-afdb-50b8bb800566"], "operator": null, "metadata": {"aucs": [0.19546519906614634, 0.40076806859716196, 0.42321380837419864, 0.19970605207578218, 0.403764721759282, 0.4268067091407498, 0.15289701122716415, 0.27740498178450657, 0.33327836948023126]}}
{"id": "325a70f7-186f-441e-a36f-2ac665d3bbc7", "fitness": 0.12303588215612951, "name": "DynamicNeighborhoodPSO", "description": "Introducing a dynamic neighborhood topology and adaptive inertia weight to further enhance exploration and exploitation balance in particle swarm optimization.", "code": "import numpy as np\n\nclass DynamicNeighborhoodPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n\n        neighborhood_size = max(2, self.population_size // 5)  # Dynamic neighborhood size\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.9 - 0.5 * progress_ratio  # Adaptive inertia weight\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                \n                # Determine local best within a neighborhood\n                neighbors_indices = np.random.choice(self.population_size, neighborhood_size, replace=False)\n                local_best_index = neighbors_indices[np.argmin(self.personal_best_scores[neighbors_indices])]\n                local_best_position = self.personal_best_positions[local_best_index]\n\n                # Update velocity with local best influence\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (local_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                # Reinitialize some particles to improve diversity\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 50, "feedback": "The algorithm DynamicNeighborhoodPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12304 with standard deviation 0.01735.", "error": "", "parent_ids": ["c0e0019d-f3e4-441b-afdb-50b8bb800566"], "operator": null, "metadata": {"aucs": [0.1327349661064663, 0.13303647886335013, 0.13339426693441658, 0.13705383427467854, 0.1373980372154242, 0.1378069389751373, 0.09852052976472303, 0.09862642983721626, 0.09875145743375324]}}
{"id": "efdbefc2-7224-421f-953d-d989cfe59495", "fitness": 0.4019971974337687, "name": "EnhancedSwarmOptimizerV4", "description": "Introducing adaptive memory for dynamic learning coefficients and implementing a hybrid local search to improve convergence and escape local optima.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV4:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.memory = np.random.rand(self.population_size, dim)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 15\n        no_progress_count = 0\n        last_best_score = np.inf\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n\n            # Adaptive memory for learning coefficients\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio + np.mean(self.memory) * 0.1\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio + np.std(self.memory) * 0.1\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                # Reinitialize some particles to improve diversity\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 5, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            # Hybrid local search: Gaussian perturbation on top performers\n            elite_idxs = np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)]\n            for idx in elite_idxs:\n                perturbed = self.personal_best_positions[idx] + np.random.normal(0, 0.1, self.dim)\n                perturbed = np.clip(perturbed, self.lower_bound, self.upper_bound)\n                if self.fitness_function_calls < self.budget:\n                    perturbed_fitness = func(perturbed)\n                    self.fitness_function_calls += 1\n                    if perturbed_fitness < self.personal_best_scores[idx]:\n                        self.personal_best_scores[idx] = perturbed_fitness\n                        self.personal_best_positions[idx] = perturbed\n\n        return self.global_best_position", "configspace": "", "generation": 51, "feedback": "The algorithm EnhancedSwarmOptimizerV4 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40200 with standard deviation 0.04804.", "error": "", "parent_ids": ["c0e0019d-f3e4-441b-afdb-50b8bb800566"], "operator": null, "metadata": {"aucs": [0.40968407264773365, 0.384013050216678, 0.47198642335551066, 0.41295960491302164, 0.38766311882761817, 0.47625726531971624, 0.38493438807790614, 0.307061985232572, 0.38341486831316207]}}
{"id": "a62c793a-e6c1-4e06-9c91-53fab29cec6f", "fitness": 0.1252454011896511, "name": "EnhancedSwarmOptimizerV4", "description": "EnhancedSwarmOptimizerV4: Introducing adaptive inertia weight scaling, elite perturbation, and decay-based reinitialization to boost convergence and escape local optima.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV4:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n        max_inertia_weight, min_inertia_weight = 0.9, 0.4\n        elite_perturbation_strength = 0.1\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = max_inertia_weight - progress_ratio * (max_inertia_weight - min_inertia_weight)\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            perturbation = elite_perturbation_strength * np.random.uniform(-1, 1, elite_positions.shape)\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = np.clip(elite_positions[i] + perturbation[i], self.lower_bound, self.upper_bound)\n\n        return self.global_best_position", "configspace": "", "generation": 52, "feedback": "The algorithm EnhancedSwarmOptimizerV4 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12525 with standard deviation 0.01844.", "error": "", "parent_ids": ["c0e0019d-f3e4-441b-afdb-50b8bb800566"], "operator": null, "metadata": {"aucs": [0.1332196471344177, 0.135587997472992, 0.13834255939773787, 0.1376077762464455, 0.14033861872916986, 0.14373455994462236, 0.09868987609841906, 0.09948594806212185, 0.10020162762093388]}}
{"id": "19850529-c9d7-4d8d-bc8c-8b98c820227b", "fitness": 0.12274202206000522, "name": "EnhancedSwarmOptimizerV3", "description": "Introducing adaptive inertia weight based on fitness improvement detection to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio + (last_best_score - self.global_best_score) / last_best_score)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                # Reinitialize some particles to improve diversity\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 53, "feedback": "The algorithm EnhancedSwarmOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12274 with standard deviation 0.01731.", "error": "", "parent_ids": ["c0e0019d-f3e4-441b-afdb-50b8bb800566"], "operator": null, "metadata": {"aucs": [0.13260302130861235, 0.13242649246722027, 0.1332009237198627, 0.13693140925746072, 0.1367300684447157, 0.13761447273918215, 0.09834159292591882, 0.09827933320541149, 0.09855088447166283]}}
{"id": "a4d76eb8-26ff-42f2-8778-0d667867c647", "fitness": 0.12319584162624252, "name": "EnhancedSwarmOptimizerV4", "description": "Implement a dynamic neighborhood topology and adaptive mutation strategy to enhance solution exploration and avoid premature convergence.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV4:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.neighbor_size = max(2, self.population_size // 10)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n\n                # Dynamic neighborhood topology\n                neighbors_idx = np.random.choice(self.population_size, self.neighbor_size, replace=False)\n                local_best = min(neighbors_idx, key=lambda idx: self.personal_best_scores[idx])\n\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.personal_best_positions[local_best] - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                # Adaptive mutation strategy\n                mutation_size = self.population_size // 4\n                mutation_indices = np.random.choice(self.population_size, size=mutation_size, replace=False)\n                for idx in mutation_indices:\n                    dimension_to_mutate = np.random.randint(self.dim)\n                    mutation_strength = np.random.uniform(-0.5, 0.5) * (self.upper_bound - self.lower_bound)\n                    self.particles[idx][dimension_to_mutate] += mutation_strength\n                    self.particles[idx] = np.clip(self.particles[idx], self.lower_bound, self.upper_bound)\n                no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 54, "feedback": "The algorithm EnhancedSwarmOptimizerV4 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12320 with standard deviation 0.01742.", "error": "", "parent_ids": ["c0e0019d-f3e4-441b-afdb-50b8bb800566"], "operator": null, "metadata": {"aucs": [0.1329081013648692, 0.13327118141941152, 0.13356419345330206, 0.13725162102446153, 0.13766689933976206, 0.13800145641560746, 0.09858114718106015, 0.09870757228124216, 0.09881040215646675]}}
{"id": "97b0e8bb-59b7-49b6-8183-d57482981ed2", "fitness": 0.12309411553075246, "name": "EnhancedSwarmOptimizerV4", "description": "EnhancedSwarmOptimizerV4: Integrating adaptive inertia and quantum-inspired particle exploration for robust convergence and diversity maintenance.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV4:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.9 - 0.5 * progress_ratio\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n            # Quantum-inspired exploration\n            explore_idxs = np.random.choice(self.population_size, size=self.population_size // 10, replace=False)\n            for idx in explore_idxs:\n                self.particles[idx] += np.random.normal(0, 0.1, self.dim)\n                self.particles[idx] = np.clip(self.particles[idx], self.lower_bound, self.upper_bound)\n\n        return self.global_best_position", "configspace": "", "generation": 55, "feedback": "The algorithm EnhancedSwarmOptimizerV4 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12309 with standard deviation 0.01737.", "error": "", "parent_ids": ["c0e0019d-f3e4-441b-afdb-50b8bb800566"], "operator": null, "metadata": {"aucs": [0.13286189727697673, 0.13306004615189537, 0.13345406514065405, 0.1371987840219816, 0.1374249081130714, 0.13787554128615342, 0.09856504528610766, 0.09863475381783227, 0.09877199868209974]}}
{"id": "345aaa5f-0667-487a-81e7-ddeb2a668683", "fitness": 0.1628765383357945, "name": "DynamicInertiaSwarmOptimizer", "description": "Introducing dynamic inertia weight adaptation and elite preservation strategy to enhance exploration and exploitation balance for improved convergence.", "code": "import numpy as np\n\nclass DynamicInertiaSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.9 - 0.5 * progress_ratio  # Dynamic inertia weight\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            elite_idxs = np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)]\n            for idx in elite_idxs:\n                if np.random.rand() < 0.5:\n                    self.particles[idx] = self.personal_best_positions[idx]\n\n        return self.global_best_position", "configspace": "", "generation": 56, "feedback": "The algorithm DynamicInertiaSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.16288 with standard deviation 0.05892.", "error": "", "parent_ids": ["c0e0019d-f3e4-441b-afdb-50b8bb800566"], "operator": null, "metadata": {"aucs": [0.13612479095422758, 0.13421904639135562, 0.2600759763512688, 0.14118303351441674, 0.13875630402901995, 0.2638926852671434, 0.09950253888252047, 0.09903063758905206, 0.19310383204314574]}}
{"id": "57bb942c-7873-48e8-9e0f-3e16c72499fb", "fitness": 0.12525470087089452, "name": "EnhancedSwarmOptimizerV3", "description": "Enhanced velocity update to improve swarm exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i] + 0.01 * (np.random.rand(self.dim) - 0.5)  # Enhanced velocity update\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                # Reinitialize some particles to improve diversity\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 57, "feedback": "The algorithm EnhancedSwarmOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12525 with standard deviation 0.01872.", "error": "", "parent_ids": ["c0e0019d-f3e4-441b-afdb-50b8bb800566"], "operator": null, "metadata": {"aucs": [0.14073605852290905, 0.13301209625030563, 0.13339354496109512, 0.14688747032082206, 0.13737027401258783, 0.13780611308164725, 0.10071777927722125, 0.0986177651862038, 0.09875120622525857]}}
{"id": "e95945ee-6014-4cdd-8d5a-686bb4b5eabf", "fitness": 0.13028622866193296, "name": "EnhancedSwarmOptimizerV4", "description": "Incorporating adaptive inertia weights and dynamic diversity reinitialization to further enhance convergence and escape local optima.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV4:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.5 + 0.4 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                # Dynamic reinitialization strategy\n                num_to_reinit = int(self.population_size * (0.1 + 0.1 * np.random.rand()))\n                reinit_indices = np.random.choice(self.population_size, size=num_to_reinit, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (num_to_reinit, self.dim))\n                no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)]\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 58, "feedback": "The algorithm EnhancedSwarmOptimizerV4 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13029 with standard deviation 0.02526.", "error": "", "parent_ids": ["c0e0019d-f3e4-441b-afdb-50b8bb800566"], "operator": null, "metadata": {"aucs": [0.13285967697185097, 0.13271984007608362, 0.16456831346262724, 0.13719646711580435, 0.137036579489615, 0.17013227673431897, 0.0985639633555776, 0.09851519886112947, 0.10098374189038928]}}
{"id": "283dc481-7a1e-41f0-95fb-48e25e631323", "fitness": 0.12312537178443345, "name": "EnhancedSwarmOptimizerV3", "description": "Enhanced convergence by fine-tuning inertia weight decay and dynamic stagnation threshold.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.7 + 0.1 * np.random.rand() * (1 - progress_ratio)  # Modified decay\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                # Reinitialize some particles to improve diversity\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 59, "feedback": "The algorithm EnhancedSwarmOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12313 with standard deviation 0.01739.", "error": "", "parent_ids": ["c0e0019d-f3e4-441b-afdb-50b8bb800566"], "operator": null, "metadata": {"aucs": [0.13292171765047023, 0.1330541199220231, 0.13351309698072855, 0.13726763334524394, 0.13741813732566333, 0.13794294380758565, 0.09858530968519874, 0.09863268006972148, 0.09879270727326595]}}
{"id": "2180a013-f12a-4ef6-829a-64ab22282a74", "fitness": 0.29908710389928034, "name": "EnhancedSwarmOptimizerV4", "description": "Introducing adaptive elitism and dynamic inertia weight adjustment to balance exploration-exploitation trade-off and enhance convergence rate.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV4:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.7 - 0.5 * progress_ratio * np.random.rand()\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 5)], max(1, self.population_size // 10), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            self.particles[elite_idxs] = elite_positions\n\n        return self.global_best_position", "configspace": "", "generation": 60, "feedback": "The algorithm EnhancedSwarmOptimizerV4 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29909 with standard deviation 0.08810.", "error": "", "parent_ids": ["c0e0019d-f3e4-441b-afdb-50b8bb800566"], "operator": null, "metadata": {"aucs": [0.424159831577969, 0.26853426941625613, 0.2533098070684171, 0.42762016353351484, 0.27286908492156015, 0.2571954235210827, 0.39956783292273734, 0.20262549140932873, 0.1859020307226572]}}
{"id": "787259a3-3b49-4e89-887d-88879b1bec05", "fitness": 0.3643772105128156, "name": "EnhancedSwarmOptimizerV4", "description": "EnhancedSwarmOptimizerV4: Introduces adaptive velocity clamping and dynamic reinitialization to better balance exploration and exploitation.  ", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV4:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp * (1 + 0.5 * progress_ratio), self.velocity_clamp * (1 + 0.5 * progress_ratio))\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                # Dynamic reinitialization to enhance exploration\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 3, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedSwarmOptimizerV4 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.36438 with standard deviation 0.13690.", "error": "", "parent_ids": ["c0e0019d-f3e4-441b-afdb-50b8bb800566"], "operator": null, "metadata": {"aucs": [0.3543589970539275, 0.2851317922880424, 0.5894548128697088, 0.3577348923467746, 0.28912893538788, 0.6267664438676037, 0.3086243369407464, 0.2380591391607343, 0.23013554469992203]}}
{"id": "9e8a4171-3c61-44aa-8d04-1079ed768fcf", "fitness": 0.3940390002543421, "name": "EnhancedSwarmOptimizerV3", "description": "Enhance global exploration by introducing small velocity perturbations to particles when progress stagnates.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.2 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                # Reinitialize some particles to improve diversity\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                self.velocities[reinit_indices] *= 0.1  # Add small perturbations to velocities\n                no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 62, "feedback": "The algorithm EnhancedSwarmOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.39404 with standard deviation 0.09083.", "error": "", "parent_ids": ["c0e0019d-f3e4-441b-afdb-50b8bb800566"], "operator": null, "metadata": {"aucs": [0.30324695554649617, 0.48822384274485797, 0.431634185024234, 0.3070989390627049, 0.49117354507506394, 0.43492528265815034, 0.21623098071916325, 0.4665981663758906, 0.4072191050825176]}}
{"id": "15418938-4a77-4975-bc75-95edd96d02b3", "fitness": 0.45757852586442344, "name": "EnhancedSwarmOptimizerV4", "description": "Introduce adaptive inertia weight and chaos-driven exploration to bolster convergence speed and solution diversity.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV4:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n\n        # Introducing chaos-driven exploration factor\n        chaos_factor = np.random.uniform(0, 1, self.dim)\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + chaos_factor * 0.2 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                # Reinitialize some particles to improve diversity\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 63, "feedback": "The algorithm EnhancedSwarmOptimizerV4 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.45758 with standard deviation 0.08133.", "error": "", "parent_ids": ["c0e0019d-f3e4-441b-afdb-50b8bb800566"], "operator": null, "metadata": {"aucs": [0.47449716372607753, 0.42137947441003365, 0.558862166783543, 0.47711177730702836, 0.42502136374569277, 0.5612761839432199, 0.3096216212056774, 0.3637223006818566, 0.5267146809766814]}}
{"id": "9a32f78d-ad72-4442-a867-30489b0ceb92", "fitness": 0.12337222804119236, "name": "EnhancedSwarmOptimizerV5", "description": "Implement feedback-driven dynamic attraction-repulsion strategy with history-guided mutation to enhance convergence efficiency and solution robustness.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV5:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial, c3_initial = 2.5, 2.5, 0.5\n        c1_final, c2_final, c3_final = 0.5, 0.5, 1.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n\n        # Chaos-driven exploration factor adjusted with feedback\n        chaos_factor = np.random.uniform(0, 1, self.dim)\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + chaos_factor * 0.2 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n            mutation_constant = c3_initial + (c3_final - c3_initial) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n                # History-guided mutation\n                if np.random.rand() < mutation_constant:\n                    historical_best = self.personal_best_positions[np.random.choice(range(self.population_size))]\n                    self.particles[i] += 0.1 * (historical_best - self.particles[i])\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 64, "feedback": "The algorithm EnhancedSwarmOptimizerV5 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12337 with standard deviation 0.01749.", "error": "", "parent_ids": ["15418938-4a77-4975-bc75-95edd96d02b3"], "operator": null, "metadata": {"aucs": [0.13364057177495992, 0.13291411035056377, 0.13382585851520667, 0.1380889613968852, 0.1372583435266146, 0.13830153944017998, 0.09883679410397073, 0.09858344951261755, 0.09890042374973274]}}
{"id": "0bd51457-8ad6-4b8e-bf8b-7e268208abf2", "fitness": 0.19626991581243183, "name": "EnhancedSwarmOptimizerV4", "description": "Enhance the algorithm by introducing dynamic reinitialization based on diversity metrics to maintain exploration and prevent premature convergence.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV4:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n\n        # Introducing chaos-driven exploration factor\n        chaos_factor = np.random.uniform(0, 1, self.dim)\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + chaos_factor * 0.2 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                diversity = np.mean(np.std(self.particles, axis=0)) # Line changed\n                reinit_ratio = 1 - diversity / (self.upper_bound - self.lower_bound) # Line changed\n                reinit_indices = np.random.choice(self.population_size, size=int(self.population_size * reinit_ratio), replace=False) # Line changed\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim)) # Line changed\n                no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 65, "feedback": "The algorithm EnhancedSwarmOptimizerV4 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19627 with standard deviation 0.10713.", "error": "", "parent_ids": ["15418938-4a77-4975-bc75-95edd96d02b3"], "operator": null, "metadata": {"aucs": [0.13344105619239677, 0.13316402366533442, 0.37081921590376776, 0.13786211269075643, 0.1375470111427971, 0.37414017692985524, 0.09876548090855486, 0.09866683515568642, 0.2820233297227376]}}
{"id": "9a82b56b-e54a-44ad-b8fb-f640b5792057", "fitness": 0.31609890249287553, "name": "EnhancedSwarmOptimizerV5", "description": "Introduce stochastic adaptive parameters and dynamic particle reinitialization to enhance convergence and escape local optima.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV5:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 8  # (Changed from 10 to 8)\n        no_progress_count = 0\n        last_best_score = np.inf\n\n        chaos_factor = np.random.uniform(0, 1, self.dim)\n        np.random.shuffle(chaos_factor)  # (Added shuffle for diversity)\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + chaos_factor * 0.2 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 3, replace=False)  # (Changed size from 1/4 to 1/3)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 66, "feedback": "The algorithm EnhancedSwarmOptimizerV5 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31610 with standard deviation 0.14927.", "error": "", "parent_ids": ["15418938-4a77-4975-bc75-95edd96d02b3"], "operator": null, "metadata": {"aucs": [0.13726083153741175, 0.3443504342849467, 0.4848283852334162, 0.15062895010455135, 0.35026675030373056, 0.5228507118037722, 0.09969982552077494, 0.29494114006868843, 0.46006309357858743]}}
{"id": "1508d641-2f84-4111-b464-e0bf6d661b19", "fitness": 0.12304382011004078, "name": "EnhancedSwarmOptimizerV5", "description": "Incorporate Lvy flight for enhanced exploration and dynamic topology adaptation to bolster solution quality and convergence.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV5:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.topology_matrix = np.random.rand(self.population_size, self.population_size) < 0.5\n\n    def levy_flight(self, L, dim):\n        u = np.random.normal(0, 1, dim) * (L ** (-1.0 / 3))\n        return u\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n\n        chaos_factor = np.random.uniform(0, 1, self.dim)\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + chaos_factor * 0.2 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] += self.levy_flight(0.1, self.dim)  # Applying Levy flight\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n            \n            for i in range(self.population_size):\n                neighbors = np.where(self.topology_matrix[i])[0]\n                if neighbors.size > 0:\n                    neighbour_idx = np.random.choice(neighbors)\n                    if self.personal_best_scores[neighbour_idx] < self.personal_best_scores[i]:\n                        self.particles[i] = self.particles[neighbour_idx]\n                        self.personal_best_positions[i] = self.personal_best_positions[neighbour_idx]\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedSwarmOptimizerV5 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12304 with standard deviation 0.01735.", "error": "", "parent_ids": ["15418938-4a77-4975-bc75-95edd96d02b3"], "operator": null, "metadata": {"aucs": [0.13262435249765825, 0.13317229462576974, 0.13339776080364707, 0.136927681946299, 0.13755317625325514, 0.13781093580042703, 0.09848150542992185, 0.09867400058790898, 0.09875267304547997]}}
{"id": "adadf611-5eed-4e81-aa6a-2f354d35f0bc", "fitness": 0.4423617874211665, "name": "EnhancedSwarmOptimizerV4", "description": "Integrate a dynamic reinitialization threshold based on convergence progress to enhance exploration and prevent stagnation.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV4:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n\n        # Introducing chaos-driven exploration factor\n        chaos_factor = np.random.uniform(0, 1, self.dim)\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + chaos_factor * 0.2 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold * (1 - progress_ratio):\n                # Reinitialize some particles to improve diversity\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 68, "feedback": "The algorithm EnhancedSwarmOptimizerV4 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.44236 with standard deviation 0.25963.", "error": "", "parent_ids": ["15418938-4a77-4975-bc75-95edd96d02b3"], "operator": null, "metadata": {"aucs": [0.13292118730156433, 0.4878304469650556, 0.7554448344861422, 0.13726662731342798, 0.4991160559573188, 0.7567022459834025, 0.09858564743954235, 0.3677732996125582, 0.7456157417314861]}}
{"id": "317d23dd-8240-4767-9c9a-44a369cc97d1", "fitness": 0.12315473356555673, "name": "EnhancedSwarmOptimizerV5", "description": "Integrate Lvy flight for enhanced exploration and dynamic parameter scaling to improve convergence in complex landscapes.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV5:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n        chaos_factor = np.random.uniform(0, 1, self.dim)\n\n        def levy_flight(Lambda=1.5):\n            sigma = (np.math.gamma(1 + Lambda) * np.sin(np.pi * Lambda / 2) / \n                     (np.math.gamma((1 + Lambda) / 2) * Lambda * 2 ** ((Lambda - 1) / 2))) ** (1 / Lambda)\n            u = np.random.normal(0, sigma, size=self.dim)\n            v = np.random.normal(0, 1, size=self.dim)\n            step = u / abs(v) ** (1 / Lambda)\n            return step\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + chaos_factor * 0.2 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                levy_step = levy_flight()\n                self.velocities[i] += levy_step\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 69, "feedback": "The algorithm EnhancedSwarmOptimizerV5 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12315 with standard deviation 0.01740.", "error": "", "parent_ids": ["15418938-4a77-4975-bc75-95edd96d02b3"], "operator": null, "metadata": {"aucs": [0.13314884116467618, 0.1330524783242797, 0.13339354496109512, 0.13752668881638752, 0.13741626212022162, 0.13780611308164725, 0.09866536224206646, 0.09863210515437826, 0.09875120622525857]}}
{"id": "3f8e48e0-a2d1-4539-a0eb-cebbdb320bb4", "fitness": 0.3638663725140744, "name": "EnhancedSwarmOptimizerV4", "description": "Introduce variable chaos-driven exploration factor updated periodically to enhance exploration.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV4:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n\n        # Introducing chaos-driven exploration factor\n        chaos_factor = np.random.uniform(0, 1, self.dim)\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + chaos_factor * 0.2 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                # Reinitialize some particles to improve diversity\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n                # Update chaos-driven exploration factor periodically\n                chaos_factor = np.random.uniform(0, 1, self.dim)\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 70, "feedback": "The algorithm EnhancedSwarmOptimizerV4 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.36387 with standard deviation 0.17488.", "error": "", "parent_ids": ["15418938-4a77-4975-bc75-95edd96d02b3"], "operator": null, "metadata": {"aucs": [0.13303457420418086, 0.44912070360219547, 0.5350136598318164, 0.13739662514419226, 0.452557568309799, 0.5376286009598129, 0.09862470476618102, 0.41585597511248007, 0.5155649406960117]}}
{"id": "929bd9ee-793b-405a-a046-7bfc4bfc9476", "fitness": 0.19594690928896152, "name": "EnhancedSwarmOptimizerV5", "description": "Introduce a dynamic chaos factor and adaptive reinitialization mechanism to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV5:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n\n        while self.fitness_function_calls < self.budget:\n            chaos_factor = 0.5 * (1 + np.sin(2 * np.pi * self.fitness_function_calls / self.dim))\n            \n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + chaos_factor * 0.2 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 3, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            elite_idxs = np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)]\n            for idx in elite_idxs:\n                self.particles[idx] = self.personal_best_positions[idx]\n\n        return self.global_best_position", "configspace": "", "generation": 71, "feedback": "The algorithm EnhancedSwarmOptimizerV5 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19595 with standard deviation 0.06845.", "error": "", "parent_ids": ["15418938-4a77-4975-bc75-95edd96d02b3"], "operator": null, "metadata": {"aucs": [0.2144447523073384, 0.2375344266849737, 0.2677122734866566, 0.2192383553218289, 0.2439528397695475, 0.2729584330047551, 0.1012580698189317, 0.10319164046549056, 0.10323139274113136]}}
{"id": "8f2cce23-3f43-4d92-acdf-0e8ff308c489", "fitness": 0.45189093217025444, "name": "EnhancedSwarmOptimizerV4", "description": "Introduce a dynamic chaos factor to enhance exploration and avoid stagnation.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV4:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n\n        chaos_factor = np.random.uniform(0, 1, self.dim)\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n\n            # Change 1: Dynamic chaos factor adjustment\n            chaos_factor = np.random.uniform(0, 1, self.dim) * (1 - progress_ratio)\n\n            inertia_weight = 0.4 + chaos_factor * 0.2 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                # Change 2: More adaptive reinitialization based on chaos factor\n                reinit_indices = np.random.choice(self.population_size, size=int(self.population_size // (4 + chaos_factor.mean() * 4)), replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 72, "feedback": "The algorithm EnhancedSwarmOptimizerV4 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.45189 with standard deviation 0.08050.", "error": "", "parent_ids": ["15418938-4a77-4975-bc75-95edd96d02b3"], "operator": null, "metadata": {"aucs": [0.43300802134611516, 0.37500247265399167, 0.5649165883224, 0.45948924361775423, 0.3784089857443298, 0.5673914749750474, 0.4053366073054747, 0.34657451248203974, 0.5368904830851373]}}
{"id": "ac47721b-0198-4146-b317-bca137863c59", "fitness": 0.43310818414335256, "name": "EnhancedSwarmOptimizerV5", "description": "Leverage multi-phase chaotic exploration and adaptive elitism to enhance convergence robustness and maintain diversity.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV5:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n        chaos_phase_duration = self.budget // 3\n\n        # Introducing dynamic chaos exploration\n        chaos_factor = np.random.uniform(0, 1, self.dim)\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + chaos_factor * 0.2 * (1 - progress_ratio)\n\n            if self.fitness_function_calls < chaos_phase_duration:\n                # Enhance chaotic exploration in initial phase\n                chaos_factor = np.random.rand(self.dim)\n            elif self.fitness_function_calls < 2 * chaos_phase_duration:\n                # Transition phase to balance exploration and exploitation\n                inertia_weight *= 0.8\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                # Reinitialize some particles with enhanced diversity strategy\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                for idx in reinit_indices:\n                    self.particles[idx] = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n                    self.velocities[idx] = np.random.uniform(-1, 1, self.dim)\n                no_progress_count = 0\n\n            # Adaptive elitist strategy to ensure diverse high-quality solutions\n            elite_idxs = np.argsort(self.personal_best_scores)[:max(1, self.population_size // 5)]\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                if np.random.rand() < 0.5:  # Introduce probabilistic elitism refresh\n                    self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 73, "feedback": "The algorithm EnhancedSwarmOptimizerV5 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.43311 with standard deviation 0.07348.", "error": "", "parent_ids": ["15418938-4a77-4975-bc75-95edd96d02b3"], "operator": null, "metadata": {"aucs": [0.38440756598863257, 0.42207218659541246, 0.5418892693482642, 0.3877182205971804, 0.4254592018462011, 0.5446103804488708, 0.33172526820147574, 0.36143717445106527, 0.4986543898130702]}}
{"id": "73f20c06-0645-4f80-95af-88e71ef7e161", "fitness": 0.1230658112852108, "name": "EnhancedSwarmOptimizerV5", "description": "Enhance convergence by incorporating Lvy flight mechanism and adaptive chaotic perturbation to improve exploration and exploitation.", "code": "import numpy as np\nfrom scipy.special import gamma\n\nclass EnhancedSwarmOptimizerV5:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n\n        chaos_factor = np.random.uniform(0, 1, self.dim)\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + chaos_factor * 0.2 * (1 - progress_ratio)\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                levy_flight = self.levy(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] += levy_flight * self.chaotic_perturbation(progress_ratio, i)\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position\n\n    def levy(self, dim, beta=1.5):\n        sigma_u = (gamma(1 + beta) * np.sin(np.pi * beta / 2) / (gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma_u, dim)\n        v = np.random.normal(0, 1, dim)\n        return u / np.abs(v) ** (1 / beta)\n\n    def chaotic_perturbation(self, progress_ratio, seed):\n        chaotic_value = (3.5699456 * seed * (1 - seed)) % 1\n        return chaotic_value * (1 - progress_ratio)", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedSwarmOptimizerV5 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12307 with standard deviation 0.01736.", "error": "", "parent_ids": ["15418938-4a77-4975-bc75-95edd96d02b3"], "operator": null, "metadata": {"aucs": [0.13282742132802616, 0.13305278713845092, 0.13339354496109512, 0.13715933593549412, 0.1374166149196988, 0.13780611308164725, 0.09855306472698488, 0.0986322132502413, 0.09875120622525857]}}
{"id": "578c80ed-ffd8-4dbc-a90b-666929e26d38", "fitness": 0.33373036868277406, "name": "EnhancedSwarmOptimizerV4", "description": "Enhance the exploration potential by tweaking the chaos factor initialization range.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV4:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n\n        # Introducing chaos-driven exploration factor\n        chaos_factor = np.random.uniform(-0.5, 0.5, self.dim)  # Modified line\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + chaos_factor * 0.2 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                # Reinitialize some particles to improve diversity\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 75, "feedback": "The algorithm EnhancedSwarmOptimizerV4 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33373 with standard deviation 0.15117.", "error": "", "parent_ids": ["15418938-4a77-4975-bc75-95edd96d02b3"], "operator": null, "metadata": {"aucs": [0.41817728826463885, 0.1334476599001908, 0.4754750043532111, 0.4210962669420898, 0.13787683846044052, 0.4784596539135737, 0.3951235412786739, 0.09875887322780708, 0.44515819180434035]}}
{"id": "3be2fa90-5d4f-46c3-ad7f-1269963cb18f", "fitness": 0.12296083407535995, "name": "EnhancedSwarmOptimizerV5", "description": "Integrate dynamic subpopulation sizes and Lvy flight-based random walks to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV5:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n\n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (1.0 / np.power(np.random.normal(0, 1), 1 / lam))\n        return u / np.linalg.norm(u)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        stagnation_threshold = 10\n        no_progress_count = 0\n        last_best_score = np.inf\n        progress_ratio_threshold = 0.5\n        \n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.6 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                if progress_ratio < progress_ratio_threshold:\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n                    self.velocities[i] *= inertia_weight\n                    self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                    self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                else:\n                    self.velocities[i] = self.levy_flight()\n\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < last_best_score:\n                last_best_score = self.global_best_score\n                no_progress_count = 0\n            else:\n                no_progress_count += 1\n\n            if no_progress_count >= stagnation_threshold:\n                reinit_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                self.particles[reinit_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(reinit_indices), self.dim))\n                no_progress_count = 0\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 76, "feedback": "The algorithm EnhancedSwarmOptimizerV5 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12296 with standard deviation 0.01740.", "error": "", "parent_ids": ["15418938-4a77-4975-bc75-95edd96d02b3"], "operator": null, "metadata": {"aucs": [0.13284138881140062, 0.13297814414363152, 0.13320092371983328, 0.13720371893996508, 0.13735979427316003, 0.13761447273919936, 0.0984250434702505, 0.09847313610913289, 0.09855088447166616]}}
{"id": "81ca54d1-007a-48c4-8057-65b88388188f", "fitness": 0.49282721111463573, "name": "EnhancedSwarmOptimizerV5", "description": "Incorporate Levy flight dynamics for enhanced exploration and adaptive restart strategy to escape local optima.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV5:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                # Apply Levy flight for global exploration\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 77, "feedback": "The algorithm EnhancedSwarmOptimizerV5 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.49283 with standard deviation 0.08127.", "error": "", "parent_ids": ["15418938-4a77-4975-bc75-95edd96d02b3"], "operator": null, "metadata": {"aucs": [0.6286432758608584, 0.4306938022948079, 0.4624808440427578, 0.6305093550759344, 0.4474155302497036, 0.4688920373984804, 0.5371058282478036, 0.3988894265352222, 0.43081480032615294]}}
{"id": "e5871352-8a99-481a-b040-39fa08cc08c9", "fitness": 0.32914566535133205, "name": "EnhancedSwarmOptimizerV5", "description": "Enhance global exploration by dynamically adjusting the search scope based on population diversity.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV5:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                # Dynamically adjust Levy flight step size based on diversity\n                diversity = np.std(self.particles, axis=0)\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim)) * diversity\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedSwarmOptimizerV5 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32915 with standard deviation 0.21049.", "error": "", "parent_ids": ["81ca54d1-007a-48c4-8057-65b88388188f"], "operator": null, "metadata": {"aucs": [0.6286432758608584, 0.1330525285869718, 0.3397537707692704, 0.6305093550759344, 0.13741631976487179, 0.3432706742655638, 0.5371058282478036, 0.0986321224326222, 0.11392711315809223]}}
{"id": "3e5de3de-1314-4b97-a24d-6d504af07f51", "fitness": 0.12308301842949065, "name": "AdaptiveSwarmOptimizer", "description": "Enhance convergence by integrating adaptive mutation based on fitness diversity and dynamic inertia adjustment for improved exploitation.", "code": "import numpy as np\n\nclass AdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def adaptive_mutation(self):\n        diversity = np.std(self.personal_best_scores)\n        mutation_rate = 0.1 * np.exp(-diversity)\n        mutation_indices = np.random.choice(self.population_size, size=int(self.population_size * mutation_rate), replace=False)\n        mutation_steps = self.levy_flight((len(mutation_indices), self.dim))\n        self.particles[mutation_indices] += mutation_steps\n        self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.9 - 0.5 * progress_ratio\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                # Apply Levy flight for global exploration\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            # Perform adaptive mutation based on fitness diversity\n            self.adaptive_mutation()\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 79, "feedback": "The algorithm AdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12308 with standard deviation 0.01737.", "error": "", "parent_ids": ["81ca54d1-007a-48c4-8057-65b88388188f"], "operator": null, "metadata": {"aucs": [0.13298054360303946, 0.1329618178976345, 0.13339354496109512, 0.13733414122974363, 0.13731293884682994, 0.13780611308164725, 0.09860684120759022, 0.0986000188125773, 0.09875120622525857]}}
{"id": "46603d6f-d3e9-4672-869c-42a4f345e9c7", "fitness": 0.12302131089695233, "name": "EnhancedSwarmOptimizerV6", "description": "Introduce adaptive parameter tuning and dynamic inertia weight adjustments to improve convergence rate and solution quality.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV6:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.7 + 0.3 * np.random.rand() * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 80, "feedback": "The algorithm EnhancedSwarmOptimizerV6 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12302 with standard deviation 0.01734.", "error": "", "parent_ids": ["81ca54d1-007a-48c4-8057-65b88388188f"], "operator": null, "metadata": {"aucs": [0.13266539069094951, 0.13305417317456203, 0.13339354496109512, 0.13697447488430314, 0.13741819817310907, 0.13780611308164725, 0.0984959981861121, 0.09863269869553415, 0.09875120622525857]}}
{"id": "7ac678c1-ab86-45d4-974d-6abf3e041d31", "fitness": 0.12303304004339013, "name": "EnhancedSwarmOptimizerV5", "description": "Refine inertia weight to improve balance between exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV5:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.6 * (1 - progress_ratio) + 0.4  # Changed line\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                # Apply Levy flight for global exploration\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedSwarmOptimizerV5 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12303 with standard deviation 0.01735.", "error": "", "parent_ids": ["81ca54d1-007a-48c4-8057-65b88388188f"], "operator": null, "metadata": {"aucs": [0.13269630724154846, 0.13306560024464253, 0.13339354496109512, 0.13700973765677915, 0.13743125655283717, 0.13780611308164725, 0.09850690101538018, 0.09863669341132253, 0.09875120622525857]}}
{"id": "77a9992c-9dca-4670-bd88-709cd263e612", "fitness": 0.1233921707482841, "name": "EnhancedSwarmOptimizerV6", "description": "Enhance swarm optimization by integrating dynamic inertia weight adjustment and adaptive mutation for improved exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV6:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def adaptive_mutation(self, particle, best_position, mutation_rate):\n        if np.random.rand() < mutation_rate:\n            mutation_vector = np.random.normal(0, 1, self.dim)\n            particle += 0.1 * (best_position - particle) * mutation_vector\n            particle = np.clip(particle, self.lower_bound, self.upper_bound)\n        return particle\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n        mutation_rate = 0.1\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                # Evaluate fitness\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                # Update personal best\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                # Update global best\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.9 - progress_ratio * 0.5  # Dynamic inertia weight\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] = (inertia_weight * self.velocities[i] +\n                                      cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i]) +\n                                      social_constant * r2 * (self.global_best_position - self.particles[i]))\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n                # Apply adaptive mutation\n                self.particles[i] = self.adaptive_mutation(self.particles[i], self.global_best_position, mutation_rate)\n\n            # Check stagnation\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 82, "feedback": "The algorithm EnhancedSwarmOptimizerV6 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12339 with standard deviation 0.01750.", "error": "", "parent_ids": ["81ca54d1-007a-48c4-8057-65b88388188f"], "operator": null, "metadata": {"aucs": [0.1333722254061691, 0.1331109366142713, 0.13396999593441483, 0.13778185121644504, 0.13748313372162602, 0.13847573315617767, 0.09874361192784664, 0.09865244721996147, 0.0989396015376447]}}
{"id": "98814fc2-7dfc-46e1-9e28-dc62379eb442", "fitness": 0.12335835519437814, "name": "EnhancedSwarmOptimizerV6", "description": "Introduce dynamic population resizing and stochastic tunneling to enhance exploration and exploitation balance and avoid premature convergence.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV6:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, max(10, dim * 2))\n        self.population_size = self.initial_population_size\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def stochastic_tunneling(self, fitness):\n        T = 1.0  # Temperature parameter\n        return 1 - np.exp(-fitness / T)\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                modified_fitness = self.stochastic_tunneling(fitness)\n                self.fitness_function_calls += 1\n\n                if modified_fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = modified_fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if modified_fitness < self.global_best_score:\n                    self.global_best_score = modified_fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n            if self.fitness_function_calls % (self.budget // 4) == 0:\n                self.population_size = max(self.initial_population_size // 2, self.population_size // 2)\n                self.particles = self.particles[:self.population_size]\n                self.velocities = self.velocities[:self.population_size]\n                self.personal_best_positions = self.personal_best_positions[:self.population_size]\n                self.personal_best_scores = self.personal_best_scores[:self.population_size]\n\n        return self.global_best_position", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedSwarmOptimizerV6 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12336 with standard deviation 0.01749.", "error": "", "parent_ids": ["81ca54d1-007a-48c4-8057-65b88388188f"], "operator": null, "metadata": {"aucs": [0.13421966750352587, 0.13271776974537997, 0.13339354496109512, 0.13875553538093155, 0.13703437394143025, 0.13780611308164725, 0.09903273565012793, 0.0985142502600066, 0.09875120622525857]}}
{"id": "b2530d53-20b8-43cb-a2e7-49377c127d36", "fitness": 0.6675231943355523, "name": "EnhancedSwarmOptimizerV6", "description": "Integrate chaotic maps for enhanced diversity and dynamically adapt exploration-exploitation trade-off for improved convergence.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV6:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n        self.chaos_sequence = self.init_chaotic_sequence()\n\n    def init_chaotic_sequence(self):\n        x = 0.7  # Initial value for chaotic map\n        chaos_seq = []\n        for _ in range(self.population_size):\n            x = 4 * x * (1 - x)  # Logistic map\n            chaos_seq.append(x)\n        return np.array(chaos_seq)\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                chaotic_factor = self.chaos_sequence[i]\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] *= chaotic_factor  # Incorporate chaotic influence\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                # Apply Levy flight for global exploration\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 84, "feedback": "The algorithm EnhancedSwarmOptimizerV6 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.66752 with standard deviation 0.02497.", "error": "", "parent_ids": ["81ca54d1-007a-48c4-8057-65b88388188f"], "operator": null, "metadata": {"aucs": [0.6675475767545282, 0.6691065301886787, 0.6959171521839346, 0.6695789434889745, 0.670933837042299, 0.6982510949658124, 0.6409131630724629, 0.6134362934037626, 0.6820241579195172]}}
{"id": "91a9cfd3-1736-4a59-accb-773cdb000cec", "fitness": 0.3195101388150501, "name": "EnhancedSwarmOptimizerV7", "description": "Introduce adaptive mutation strategies and dynamic chaos scaling to enhance exploration and exploitation balance in swarm optimization.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV7:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n        self.chaos_sequence = self.init_chaotic_sequence()\n\n    def init_chaotic_sequence(self):\n        x = 0.7  # Initial value for chaotic map\n        chaos_seq = []\n        for _ in range(self.population_size):\n            x = 4 * x * (1 - x)  # Logistic map\n            chaos_seq.append(x)\n        return np.array(chaos_seq)\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def mutate(self, particle, mutation_rate):\n        if np.random.rand() < mutation_rate:\n            mutation_vector = np.random.normal(0, 0.1, self.dim)\n            particle += mutation_vector\n            particle = np.clip(particle, self.lower_bound, self.upper_bound)\n        return particle\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                chaotic_factor = self.chaos_sequence[i] * (1 + progress_ratio)  # Adaptive chaos scaling\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] *= chaotic_factor\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n                # Apply mutation\n                mutation_rate = 0.1 * (1 - progress_ratio)\n                self.particles[i] = self.mutate(self.particles[i], mutation_rate)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedSwarmOptimizerV7 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31951 with standard deviation 0.15789.", "error": "", "parent_ids": ["b2530d53-20b8-43cb-a2e7-49377c127d36"], "operator": null, "metadata": {"aucs": [0.3571148306719887, 0.5514455249983183, 0.14463512366283449, 0.360833938837109, 0.5594759385382526, 0.15145357641397628, 0.2816445626352242, 0.3670635781300593, 0.10192417544768795]}}
{"id": "adf05246-691b-46fd-9e7a-068a8711078b", "fitness": 0.6675231943355523, "name": "EnhancedSwarmOptimizerV6", "description": "Enhance global exploration by incorporating an adaptive Levy flight scale factor based on current diversity.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV6:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n        self.chaos_sequence = self.init_chaotic_sequence()\n\n    def init_chaotic_sequence(self):\n        x = 0.7  # Initial value for chaotic map\n        chaos_seq = []\n        for _ in range(self.population_size):\n            x = 4 * x * (1 - x)  # Logistic map\n            chaos_seq.append(x)\n        return np.array(chaos_seq)\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                chaotic_factor = self.chaos_sequence[i]\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] *= chaotic_factor  # Incorporate chaotic influence\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                # Apply adaptive Levy flight for global exploration\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                diversity = np.std(self.particles, axis=0).mean()\n                scale_factor = 0.01 + 0.99 * (diversity / (self.upper_bound - self.lower_bound))\n                steps = self.levy_flight((len(levy_indices), self.dim)) * scale_factor\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 86, "feedback": "The algorithm EnhancedSwarmOptimizerV6 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.66752 with standard deviation 0.02497.", "error": "", "parent_ids": ["b2530d53-20b8-43cb-a2e7-49377c127d36"], "operator": null, "metadata": {"aucs": [0.6675475767545282, 0.6691065301886787, 0.6959171521839346, 0.6695789434889745, 0.670933837042299, 0.6982510949658124, 0.6409131630724629, 0.6134362934037626, 0.6820241579195172]}}
{"id": "c2effcaa-ac81-46af-9306-e4736f5744f1", "fitness": 0.3425821778458984, "name": "EnhancedSwarmOptimizerV6", "description": "Enhance velocity update by introducing adaptive chaotic influence for improved exploration.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV6:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n        self.chaos_sequence = self.init_chaotic_sequence()\n\n    def init_chaotic_sequence(self):\n        x = 0.7  # Initial value for chaotic map\n        chaos_seq = []\n        for _ in range(self.population_size):\n            x = 4 * x * (1 - x)  # Logistic map\n            chaos_seq.append(x)\n        return np.array(chaos_seq)\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                chaotic_factor = self.chaos_sequence[i] * (1 - progress_ratio)  # Adaptive chaotic influence\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] *= chaotic_factor  # Incorporate chaotic influence\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                # Apply Levy flight for global exploration\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedSwarmOptimizerV6 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.34258 with standard deviation 0.30958.", "error": "", "parent_ids": ["b2530d53-20b8-43cb-a2e7-49377c127d36"], "operator": null, "metadata": {"aucs": [0.782415880370144, 0.1339187602095162, 0.13432196918992467, 0.7835071982796579, 0.13842063610342958, 0.13887125216214002, 0.7737962710355454, 0.09891753522199209, 0.09907009804073585]}}
{"id": "79091a30-c0c2-4b9f-9483-bc2900267266", "fitness": 0.46504950610026263, "name": "EnhancedSwarmOptimizerV6", "description": "Enhance exploration by applying Levy flight more frequently based on a dynamic threshold for stagnation detection.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV6:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 5 # Changed from 10 to 5 for more frequent explorations\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n        self.chaos_sequence = self.init_chaotic_sequence()\n\n    def init_chaotic_sequence(self):\n        x = 0.7  # Initial value for chaotic map\n        chaos_seq = []\n        for _ in range(self.population_size):\n            x = 4 * x * (1 - x)  # Logistic map\n            chaos_seq.append(x)\n        return np.array(chaos_seq)\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                chaotic_factor = self.chaos_sequence[i]\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] *= chaotic_factor  # Incorporate chaotic influence\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                # Apply Levy flight for global exploration\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 88, "feedback": "The algorithm EnhancedSwarmOptimizerV6 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.46505 with standard deviation 0.07474.", "error": "", "parent_ids": ["b2530d53-20b8-43cb-a2e7-49377c127d36"], "operator": null, "metadata": {"aucs": [0.4154729875143113, 0.5453735596402705, 0.4856140413321127, 0.4401128037327545, 0.548086477113419, 0.5224821579756347, 0.36548440267280335, 0.5261430034436625, 0.3366761214773947]}}
{"id": "0a6b6260-958c-46b7-ba7f-7f72d0f30b0a", "fitness": 0.12336458610787843, "name": "EnhancedSwarmOptimizerV6", "description": "Introduce adaptive mutation to enhance exploration through chaotic sequences for improved convergence.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV6:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n        self.chaos_sequence = self.init_chaotic_sequence()\n\n    def init_chaotic_sequence(self):\n        x = 0.7  # Initial value for chaotic map\n        chaos_seq = []\n        for _ in range(self.population_size):\n            x = 4 * x * (1 - x)  # Logistic map\n            chaos_seq.append(x)\n        return np.array(chaos_seq)\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                chaotic_factor = self.chaos_sequence[i]\n\n                # Adaptive mutation to enhance exploration\n                if np.random.rand() < chaotic_factor:\n                    mutation_strength = chaotic_factor * (self.upper_bound - self.lower_bound)\n                    self.particles[i] += np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] *= chaotic_factor  # Incorporate chaotic influence\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                # Apply Levy flight for global exploration\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 89, "feedback": "The algorithm EnhancedSwarmOptimizerV6 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12336 with standard deviation 0.01749.", "error": "", "parent_ids": ["b2530d53-20b8-43cb-a2e7-49377c127d36"], "operator": null, "metadata": {"aucs": [0.13383168641178467, 0.1328092211419556, 0.13371222986946363, 0.13830845398710645, 0.1371385821030685, 0.13817184321735843, 0.098902104877852, 0.09854663931937768, 0.09886051404293883]}}
{"id": "552282b1-2b4d-43db-948c-4bc672e3d6a3", "fitness": 0.12294912921654418, "name": "EnhancedSwarmOptimizerV7", "description": "Introduce adaptive chaotic perturbations and enhanced elitism to further refine convergence balance and exploration-exploitation.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV7:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.adaptive_chaotic_factor = 0.5\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n        self.chaos_sequence = self.init_chaotic_sequence()\n\n    def init_chaotic_sequence(self):\n        x = 0.7  # Initial value for chaotic map\n        chaos_seq = []\n        for _ in range(self.population_size):\n            x = 4 * x * (1 - x)  # Logistic map\n            chaos_seq.append(x)\n        return np.array(chaos_seq)\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                chaotic_factor = self.chaos_sequence[i] + self.adaptive_chaotic_factor * (1 - progress_ratio)\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] *= chaotic_factor  # Adaptive chaotic influence\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                # Apply Levy flight for global exploration\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 3, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            # Enhanced elitist strategy to preserve best solutions\n            elite_idxs = np.argsort(self.personal_best_scores)[:max(1, self.population_size // 5)]\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i, pos in zip(elite_idxs, elite_positions):\n                self.particles[i] = pos\n\n        return self.global_best_position", "configspace": "", "generation": 90, "feedback": "The algorithm EnhancedSwarmOptimizerV7 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12295 with standard deviation 0.01731.", "error": "", "parent_ids": ["b2530d53-20b8-43cb-a2e7-49377c127d36"], "operator": null, "metadata": {"aucs": [0.13275160455638713, 0.13270735300591197, 0.13339354496109512, 0.13707281316987996, 0.13702233614095005, 0.13780611308164725, 0.09852639534683871, 0.09851079646092897, 0.09875120622525857]}}
{"id": "6c93912f-aa02-43a5-94be-b70a42a44586", "fitness": 0.48224996462496467, "name": "EnhancedSwarmOptimizerV7", "description": "Incorporate adaptive learning rates with sinusoidal variations and a multi-phase update strategy to enhance global convergence and reduce premature stagnation.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV7:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n        self.chaos_sequence = self.init_chaotic_sequence()\n        self.learning_rate_sequence = self.init_learning_rates()\n\n    def init_chaotic_sequence(self):\n        x = 0.7\n        chaos_seq = []\n        for _ in range(self.population_size):\n            x = 4 * x * (1 - x)\n            chaos_seq.append(x)\n        return np.array(chaos_seq)\n\n    def init_learning_rates(self):\n        return np.sin(np.linspace(0, np.pi, self.budget))\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                chaotic_factor = self.chaos_sequence[i]\n                learning_rate = self.learning_rate_sequence[self.fitness_function_calls - 1]\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += learning_rate * cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += learning_rate * social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] *= chaotic_factor\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= 10:\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 91, "feedback": "The algorithm EnhancedSwarmOptimizerV7 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.48225 with standard deviation 0.18982.", "error": "", "parent_ids": ["b2530d53-20b8-43cb-a2e7-49377c127d36"], "operator": null, "metadata": {"aucs": [0.6438835082733231, 0.5722606819378813, 0.17827708147880028, 0.645815123296289, 0.584259711982374, 0.4671538415365124, 0.6016935073502145, 0.5420556423920526, 0.10485058337723552]}}
{"id": "9518bb0f-2a0b-4761-be4d-b08582ea7b2d", "fitness": 0.515497484294834, "name": "EnhancedSwarmOptimizerV7", "description": "Introduce adaptive inertia weight and diversity-enhancing mutation for improved convergence.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV7:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n        self.chaos_sequence = self.init_chaotic_sequence()\n\n    def init_chaotic_sequence(self):\n        x = 0.7  # Initial value for chaotic map\n        chaos_seq = []\n        for _ in range(self.population_size):\n            x = 4 * x * (1 - x)  # Logistic map\n            chaos_seq.append(x)\n        return np.array(chaos_seq)\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.7 - 0.3 * progress_ratio  # Adaptive inertia weight\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                chaotic_factor = self.chaos_sequence[i]\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] *= chaotic_factor  # Incorporate chaotic influence\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                # Apply Levy flight for global exploration\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n                # Introducing diversity-enhancing mutation\n                mutation_indices = np.random.choice(self.population_size, size=self.population_size // 10, replace=False)\n                mutation_steps = np.random.uniform(-1, 1, (len(mutation_indices), self.dim))\n                self.particles[mutation_indices] += mutation_steps\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedSwarmOptimizerV7 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.51550 with standard deviation 0.28002.", "error": "", "parent_ids": ["b2530d53-20b8-43cb-a2e7-49377c127d36"], "operator": null, "metadata": {"aucs": [0.1328040776327407, 0.6780912316221872, 0.7686863330527138, 0.1371327257782381, 0.6797281964717969, 0.7698679628235368, 0.09854481112129143, 0.6652445235063801, 0.7093774966446216]}}
{"id": "2e5e02a5-20df-45fb-9af1-56643d5d867f", "fitness": 0.4880164255256947, "name": "EnhancedSwarmOptimizerV6", "description": "Introduced a dynamic velocity scaling strategy based on the fitness progress to enhance adaptability.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV6:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n        self.chaos_sequence = self.init_chaotic_sequence()\n\n    def init_chaotic_sequence(self):\n        x = 0.7  # Initial value for chaotic map\n        chaos_seq = []\n        for _ in range(self.population_size):\n            x = 4 * x * (1 - x)  # Logistic map\n            chaos_seq.append(x)\n        return np.array(chaos_seq)\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                chaotic_factor = self.chaos_sequence[i]\n                velocity_scale = 1 + 0.5 * (self.last_best_score - self.global_best_score) / self.global_best_score\n                self.velocities[i] *= inertia_weight * velocity_scale\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] *= chaotic_factor  # Incorporate chaotic influence\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                # Apply Levy flight for global exploration\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 93, "feedback": "The algorithm EnhancedSwarmOptimizerV6 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.48802 with standard deviation 0.21948.", "error": "", "parent_ids": ["b2530d53-20b8-43cb-a2e7-49377c127d36"], "operator": null, "metadata": {"aucs": [0.13277003325189551, 0.6344555029241159, 0.7001584870294817, 0.13748521666176217, 0.5949128579612145, 0.7108210736758236, 0.3170155282884557, 0.6527999952506164, 0.5117291346878869]}}
{"id": "997ce33b-55e1-49f6-b9b5-f1016a8cac9c", "fitness": 0.4197315245395232, "name": "EnhancedSwarmOptimizerV6", "description": "Introduce an adaptive inertia weight update mechanism for improved convergence speed and stability.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV6:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n        self.chaos_sequence = self.init_chaotic_sequence()\n\n    def init_chaotic_sequence(self):\n        x = 0.7  # Initial value for chaotic map\n        chaos_seq = []\n        for _ in range(self.population_size):\n            x = 4 * x * (1 - x)  # Logistic map\n            chaos_seq.append(x)\n        return np.array(chaos_seq)\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 * (1 - progress_ratio**2)  # Modified line for adaptive inertia weight\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                chaotic_factor = self.chaos_sequence[i]\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] *= chaotic_factor  # Incorporate chaotic influence\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                # Apply Levy flight for global exploration\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 94, "feedback": "The algorithm EnhancedSwarmOptimizerV6 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41973 with standard deviation 0.21742.", "error": "", "parent_ids": ["b2530d53-20b8-43cb-a2e7-49377c127d36"], "operator": null, "metadata": {"aucs": [0.133023030364258, 0.5356933190195228, 0.6551317248067574, 0.13738304141346014, 0.5502798819585525, 0.656945680924969, 0.09862120847212708, 0.46286765221338666, 0.547638181682676]}}
{"id": "65ebef41-5e33-4557-bd78-6148cb51c1ed", "fitness": 0.4958412873138007, "name": "EnhancedSwarmOptimizerV6", "description": "Introduce a dynamic adjustment to the inertia weight to improve convergence speed.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV6:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n        self.chaos_sequence = self.init_chaotic_sequence()\n\n    def init_chaotic_sequence(self):\n        x = 0.7  # Initial value for chaotic map\n        chaos_seq = []\n        for _ in range(self.population_size):\n            x = 4 * x * (1 - x)  # Logistic map\n            chaos_seq.append(x)\n        return np.array(chaos_seq)\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 + 0.3 * (1 - progress_ratio)  # Adjusted line\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                chaotic_factor = self.chaos_sequence[i]\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] *= chaotic_factor  # Incorporate chaotic influence\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                # Apply Levy flight for global exploration\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 95, "feedback": "The algorithm EnhancedSwarmOptimizerV6 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.49584 with standard deviation 0.26390.", "error": "", "parent_ids": ["b2530d53-20b8-43cb-a2e7-49377c127d36"], "operator": null, "metadata": {"aucs": [0.133303839472801, 0.6780912316221872, 0.6996971531368452, 0.13777408604144636, 0.6797281964717969, 0.7013100359706639, 0.09866728947152736, 0.6652445235063801, 0.6687552301305582]}}
{"id": "8556953a-a369-4381-aa82-a822099de10e", "fitness": 0.4194811682687051, "name": "QuantumSwarmOptimizer", "description": "Introduce adaptive inertia weight based on historical progress and integrate quantum-inspired exploration to enhance global search capability.", "code": "import numpy as np\n\nclass QuantumSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n        self.chaos_sequence = self.init_chaotic_sequence()\n\n    def init_chaotic_sequence(self):\n        x = 0.7\n        chaos_seq = []\n        for _ in range(self.population_size):\n            x = 4 * x * (1 - x)\n            chaos_seq.append(x)\n        return np.array(chaos_seq)\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def quantum_explore(self, particle, global_best):\n        delta = np.random.uniform(-1, 1, self.dim) * np.abs(global_best - particle)\n        return particle + delta\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.9 - 0.5 * (self.fitness_function_calls / (self.budget / 2))\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                chaotic_factor = self.chaos_sequence[i]\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] *= chaotic_factor\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                # Apply Levy flight for global exploration\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                for idx in levy_indices:\n                    self.particles[idx] = self.quantum_explore(self.particles[idx], self.global_best_position)\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 96, "feedback": "The algorithm QuantumSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41948 with standard deviation 0.21055.", "error": "", "parent_ids": ["b2530d53-20b8-43cb-a2e7-49377c127d36"], "operator": null, "metadata": {"aucs": [0.5772100513674151, 0.13304412543954858, 0.580348677255621, 0.5796823252427963, 0.13740702716768072, 0.5828513660583354, 0.5238022964983514, 0.0986287513559102, 0.5623558940326869]}}
{"id": "87e89676-64bd-4a51-a839-9a445d8fc97e", "fitness": 0.12349753427919152, "name": "EnhancedSwarmOptimizerV7", "description": "Incorporate adaptive velocity clamping and Gaussian mutation for enhanced exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV7:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n        self.chaos_sequence = self.init_chaotic_sequence()\n\n    def init_chaotic_sequence(self):\n        x = 0.7  # Initial value for chaotic map\n        chaos_seq = []\n        for _ in range(self.population_size):\n            x = 4 * x * (1 - x)  # Logistic map\n            chaos_seq.append(x)\n        return np.array(chaos_seq)\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                chaotic_factor = self.chaos_sequence[i]\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] *= chaotic_factor  # Incorporate chaotic influence\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp * (1 + progress_ratio), self.velocity_clamp * (1 + progress_ratio))\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                # Apply Levy flight for global exploration\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i] + np.random.normal(0, 0.1, self.dim)  # Apply Gaussian mutation\n\n        return self.global_best_position", "configspace": "", "generation": 97, "feedback": "The algorithm EnhancedSwarmOptimizerV7 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12350 with standard deviation 0.01756.", "error": "", "parent_ids": ["b2530d53-20b8-43cb-a2e7-49377c127d36"], "operator": null, "metadata": {"aucs": [0.13270412087444117, 0.13473791600153062, 0.13339354496109512, 0.1370186554335605, 0.13937348620818824, 0.13780611308164725, 0.09850964841937415, 0.09918311730762808, 0.09875120622525857]}}
{"id": "dc8c34c6-4b72-4e1d-b3f7-d40a587b32b4", "fitness": 0.5251834417074092, "name": "QuantumEnhancedSwarmOptimizer", "description": "Utilize adaptive local search mechanisms and quantum-inspired operators to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass QuantumEnhancedSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n        self.chaos_sequence = self.init_chaotic_sequence()\n\n    def init_chaotic_sequence(self):\n        x = 0.7  # Initial value for chaotic map\n        chaos_seq = []\n        for _ in range(self.population_size):\n            x = 4 * x * (1 - x)  # Logistic map\n            chaos_seq.append(x)\n        return np.array(chaos_seq)\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def quantum_operator(self, position, best_position):\n        alpha = 0.1  # Quantum operator scale\n        theta = np.random.uniform(0, 2 * np.pi, self.dim)\n        q_step = alpha * np.cos(theta) * (self.global_best_position - position) + \\\n                 alpha * np.sin(theta) * (best_position - position)\n        return q_step\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                chaotic_factor = self.chaos_sequence[i]\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] *= chaotic_factor  # Incorporate chaotic influence\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n\n                quantum_step = self.quantum_operator(self.particles[i], self.personal_best_positions[i])\n                self.particles[i] += self.velocities[i] + quantum_step\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                # Apply Levy flight for global exploration\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n            \n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)]\n            elite_positions = self.personal_best_positions[elite_idxs]\n            random_idxs = np.random.choice(self.population_size, max(1, self.population_size // 20), replace=False)\n            for idx in random_idxs:\n                self.particles[idx] = elite_positions[np.random.randint(len(elite_positions))]\n\n        return self.global_best_position", "configspace": "", "generation": 98, "feedback": "The algorithm QuantumEnhancedSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.52518 with standard deviation 0.28587.", "error": "", "parent_ids": ["b2530d53-20b8-43cb-a2e7-49377c127d36"], "operator": null, "metadata": {"aucs": [0.685604774154323, 0.13471581326092152, 0.7720620775553872, 0.6883813686512765, 0.13932380183499093, 0.7732427632092351, 0.6712547595996516, 0.0992044271817164, 0.7628611899191802]}}
{"id": "4e41e12e-fc8d-48a0-abf1-a6d64b209b52", "fitness": 0.443287875990204, "name": "EnhancedSwarmOptimizerV6", "description": "Introduce adaptive learning rates for chaotic sequences, improving convergence speed and diversity maintenance.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizerV6:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, max(10, dim * 2))\n        self.particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.fitness_function_calls = 0\n        self.velocity_clamp = 0.1 * (self.upper_bound - self.lower_bound)\n        self.stagnation_threshold = 10\n        self.no_progress_count = 0\n        self.last_best_score = np.inf\n        self.chaos_sequence = self.init_chaotic_sequence()\n\n    def init_chaotic_sequence(self):\n        x = 0.7  # Initial value for chaotic map\n        chaos_seq = []\n        for _ in range(self.population_size):\n            x = 4 * x * (1 - x)  # Logistic map\n            chaos_seq.append(x)\n        return np.array(chaos_seq)\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        c1_initial, c2_initial = 2.5, 2.5\n        c1_final, c2_final = 0.5, 0.5\n\n        while self.fitness_function_calls < self.budget:\n            for i in range(self.population_size):\n                if self.fitness_function_calls >= self.budget:\n                    break\n\n                fitness = func(self.particles[i])\n                self.fitness_function_calls += 1\n\n                if fitness < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if fitness < self.global_best_score:\n                    self.global_best_score = fitness\n                    self.global_best_position = self.particles[i]\n\n            progress_ratio = self.fitness_function_calls / self.budget\n            inertia_weight = 0.4 * (1 - progress_ratio)\n\n            cognitive_constant = c1_initial - (c1_initial - c1_final) * progress_ratio\n            social_constant = c2_initial - (c2_initial - c2_final) * progress_ratio\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                chaotic_factor = self.chaos_sequence[i] * (1 + progress_ratio)  # Adaptive learning rate\n                self.velocities[i] *= inertia_weight\n                self.velocities[i] += cognitive_constant * r1 * (self.personal_best_positions[i] - self.particles[i])\n                self.velocities[i] += social_constant * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] *= chaotic_factor  # Incorporate chaotic influence\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lower_bound, self.upper_bound)\n\n            if self.global_best_score < self.last_best_score:\n                self.last_best_score = self.global_best_score\n                self.no_progress_count = 0\n            else:\n                self.no_progress_count += 1\n\n            if self.no_progress_count >= self.stagnation_threshold:\n                # Apply Levy flight for global exploration\n                levy_indices = np.random.choice(self.population_size, size=self.population_size // 4, replace=False)\n                steps = self.levy_flight((len(levy_indices), self.dim))\n                self.particles[levy_indices] += steps\n                self.particles = np.clip(self.particles, self.lower_bound, self.upper_bound)\n                self.no_progress_count = 0\n\n            # Improved elitist strategy to preserve better solutions\n            elite_idxs = np.random.choice(np.argsort(self.personal_best_scores)[:max(1, self.population_size // 10)], max(1, self.population_size // 20), replace=False)\n            elite_positions = self.personal_best_positions[elite_idxs]\n            for i in range(len(elite_idxs)):\n                self.particles[elite_idxs[i]] = elite_positions[i]\n\n        return self.global_best_position", "configspace": "", "generation": 99, "feedback": "The algorithm EnhancedSwarmOptimizerV6 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.44329 with standard deviation 0.25889.", "error": "", "parent_ids": ["b2530d53-20b8-43cb-a2e7-49377c127d36"], "operator": null, "metadata": {"aucs": [0.13302992502692323, 0.45657811510941504, 0.7582452617218751, 0.13739094267209806, 0.48123862632554326, 0.759551594945443, 0.09862358795266069, 0.41662890750292814, 0.7483039226549498]}}
