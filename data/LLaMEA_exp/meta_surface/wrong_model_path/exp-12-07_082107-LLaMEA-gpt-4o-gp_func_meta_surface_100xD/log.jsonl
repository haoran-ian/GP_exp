{"id": "535ce6ca-c4df-4bba-a672-8aca07a991fc", "fitness": 0.12271897118159442, "name": "NovelMetaheuristic", "description": "This algorithm combines differential evolution with adaptive learning rates to efficiently explore and exploit the search space for black box optimization.", "code": "import numpy as np\n\nclass NovelMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, self.budget // 10)\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Mutation\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 0, "feedback": "The algorithm NovelMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12272 with standard deviation 0.01722.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13270267213018938, 0.13267465653697486, 0.13264417310297383, 0.13701699904549458, 0.13698504311175663, 0.13695028118528774, 0.09850914315169679, 0.09849926610795101, 0.09848850626202488]}}
{"id": "99246ad9-e63b-4a62-a577-40e99e0b0cef", "fitness": 0.12478597846841676, "name": "RefinedMetaheuristic", "description": "This refined algorithm enhances differential evolution by incorporating an adaptive population size and dynamic parameter adjustment to improve convergence speed and solution quality for black box optimization.", "code": "import numpy as np\n\nclass RefinedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n\n    def adapt_parameters(self, generation):\n        # Dynamically adjust parameters based on generation\n        self.f = max(0.1, 0.5 - 0.4 * (generation / (self.budget // self.population_size)))\n        self.cr = min(1.0, 0.9 + 0.1 * np.sin(3.14 * generation / (self.budget // self.population_size)))\n\n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2) \n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 1, "feedback": "The algorithm RefinedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12479 with standard deviation 0.01839.", "error": "", "parent_ids": ["535ce6ca-c4df-4bba-a672-8aca07a991fc"], "operator": null, "metadata": {"aucs": [0.13998319845088314, 0.1328255664156197, 0.1326905655209215, 0.14551958024265232, 0.1371571975978979, 0.1370032543745996, 0.10083721764284737, 0.09855244264116647, 0.09850478332916268]}}
{"id": "8bc70ec1-84a9-4294-ba23-035bab712563", "fitness": 0.1337486518425886, "name": "EnhancedMetaheuristic", "description": "This enhanced algorithm integrates chaotic maps for parameter adaptation and a feedback mechanism to dynamically adjust crossover and mutation rates, improving the convergence rate and robustness of differential evolution for black-box optimization.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        # Logistic map for chaotic behavior\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)  # Increase differential weight slightly\n            self.cr = max(0.1, self.cr - 0.05)  # Decrease crossover probability slightly\n    \n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 2, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13375 with standard deviation 0.03016.", "error": "", "parent_ids": ["99246ad9-e63b-4a62-a577-40e99e0b0cef"], "operator": null, "metadata": {"aucs": [0.1340872449907652, 0.13266277658046366, 0.17880815221448498, 0.13862920009458535, 0.1369714935162908, 0.18447034388024008, 0.09895665953000443, 0.09849507598146656, 0.10065691979499647]}}
{"id": "d356c277-2013-400c-b357-4a6181de2607", "fitness": 0.12274190676473827, "name": "EnhancedMetaheuristic", "description": "This refined algorithm enhances the exploration-exploitation balance by incorporating a dynamic mutation strategy and adaptive crossover rates, improving robustness and performance in varying conditions.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        # Logistic map for chaotic behavior\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)  # Increase differential weight slightly\n            self.cr = max(0.1, self.cr - 0.05)  # Decrease crossover probability slightly\n    \n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def dynamic_mutation_strategy(self, generation):\n        # Dynamic adjustment of mutation strategy based on generation\n        return np.random.uniform(0.4, 0.9) * (1 - generation / self.budget) + 0.1\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n                \n                # Dynamic Mutation Strategy\n                self.f = self.dynamic_mutation_strategy(generation)\n\n                # Mutation\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12274 with standard deviation 0.01723.", "error": "", "parent_ids": ["8bc70ec1-84a9-4294-ba23-035bab712563"], "operator": null, "metadata": {"aucs": [0.1327229960303543, 0.13274940402022584, 0.13263189874191417, 0.13704017915936284, 0.13707032325531743, 0.13693628601235097, 0.09851631173695985, 0.09852559097476954, 0.0984841709513895]}}
{"id": "646c0a13-0101-4c56-8acb-8197829cb456", "fitness": -Infinity, "name": "RefinedMetaheuristic", "description": "This refined algorithm incorporates adaptive multi-population strategies with hybridization of differential evolution and particle swarm techniques, enhancing its exploration and exploitation balance for superior performance in black-box optimization.", "code": "import numpy as np\n\nclass RefinedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.global_best = None\n        self.global_best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n    \n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            self.velocity = self.velocity[:self.population_size]\n            self.personal_best = self.personal_best[:self.population_size]\n            self.personal_best_fitness = self.personal_best_fitness[:self.population_size]\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Particle Swarm Update\n                inertia = 0.7\n                cognitive_coeff = 1.5\n                social_coeff = 1.5\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                self.velocity[i] = (inertia * self.velocity[i] +\n                                    cognitive_coeff * r1 * (self.personal_best[i] - self.population[i]) +\n                                    social_coeff * r2 * (self.global_best - self.population[i]))\n                self.velocity[i] = np.clip(self.velocity[i], -1, 1)\n                candidate = self.population[i] + self.velocity[i]\n                candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n\n                # Differential Evolution Mutation and Crossover\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, candidate)\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection and Personal Best Update\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n                if trial_fitness < self.personal_best_fitness[i]:\n                    self.personal_best[i] = trial\n                    self.personal_best_fitness[i] = trial_fitness\n\n                # Global Best Update\n                if trial_fitness < self.global_best_fitness:\n                    self.global_best = trial\n                    self.global_best_fitness = trial_fitness\n\n            generation += 1\n        \n        return self.global_best, self.global_best_fitness", "configspace": "", "generation": 4, "feedback": "An exception occurred: TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\").", "error": "TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\")", "parent_ids": ["8bc70ec1-84a9-4294-ba23-035bab712563"], "operator": null, "metadata": {}}
{"id": "8795cd4d-ddca-48d8-b344-e71cbf6359cc", "fitness": 0.12295130442279525, "name": "EnhancedMetaheuristic", "description": "Integrate adaptive opposition-based learning feature to enhance exploration and exploitation balance in differential evolution.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n    \n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def opposition_based_learning(self):\n        # Generate opposite population\n        opposite_population = self.lower_bound + self.upper_bound - self.population\n        return opposite_population\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            if generation < 5:  # Apply OBL for the first few generations\n                opposite_population = self.opposition_based_learning()\n                for i in range(self.population_size):\n                    if self.evaluations >= self.budget:\n                        break\n                    opposite_fitness = func(opposite_population[i])\n                    self.evaluations += 1\n                    if opposite_fitness < self.fitness[i]:\n                        self.population[i] = opposite_population[i]\n                        self.fitness[i] = opposite_fitness\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12295 with standard deviation 0.01731.", "error": "", "parent_ids": ["8bc70ec1-84a9-4294-ba23-035bab712563"], "operator": null, "metadata": {"aucs": [0.1326579372737915, 0.13281938780366365, 0.13338304692649228, 0.13696599334420756, 0.1371502133917052, 0.13779413602853874, 0.09849334171371193, 0.09855017457741255, 0.09874750874563387]}}
{"id": "8749fc12-6ab2-491d-ab18-21da7de12691", "fitness": 0.12274893463956757, "name": "EnhancedMetaheuristic", "description": "This refined algorithm enhances parameter adaptation using a multi-chaotic map approach and incorporates an elitism strategy to preserve the best solutions, improving the balance between exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.best_solution = None  # Initialize the best solution\n\n    def chaotic_map(self, value, map_type='logistic'):\n        if map_type == 'logistic':\n            return 4.0 * value * (1.0 - value)\n        elif map_type == 'sine':\n            return np.abs(np.sin(np.pi * value))\n    \n    def adapt_parameters(self, generation):\n        # Use multi-chaotic map for parameter adaptation\n        map_type = 'sine' if generation % 2 == 0 else 'logistic'\n        self.f = max(0.1, self.chaotic_map(self.f, map_type))\n        self.cr = min(1.0, self.chaotic_map(self.cr, map_type))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n    \n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial  # Preserve the best solution\n\n            generation += 1\n        \n        return self.best_solution, self.best_fitness  # Return the best found solution", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12275 with standard deviation 0.01723.", "error": "", "parent_ids": ["8bc70ec1-84a9-4294-ba23-035bab712563"], "operator": null, "metadata": {"aucs": [0.13275216024269298, 0.13271684274626683, 0.13266066048791392, 0.13707345368488977, 0.13703315867701293, 0.13696908082315162, 0.09852658175654949, 0.09851414485599119, 0.09849432848163953]}}
{"id": "aabf799d-0060-4187-b78a-c2d6f33d9e71", "fitness": 0.12841747802056752, "name": "EnhancedMetaheuristic", "description": "This refined EnhancedMetaheuristic algorithm introduces a dynamic scaling factor, increasing exploration in early stages and focusing on exploitation as iterations progress, boosting convergence speed while maintaining robustness in black-box optimization.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        # Logistic map for chaotic behavior\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)  # Increase differential weight slightly\n            self.cr = max(0.1, self.cr - 0.05)  # Decrease crossover probability slightly\n    \n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                dynamic_scaling = 1 - (self.evaluations / self.budget)  # Added line for dynamic scaling\n                mutant = self.population[a] + dynamic_scaling * self.f * (self.population[b] - self.population[c])  # Modified line\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12842 with standard deviation 0.02182.", "error": "", "parent_ids": ["8bc70ec1-84a9-4294-ba23-035bab712563"], "operator": null, "metadata": {"aucs": [0.132647293850098, 0.1327155385983324, 0.15320677169615704, 0.13695384862694693, 0.13703168262737875, 0.16186586924005408, 0.09848959557527992, 0.09851366873208545, 0.10433303323877507]}}
{"id": "8ecbc375-cc55-4843-8329-9b64cd9380d7", "fitness": -Infinity, "name": "RefinedMetaheuristic", "description": "This refined algorithm introduces adaptive population size control based on diversity measures and utilizes a Lévy flight mechanism for mutation to enhance exploration and exploitation balance in differential evolution for black-box optimization.", "code": "import numpy as np\n\nclass RefinedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        # Logistic map for chaotic behavior\n        return 4.0 * value * (1.0 - value)\n\n    def levy_flight(self, lam=1.5):\n        # Lévy flight step for mutation\n        sigma = (np.gamma(1 + lam) * np.sin(np.pi * lam / 2) /\n                 (np.gamma((1 + lam) / 2) * lam * 2 ** ((lam - 1) / 2))) ** (1 / lam)\n        u = np.random.normal(0, sigma, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / np.abs(v) ** (1 / lam)\n        return step\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)  # Increase differential weight slightly\n            self.cr = max(0.1, self.cr - 0.05)  # Decrease crossover probability slightly\n    \n    def diversity_control(self):\n        # Adjust population size based on diversity\n        diversity = np.mean(np.std(self.population, axis=0))\n        if diversity < 1e-3:\n            self.population_size = max(5, self.population_size // 2)\n        elif diversity > 1.0:\n            self.population_size = min(self.initial_population_size, self.population_size + 5)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.diversity_control()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation with Lévy flight\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c]) + self.levy_flight()\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 8, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_ids": ["8bc70ec1-84a9-4294-ba23-035bab712563"], "operator": null, "metadata": {}}
{"id": "95009fa6-56eb-441e-b557-ee0969db6028", "fitness": 0.12273605967792639, "name": "RefinedMetaheuristic", "description": "This updated algorithm refines the adaptation of parameters using a combination of chaotic maps and adaptive learning rates to enhance convergence speed and solution quality for black-box optimization.", "code": "import numpy as np\n\nclass RefinedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.learning_rate_f = 0.05\n        self.learning_rate_cr = 0.05\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f += self.learning_rate_f * (0.5 - self.f)\n        self.cr += self.learning_rate_cr * (0.9 - self.cr)\n        self.f = max(0.1, min(0.9, self.chaotic_map(self.f)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr)))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.learning_rate_f = min(0.1, self.learning_rate_f * 1.1)\n            self.learning_rate_cr = max(0.01, self.learning_rate_cr * 0.9)\n        else:\n            self.learning_rate_f = max(0.01, self.learning_rate_f * 0.9)\n            self.learning_rate_cr = min(0.1, self.learning_rate_cr * 1.1)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 9, "feedback": "The algorithm RefinedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12274 with standard deviation 0.01722.", "error": "", "parent_ids": ["8bc70ec1-84a9-4294-ba23-035bab712563"], "operator": null, "metadata": {"aucs": [0.13269877597391833, 0.13268517269156366, 0.13269924917582054, 0.1370125518123777, 0.13699703990300893, 0.13701316453474965, 0.09850777394639165, 0.09850297140808306, 0.09850783765542381]}}
{"id": "d84d8905-c679-484a-8e42-ec46c540ce0e", "fitness": 0.1337486518425886, "name": "EnhancedMetaheuristic", "description": "Introduced a dynamic mutation factor increment for better diversity handling and convergence acceleration in differential evolution. ", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        # Logistic map for chaotic behavior\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.1)  # Increase differential weight more to promote exploration\n            self.cr = max(0.1, self.cr - 0.05)  # Decrease crossover probability slightly\n    \n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 10, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13375 with standard deviation 0.03016.", "error": "", "parent_ids": ["8bc70ec1-84a9-4294-ba23-035bab712563"], "operator": null, "metadata": {"aucs": [0.1340872449907652, 0.13266277658046366, 0.17880815221448498, 0.13862920009458535, 0.1369714935162908, 0.18447034388024008, 0.09895665953000443, 0.09849507598146656, 0.10065691979499647]}}
{"id": "c90df7fd-fadc-4078-92e2-d0a745981f39", "fitness": 0.12278332641819698, "name": "EnhancedMetaheuristic", "description": "A slight increase in crossover probability and differential weight was made to potentially enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.55  # Initial differential weight, slightly increased\n        self.cr = 0.92  # Initial crossover probability, slightly increased\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        # Logistic map for chaotic behavior\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)  # Increase differential weight slightly\n            self.cr = max(0.1, self.cr - 0.05)  # Decrease crossover probability slightly\n    \n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 11, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12278 with standard deviation 0.01724.", "error": "", "parent_ids": ["8bc70ec1-84a9-4294-ba23-035bab712563"], "operator": null, "metadata": {"aucs": [0.13266826808920862, 0.13265268188884627, 0.13293288434978878, 0.13697778562409646, 0.13695998412147437, 0.13727969565148201, 0.0984969720236003, 0.0984915097775817, 0.09859015623769429]}}
{"id": "c588c2a0-0667-4c74-8175-f2680533e111", "fitness": 0.17873477225744752, "name": "EnhancedMetaheuristic", "description": "This refined algorithm enhances the selection strategy by incorporating a dynamic elitism mechanism, ensuring that the top performers are retained across generations to improve convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        # Logistic map for chaotic behavior\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)  # Increase differential weight slightly\n            self.cr = max(0.1, self.cr - 0.05)  # Decrease crossover probability slightly\n    \n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection with dynamic elitism\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            # Retain the best individual in the population\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17873 with standard deviation 0.08289.", "error": "", "parent_ids": ["8bc70ec1-84a9-4294-ba23-035bab712563"], "operator": null, "metadata": {"aucs": [0.13263561019564651, 0.1326624287197692, 0.28483958572352974, 0.13694051744380242, 0.13697109681927966, 0.3355109210487186, 0.09848548234168941, 0.09849495321361001, 0.252072354810982]}}
{"id": "e0d1e4e8-ed15-44e0-a37e-89c0b779ebe3", "fitness": 0.12368096747779316, "name": "AdvancedMetaheuristic", "description": "This advanced algorithm incorporates adaptive chaotic differential weight and crossover probability adjustments with iterative elitism to enhance convergence speed and efficacy.", "code": "import numpy as np\n\nclass AdvancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.7  # Start with slightly higher differential weight\n        self.cr = 0.8  # Start with slightly lower crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.4, self.chaotic_map(self.f))\n        self.cr = min(0.9, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n    \n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 13, "feedback": "The algorithm AdvancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12368 with standard deviation 0.01770.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.1358163550969138, 0.13276378768427521, 0.132918166186212, 0.14072551535994782, 0.13708670975667825, 0.13726439929976786, 0.09944015198730172, 0.09853069069493847, 0.09858293123410333]}}
{"id": "6f9bff5b-6496-4b24-99a3-652e525cb02e", "fitness": 0.17873477225744752, "name": "AdaptiveEnhancedMetaheuristic", "description": "This novel algorithm introduces adaptive learning rates and reinforced dynamic elitism, which adjust the balance between exploration and exploitation dynamically, enhancing convergence and solution robustness.", "code": "import numpy as np\n\nclass AdaptiveEnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.learning_rate = 0.1\n\n    def reinforced_chaotic_map(self, value):\n        # Logistic map for chaotic behavior with reinforcement\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation with reinforcement\n        self.f = max(0.1, self.reinforced_chaotic_map(self.f))\n        self.cr = min(1.0, self.reinforced_chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        # Adapt learning rate to control parameter adjustment\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + self.learning_rate * 0.05)  # Increase differential weight\n            self.cr = max(0.1, self.cr - self.learning_rate * 0.05)  # Decrease crossover probability\n            self.learning_rate = min(0.2, self.learning_rate + 0.01)  # Increase learning rate\n        else:\n            self.learning_rate = max(0.05, self.learning_rate * 0.99)  # Decay learning rate\n    \n    def adapt_population_size(self):\n        # Reduce population size more dynamically\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, int((self.budget - self.evaluations) / 1.5))\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection with dynamic elitism\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            # Reinforced retention of the best individual\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 14, "feedback": "The algorithm AdaptiveEnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17873 with standard deviation 0.08289.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.13263561019564651, 0.1326624287197692, 0.28483958572352974, 0.13694051744380242, 0.13697109681927966, 0.3355109210487186, 0.09848548234168941, 0.09849495321361001, 0.252072354810982]}}
{"id": "bcb046fe-fa21-4bb5-977c-80ee27051f72", "fitness": 0.12278992198658036, "name": "RefinedAdaptiveMetaheuristic", "description": "This algorithm introduces adaptive exploration through self-tuning chaotic maps and adaptive mutation strategies, enhancing search efficiency by dynamically balancing exploration and exploitation.", "code": "import numpy as np\n\nclass RefinedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        # Sine map for chaotic behavior\n        return np.abs(np.sin(np.pi * value))\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f + generation / self.budget))\n        self.cr = max(0.1, self.chaotic_map(self.cr + generation / self.budget))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.1 * (current_fitness - trial_fitness) / abs(current_fitness))  # Scale increase\n            self.cr = max(0.1, self.cr - 0.1 * (current_fitness - trial_fitness) / abs(trial_fitness))  # Scale decrease\n    \n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation with adaptive scaling\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection with dynamic elitism\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            # Retain the best individual in the population\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 15, "feedback": "The algorithm RefinedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12279 with standard deviation 0.01725.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.13274102066701476, 0.1326500836019543, 0.13288660314382494, 0.1370607373470858, 0.1369570206909908, 0.13722735756769722, 0.0985226686683569, 0.0984905933099669, 0.09857321288233167]}}
{"id": "2080445b-55dc-42e4-9a51-cb15f413084d", "fitness": 0.17873477225744752, "name": "EnhancedMetaheuristic", "description": "This refined algorithm incorporates a novel adaptive parameter control strategy, dynamically adjusting mutation and crossover rates using a hybrid approach combining chaotic maps and success-based adaptation to enhance convergence and robustness.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n\n    def hybrid_adjust_rates(self, trial_fitness, generation):\n        improvement = (self.best_fitness - trial_fitness) / (abs(self.best_fitness) + 1e-10)\n        if improvement > 0:\n            self.f = min(0.9, self.f + 0.01 * improvement)\n            self.cr = max(0.1, self.cr - 0.01 * improvement)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n                self.hybrid_adjust_rates(trial_fitness, generation)\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 16, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17873 with standard deviation 0.08289.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.13263561019564651, 0.1326624287197692, 0.28483958572352974, 0.13694051744380242, 0.13697109681927966, 0.3355109210487186, 0.09848548234168941, 0.09849495321361001, 0.252072354810982]}}
{"id": "862b13ef-181c-4148-8c64-9a66adad9af3", "fitness": 0.12311916195497766, "name": "AdvancedMetaheuristic", "description": "This advanced metaheuristic introduces a multiscale exploration approach with adaptive lambda scaling, allowing for diverse search patterns and enhanced convergence through dynamic adaptation of mutation scaling based on convergence progress.", "code": "import numpy as np\n\nclass AdvancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.lambda_scale = 1.0\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n        self.lambda_scale *= 0.99  # Gradually reduce the scale to focus the search\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n            self.lambda_scale = min(1.0, self.lambda_scale * 1.01)  # Slightly increase scale if improving\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutation_factor = self.f * self.lambda_scale\n                mutant = self.population[a] + mutation_factor * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 17, "feedback": "The algorithm AdvancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12312 with standard deviation 0.01738.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.13349720774678386, 0.13315322244231953, 0.13281609729311006, 0.1379251937558884, 0.1375315438909146, 0.13714640342938966, 0.09878658880562574, 0.09866710558765701, 0.0985490946431099]}}
{"id": "4507b3c0-0805-4de5-8fce-e0cc8db29d27", "fitness": 0.17873477225744752, "name": "EnhancedMetaheuristic", "description": "This enhanced algorithm integrates dynamic parameter adaptation with an adaptive population contraction strategy, optimizing exploration and exploitation over the black-box function landscape for improved convergence.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        # Logistic map for chaotic behavior\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)  # Increase differential weight slightly\n            self.cr = max(0.1, self.cr - 0.05)  # Decrease crossover probability slightly\n    \n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def elitism(self):\n        # Retain the best individual in the population\n        best_idx = np.argmin(self.fitness)\n        elite = self.population[best_idx].copy()\n        self.population[0] = elite\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters()\n\n                # Mutation\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection with dynamic elitism\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            self.elitism()\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17873 with standard deviation 0.08289.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.13263561019564651, 0.1326624287197692, 0.28483958572352974, 0.13694051744380242, 0.13697109681927966, 0.3355109210487186, 0.09848548234168941, 0.09849495321361001, 0.252072354810982]}}
{"id": "9c3d8ef6-c92d-4aee-a086-dda37f523ec3", "fitness": 0.12281448090520192, "name": "EnhancedMetaheuristic", "description": "This adaptive algorithm integrates a diversity-preserving strategy with a dynamic scaling mechanism to enhance exploration and exploitation, ensuring robust convergence across diverse problem landscapes.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.1)  # Increase differential weight\n            self.cr = max(0.1, self.cr - 0.1)  # Decrease crossover probability\n    \n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def diversify_population(self):\n        # Introduce diversity by replacing some individuals with new random ones\n        if self.evaluations < self.budget // 2:\n            num_replacements = self.population_size // 5\n            for i in np.random.choice(self.population_size, num_replacements, replace=False):\n                self.population[i] = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            if generation % 5 == 0:  # Regularly introduce diversity\n                self.diversify_population()\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 19, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12281 with standard deviation 0.01726.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.1327243477143324, 0.13298323891263142, 0.1326587803667849, 0.13704173511001538, 0.13733761175022907, 0.13696693858339093, 0.09851676829481548, 0.09860724508197427, 0.09849366233264323]}}
{"id": "0bceaf1a-46ec-4645-b77f-c1d164308a16", "fitness": 0.12280187639184136, "name": "EnhancedMetaheuristic", "description": "Introduce a dynamic adaptive control for the mutation factor to enhance search exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n    \n    def dynamic_mutation_factor(self, gen_factor):\n        return 0.5 + 0.3 * gen_factor\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                gen_factor = (self.budget - self.evaluations) / self.budget\n                self.f = self.dynamic_mutation_factor(gen_factor)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 20, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12280 with standard deviation 0.01725.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.1330024786341617, 0.13267450222120514, 0.13264425679801717, 0.1373628562439767, 0.13698486483490913, 0.13695037665423326, 0.09860980145227782, 0.09849921492162794, 0.09848853576616334]}}
{"id": "f8ed4c46-6f8f-4d01-84bd-d834faa404ff", "fitness": 0.12695343509450974, "name": "EnhancedMetaheuristic", "description": "This enhanced algorithm integrates an adaptive differential weight and crossover probability strategy, dynamically adjusting based on current performance to enhance convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.best_individual = None\n\n    def chaotic_map(self, value):\n        # Logistic map for chaotic behavior\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, success_rate):\n        # Dynamic adaptation of parameters based on success rate\n        self.f = max(0.1, min(0.9, self.f + 0.1 * (success_rate - 0.2)))\n        self.cr = max(0.1, min(1.0, self.cr + 0.1 * (0.8 - success_rate)))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.02)  # Increase differential weight slightly\n            self.cr = max(0.1, self.cr - 0.02)  # Decrease crossover probability slightly\n\n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            success_count = 0\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    success_count += 1\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_individual = trial.copy()\n\n            success_rate = success_count / self.population_size\n            self.adapt_parameters(success_rate)\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        return self.best_individual, self.best_fitness", "configspace": "", "generation": 21, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12695 with standard deviation 0.02114.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.13271047428452565, 0.1508537174913136, 0.13269723250274312, 0.1370259087695479, 0.15559719230332303, 0.13701080730255, 0.0985118793181271, 0.09966649691067153, 0.09850720696778581]}}
{"id": "6539b029-18d5-4608-8a1c-2cddafdf8b05", "fitness": 0.12294363779204717, "name": "PredatorPreyMetaheuristic", "description": "This evolved algorithm incorporates a dynamic adaptive learning rate inspired by natural predator-prey interactions, promoting diversity and convergence by adjusting exploration and exploitation based on environmental feedback.", "code": "import numpy as np\n\nclass PredatorPreyMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def dynamic_interaction(self, prey_fitness, predator_fitness):\n        # Adapt interaction rate based on relative fitness\n        return max(0.1, min(0.9, 1 - np.tanh(prey_fitness - predator_fitness)))\n    \n    def adapt_parameters(self, generation):\n        # Use a dynamic approach for parameter adaptation\n        self.f = max(0.1, np.random.rand() * (0.9 - 0.1) + 0.1)\n        self.cr = min(1.0, np.random.rand() * (1.0 - 0.1) + 0.1)\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)  # Increase differential weight slightly\n            self.cr = max(0.1, self.cr - 0.05)  # Decrease crossover probability slightly\n\n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection with predator-prey dynamic\n                if trial_fitness < self.fitness[i]:\n                    interaction_rate = self.dynamic_interaction(trial_fitness, self.fitness[i])\n                    self.fitness[i] = (1 - interaction_rate) * self.fitness[i] + interaction_rate * trial_fitness\n                    self.population[i] = (1 - interaction_rate) * self.population[i] + interaction_rate * trial\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            # Retain the best individual in the population\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 22, "feedback": "The algorithm PredatorPreyMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12294 with standard deviation 0.01731.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.13272075679497908, 0.13271429857971673, 0.13339762476672878, 0.1370376212914548, 0.13703025507609623, 0.1378107801470202, 0.09851552753253012, 0.09851325017877954, 0.0987526257611192]}}
{"id": "3f57513d-cafd-47c3-b672-03cde6b6d286", "fitness": 0.16338799435875184, "name": "DynamicLeagueMetaheuristic", "description": "This innovative algorithm incorporates a dynamic league system, where solutions are grouped into leagues based on performance, promoting exploration and exploitation through inter-league competition and intra-league collaboration to enhance convergence and solution quality.", "code": "import numpy as np\n\nclass DynamicLeagueMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 8)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.league_threshold = self.population_size // 3\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        # Logistic map for chaotic behavior\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)  # Increase differential weight slightly\n            self.cr = max(0.1, self.cr - 0.05)  # Decrease crossover probability slightly\n    \n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def league_sort(self):\n        # Sort population into leagues\n        indices = np.argsort(self.fitness)\n        return indices[:self.league_threshold], indices[self.league_threshold:]\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            top_league, lower_league = self.league_sort()\n\n            for league, league_size in zip([top_league, lower_league], [self.league_threshold, self.population_size - self.league_threshold]):\n                for i in league:\n                    if self.evaluations >= self.budget:\n                        break\n\n                    self.adapt_parameters(generation)\n\n                    # Mutation\n                    indices = [idx for idx in range(league_size) if idx != i]\n                    a, b, c = np.random.choice(indices, 3, replace=False)\n                    mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                    mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                    # Crossover\n                    crossover_mask = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover_mask, mutant, self.population[i])\n\n                    # Evaluate\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n\n                    # Selection with dynamic elitism\n                    if trial_fitness < self.fitness[i]:\n                        self.population[i] = trial\n                        self.fitness[i] = trial_fitness\n                        self.adjust_rates(trial_fitness, self.fitness[i])\n\n                    # Update best fitness\n                    if trial_fitness < self.best_fitness:\n                        self.best_fitness = trial_fitness\n\n                # Retain the best individual in the top league\n                best_idx = np.argmin(self.fitness[league])\n                elite = self.population[league[best_idx]].copy()\n                self.population[league[0]] = elite\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 23, "feedback": "The algorithm DynamicLeagueMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.16339 with standard deviation 0.06983.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.2882636750697424, 0.1328710521824017, 0.13265563213736398, 0.2919133309640083, 0.13720929450988706, 0.13696334795339937, 0.15355488679447438, 0.09856817777089522, 0.09849255184659433]}}
{"id": "c15bf6bf-35b5-4bf0-93ef-f2b627261d57", "fitness": 0.12270984056531004, "name": "EnhancedMetaheuristic", "description": "This refined algorithm enhances parameter adjustment by slightly increasing the adaptation rate for faster convergence while maintaining solution quality.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        # Logistic map for chaotic behavior\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f) + 0.01)  # Slightly increase adaptation rate\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)  # Increase differential weight slightly\n            self.cr = max(0.1, self.cr - 0.05)  # Decrease crossover probability slightly\n    \n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection with dynamic elitism\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            # Retain the best individual in the population\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 24, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12271 with standard deviation 0.01721.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.13263434513894523, 0.13269082192668424, 0.1326633848723081, 0.13693907504463598, 0.13700353525278952, 0.1369721887200378, 0.09848503550440202, 0.09850489012785024, 0.09849528850013711]}}
{"id": "2ad395fc-ebbe-428d-8b7f-d33549dd93ee", "fitness": 0.123028414439525, "name": "EnhancedMetaheuristic", "description": "This refined algorithm further enhances convergence speed and solution quality by adapting mutation strategies based on performance trends.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        # Logistic map for chaotic behavior\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)  # Increase differential weight slightly\n            self.cr = max(0.1, self.cr - 0.05)  # Decrease crossover probability slightly\n    \n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation strategy change\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                factor = np.random.uniform(0.5, 1.0)  # Dynamic factor adjustment\n                mutant = self.population[a] + factor * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection with dynamic elitism\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            # Retain the best individual in the population\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 25, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12303 with standard deviation 0.01735.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.13265903424934822, 0.13378885606604396, 0.13269228593107552, 0.1369672273260777, 0.13826857608452436, 0.13700516863451662, 0.09849375304662533, 0.0988753707750224, 0.09850545784249087]}}
{"id": "a37b9033-54eb-48ef-a19a-65356aa1a0b0", "fitness": 0.12288636906636051, "name": "EnhancedMetaheuristic", "description": "This optimized algorithm employs adaptive learning rates to dynamically adjust exploration and exploitation, enhancing convergence speed and solution accuracy.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        # Logistic map for chaotic behavior\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.03)  # Adjust differential weight\n            self.cr = max(0.1, self.cr - 0.03)  # Adjust crossover probability\n    \n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        self.cr = 1.0 - (self.evaluations / self.budget)  # Adaptive crossover probability\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection with dynamic elitism\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            # Retain the best individual in the population\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 26, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12289 with standard deviation 0.01729.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.1326161606592937, 0.1326162525869249, 0.13339354496109512, 0.13691834239062484, 0.13691844719494517, 0.13780611308164725, 0.09847861100646704, 0.098478643490988, 0.09875120622525857]}}
{"id": "a32161bc-5f39-4afd-aab0-0033c5e8c1c8", "fitness": 0.1227288895943822, "name": "EnhancedMetaheuristic", "description": "This refined algorithm incorporates adaptive learning for mutation control, adjusting the differential weight based on historical performance to enhance convergence.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        # Logistic map for chaotic behavior\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)  # Increase differential weight slightly\n            self.cr = max(0.1, self.cr - 0.05)  # Decrease crossover probability slightly\n        self.f = (self.f + abs(trial_fitness - current_fitness) / 10) % 0.9  # Adaptive learning for mutation control\n    \n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection with dynamic elitism\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            # Retain the best individual in the population\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 27, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12273 with standard deviation 0.01722.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.1327001498897611, 0.1326621696211272, 0.13269499733621393, 0.13701411911201045, 0.13697080187717403, 0.1370083249762104, 0.09850825811398178, 0.09849486101274252, 0.09850632441021856]}}
{"id": "009bb0c9-73cb-4a68-a0a9-aa692c4351a5", "fitness": 0.1229001453015955, "name": "EnhancedMetaheuristic", "description": "Introducing stochastic ranking in the selection process to balance exploration and exploitation effectively.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        # Logistic map for chaotic behavior\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)  # Increase differential weight slightly\n            self.cr = max(0.1, self.cr - 0.05)  # Decrease crossover probability slightly\n    \n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def stochastic_ranking(self, trial_fitness, current_fitness):\n        # Probabilistically decide to accept worse solutions\n        if np.random.rand() < 0.45:\n            return trial_fitness < current_fitness\n        else:\n            return current_fitness <= trial_fitness\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection with dynamic elitism\n                if self.stochastic_ranking(trial_fitness, self.fitness[i]):\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            # Retain the best individual in the population\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 28, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12290 with standard deviation 0.01729.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.13263954558746616, 0.1326495865461591, 0.13338655036131863, 0.13694500520849817, 0.13695645519731203, 0.13779812865897356, 0.09848687139803058, 0.09849041596880259, 0.0987487487877986]}}
{"id": "9767d6c2-b5ed-4bff-beb1-3f1611e05e1d", "fitness": 0.12277535989544945, "name": "EnhancedMetaheuristic", "description": "Improved adaptation of parameters and mutation strategy to enhance convergence efficiency and solution quality.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        # Logistic map for chaotic behavior\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n        self.f += 0.1 * np.sin(generation)  # Slightly adjust f with a sinusoidal component\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)  # Increase differential weight slightly\n            self.cr = max(0.1, self.cr - 0.05)  # Decrease crossover probability slightly\n    \n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation with dynamic factor\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection with dynamic elitism\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            # Retain the best individual in the population\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 29, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12278 with standard deviation 0.01724.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.13287064269091797, 0.13267685942873875, 0.13267755808019321, 0.1372086405351992, 0.1369875538563241, 0.1369883515190573, 0.0985682965599396, 0.09850004557205894, 0.09850029081661604]}}
{"id": "73dd7d82-c2bf-4a28-bf8b-8452f02ecd04", "fitness": 0.12298826408478132, "name": "EnhancedMetaheuristic", "description": "Optimized parameter adaptation using cosine map to enhance convergence speed.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        # Use cosine map for chaotic behavior\n        return 0.5 * (1 + np.cos(np.pi * value))\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)  # Increase differential weight slightly\n            self.cr = max(0.1, self.cr - 0.05)  # Decrease crossover probability slightly\n    \n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection with dynamic elitism\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            # Retain the best individual in the population\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 30, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12299 with standard deviation 0.01733.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.13267124027787414, 0.1329290334504989, 0.13339354492670863, 0.13698114609065482, 0.13727547716080013, 0.1378061130424424, 0.09849806207431999, 0.09858855352662332, 0.09875120621310973]}}
{"id": "1835c22a-6eff-4904-9a57-e2cd99505078", "fitness": 0.12273133967208968, "name": "EnhancedMetaheuristic", "description": "This enhanced metaheuristic incorporates adaptive mutation scaling to dynamically adjust diversification and intensification, improving search efficiency.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        # Logistic map for chaotic behavior\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)  # Increase differential weight slightly\n            self.cr = max(0.1, self.cr - 0.05)  # Decrease crossover probability slightly\n\n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation with adaptive scaling\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                scaling_factor = 0.5 + 0.3 * np.random.rand()  # Added adaptive scaling\n                mutant = self.population[a] + scaling_factor * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Selection with dynamic elitism\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            # Retain the best individual in the population\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 31, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12273 with standard deviation 0.01722.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.13265655907256269, 0.13270713383491406, 0.1327024546212332, 0.13696440589252623, 0.13702208087894618, 0.1370167488234818, 0.09849287773871329, 0.0985107266945191, 0.09850906949191063]}}
{"id": "19c44c28-34bf-4209-9fcd-82d9d9a0e748", "fitness": 0.17873477225744752, "name": "ImprovedMetaheuristic", "description": "This improved algorithm integrates adaptive learning rates with differential evolution, enhancing exploration and exploitation by dynamically adjusting parameters based on population diversity and convergence rate.", "code": "import numpy as np\n\nclass ImprovedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def diversity(self):\n        return np.mean(np.std(self.population, axis=0))\n\n    def adapt_parameters(self, generation, diversity_level):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n        if diversity_level < 0.1:\n            self.f = max(0.1, self.f * 0.9)\n            self.cr = min(1.0, self.cr * 1.1)\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            diversity_level = self.diversity()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation, diversity_level)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 32, "feedback": "The algorithm ImprovedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17873 with standard deviation 0.08289.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.13263561019564651, 0.1326624287197692, 0.28483958572352974, 0.13694051744380242, 0.13697109681927966, 0.3355109210487186, 0.09848548234168941, 0.09849495321361001, 0.252072354810982]}}
{"id": "c5e2f0d2-e3e2-4eac-9eb0-42e745c68d83", "fitness": 0.29875042437388366, "name": "DynamicMemoryMetaheuristic", "description": "This novel algorithm introduces a dynamic memory-based component that adaptively preserves and reintroduces promising solutions throughout the search process, enhancing exploration and exploitation for better convergence.", "code": "import numpy as np\n\nclass DynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5  # Initial differential weight\n        self.cr = 0.9  # Initial crossover probability\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []  # Memory to store promising solutions\n\n    def chaotic_map(self, value):\n        # Logistic map for chaotic behavior\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        # Use chaotic map for parameter adaptation\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)  # Increase differential weight slightly\n            self.cr = max(0.1, self.cr - 0.05)  # Decrease crossover probability slightly\n    \n    def adapt_population_size(self):\n        # Reduce population size gradually as budget decreases\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        # Maintain a set of promising solutions\n        self.memory.append((trial.copy(), trial_fitness))\n        # Keep memory size manageable\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.1:  # 10% chance to reintroduce\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                # Mutation\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Attempt to reintroduce from memory\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                # Evaluate\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                # Update memory\n                self.update_memory(trial, trial_fitness)\n\n                # Selection with dynamic elitism\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                # Update best fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            # Retain the best individual in the population\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        # Return the best found solution\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 33, "feedback": "The algorithm DynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29875 with standard deviation 0.13538.", "error": "", "parent_ids": ["c588c2a0-0667-4c74-8175-f2680533e111"], "operator": null, "metadata": {"aucs": [0.13268433899150944, 0.4557948684768057, 0.33249532528453696, 0.13699609063007878, 0.45902145025873453, 0.33907213593241026, 0.09850267508322275, 0.43287924214648876, 0.30130769256116585]}}
{"id": "3337dbfc-3865-4798-a1ae-deca4da6f54b", "fitness": 0.3334767132139587, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "This refined algorithm integrates adaptive chaotic mapping with memory-augmented differential evolution, emphasizing dynamic parameter tuning and elite reintroduction for enhanced convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.15:  # Increased to 15%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 34, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33348 with standard deviation 0.18258.", "error": "", "parent_ids": ["c5e2f0d2-e3e2-4eac-9eb0-42e745c68d83"], "operator": null, "metadata": {"aucs": [0.13283533397150116, 0.3360546267373431, 0.576120612454551, 0.13716840224470483, 0.3396435659575169, 0.5789038319941608, 0.0985557967462819, 0.26361789746661923, 0.5383903513529495]}}
{"id": "592fe456-313c-40ea-b3db-4268babc5813", "fitness": -Infinity, "name": "ImprovedDynamicMemoryMetaheuristic", "description": "Introducing adaptive Lévy flight mutations and dynamic crossover strategies to enhance exploration and convergence in chaotic memory-augmented differential evolution.", "code": "import numpy as np\n\nclass ImprovedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = 0.9 - 0.7 * (self.evaluations / self.budget)  # Dynamic crossover\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.2:  # Increased to 20%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def levy_flight(self, step_size):\n        beta = 1.5\n        sigma = (np.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1 / beta)\n        u = np.random.normal(0, sigma, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / np.abs(v)**(1 / beta)\n        return step_size * step\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                if np.random.rand() < 0.3:  # Lévy flight probability\n                    trial += self.levy_flight(0.1)\n\n                trial = np.clip(trial, self.lower_bound, self.upper_bound)\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 35, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_ids": ["3337dbfc-3865-4798-a1ae-deca4da6f54b"], "operator": null, "metadata": {}}
{"id": "96732a9a-ab41-4d24-980e-65aec511529f", "fitness": 0.18134949553078877, "name": "EnhancedDynamicMemoryMetaheuristicV2", "description": "This enhanced algorithm optimizes adaptive parameter tuning and reintroduction strategies by incorporating self-adaptive crossover rates and global best guidance for improved convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristicV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, fitness_improvement):\n        if fitness_improvement:\n            self.f = min(0.9, self.f + 0.05)\n        else:\n            self.f = max(0.1, self.f - 0.03)\n        self.cr = np.random.normal(loc=0.5, scale=0.1)\n        self.cr = np.clip(self.cr, 0.1, 0.9)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.15:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                if self.best_solution is not None:\n                    r = np.random.rand(self.dim) < 0.1\n                    trial = np.where(r, self.best_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                fitness_improvement = trial_fitness < self.fitness[i]\n                self.adapt_parameters(fitness_improvement)\n\n                self.update_memory(trial, trial_fitness)\n\n                if fitness_improvement:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial.copy()\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 36, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristicV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18135 with standard deviation 0.08153.", "error": "", "parent_ids": ["3337dbfc-3865-4798-a1ae-deca4da6f54b"], "operator": null, "metadata": {"aucs": [0.13271301226375531, 0.13808574858853717, 0.3021659537277873, 0.13702905051612901, 0.14349688444595077, 0.3057157104929462, 0.09851243497146267, 0.1000667911107268, 0.27435987365980374]}}
{"id": "06c95d16-4030-4827-a248-42858254bb64", "fitness": 0.12285883693276661, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "This improved algorithm incorporates an adaptive Lévy flight mechanism with a redefined memory reintroduction strategy, enhancing exploration and exploitation balance with adaptive step-size adjustments.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def levy_flight(self, dimension):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma, dimension)\n        v = np.random.normal(0, 1, dimension)\n        return u / (np.abs(v) ** (1 / beta))\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if len(self.memory) > 1 and np.random.rand() < 0.20:  # 20% chance\n            idx1, idx2 = np.random.choice(len(self.memory), 2, replace=False)\n            return (self.memory[idx1][0] + self.memory[idx2][0]) / 2\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Apply Lévy flight to enhance exploration\n                trial += self.levy_flight(self.dim)\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 37, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12286 with standard deviation 0.01727.", "error": "", "parent_ids": ["3337dbfc-3865-4798-a1ae-deca4da6f54b"], "operator": null, "metadata": {"aucs": [0.13267135309034173, 0.13301197959761712, 0.13284312655426667, 0.13698127523745385, 0.137370199924144, 0.13717724025856948, 0.09849810118034796, 0.0986176410417936, 0.09855861551036493]}}
{"id": "abd6a304-920a-45da-b087-e47201c76984", "fitness": 0.2859299965011032, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "Enhanced chaotic mapping and memory reintroduction with adaptive parameter adjustment for improved convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        if generation % 10 == 0:  # Change 1: Adapt parameters every 10 generations\n            self.f = max(0.1, self.chaotic_map(self.f))\n            self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.2:  # Change 2: Increased reintroduction probability to 20%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 38, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28593 with standard deviation 0.11963.", "error": "", "parent_ids": ["3337dbfc-3865-4798-a1ae-deca4da6f54b"], "operator": null, "metadata": {"aucs": [0.41614501997167697, 0.13277943417968507, 0.347202878102, 0.41935605565458545, 0.1371045749984089, 0.3504703001219629, 0.35040217969870213, 0.09853618123002328, 0.321373344552884]}}
{"id": "4f5cc630-c8f6-42ba-b908-eccd3062b92c", "fitness": 0.12290977974100176, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "This enhanced algorithm optimizes adaptive parameters with a novel mutation strategy and improved memory utilization for superior convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.20:  # Modified probability to 20%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 39, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12291 with standard deviation 0.01730.", "error": "", "parent_ids": ["3337dbfc-3865-4798-a1ae-deca4da6f54b"], "operator": null, "metadata": {"aucs": [0.13271506384126608, 0.13311458302682455, 0.132880757299362, 0.13703113524069177, 0.13748731879671539, 0.1372203858284765, 0.09851350969184425, 0.09865369496620668, 0.09857156897762853]}}
{"id": "ed62849c-d7cd-48f2-93a8-d1c9e5e89924", "fitness": 0.22403566265076139, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "Introduce diversity control and adaptive mutation scaling to balance exploration and exploitation in memory-augmented differential evolution.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.diversity_threshold = 0.1  # New: diversity control threshold\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.15:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def evaluate_diversity(self):\n        # New: Evaluate population diversity\n        return np.std(self.population, axis=0).mean()\n\n    def adapt_mutation_scaling(self):\n        # New: Adjust mutation scaling based on diversity\n        diversity = self.evaluate_diversity()\n        if diversity < self.diversity_threshold:\n            self.f = max(0.1, self.f - 0.1)\n        else:\n            self.f = min(0.9, self.f + 0.1)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n                self.adapt_mutation_scaling()  # New: Call adaptive mutation scaling\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22404 with standard deviation 0.14417.", "error": "", "parent_ids": ["3337dbfc-3865-4798-a1ae-deca4da6f54b"], "operator": null, "metadata": {"aucs": [0.13272818170577194, 0.13264116173610085, 0.43185532902319024, 0.13704609554665892, 0.13694684768534315, 0.440139602507979, 0.09851813804186327, 0.09848744257810882, 0.40795816503183624]}}
{"id": "d5f16e5c-6146-42ee-b928-bd8a8bd0a2c3", "fitness": 0.12290977974100176, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "Slightly increase the probability of reintroducing solutions from memory to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.2:  # Changed from 15% to 20%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 41, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12291 with standard deviation 0.01730.", "error": "", "parent_ids": ["3337dbfc-3865-4798-a1ae-deca4da6f54b"], "operator": null, "metadata": {"aucs": [0.13271506384126608, 0.13311458302682455, 0.132880757299362, 0.13703113524069177, 0.13748731879671539, 0.1372203858284765, 0.09851350969184425, 0.09865369496620668, 0.09857156897762853]}}
{"id": "e65ec14a-a5d7-4bb1-8d85-b6a176ad8a69", "fitness": 0.32011573578378877, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "This novel algorithm leverages adaptive chaotic mapping with memory-augmented differential evolution, including a new stochastic reinitialization strategy and step-size adaptation for improved diversity and convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.step_size = 0.1\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.15:  # Retained at 15%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def stochastic_reinitialization(self):\n        if np.random.rand() < 0.1:  # Introduce new strategy with 10% probability\n            return np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                stochastic_solution = self.stochastic_reinitialization()\n                if stochastic_solution is not None:\n                    trial = stochastic_solution\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32012 with standard deviation 0.14854.", "error": "", "parent_ids": ["3337dbfc-3865-4798-a1ae-deca4da6f54b"], "operator": null, "metadata": {"aucs": [0.4962940460078975, 0.4115226742455321, 0.13349165229325632, 0.498802143544682, 0.414919984517098, 0.13791838983531102, 0.33938157615617937, 0.34992587883725423, 0.0987852766168883]}}
{"id": "66cdb9e6-8418-466b-84db-255b2c77dd20", "fitness": 0.12271875265080201, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "This algorithm augments the original with a new hybrid mutation strategy integrating opposition-based learning and a more aggressive reintroduction from memory to enhance exploration and convergence speed.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr) + 0.05)  # Adjust cr slightly\n                                                            \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:  # Increased probability to 25%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def opposition_based_mutation(self, individual):\n        opposite = self.lower_bound + self.upper_bound - individual\n        return np.clip(opposite, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.opposition_based_mutation(self.population[a])  # Use opposition-based mutation\n                mutant = np.clip(mutant + self.f * (self.population[b] - self.population[c]), self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 43, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12272 with standard deviation 0.01722.", "error": "", "parent_ids": ["3337dbfc-3865-4798-a1ae-deca4da6f54b"], "operator": null, "metadata": {"aucs": [0.13270891488243297, 0.1326513955997467, 0.13266040135288326, 0.13702411170025663, 0.13695851665951075, 0.13696878760585762, 0.09851135564980762, 0.09849105665919222, 0.0984942337475303]}}
{"id": "39c97c26-0aef-46cd-9c72-8bc8d7f443dc", "fitness": 0.1770826749989254, "name": "EnhancedHybridMetaheuristic", "description": "This improved algorithm introduces a hybrid self-adaptive parameter control strategy in combination with dynamic memory reintroduction, aiming for robust convergence across diverse optimization landscapes.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, self.budget // 10)\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        if generation % 2 == 0:\n            self.f = np.random.uniform(0.4, 0.9)\n            self.cr = np.random.uniform(0.1, 1.0)\n        else:\n            self.f = max(0.1, self.chaotic_map(self.f))\n            self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size:\n            self.population_size = max(5, remaining_budget // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.15:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17708 with standard deviation 0.07977.", "error": "", "parent_ids": ["3337dbfc-3865-4798-a1ae-deca4da6f54b"], "operator": null, "metadata": {"aucs": [0.30358590661596596, 0.1326621308532132, 0.13274186929734766, 0.30738918395769155, 0.13697075950666537, 0.13706170639493787, 0.24631470730177973, 0.09849484470574832, 0.09852296635697899]}}
{"id": "8f975cce-f8ec-4366-8794-e20e31d2f8d9", "fitness": 0.12288636875372808, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "Leveraging stochastic population variance with self-adapting crossover and mutation intensities for balanced exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self):\n        # Adjust mutation factor and crossover rate based on the best solution\n        self.f = max(0.1, self.chaotic_map(np.min(self.fitness) / self.best_fitness))\n        self.cr = min(1.0, self.chaotic_map(np.max(self.fitness) / self.best_fitness))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.15:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters()\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 45, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12289 with standard deviation 0.01729.", "error": "", "parent_ids": ["3337dbfc-3865-4798-a1ae-deca4da6f54b"], "operator": null, "metadata": {"aucs": [0.1326161592070485, 0.13261625284018863, 0.1333935450316468, 0.1369183407349932, 0.13691844748367754, 0.13780611316207847, 0.09847861049323037, 0.09847864358049518, 0.09875120625019373]}}
{"id": "5ea53380-9ee8-4b45-bf19-1f487485d08b", "fitness": -Infinity, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "This enhanced metaheuristic algorithm introduces multi-strategy differential evolution with adaptive local search and fitness-weighted reintroduction for superior convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:  # Increased to 25%\n            weights = np.array([1.0 / (m[1] + 1e-9) for m in self.memory])\n            weights /= weights.sum()\n            idx = np.random.choice(len(self.memory), p=weights)\n            return self.memory[idx][0]\n        return None\n\n    def local_search(self, solution):\n        local_step = np.random.uniform(-0.1, 0.1, self.dim)\n        local_solution = np.clip(solution + local_step, self.lower_bound, self.upper_bound)\n        return local_solution\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                local_trial = self.local_search(self.population[i])\n                local_fitness = func(local_trial)\n                self.evaluations += 1\n\n                if local_fitness < self.fitness[i]:\n                    self.population[i] = local_trial\n                    self.fitness[i] = local_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 46, "feedback": "An exception occurred: ValueError('probabilities are not non-negative').", "error": "ValueError('probabilities are not non-negative')", "parent_ids": ["3337dbfc-3865-4798-a1ae-deca4da6f54b"], "operator": null, "metadata": {}}
{"id": "f9f7976d-adc1-4235-a50f-cb5e1d621925", "fitness": 0.212453415382921, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "The algorithm incorporates adaptive chaotic mapping with memory-augmented differential evolution, now introducing a dynamic learning phase for parameters and population diversity management, aiming for accelerated convergence and robustness.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.learning_phase = True\n        self.learning_threshold = self.budget // 4\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        if self.learning_phase:\n            self.f = max(0.1, self.chaotic_map(self.f))\n            self.cr = min(1.0, self.chaotic_map(self.cr))\n        else:\n            self.f = 0.5 + 0.3 * np.sin(np.pi * generation / self.budget)\n            self.cr = 0.9 - 0.4 * np.cos(np.pi * generation / self.budget)\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.05)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.15:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def manage_diversity(self):\n        if np.std(self.fitness) < 0.01:\n            for i in range(self.population_size):\n                if np.random.rand() < 0.1:\n                    self.population[i] = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            if self.evaluations > self.learning_threshold and self.learning_phase:\n                self.learning_phase = False\n\n            self.manage_diversity()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 47, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.21245 with standard deviation 0.07980.", "error": "", "parent_ids": ["3337dbfc-3865-4798-a1ae-deca4da6f54b"], "operator": null, "metadata": {"aucs": [0.13277382617646916, 0.22471535539780174, 0.31492094799247494, 0.13709817307931482, 0.23103448441768581, 0.35021456017118324, 0.09853421142473928, 0.17729605249963376, 0.24549312728698625]}}
{"id": "72a2d33d-a0e0-4863-8c79-81153801b5b1", "fitness": 0.1693892175423729, "name": "ChaoticMemoryDifferentialEvolution", "description": "This algorithm enhances convergence by employing adaptive memory-enhanced chaos-driven differential evolution, incorporating elite reintroduction with diversity preservation and parameter stabilization strategies.", "code": "import numpy as np\n\nclass ChaoticMemoryDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self):\n        self.f = max(0.1, min(0.9, self.chaotic_map(self.f)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr)))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.02)\n            self.cr = max(0.1, self.cr - 0.02)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.10:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def maintain_diversity(self):\n        return np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters()\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                if np.random.rand() < 0.05:\n                    trial = self.maintain_diversity()\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 48, "feedback": "The algorithm ChaoticMemoryDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.16939 with standard deviation 0.07062.", "error": "", "parent_ids": ["3337dbfc-3865-4798-a1ae-deca4da6f54b"], "operator": null, "metadata": {"aucs": [0.13274398890213845, 0.28094951260995393, 0.1337977359093685, 0.13706415407249484, 0.2975683626253025, 0.13827321007935967, 0.09852367114846738, 0.20669650666772021, 0.09888581586655054]}}
{"id": "00eec5c0-9b04-4f41-8771-5a0d78491f61", "fitness": 0.1228125129387598, "name": "EnhancedAdaptiveMemoryMetaheuristic", "description": "This novel approach combines adaptive parameter control with an elite-driven memory reintroduction strategy, emphasizing convergence efficiency through adaptive mutation and crossover rates.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        improvement = max(0.01, (current_fitness - trial_fitness) / abs(current_fitness))\n        self.f = min(0.9, self.f + improvement * 0.1)\n        self.cr = max(0.1, self.cr - improvement * 0.1)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.2:  # Increased to 20%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n                self.update_memory(trial, trial_fitness)\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 49, "feedback": "The algorithm EnhancedAdaptiveMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12281 with standard deviation 0.01726.", "error": "", "parent_ids": ["3337dbfc-3865-4798-a1ae-deca4da6f54b"], "operator": null, "metadata": {"aucs": [0.13276824039856738, 0.13279070231859302, 0.1328062696484258, 0.13709188763868818, 0.13711742944228622, 0.13713521684643137, 0.09851712147633895, 0.0985401518230119, 0.09854559685649533]}}
{"id": "e57fbe7a-c75f-410d-9f15-476fd91f0fdd", "fitness": 0.4262505509156563, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "This improved algorithm leverages adaptive mutation scaling and dynamic crossover probability, combined with elite reintroduction, to optimize convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6  # Changed initial mutation factor\n        self.cr = 0.7  # Changed initial crossover rate\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)  # Adjusted crossover decrement\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.20:  # Increased to 20%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 50, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42625 with standard deviation 0.16255.", "error": "", "parent_ids": ["3337dbfc-3865-4798-a1ae-deca4da6f54b"], "operator": null, "metadata": {"aucs": [0.4859154563657757, 0.22487963786644283, 0.6087655983898498, 0.49539615123118863, 0.2289525038460567, 0.6111930939091916, 0.3957851575456639, 0.19356822451710642, 0.5917991345696312]}}
{"id": "14ba2d9d-f68b-451d-af9b-7ad13d0297ff", "fitness": 0.27968703797478356, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "This refined algorithm introduces chaotic adaptation of mutation and crossover rates, coupled with periodic elite reintroduction to enhance convergence and exploration balance.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.02)  # Adjusted to slower increment\n            self.cr = max(0.1, self.cr - 0.02)  # Adjusted to slower decrement\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:  # Increased probability to 25%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 51, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27969 with standard deviation 0.11664.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.13263532614910867, 0.34200784533197015, 0.41125939036648484, 0.13694019371184296, 0.3457023252945992, 0.4142905232655002, 0.09848538182086475, 0.2953307258013319, 0.3405316300313491]}}
{"id": "cc66828f-c764-4d9c-a3ba-9ad693eafee0", "fitness": 0.3156333089039954, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "This slightly refined algorithm improves the handling of mutation factor scaling to enhance convergence speed.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6  # Changed initial mutation factor\n        self.cr = 0.7  # Changed initial crossover rate\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.15, self.chaotic_map(self.f))  # Updated mutation factor scaling\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)  # Adjusted crossover decrement\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.20:  # Increased to 20%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 52, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31563 with standard deviation 0.10425.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.4777166430682618, 0.2747391693098641, 0.2447854259013953, 0.4807804109847794, 0.27843160003329603, 0.24855406353620335, 0.4140553028501005, 0.20668167538388316, 0.21495548906817463]}}
{"id": "9982a603-5c05-4906-a7b3-12028aa1a461", "fitness": 0.17578973811780782, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "This refined algorithm enhances convergence by introducing a log-sigmoid adaptive mutation factor and a flexible elite memory size, improving memory utilization and search efficacy.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6  # Changed initial mutation factor\n        self.cr = 0.7  # Changed initial crossover rate\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, 1 / (1 + np.exp(-10 * (self.f - 0.5))))  # Log-sigmoid adaptation\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)  # Adjusted crossover decrement\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size + 5:  # Flexible memory size\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size + 5]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.20:  # Increased to 20%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 53, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17579 with standard deviation 0.07738.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.13269739948208426, 0.28448618780113466, 0.13264794801509816, 0.13701103529316905, 0.3071365959752158, 0.13695458657323156, 0.09850721282499764, 0.25417683931624746, 0.0984898377790917]}}
{"id": "ab2fb848-ad45-4d6c-9f5a-cd51676ebfba", "fitness": 0.12279592066763786, "name": "RefinedEnhancedDynamicMemoryMetaheuristic", "description": "This refined algorithm integrates a hybrid chaotic map for adaptive parameter tuning and includes a memory-based perturbation strategy to enhance global exploration.", "code": "import numpy as np\n\nclass RefinedEnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def hybrid_chaotic_map(self, value, generation):\n        logistic = 3.8 * value * (1.0 - value)\n        sin = np.sin(3.14 * value) + generation * 0.001\n        return (logistic + sin) / 2\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.hybrid_chaotic_map(self.f, generation))\n        self.cr = min(1.0, self.hybrid_chaotic_map(self.cr, generation))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def perturb_memory_solution(self, solution):\n        noise = np.random.uniform(-0.1, 0.1, self.dim)\n        perturbed_solution = solution + noise\n        return np.clip(perturbed_solution, self.lower_bound, self.upper_bound)\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.20:\n            idx = np.random.choice(len(self.memory))\n            return self.perturb_memory_solution(self.memory[idx][0])\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 54, "feedback": "The algorithm RefinedEnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12280 with standard deviation 0.01725.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.13296461158673833, 0.13269396705763048, 0.13264073249407526, 0.13731599283065987, 0.1370070697006872, 0.13694636020887452, 0.09860118991759292, 0.09850607401583356, 0.09848728819664876]}}
{"id": "81a9d7ca-7f8f-4382-900a-37001975de04", "fitness": 0.41498516352389747, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "Introducing adaptive elite preservation and memory influence to enhance the search efficiency.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6 \n        self.cr = 0.7 \n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03) \n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:  # Increased to 25%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None and np.random.rand() < 0.5:  # 50% chance to prioritize memory\n                    trial = memory_solution\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[np.random.randint(1, self.population_size)] = elite  # Preserve elite randomly\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 55, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41499 with standard deviation 0.15476.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.5268799355189347, 0.23140748807674405, 0.5441401296620783, 0.53019001840771, 0.236256999387236, 0.5466608234136959, 0.5046659882818878, 0.1357509318838298, 0.47891415708296037]}}
{"id": "73468fa6-9c60-4ef7-a077-f7fa5789884c", "fitness": 0.27968703797478356, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "This improved algorithm leverages adaptive mutation scaling and dynamic crossover probability, combined with elite reintroduction and enhanced chaotic adaptation, to optimize convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6  # Changed initial mutation factor\n        self.cr = 0.7  # Changed initial crossover rate\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)  # Adjusted crossover decrement\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:  # Increased to 25%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 56, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27969 with standard deviation 0.11664.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.13263532614910867, 0.34200784533197015, 0.41125939036648484, 0.13694019371184296, 0.3457023252945992, 0.4142905232655002, 0.09848538182086475, 0.2953307258013319, 0.3405316300313491]}}
{"id": "e4c1bbfc-2c44-4402-bfbd-9f41ef39941d", "fitness": 0.3721066415445613, "name": "RefinedDynamicMemoryMetaheuristic", "description": "This refined algorithm introduces differential grouping for dynamic dimensional adaptation and integrates a Lévy flight mechanism for global exploration, enhancing convergence in high-dimensional spaces.", "code": "import numpy as np\n\nclass RefinedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.dim_group_size = max(1, self.dim // 5)\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def levy_flight(self, L):\n        u = np.random.normal(0, 1, L)\n        v = np.random.normal(0, 1, L)\n        step = u / np.abs(v) ** (1/3)\n        return step\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.20:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Apply Lévy flight to a random dimension group\n                if np.random.rand() < 0.1:  # 10% chance for global exploration\n                    start_dim = np.random.randint(0, self.dim - self.dim_group_size)\n                    end_dim = start_dim + self.dim_group_size\n                    trial[start_dim:end_dim] += self.levy_flight(end_dim - start_dim)\n                    trial = np.clip(trial, self.lower_bound, self.upper_bound)\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 57, "feedback": "The algorithm RefinedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.37211 with standard deviation 0.17828.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.6358187534356408, 0.3764792005551736, 0.19317738261252504, 0.6376545667525693, 0.3802860480626482, 0.1973486561430422, 0.493564452979787, 0.31902039538389715, 0.11561031797576893]}}
{"id": "baa58dbd-ebfe-4dda-a02d-b0cc686c15c3", "fitness": 0.1228960574294258, "name": "AdvancedLogDecayMetaheuristic", "description": "An advanced heuristic using a logarithmic-decay mutation strategy and dynamic crossover for improved exploration and exploitation balance in black-box optimization.", "code": "import numpy as np\n\nclass AdvancedLogDecayMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def logarithmic_decay(self, generation, total_generations):\n        return self.f * (1 - np.log1p(generation) / np.log1p(total_generations))\n\n    def adapt_parameters(self, generation, total_generations):\n        self.f = self.logarithmic_decay(generation, total_generations)\n        self.cr = min(1.0, 1.0 / (1.0 + np.exp(-0.1 * (generation - total_generations / 2))))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.30:  # Changed to 30%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        total_generations = self.budget // self.population_size\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation, total_generations)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 58, "feedback": "The algorithm AdvancedLogDecayMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12290 with standard deviation 0.01729.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.13262593313210969, 0.13263161510565902, 0.13340338613565828, 0.1369294840727746, 0.13693596246344242, 0.13781737108060443, 0.09848206386202951, 0.09848407098237288, 0.09875463003018137]}}
{"id": "3a24ca66-c58b-484a-876a-0b2b514d558f", "fitness": 0.14151099984515103, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "This improved algorithm utilizes adaptive mutation scaling, dynamic crossover probability, elite reintroduction, and a chaotic weight factor to enhance convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6  # Changed initial mutation factor\n        self.cr = 0.7  # Changed initial crossover rate\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.weight_factor = 0.5  # New weight factor for chaotic map\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)  # Adjusted crossover decrement\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.20:  # Increased to 20%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution * self.weight_factor, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 59, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.14151 with standard deviation 0.04396.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.2163609929856547, 0.1327531669003208, 0.13268918636300042, 0.2202866796000149, 0.1370746126973167, 0.13700162329823407, 0.10040143630515042, 0.09852692141974029, 0.09850437903692699]}}
{"id": "b58ca32a-8ada-4ffd-95c9-4469fef349ff", "fitness": 0.13642334394045064, "name": "EnhancedNonlinearDynamicMemoryMetaheuristic", "description": "The novel EnhancedNonlinearDynamicMemoryMetaheuristic leverages chaotic nonlinear mappings for parameter adaptation, incorporates probabilistic elite reintroduction, and dynamically adjusts mutation and crossover rates to enhance convergence efficiency in diverse black box optimization problems.", "code": "import numpy as np\n\nclass EnhancedNonlinearDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.7  # Slightly increased initial mutation factor\n        self.cr = 0.8  # Slightly increased initial crossover rate\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return np.sin(value * np.pi)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, abs(self.chaotic_map(self.f)))\n        self.cr = min(1.0, abs(self.chaotic_map(self.cr)))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.1)\n            self.cr = max(0.1, self.cr - 0.05)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, int((self.budget - self.evaluations) / 1.5))\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:  # Increased to 25%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 60, "feedback": "The algorithm EnhancedNonlinearDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13642 with standard deviation 0.03632.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.17580817201934174, 0.13298234040589507, 0.1326887103051413, 0.21464906015328844, 0.1373365009055979, 0.13700108714957238, 0.10023297969373146, 0.09860704336541637, 0.09850420146607108]}}
{"id": "bcd51479-1d74-4770-9b57-a169465b290a", "fitness": 0.32344502000728437, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "Introduced an adaptive scaling factor for memory reintroduction probability to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6  # Changed initial mutation factor\n        self.cr = 0.7  # Changed initial crossover rate\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)  # Adjusted crossover decrement\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        prob = 0.10 + 0.10 * (1 - self.best_fitness / (abs(self.best_fitness) + 1e-8))  # Adapted probability\n        if self.memory and np.random.rand() < prob:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32345 with standard deviation 0.25678.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.1327626468690647, 0.16265874371782185, 0.7044571566325374, 0.13708557288316348, 0.20247773394206914, 0.7059247191842906, 0.09853005962764372, 0.12610909001083426, 0.6409994571981343]}}
{"id": "6f0356f6-aa54-47f0-b025-1f5854389815", "fitness": 0.24286152361697574, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "This enhanced algorithm utilizes self-adaptive mutation and crossover parameters, coupled with strategic memory reintroduction and population diversity preservation, to ensure robust global optimization capability.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.8\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.03)\n            self.cr = max(0.1, self.cr - 0.02)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def maintain_diversity(self):\n        return np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                if np.random.rand() < 0.1:\n                    trial = self.maintain_diversity()\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 62, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24286 with standard deviation 0.08959.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.13390169305949817, 0.3388329084420527, 0.27601597808999745, 0.13840175579413017, 0.3422687456048368, 0.30397931936632383, 0.09890992417130406, 0.31216889552561133, 0.24127449249902733]}}
{"id": "efb08a7f-9001-4422-8f86-56ae0fd5c5b8", "fitness": 0.27968703797478356, "name": "AdvancedChaoticMetaheuristic", "description": "This enhanced algorithm incorporates adaptive learning rates and differential elitism, leveraging chaotic mappings and memory-driven mutation, to improve convergence and exploration in optimization tasks.", "code": "import numpy as np\n\nclass AdvancedChaoticMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:  # Increased to 25%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 63, "feedback": "The algorithm AdvancedChaoticMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27969 with standard deviation 0.11664.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.13263532614910867, 0.34200784533197015, 0.41125939036648484, 0.13694019371184296, 0.3457023252945992, 0.4142905232655002, 0.09848538182086475, 0.2953307258013319, 0.3405316300313491]}}
{"id": "492198d9-9bf9-4ed7-828e-44da8d7a26d0", "fitness": 0.12336771755160772, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "This enhanced algorithm introduces dynamic adaptive mutation and crossover strategies using fitness-based momentum, alongside a hybrid memory mechanism to optimize exploitation and exploration balance.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.5\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.momentum = 0.1\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n        else:\n            self.f = max(0.1, self.f - self.momentum * (trial_fitness - current_fitness) / (abs(current_fitness) + 1e-8))\n            self.cr = min(0.9, self.cr + self.momentum * (trial_fitness - current_fitness) / (abs(current_fitness) + 1e-8))\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size // 2] + self.memory[-self.population_size // 2:]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 64, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12337 with standard deviation 0.01758.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.13263776358845525, 0.13267359141988488, 0.1349920311730739, 0.1369429728987014, 0.13698383015483873, 0.14001393081889602, 0.09848624268417727, 0.09849888778737292, 0.09908020743906909]}}
{"id": "170ee9d8-5dfe-411c-a607-cd455d496e1b", "fitness": 0.2028373903310663, "name": "RefinedChaoticEliteReintroduction", "description": "This refined algorithm integrates a chaotic map-based parameter adaptation with a hybridized elite reintroduction and differential crossover for enhanced convergence speed and solution accuracy.", "code": "import numpy as np\n\nclass RefinedChaoticEliteReintroduction:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.7  # Slightly higher initial mutation factor\n        self.cr = 0.8  # Slightly higher initial crossover rate\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        if generation % 5 == 0:  # Adapt parameters periodically\n            self.f = max(0.1, self.chaotic_map(self.f))\n            self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.02)  # Slightly reduced decrement\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:  # Increased probability to 25%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 65, "feedback": "The algorithm RefinedChaoticEliteReintroduction got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20284 with standard deviation 0.03647.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.23196855793668747, 0.19917251168060313, 0.20082339941956995, 0.28478927539134136, 0.20492273092408708, 0.20493103287495074, 0.18048809336556615, 0.16522437274758361, 0.15321653863920703]}}
{"id": "cc06a424-ae7a-40fd-97d1-4097302514da", "fitness": 0.4262505509156563, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "Enhanced mutation diversity by replacing one parent with a memory solution, improving exploration.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6  # Changed initial mutation factor\n        self.cr = 0.7  # Changed initial crossover rate\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)  # Adjusted crossover decrement\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.20:  # Increased to 20%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 66, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42625 with standard deviation 0.16255.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.4859154563657757, 0.22487963786644283, 0.6087655983898498, 0.49539615123118863, 0.2289525038460567, 0.6111930939091916, 0.3957851575456639, 0.19356822451710642, 0.5917991345696312]}}
{"id": "2ae79762-b4bd-4a76-a409-127d342f28a4", "fitness": 0.27968703797478356, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "This refined algorithm incorporates a dynamic learning rate and a more balanced memory reintroduction mechanism to enhance adaptive parameter tuning and convergence speed.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.04)  # Adjusted mutation increment\n            self.cr = max(0.1, self.cr - 0.02)  # Adjusted crossover decrement\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:  # Increased reintroduction probability\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27969 with standard deviation 0.11664.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.13263532614910867, 0.34200784533197015, 0.41125939036648484, 0.13694019371184296, 0.3457023252945992, 0.4142905232655002, 0.09848538182086475, 0.2953307258013319, 0.3405316300313491]}}
{"id": "e8fcac85-a2ec-4928-9255-7081552d0bbd", "fitness": 0.27968703797478356, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "This algorithm enhances efficiency by leveraging adaptive mutation scaling and dynamic crossover probability, combined with elite reintroduction, to optimize convergence while ensuring robust exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6  # Changed initial mutation factor\n        self.cr = 0.7  # Changed initial crossover rate\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)  # Adjusted crossover decrement\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:  # Increased to 25%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 68, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27969 with standard deviation 0.11664.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.13263532614910867, 0.34200784533197015, 0.41125939036648484, 0.13694019371184296, 0.3457023252945992, 0.4142905232655002, 0.09848538182086475, 0.2953307258013319, 0.3405316300313491]}}
{"id": "524083e2-7daa-4089-8142-1f882c4470d6", "fitness": 0.5355421719558473, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "Slight increase in memory utilization rate to enhance convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6  # Changed initial mutation factor\n        self.cr = 0.7  # Changed initial crossover rate\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)  # Adjusted crossover decrement\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.21:  # Increased to 21%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 69, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.53554 with standard deviation 0.08674.", "error": "", "parent_ids": ["e57fbe7a-c75f-410d-9f15-476fd91f0fdd"], "operator": null, "metadata": {"aucs": [0.669408564295888, 0.4959808476717251, 0.46779725701460606, 0.6716599631583449, 0.4985348984909579, 0.4710136230616997, 0.6243017861850378, 0.47590920583546314, 0.44527340188890285]}}
{"id": "443d57d5-aadc-4bc1-a832-e9d1262de18d", "fitness": 0.12278563348854662, "name": "AdaptivePopulationClusteringMetaheuristic", "description": "Adaptive Population Clustering to Enhance Solution Diversity and Convergence Efficiency.", "code": "import numpy as np\n\nclass AdaptivePopulationClusteringMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n    \n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.21:\n            return self.memory[np.random.choice(len(self.memory))][0]\n        return None\n\n    def cluster_population(self):\n        clusters = []\n        for i in range(0, self.population_size, 5):\n            cluster = self.population[i:i+5]\n            clusters.append(cluster)\n        return clusters\n    \n    def explore_clusters(self, func, generation):\n        for cluster in self.cluster_population():\n            best_local_idx = np.argmin([func(ind) for ind in cluster])\n            cluster_best = cluster[best_local_idx]\n            for ind in cluster:\n                if np.random.rand() < 0.1:  # Small probability perturbation\n                    direction = np.random.uniform(-1, 1, self.dim)\n                    ind += direction * 0.5 * (cluster_best - ind)\n                    ind = np.clip(ind, self.lower_bound, self.upper_bound)\n                    trial_fitness = func(ind)\n                    self.evaluations += 1\n                    self.update_memory(ind, trial_fitness)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            self.explore_clusters(func, generation)\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 70, "feedback": "The algorithm AdaptivePopulationClusteringMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12279 with standard deviation 0.01724.", "error": "", "parent_ids": ["524083e2-7daa-4089-8142-1f882c4470d6"], "operator": null, "metadata": {"aucs": [0.13266953982076368, 0.13275599226840462, 0.1328366635257603, 0.13697920698765853, 0.13707781097734395, 0.13717020372660071, 0.09849746182766506, 0.09852795225938371, 0.09855587000333899]}}
{"id": "8a663ce2-73ca-4145-8b1a-3d950c1f8f1f", "fitness": 0.5355421719558473, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "Introduced an elite preservation strategy to maintain the best individual across generations.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6  # Changed initial mutation factor\n        self.cr = 0.7  # Changed initial crossover rate\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)  # Adjusted crossover decrement\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.21:  # Increased to 21%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        best_individual = None  # Line change: Initialize storage for the best individual\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    best_individual = trial.copy()  # Preserve the best individual\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite if best_individual is None else best_individual  # Line change: Use the best preserved individual\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 71, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.53554 with standard deviation 0.08674.", "error": "", "parent_ids": ["524083e2-7daa-4089-8142-1f882c4470d6"], "operator": null, "metadata": {"aucs": [0.669408564295888, 0.4959808476717251, 0.46779725701460606, 0.6716599631583449, 0.4985348984909579, 0.4710136230616997, 0.6243017861850378, 0.47590920583546314, 0.44527340188890285]}}
{"id": "71c3ff52-6a98-480b-b22f-3bd4b8645d9b", "fitness": 0.4813440942126094, "name": "LevyEnhancedMetaheuristic", "description": "Utilize Levy flights for exploration to escape local optima and enhance global search capabilities.", "code": "import numpy as np\n\nclass LevyEnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.21:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def levy_flight(self, step_size=0.01):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / abs(v) ** (1 / beta)\n        return step_size * step\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                if np.random.rand() < 0.1:  # Apply Levy flight with a 10% probability\n                    trial += self.levy_flight()\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 72, "feedback": "The algorithm LevyEnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.48134 with standard deviation 0.14800.", "error": "", "parent_ids": ["524083e2-7daa-4089-8142-1f882c4470d6"], "operator": null, "metadata": {"aucs": [0.6956901338896704, 0.41691660586682666, 0.38148135789877236, 0.6975858071246683, 0.43457907462528333, 0.39981352832688555, 0.6591532312786477, 0.3244086073719876, 0.3224685015307418]}}
{"id": "2ed44ed4-fc01-4ccb-855e-1992adc0e1f7", "fitness": 0.12275690586138498, "name": "HybridAdaptiveMemoryMetaheuristic", "description": "Hybrid Differential Evolution with Adaptive Memory Integration to dynamically balance exploration and exploitation.", "code": "import numpy as np\n\nclass HybridAdaptiveMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.memory_weight = 0.2\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < self.memory_weight:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    hybrid_trial = self.memory_weight * memory_solution + (1 - self.memory_weight) * trial\n                    hybrid_trial = np.clip(hybrid_trial, self.lower_bound, self.upper_bound)\n                    trial = hybrid_trial\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 73, "feedback": "The algorithm HybridAdaptiveMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12276 with standard deviation 0.01723.", "error": "", "parent_ids": ["524083e2-7daa-4089-8142-1f882c4470d6"], "operator": null, "metadata": {"aucs": [0.13265185580667338, 0.13271258578556888, 0.1327940042545357, 0.1369590425777515, 0.1370283023294805, 0.13712122492162881, 0.09849121750188272, 0.09851264496916512, 0.09854127460577822]}}
{"id": "ecb630ac-03d2-46de-98b4-20dbcf0629d0", "fitness": 0.43003215790052857, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "Introduce adaptive elitism and dynamic crossover rate based on diversity to enhance convergence and exploration balance.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation, diversity):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr) + 0.5 * diversity)\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.21:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def calculate_diversity(self):\n        mean_vector = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - mean_vector, axis=1))\n        return diversity / np.sqrt(self.dim)\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            diversity = self.calculate_diversity()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation, diversity)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.43003 with standard deviation 0.09918.", "error": "", "parent_ids": ["524083e2-7daa-4089-8142-1f882c4470d6"], "operator": null, "metadata": {"aucs": [0.5837008357907207, 0.39382304379519073, 0.3629064361452021, 0.5933977744069073, 0.39697753760720433, 0.36617359461617616, 0.5140485976894043, 0.33214231227582713, 0.3271192887781238]}}
{"id": "8ff0ab9d-3a2b-4d5a-b473-57f37111b308", "fitness": 0.3804013933311541, "name": "StochasticWeightEnhancedMetaheuristic", "description": "Utilize a stochastic weight adjustment strategy with historical data influence to enhance convergence efficiency.", "code": "import numpy as np\n\nclass StochasticWeightEnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.memory_size = self.initial_population_size // 2\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        adjustment_factor = np.random.rand() * 0.1\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + adjustment_factor)\n            self.cr = max(0.1, self.cr - adjustment_factor)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.memory_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.memory_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:  # Increased to 25%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters()\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 75, "feedback": "The algorithm StochasticWeightEnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.38040 with standard deviation 0.19032.", "error": "", "parent_ids": ["524083e2-7daa-4089-8142-1f882c4470d6"], "operator": null, "metadata": {"aucs": [0.4489167321353642, 0.5758432539806229, 0.13354259469174257, 0.45214621338833194, 0.5968251584901502, 0.13797668694274923, 0.4258146297601917, 0.5537442991123711, 0.09880297147886286]}}
{"id": "4c7ab9c2-bd97-4b1c-8a85-28926ca16c0e", "fitness": 0.27968703797478356, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "Enhanced adaptive memory utilization for better exploration-exploitation balance in black box optimization. ", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6  # Changed initial mutation factor\n        self.cr = 0.7  # Changed initial crossover rate\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)  # Adjusted crossover decrement\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:  # Increased to 25%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 76, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27969 with standard deviation 0.11664.", "error": "", "parent_ids": ["524083e2-7daa-4089-8142-1f882c4470d6"], "operator": null, "metadata": {"aucs": [0.13263532614910867, 0.34200784533197015, 0.41125939036648484, 0.13694019371184296, 0.3457023252945992, 0.4142905232655002, 0.09848538182086475, 0.2953307258013319, 0.3405316300313491]}}
{"id": "d5375910-44dc-4a2b-9c67-57e7d817b9a3", "fitness": 0.21962537948490535, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "Introducing dynamic population diversity and adaptive mutation based on a success history to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6  # Initial mutation factor\n        self.cr = 0.7  # Initial crossover rate\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.success_history = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        if generation % 5 == 0 and self.success_history:\n            success_rate = np.mean(self.success_history[-5:])\n            self.f = np.clip(self.f + 0.1 * (0.5 - success_rate), 0.4, 0.9)\n            self.cr = np.clip(self.cr + 0.1 * (success_rate - 0.5), 0.1, 0.9)\n        else:\n            self.f = max(0.1, self.chaotic_map(self.f))\n            self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        success = int(trial_fitness < current_fitness)\n        self.success_history.append(success)\n        if len(self.success_history) > 100:\n            self.success_history.pop(0)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.21:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def enhance_population_diversity(self):\n        diversity_metric = np.std(self.population)\n        if diversity_metric < 1e-2:\n            perturbation = np.random.normal(0, 0.1, self.population.shape)\n            self.population += perturbation\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n                    self.fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            self.enhance_population_diversity()\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 77, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.21963 with standard deviation 0.07532.", "error": "", "parent_ids": ["524083e2-7daa-4089-8142-1f882c4470d6"], "operator": null, "metadata": {"aucs": [0.30073851931228557, 0.25812754656574655, 0.13507544679169758, 0.3184034230938577, 0.26208804437581634, 0.13974354126166333, 0.27121852497850396, 0.19191470420291956, 0.09931866478165763]}}
{"id": "33a7c4e5-4fa8-4cbb-91d9-6f6b2d756d62", "fitness": 0.5075273394890645, "name": "AdaptiveChaosMemoryMetaheuristic", "description": "Introduce adaptive chaos-based mutation rate adjustment and elite-based memory reintroduction to enhance convergence and exploration balance.", "code": "import numpy as np\n\nclass AdaptiveChaosMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value, a=3.7):\n        return a * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f, a=3.9))\n        self.cr = min(1.0, self.chaotic_map(self.cr, a=3.9))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self, elite):\n        if self.memory and np.random.rand() < 0.25:  # Increased to 25%\n            idx = np.random.choice(len(self.memory))\n            memory_candidate = self.memory[idx][0]\n            return np.where(np.random.rand(self.dim) < self.cr, elite, memory_candidate)\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                best_idx = np.argmin(self.fitness)\n                elite = self.population[best_idx].copy()\n\n                memory_solution = self.reintroduce_from_memory(elite)\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 78, "feedback": "The algorithm AdaptiveChaosMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.50753 with standard deviation 0.03726.", "error": "", "parent_ids": ["524083e2-7daa-4089-8142-1f882c4470d6"], "operator": null, "metadata": {"aucs": [0.5579663461182611, 0.4781039719188275, 0.5148238862389264, 0.5604408222792538, 0.48082652363035694, 0.5174514843988607, 0.5377752056669224, 0.45701034595151857, 0.463347469198652]}}
{"id": "0c0ecdb2-e3c0-4d37-8e50-7ec8a1b7422c", "fitness": 0.5355421719558473, "name": "EnhancedDynamicMemoryMetaheuristic", "description": "Enhance convergence by fine-tuning the mutation factor increment strategy.", "code": "import numpy as np\n\nclass EnhancedDynamicMemoryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6  # Changed initial mutation factor\n        self.cr = 0.7  # Changed initial crossover rate\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.03)  # Fine-tuned mutation increment\n            self.cr = max(0.1, self.cr - 0.03)  # Adjusted crossover decrement\n\n    def adapt_population_size(self):\n        if self.budget - self.evaluations < self.population_size:\n            self.population_size = max(5, (self.budget - self.evaluations) // 2)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.21:  # Increased to 21%\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n            \n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            self.population[0] = elite\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 79, "feedback": "The algorithm EnhancedDynamicMemoryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.53554 with standard deviation 0.08674.", "error": "", "parent_ids": ["524083e2-7daa-4089-8142-1f882c4470d6"], "operator": null, "metadata": {"aucs": [0.669408564295888, 0.4959808476717251, 0.46779725701460606, 0.6716599631583449, 0.4985348984909579, 0.4710136230616997, 0.6243017861850378, 0.47590920583546314, 0.44527340188890285]}}
{"id": "a40040e2-aa63-4f94-a24a-7f11b9fff32a", "fitness": 0.5930404525323573, "name": "EliteGuidedAdaptiveMetaheuristic", "description": "Integrate elite-guided mutation and adaptive population resizing for improved convergence.", "code": "import numpy as np\n\nclass EliteGuidedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, value):\n        return 4.0 * value * (1.0 - value)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f))\n        self.cr = min(1.0, self.chaotic_map(self.cr))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n                mutant = elite + self.f * (self.population[a] - self.population[b])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 80, "feedback": "The algorithm EliteGuidedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.59304 with standard deviation 0.25160.", "error": "", "parent_ids": ["524083e2-7daa-4089-8142-1f882c4470d6"], "operator": null, "metadata": {"aucs": [0.2500169424059552, 0.7429765894791829, 0.8022566140039822, 0.2541220797350874, 0.7446103144109695, 0.8033199696266997, 0.21365208300926175, 0.7322610116728365, 0.7941484684472395]}}
{"id": "0722c836-b855-4351-9afd-4bd0fe0b9b37", "fitness": 0.6323246022240975, "name": "EnhancedEliteGuidedAdaptiveMetaheuristic", "description": "Use an enhanced chaotic adaptation of mutation and crossover rates with memory-based elite reintroduction for robust convergence.", "code": "import numpy as np\n\nclass EnhancedEliteGuidedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr - generation / (self.budget + 1))))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n                mutant = elite + self.f * (self.population[a] - self.population[b])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedEliteGuidedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.63232 with standard deviation 0.08069.", "error": "", "parent_ids": ["a40040e2-aa63-4f94-a24a-7f11b9fff32a"], "operator": null, "metadata": {"aucs": [0.6737937839989354, 0.7093304984020771, 0.5301951919266348, 0.6754841268299425, 0.7107818590273894, 0.5326180763603612, 0.6606719890677528, 0.697845706232405, 0.5002001881713793]}}
{"id": "f0152474-bda4-4ac2-9b15-c49e968be3bc", "fitness": -Infinity, "name": "EnhancedEliteGuidedAdaptiveMetaheuristic", "description": "Integrate adaptive learning strategies with a dynamic neighborhood and chaotic mutation to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedEliteGuidedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr - generation / (self.budget + 1))))\n        self.cr += 0.01 * np.sin(generation)  # Added sinusoidal adjustment for crossover rate\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n        else:\n            self.f = max(0.1, self.f - 0.03)  # Adjust mutation rate downward on non-improvement\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def select_best_neighborhood(self, population, fitness, i):\n        neighborhood_size = min(5, self.population_size)\n        neighbors = np.random.choice([j for j in range(self.population_size) if j != i], neighborhood_size, replace=False)\n        best_neighbor_idx = neighbors[np.argmin(fitness[neighbors])]\n        return population[best_neighbor_idx]\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                best_neighbor = self.select_best_neighborhood(self.population, self.fitness, i)\n                mutant = elite + self.f * (best_neighbor - self.population[i])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 82, "feedback": "An exception occurred: ValueError(\"Cannot take a larger sample than population when 'replace=False'\").", "error": "ValueError(\"Cannot take a larger sample than population when 'replace=False'\")", "parent_ids": ["0722c836-b855-4351-9afd-4bd0fe0b9b37"], "operator": null, "metadata": {}}
{"id": "e95b5f70-f6ae-4404-af3a-91c7cbcef170", "fitness": 0.4753188054043355, "name": "EnhancedEliteGuidedAdaptiveMetaheuristic", "description": "Integrate adaptive memory-based mutation selection to enhance exploration and exploitation balance in the optimization process.", "code": "import numpy as np\n\nclass EnhancedEliteGuidedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr - generation / (self.budget + 1))))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def adaptive_mutation(self, elite, a, b):\n        memory_solution = self.reintroduce_from_memory()\n        if memory_solution is not None and np.random.rand() < 0.5:\n            return memory_solution + self.f * (elite - self.population[a])\n        return elite + self.f * (self.population[a] - self.population[b])\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n\n                mutant = self.adaptive_mutation(elite, a, b)\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedEliteGuidedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.47532 with standard deviation 0.15169.", "error": "", "parent_ids": ["0722c836-b855-4351-9afd-4bd0fe0b9b37"], "operator": null, "metadata": {"aucs": [0.5334404060608289, 0.6283460096674527, 0.2765252172091003, 0.555939585668209, 0.6304562784664791, 0.28021100651939956, 0.5125500958611966, 0.6128054786609086, 0.24759517052544533]}}
{"id": "5d5b171d-614a-4207-8e9a-c19bf46bdedb", "fitness": -Infinity, "name": "EnhancedAdaptiveLevyEliteMetaheuristic", "description": "Integrate an adaptive strategy based on Levy flights for enhanced exploration combined with elite-guided exploitation to improve convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedAdaptiveLevyEliteMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n    \n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr - generation / (self.budget + 1))))\n\n    def levy_flight(self):\n        beta = 1.5\n        sigma = (np.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1 / beta)\n        u = np.random.randn(self.dim) * sigma\n        v = np.random.randn(self.dim)\n        step = u / abs(v)**(1 / beta)\n        return step\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        if self.memory and np.random.rand() < 0.25:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n                mutant = elite + self.f * (self.population[a] - self.population[b])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                levy_step = self.levy_flight()\n                trial = np.clip(trial + levy_step, self.lower_bound, self.upper_bound)\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 84, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_ids": ["0722c836-b855-4351-9afd-4bd0fe0b9b37"], "operator": null, "metadata": {}}
{"id": "0a0fb6a3-fc8c-4bff-b947-d6f090b89777", "fitness": 0.4534600269487136, "name": "EnhancedEliteGuidedAdaptiveMetaheuristic", "description": "Introduce a dynamic adjustment to the reintroduction probability for more effective exploitation of memory solutions.", "code": "import numpy as np\n\nclass EnhancedEliteGuidedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr - generation / (self.budget + 1))))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self, generation):\n        reintroduction_prob = 0.25 * (1 + generation / self.budget)\n        if self.memory and np.random.rand() < reintroduction_prob:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n                mutant = elite + self.f * (self.population[a] - self.population[b])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory(generation)\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedEliteGuidedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.45346 with standard deviation 0.16673.", "error": "", "parent_ids": ["0722c836-b855-4351-9afd-4bd0fe0b9b37"], "operator": null, "metadata": {"aucs": [0.6738300322456867, 0.25902087576511446, 0.4601982822671057, 0.6755240439410276, 0.2628581712390422, 0.46306579612890975, 0.6193097589168634, 0.22921042170673556, 0.43812286032793746]}}
{"id": "faa3f92e-cb90-417b-9f1c-9efecfd5a5d1", "fitness": -Infinity, "name": "AdaptiveEntropyEnhancedMetaheuristic", "description": "Introduce adaptive memory usage with entropy-based selection to balance exploration and exploitation for improved convergence.", "code": "import numpy as np\n\nclass AdaptiveEntropyEnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n\n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr - generation / (self.budget + 1))))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def entropy_based_selection(self):\n        if not self.memory:\n            return None\n        fitness_values = np.array([fit for _, fit in self.memory])\n        probabilities = np.exp(-fitness_values / (np.std(fitness_values) + 1e-9))\n        probabilities /= probabilities.sum()\n        idx = np.random.choice(len(self.memory), p=probabilities)\n        return self.memory[idx][0]\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n                mutant = elite + self.f * (self.population[a] - self.population[b])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.entropy_based_selection()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n                    self.fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 86, "feedback": "An exception occurred: ValueError('probabilities contain NaN').", "error": "ValueError('probabilities contain NaN')", "parent_ids": ["0722c836-b855-4351-9afd-4bd0fe0b9b37"], "operator": null, "metadata": {}}
{"id": "212208bb-9da4-4ec2-885a-2d651d2a72b0", "fitness": 0.708080737655473, "name": "EnhancedEliteGuidedAdaptiveMetaheuristic", "description": "Introduce adaptive elite reintroduction probability and enhance the chaotic map for dynamic exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedEliteGuidedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.reintroduction_prob = 0.25\n\n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x) + 0.1 * np.sin(x)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr - generation / (self.budget + 1))))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        probability = min(0.5, self.reintroduction_prob + 0.01 * len(self.memory) / self.population_size)\n        if self.memory and np.random.rand() < probability:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n                mutant = elite + self.f * (self.population[a] - self.population[b])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedEliteGuidedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.70808 with standard deviation 0.04221.", "error": "", "parent_ids": ["0722c836-b855-4351-9afd-4bd0fe0b9b37"], "operator": null, "metadata": {"aucs": [0.7432340424164374, 0.7522886687526192, 0.6606341161387441, 0.7446884448351614, 0.753521716384449, 0.6645504497659446, 0.691684228918102, 0.723292237898089, 0.6388327337897096]}}
{"id": "a2bb519a-b4cb-4de3-ac19-5a9cfd6a72c7", "fitness": 0.48536124320116947, "name": "EnhancedEliteGuidedAdaptiveMetaheuristic", "description": "Introduce multi-elite selection and adaptive mutation scaling for enhanced exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedEliteGuidedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.reintroduction_prob = 0.25\n        self.multi_elite_fraction = 0.2\n\n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x) + 0.1 * np.sin(x)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr - generation / (self.budget + 1))))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        probability = min(0.5, self.reintroduction_prob + 0.01 * len(self.memory) / self.population_size)\n        if self.memory and np.random.rand() < probability:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def select_multi_elite(self):\n        elite_count = max(1, int(self.multi_elite_fraction * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:elite_count]\n        return self.population[elite_indices]\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            elites = self.select_multi_elite()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n                random_elite = elites[np.random.randint(len(elites))]\n                mutant = random_elite + self.f * (self.population[a] - self.population[b])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 88, "feedback": "The algorithm EnhancedEliteGuidedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.48536 with standard deviation 0.14573.", "error": "", "parent_ids": ["212208bb-9da4-4ec2-885a-2d651d2a72b0"], "operator": null, "metadata": {"aucs": [0.5859037599191224, 0.29214081040757633, 0.5916319712307265, 0.6241009543691577, 0.29594955265593925, 0.5938233387283678, 0.5541550920697357, 0.2556594288490539, 0.5748862805808459]}}
{"id": "938ca860-efd9-49b4-b146-76b5d77c2278", "fitness": 0.6856961324357659, "name": "RefinedEliteGuidedAdaptiveMetaheuristic", "description": "Introduce self-adaptive chaos parameter tuning and hybrid elitist learning to dynamically enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass RefinedEliteGuidedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.reintroduction_prob = 0.25\n        self.chaos_param = 0.5\n\n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x) + self.chaos_param * np.sin(x)\n\n    def adapt_parameters(self, generation):\n        self.chaos_param = np.random.uniform(0.1, 0.5)\n        self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr - generation / (self.budget + 1))))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        probability = min(0.5, self.reintroduction_prob + 0.01 * len(self.memory) / self.population_size)\n        if self.memory and np.random.rand() < probability:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n                mutant = elite + self.f * (self.population[a] - self.population[b])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 89, "feedback": "The algorithm RefinedEliteGuidedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.68570 with standard deviation 0.01498.", "error": "", "parent_ids": ["212208bb-9da4-4ec2-885a-2d651d2a72b0"], "operator": null, "metadata": {"aucs": [0.7104173767307078, 0.6859138111668649, 0.6798241741613752, 0.7119224746295261, 0.6875965959421838, 0.6815442299230252, 0.6750932227100108, 0.6731308436639751, 0.6658224629942241]}}
{"id": "605b570d-3205-4baa-ab9b-b3b7bcb56d12", "fitness": 0.3812127468306139, "name": "EnhancedEliteGuidedAdaptiveMetaheuristic", "description": "Introduce adaptive mutation scaling and adaptive crossover rates using historical success to enhance search efficiency and convergence speed.", "code": "import numpy as np\n\nclass EnhancedEliteGuidedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.reintroduction_prob = 0.25\n        self.success_history = []\n\n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x) + 0.1 * np.sin(x)\n\n    def adapt_parameters(self, generation):\n        if self.success_history:\n            avg_success = np.mean(self.success_history[-10:])\n            self.f = min(0.9, max(0.4, 0.6 + 0.3 * (avg_success - 0.5)))\n            self.cr = min(0.9, max(0.1, 0.7 - 0.5 * (avg_success - 0.5)))\n        else:\n            self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n            self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr - generation / (self.budget + 1))))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        probability = min(0.5, self.reintroduction_prob + 0.01 * len(self.memory) / self.population_size)\n        if self.memory and np.random.rand() < probability:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            self.best_fitness = self.fitness[best_idx]\n            elite = self.population[best_idx].copy()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n                mutant = elite + self.f * (self.population[a] - self.population[b])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n                    self.success_history.append(1)\n                else:\n                    self.success_history.append(0)\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 90, "feedback": "The algorithm EnhancedEliteGuidedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.38121 with standard deviation 0.13607.", "error": "", "parent_ids": ["212208bb-9da4-4ec2-885a-2d651d2a72b0"], "operator": null, "metadata": {"aucs": [0.260995765146385, 0.5765752845564076, 0.34684042009258376, 0.2648034085710361, 0.5787594107979912, 0.3502003942121851, 0.21154282359729715, 0.5376493892111129, 0.30354782529052593]}}
{"id": "760c91d2-f351-4bc1-87aa-bb6c7a0617cb", "fitness": 0.6077008995805699, "name": "EnhancedEliteGuidedAdaptiveMetaheuristic", "description": "Integrate adaptive dynamic mutation scaling and a memory-based elite crossover mechanism to enhance exploration and solution quality.", "code": "import numpy as np\n\nclass EnhancedEliteGuidedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.reintroduction_prob = 0.25\n\n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x) + 0.1 * np.sin(x)\n    \n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr - generation / (self.budget + 1))))\n    \n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        probability = min(0.5, self.reintroduction_prob + 0.01 * len(self.memory) / self.population_size)\n        if self.memory and np.random.rand() < probability:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def dynamic_mutation_scaling(self, fitness):\n        return 0.5 + 0.5 * (1 - (fitness - min(fitness)) / (max(fitness) - min(fitness) + 1e-9))\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n            dynamic_scaling = self.dynamic_mutation_scaling(self.fitness)\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n                mutant = elite + dynamic_scaling[i] * (self.population[a] - self.population[b])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 91, "feedback": "The algorithm EnhancedEliteGuidedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.60770 with standard deviation 0.27982.", "error": "", "parent_ids": ["212208bb-9da4-4ec2-885a-2d651d2a72b0"], "operator": null, "metadata": {"aucs": [0.2718482991857486, 0.9608628683175413, 0.6103598398415014, 0.2755297467929926, 0.9610570176710248, 0.6123789163395021, 0.2879316978738393, 0.9593224046746611, 0.5300173055283176]}}
{"id": "9a2280a0-3ba7-429a-b85b-6f1e245608d2", "fitness": 0.708080737655473, "name": "EnhancedEliteGuidedAdaptiveMetaheuristic", "description": "Enhance exploration-exploitation balance with a dynamic learning coefficient influenced by a self-adaptive mechanism and diversity awareness.", "code": "import numpy as np\n\nclass EnhancedEliteGuidedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.reintroduction_prob = 0.25\n        self.learning_rate = 0.1\n\n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x) + 0.1 * np.sin(x)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr - generation / (self.budget + 1))))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + self.learning_rate)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        probability = min(0.5, self.reintroduction_prob + 0.01 * len(self.memory) / self.population_size)\n        if self.memory and np.random.rand() < probability:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def update_learning_rate(self, diversity):\n        self.learning_rate = 0.1 + 0.5 * (1.0 - diversity)\n\n    def calculate_diversity(self):\n        mean_individual = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - mean_individual, axis=1))\n        return diversity / np.sqrt(self.dim)\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n\n            diversity = self.calculate_diversity()\n            self.update_learning_rate(diversity)\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n                mutant = elite + self.f * (self.population[a] - self.population[b])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedEliteGuidedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.70808 with standard deviation 0.04221.", "error": "", "parent_ids": ["212208bb-9da4-4ec2-885a-2d651d2a72b0"], "operator": null, "metadata": {"aucs": [0.7432340424164374, 0.7522886687526192, 0.6606341161387441, 0.7446884448351614, 0.753521716384449, 0.6645504497659446, 0.691684228918102, 0.723292237898089, 0.6388327337897096]}}
{"id": "02c5bff5-8b5a-4e0e-ac21-de6a858c3b07", "fitness": 0.1874050502766267, "name": "EnhancedEliteGuidedAdaptiveMetaheuristic", "description": "Leverage adaptive levy flights and dynamic differential evolution parameters to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedEliteGuidedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.reintroduction_prob = 0.25\n        self.levy_factor = 0.1\n\n    def levy_flight(self, x):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / abs(v) ** (1 / beta)\n        return x + step * self.levy_factor\n\n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x) + 0.1 * np.sin(x)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr - generation / (self.budget + 1))))\n        if generation % 5 == 0:\n            self.levy_factor = max(0.01, self.levy_factor * 0.9)\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        probability = min(0.5, self.reintroduction_prob + 0.01 * len(self.memory) / self.population_size)\n        if self.memory and np.random.rand() < probability:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n                mutant = elite + self.f * (self.population[a] - self.population[b])\n                mutant = self.levy_flight(np.clip(mutant, self.lower_bound, self.upper_bound))\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 93, "feedback": "The algorithm EnhancedEliteGuidedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18741 with standard deviation 0.11756.", "error": "", "parent_ids": ["212208bb-9da4-4ec2-885a-2d651d2a72b0"], "operator": null, "metadata": {"aucs": [0.40339857089803854, 0.1334726121900881, 0.13605223963621516, 0.4077720483411025, 0.13789811552908138, 0.14087864277245798, 0.128760186842086, 0.09877659984705578, 0.09963643643351494]}}
{"id": "25bc503f-ef84-4e5e-a369-c05f54575634", "fitness": 0.6277358563882349, "name": "EnhancedEliteGuidedAdaptiveMetaheuristic", "description": "Introduce adaptive scaling factor for chaotic map to enhance dynamic exploration-exploitation control.", "code": "import numpy as np\n\nclass EnhancedEliteGuidedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.reintroduction_prob = 0.25\n\n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x) + 0.1 * np.sin(0.5 * x)  # Change made here\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr - generation / (self.budget + 1))))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        probability = min(0.5, self.reintroduction_prob + 0.01 * len(self.memory) / self.population_size)\n        if self.memory and np.random.rand() < probability:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n                mutant = elite + self.f * (self.population[a] - self.population[b])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 94, "feedback": "The algorithm EnhancedEliteGuidedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.62774 with standard deviation 0.04926.", "error": "", "parent_ids": ["212208bb-9da4-4ec2-885a-2d651d2a72b0"], "operator": null, "metadata": {"aucs": [0.6911578152849132, 0.5767203446386507, 0.6526392273713071, 0.6926918595530704, 0.5790777124983986, 0.6640367500805135, 0.607406678149129, 0.5490418593054601, 0.6368504606126716]}}
{"id": "f5cb2727-2442-424c-b8b2-69997ff923ff", "fitness": 0.32199537738911954, "name": "RefinedEnhancedEliteGuidedAdaptiveMetaheuristic", "description": "Introduce dynamic mutation scale and enhanced memory-based perturbation for improved convergence and diversity.", "code": "import numpy as np\n\nclass RefinedEnhancedEliteGuidedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.reintroduction_prob = 0.25\n\n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x) + 0.1 * np.sin(x)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr - generation / (self.budget + 1))))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def dynamic_mutation_scale(self, trial_fitness):\n        return 0.5 * (1.0 + np.tanh(5.0 * (self.best_fitness - trial_fitness)))\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        probability = min(0.5, self.reintroduction_prob + 0.01 * len(self.memory) / self.population_size)\n        if self.memory and np.random.rand() < probability:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def enhanced_memory_perturbation(self, elite):\n        perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n        if self.memory:\n            idx = np.random.choice(len(self.memory))\n            memory_solution = self.memory[idx][0]\n            return memory_solution + perturbation * (elite - memory_solution)\n        return elite + perturbation\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n                mutation_scale = self.dynamic_mutation_scale(self.fitness[i])\n                mutant = elite + mutation_scale * (self.population[a] - self.population[b])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial = self.enhanced_memory_perturbation(trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 95, "feedback": "The algorithm RefinedEnhancedEliteGuidedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32200 with standard deviation 0.28208.", "error": "", "parent_ids": ["212208bb-9da4-4ec2-885a-2d651d2a72b0"], "operator": null, "metadata": {"aucs": [0.13286182571162797, 0.1326801380065039, 0.723645872904811, 0.13719858762787085, 0.13699129182361514, 0.7250957145798115, 0.09856518218967603, 0.09850120392622497, 0.7124185797319346]}}
{"id": "e661f6c0-37ab-461c-b188-7d9a24f6429e", "fitness": 0.6890029546555129, "name": "EnhancedEliteGuidedAdaptiveMetaheuristic", "description": "Fine-tune the crossover rate adaptation in the EnhancedEliteGuidedAdaptiveMetaheuristic for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedEliteGuidedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.reintroduction_prob = 0.25\n\n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x) + 0.1 * np.sin(x)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n        # Change made here for crossover rate adaptation\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr + np.sqrt(generation) / (self.budget + 1))))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        probability = min(0.5, self.reintroduction_prob + 0.01 * len(self.memory) / self.population_size)\n        if self.memory and np.random.rand() < probability:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n                mutant = elite + self.f * (self.population[a] - self.population[b])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 96, "feedback": "The algorithm EnhancedEliteGuidedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.68900 with standard deviation 0.04525.", "error": "", "parent_ids": ["212208bb-9da4-4ec2-885a-2d651d2a72b0"], "operator": null, "metadata": {"aucs": [0.729893437541524, 0.6372246229735796, 0.7260820154470278, 0.7313405487264826, 0.6390639254360992, 0.7275177313248947, 0.7135386391214544, 0.6094751702955636, 0.6868905010329893]}}
{"id": "2029770e-699c-4afc-87d5-08fb12783459", "fitness": 0.5312079164684258, "name": "EnhancedEliteGuidedAdaptiveMetaheuristic", "description": "Enhance exploration via Lévy flights and adaptive parameter mutation to improve search efficiency and convergence in complex landscapes.", "code": "import numpy as np\n\nclass EnhancedEliteGuidedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.reintroduction_prob = 0.25\n\n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x) + 0.1 * np.sin(x)\n\n    def levy_flight(self, L=1.5):\n        u = np.random.normal(0, 1, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / np.abs(v) ** (1 / L)\n        return step\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr - generation / (self.budget + 1))))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        probability = min(0.5, self.reintroduction_prob + 0.01 * len(self.memory) / self.population_size)\n        if self.memory and np.random.rand() < probability:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n                \n                if np.random.rand() < 0.5:\n                    mutant = elite + self.f * (self.population[a] - self.population[b])\n                else:\n                    step = self.levy_flight()\n                    mutant = self.population[i] + step\n                \n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 97, "feedback": "The algorithm EnhancedEliteGuidedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.53121 with standard deviation 0.18874.", "error": "", "parent_ids": ["212208bb-9da4-4ec2-885a-2d651d2a72b0"], "operator": null, "metadata": {"aucs": [0.70885946720871, 0.6375355948820114, 0.29993263370393075, 0.7104942675909169, 0.6394351163989191, 0.30415715741459093, 0.655282058768982, 0.6191236624116826, 0.206051289836089]}}
{"id": "1c74b8e1-e168-44e8-afa9-d405dfdd7f08", "fitness": 0.1229601226164905, "name": "EnhancedEliteGuidedAdaptiveMetaheuristic", "description": "Enhance the diversity of solutions by introducing a random perturbation to the trial solution.", "code": "import numpy as np\n\nclass EnhancedEliteGuidedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.reintroduction_prob = 0.25\n\n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x) + 0.1 * np.sin(x)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr - generation / (self.budget + 1))))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        probability = min(0.5, self.reintroduction_prob + 0.01 * len(self.memory) / self.population_size)\n        if self.memory and np.random.rand() < probability:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n                mutant = elite + self.f * (self.population[a] - self.population[b])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                # Change starts here\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                trial = np.clip(trial + perturbation, self.lower_bound, self.upper_bound)\n                # Change ends here\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 98, "feedback": "The algorithm EnhancedEliteGuidedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12296 with standard deviation 0.01732.", "error": "", "parent_ids": ["212208bb-9da4-4ec2-885a-2d651d2a72b0"], "operator": null, "metadata": {"aucs": [0.13267168822996422, 0.1326846304969338, 0.13353603837388295, 0.136981656321007, 0.13699641780198235, 0.13796982660541934, 0.09849822103781636, 0.0985027854535293, 0.09879983922787916]}}
{"id": "714e8097-815a-42f2-96cf-5c39f10e566b", "fitness": 0.6664893964777474, "name": "EnhancedEliteGuidedAdaptiveMetaheuristic", "description": "Slightly increase the reintroduction probability to enhance exploration capabilities.", "code": "import numpy as np\n\nclass EnhancedEliteGuidedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(50, self.budget // 10)\n        self.population_size = self.initial_population_size\n        self.f = 0.6\n        self.cr = 0.7\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.best_fitness = np.inf\n        self.memory = []\n        self.reintroduction_prob = 0.30  # Modified from 0.25 to 0.30\n\n    def chaotic_map(self, x):\n        return 4.0 * x * (1.0 - x) + 0.1 * np.sin(x)\n\n    def adapt_parameters(self, generation):\n        self.f = max(0.1, self.chaotic_map(self.f + generation / (self.budget + 1)))\n        self.cr = max(0.1, min(1.0, self.chaotic_map(self.cr - generation / (self.budget + 1))))\n\n    def adjust_rates(self, trial_fitness, current_fitness):\n        if trial_fitness < current_fitness:\n            self.f = min(0.9, self.f + 0.05)\n            self.cr = max(0.1, self.cr - 0.03)\n\n    def adapt_population_size(self):\n        remaining_budget = self.budget - self.evaluations\n        if remaining_budget < self.population_size * 2:\n            self.population_size = max(5, remaining_budget // 3)\n\n    def update_memory(self, trial, trial_fitness):\n        self.memory.append((trial.copy(), trial_fitness))\n        if len(self.memory) > self.population_size:\n            self.memory.sort(key=lambda x: x[1])\n            self.memory = self.memory[:self.population_size]\n\n    def reintroduce_from_memory(self):\n        probability = min(0.5, self.reintroduction_prob + 0.01 * len(self.memory) / self.population_size)\n        if self.memory and np.random.rand() < probability:\n            idx = np.random.choice(len(self.memory))\n            return self.memory[idx][0]\n        return None\n\n    def __call__(self, func):\n        generation = 0\n        self.fitness = np.array([func(ind) for ind in self.population])\n        self.evaluations += self.population_size\n\n        while self.evaluations < self.budget:\n            self.adapt_population_size()\n            self.population = self.population[:self.population_size]\n            self.fitness = self.fitness[:self.population_size]\n\n            best_idx = np.argmin(self.fitness)\n            elite = self.population[best_idx].copy()\n\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.adapt_parameters(generation)\n\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b = np.random.choice(indices, 2, replace=False)\n                mutant = elite + self.f * (self.population[a] - self.population[b])\n                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n                crossover_mask = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                memory_solution = self.reintroduce_from_memory()\n                if memory_solution is not None:\n                    trial = np.where(crossover_mask, memory_solution, trial)\n\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                self.update_memory(trial, trial_fitness)\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    self.adjust_rates(trial_fitness, self.fitness[i])\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n\n            generation += 1\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "configspace": "", "generation": 99, "feedback": "The algorithm EnhancedEliteGuidedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.66649 with standard deviation 0.03207.", "error": "", "parent_ids": ["212208bb-9da4-4ec2-885a-2d651d2a72b0"], "operator": null, "metadata": {"aucs": [0.7116016731136239, 0.6574828573387218, 0.6708884137720752, 0.7131014847570812, 0.6594647357252859, 0.6725764866748201, 0.6772393053068012, 0.622838007164674, 0.6132116044466431]}}
