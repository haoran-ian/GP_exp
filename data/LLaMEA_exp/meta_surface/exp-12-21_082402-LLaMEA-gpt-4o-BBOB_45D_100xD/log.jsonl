{"id": "730c16c4-d810-4c62-9ae4-765e6a82e294", "fitness": -Infinity, "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic combining Differential Evolution and Adaptive Neighborhood Search to efficiently explore and exploit the search space within a limited budget.", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 0, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ubuntu/GP_Compare/LLaMEA/llamea/llamea.py\", line 188, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ubuntu/GP_Compare/LLaMEA/llamea/llamea.py\", line 247, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"scripts/5_algorithm_generation.py\", line 95, in evaluateBBOB\n    algorithm(problem)\n  File \"<string>\", line 58, in __call__\nNameError: name 'evals' is not defined\n.", "error": "NameError(\"name 'evals' is not defined\")Traceback (most recent call last):\n  File \"/home/ubuntu/GP_Compare/LLaMEA/llamea/llamea.py\", line 188, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ubuntu/GP_Compare/LLaMEA/llamea/llamea.py\", line 247, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"scripts/5_algorithm_generation.py\", line 95, in evaluateBBOB\n    algorithm(problem)\n  File \"<string>\", line 58, in __call__\nNameError: name 'evals' is not defined\n", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "9912ede8-5042-47c2-bdcc-932a3fac6b87", "fitness": 0.15433996368045363, "name": "HybridMetaheuristic", "description": "A refined hybrid metaheuristic enhancing exploration with adaptive parameter tuning in Differential Evolution.", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget)) # Change 1\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1]) # Change 2\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness) # Change 3\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 1, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.15434 with standard deviation 0.30281.", "error": "", "parent_ids": ["730c16c4-d810-4c62-9ae4-765e6a82e294"], "operator": null, "metadata": {"aucs": [1.0, 0.12693355886894675, 0.13484568806191388, 0.04439040932837657, 0.04118800694350422, 0.041035343254674284, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "0c143ac0-b3a6-45de-b80c-d4068ea78e51", "fitness": -Infinity, "name": "EnhancedHybridMetaheuristic", "description": "An enhanced Differential Evolution with dynamic population resizing and adaptive local search to improve convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n                # Dynamic population resizing\n                if evals % (self.budget // 10) == 0 and pop_size < 50:\n                    new_individuals = np.random.rand(5, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                    pop = np.vstack([pop, new_individuals])\n                    fitness = np.append(fitness, [func(ind) for ind in new_individuals])\n                    pop_size = len(pop)\n                    evals += 5\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_local_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                f_candidate = func(candidate)\n                evals += 1\n                if f_candidate < fitness[best_idx]:\n                    best_idx = np.argmin(fitness)\n                    best_ind = candidate\n                    fitness[best_idx] = f_candidate\n                if evals >= self.budget:\n                    break\n                # Adaptive neighborhood adjustment\n                if f_candidate < fitness[best_idx]:\n                    neighborhood_size *= 0.9\n                else:\n                    neighborhood_size *= 1.1\n\n        adaptive_local_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 2, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'pop_size' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'pop_size' referenced before assignment\")", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {}}
{"id": "27e6e7e4-7e59-45d8-af5d-6db85ec35d22", "fitness": 0.05823378439393637, "name": "HybridMetaheuristic", "description": "An optimized hybrid metaheuristic extending exploration through stochastic ranking in Differential Evolution.", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget)) # Change 1\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 3, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05823 with standard deviation 0.05514.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13504438642134498, 0.12693355886894675, 0.13484568806191388, 0.04439040932837657, 0.04118800694350422, 0.041035343254674284, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "8c4a4b82-c873-422a-95d9-fcd3cc85347a", "fitness": -Infinity, "name": "EnhancedHybridMetaheuristic", "description": "An enhanced hybrid metaheuristic integrating dynamic population adjustment and adaptive learning rates within Differential Evolution to bolster convergence speed.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            historical_fitness = np.full(self.budget, np.inf)\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget)) # Adjusted based on the ratio of current evaluations to budget\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    historical_fitness[evals-1] = f_trial\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n\n                    # Dynamic population adjustment\n                    if evals >= 10 and evals % 10 == 0:\n                        recent_fitness = historical_fitness[evals-10:evals]\n                        if np.std(recent_fitness) < 0.01 * np.mean(recent_fitness):\n                            pop_size = max(2, pop_size - 1)\n                        elif np.std(recent_fitness) > 0.1 * np.mean(recent_fitness):\n                            pop_size += 1\n                            new_individual = np.random.rand(self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                            pop = np.vstack((pop, new_individual))\n                            fitness = np.append(fitness, func(new_individual))\n                            evals += 1\n\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                search_intensity = 1 - (evals / self.budget) # Decrease search intensity over time\n                for i in range(pop_size):\n                    candidate = best_ind + search_intensity * neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 4, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'pop_size' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'pop_size' referenced before assignment\")", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {}}
{"id": "e47dcafe-8e37-43e4-92c9-e92c456402af", "fitness": 0.06162776683375664, "name": "HybridMetaheuristic", "description": "Improved hybrid metaheuristic with adaptive mutation scaling and enhanced neighborhood exploration.", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget)) # Change 1\n                    F_dynamic = F_dynamic * (0.5 + 0.5 * np.random.rand()) # Adaptive mutation scaling\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1]) # Change 2\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness) # Change 3\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n            neighborhood_size *= 0.9 # Enhance neighborhood exploration\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 5, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06163 with standard deviation 0.05497.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.1273550571111639, 0.14150995139273148, 0.1291000208408456, 0.06716273558761188, 0.04200542288776499, 0.04685004701702522, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "dd44caf6-0ec4-4746-82b1-20736e505c71", "fitness": 0.05737353050922789, "name": "HybridMetaheuristic", "description": "Enhancing hybrid metaheuristic with adaptive crossover rate and dynamic neighborhood scaling.", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget)) # Change 1\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    CR_dynamic = CR * (1 - (evals / self.budget)) # Change 1\n                    cross_points = np.random.rand(self.dim) < CR_dynamic\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n                neighborhood_size *= 0.95 # Change 2\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 6, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05737 with standard deviation 0.05446.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13248146502726954, 0.13399323226454296, 0.1251833437683313, 0.04031256578121589, 0.044657080240355196, 0.03906742083466941, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "f23f912c-352d-4011-8c04-7314c18758e6", "fitness": 0.05692984826726391, "name": "HybridMetaheuristic", "description": "Enhanced adaptive parameter tuning by incorporating a dynamic crossover rate in Differential Evolution.", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    CR_dynamic = CR - (0.5 * (evals / self.budget))  # Change 1\n                    cross_points = np.random.rand(self.dim) < CR_dynamic\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 7, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05693 with standard deviation 0.05518.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.1363387857523053, 0.13308214572471921, 0.12532511351586895, 0.03710239126065895, 0.040912578739488614, 0.038940952745667445, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "1dc8aed1-dcf2-4017-ab91-8c8210af5fb9", "fitness": 0.05737353050922789, "name": "HybridMetaheuristic", "description": "An improved version of the hybrid metaheuristic leveraging an adaptive crossover rate to enhance convergence speed. ", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget)) # Change 1\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1]) # Change 2\n                    CR_dynamic = CR * (1 - (evals / self.budget)) # Change 4\n                    cross_points = np.random.rand(self.dim) < CR_dynamic\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness) # Change 3\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 8, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05737 with standard deviation 0.05446.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13248146502726954, 0.13399323226454296, 0.1251833437683313, 0.04031256578121589, 0.044657080240355196, 0.03906742083466941, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "2f46152e-e491-444f-b95a-2884d1fa439e", "fitness": 0.05737353050922789, "name": "HybridMetaheuristic", "description": "An enhanced hybrid metaheuristic utilizing dynamic crossover rate adjustment in Differential Evolution.", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget)) # Change 1\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1]) # Change 2\n                    CR_dynamic = CR * (1 - (evals / self.budget)) # Modified line\n                    cross_points = np.random.rand(self.dim) < CR_dynamic\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness) # Change 3\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 9, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05737 with standard deviation 0.05446.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13248146502726954, 0.13399323226454296, 0.1251833437683313, 0.04031256578121589, 0.044657080240355196, 0.03906742083466941, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "1edddbe4-63d8-4532-bd3f-325e854ea6e7", "fitness": 0.05766462871246842, "name": "HybridMetaheuristic", "description": "Enhanced mutation strategy with adaptive scaling factor in Differential Evolution improves convergence.", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget)) + 0.2 * np.random.rand() # Change 1\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 10, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05766 with standard deviation 0.05588.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13335586958158208, 0.13282958415569068, 0.1338629888531, 0.04010261670528781, 0.042114824715075105, 0.0360491077348134, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "5a5e091e-093a-44a4-9680-b7950d3776e3", "fitness": 0.05655765591361259, "name": "HybridMetaheuristic", "description": "Introduce dynamic crossover rate adaptation in Differential Evolution to further enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    CR_dynamic = CR * (1 - (evals / (2 * self.budget))) # Change 1\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR_dynamic # Change 2\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 11, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05656 with standard deviation 0.05445.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13473803832930098, 0.12775441708099344, 0.12783501154445598, 0.03632191368916149, 0.04039873718465081, 0.0413041187272839, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "3ad280c6-275d-4f03-99d7-94a5d4de1795", "fitness": 0.05743510323699132, "name": "HybridMetaheuristic", "description": "Incorporate a dynamic crossover rate for enhanced convergence within Differential Evolution.", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget)) # Change 1\n                    CR_dynamic = CR * (evals / self.budget) # Change 4\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1]) # Change 2\n                    cross_points = np.random.rand(self.dim) < CR_dynamic\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness) # Change 3\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 12, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05744 with standard deviation 0.05495.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13207458487545642, 0.13437898419905414, 0.12799089566098598, 0.03609899305216535, 0.04204252557814836, 0.043663279100444874, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "32cd0634-0c24-407f-97d6-bfa7e19c0e5a", "fitness": 0.05823378439393637, "name": "EnhancedHybridMetaheuristic", "description": "A novel metaheuristic that combines adaptive Differential Evolution with a dynamic search neighborhood to enhance convergence speed and precision.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F_base = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_base * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness, evals\n\n        pop, fitness, evals = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            dynamic_factor = 0.5\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + (neighborhood_size * dynamic_factor) * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                        dynamic_factor = 0.5  # Reset dynamic factor on improvement\n                    else:\n                        dynamic_factor *= 0.95  # Gradually reduce neighborhood size if no improvement\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 13, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05823 with standard deviation 0.05514.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13504438642134498, 0.12693355886894675, 0.13484568806191388, 0.04439040932837657, 0.04118800694350422, 0.041035343254674284, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "14b1f734-2d91-424f-aa1f-83d9cb20f73b", "fitness": 0.05702370995503489, "name": "HybridMetaheuristicImproved", "description": "A hybrid metaheuristic combining adaptive differential evolution with diversity enhancement and focused local exploration.", "code": "import numpy as np\n\nclass HybridMetaheuristicImproved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 15\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            diversity_factor = 0.1\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    \n                    # Diversity enhancement\n                    if np.random.rand() < diversity_factor:\n                        mutant += np.random.normal(0, 0.1, self.dim)\n\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            initial_neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            neighborhood_size = initial_neighborhood_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                        neighborhood_size = initial_neighborhood_size\n                    else:\n                        neighborhood_size *= 0.95  # Reduce neighborhood size\n\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 14, "feedback": "The algorithm HybridMetaheuristicImproved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05702 with standard deviation 0.05452.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13269364346231283, 0.12829434748994506, 0.13055992439027697, 0.04166500449218402, 0.03667106900157502, 0.04266273409235344, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "c850a3ab-dc87-4b40-b2a3-3ed203576627", "fitness": 0.05749048346630095, "name": "HybridMetaheuristic", "description": "Introducing self-adaptive crossover rate in Differential Evolution to enhance diversity and convergence.", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    CR_dynamic = CR * (0.5 + 0.5 * np.random.rand()) # Change 1\n                    cross_points = np.random.rand(self.dim) < CR_dynamic # Change 2\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 15, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05749 with standard deviation 0.05488.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.12811717639871734, 0.1273231108510654, 0.1385458932537733, 0.03910702977430425, 0.04268491684192954, 0.04096955741025199, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "2d264386-bd7f-4f1a-867d-865b57192954", "fitness": 0.05126195659677021, "name": "EnhancedHybridMetaheuristic", "description": "An enhanced hybrid metaheuristic incorporating Lvy flights for improved exploration and adaptive mutation factor in Differential Evolution.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def levy_flight(Lambda):\n            sigma1 = np.power((np.math.gamma(1 + Lambda) * np.sin(np.pi * Lambda / 2)) / (np.math.gamma((1 + Lambda) / 2) * Lambda * np.power(2, (Lambda - 1) / 2)), 1 / Lambda)\n            sigma2 = 1\n            u = np.random.normal(0, sigma1, size=self.dim)\n            v = np.random.normal(0, sigma2, size=self.dim)\n            step = u / np.power(np.fabs(v), 1 / Lambda)\n            return step\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * np.sin((np.pi / 2) * (1 - (evals / self.budget)))  # Change 1\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1]) + levy_flight(1.5) * (bounds[:, 1] - bounds[:, 0]) * 0.1  # Change 2\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 16, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05126 with standard deviation 0.04858.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.12479151433822555, 0.1125307012628336, 0.11169672294218747, 0.041110590560147875, 0.0343226382701709, 0.036238775330699746, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "8b4a27c1-a70e-4086-b272-b36ab89152a9", "fitness": 0.05823378439393637, "name": "EnhancedHybridMetaheuristic", "description": "An enhanced hybrid metaheuristic integrating adaptive parameter tuning and local search with dynamic population strategies in Differential Evolution.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        initial_pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(initial_pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = initial_pop_size\n\n            while evals < self.budget:\n                for i in range(len(pop)):\n                    indices = list(range(len(pop)))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n                # Dynamic population adjustment\n                pop_size = initial_pop_size + int((self.budget - evals) / self.budget * initial_pop_size)\n                pop = pop[:pop_size]\n                fitness = fitness[:pop_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for _ in range(initial_pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = np.argmin(np.r_[fitness, f_candidate])\n                        best_ind = candidate\n                        fitness = np.r_[fitness, f_candidate][:initial_pop_size]\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05823 with standard deviation 0.05514.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13504438642134498, 0.12693355886894675, 0.13484568806191388, 0.04439040932837657, 0.04118800694350422, 0.041035343254674284, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "13d68ab1-0812-4dd4-a9c7-ee3784bba286", "fitness": 0.05627856969667085, "name": "EnhancedHybridMetaheuristic", "description": "An enhanced hybrid metaheuristic incorporating adaptive population size and dynamic crossover in Differential Evolution for efficient exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        initial_pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def adaptive_differential_evolution():\n            pop_size = initial_pop_size\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR * (1 - (evals / self.budget)) # Dynamic CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population size reduction\n                if evals / self.budget > 0.5 and pop_size > 4:\n                    pop_size = max(4, int(pop_size * 0.9))\n                    pop = pop[:pop_size]\n                    fitness = fitness[:pop_size]\n\n            return pop, fitness\n\n        pop, fitness = adaptive_differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(initial_pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05628 with standard deviation 0.05308.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.12860354217648318, 0.129484621366741, 0.12444573371894951, 0.03927984955763386, 0.042860590533230325, 0.041166123250333087, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "16ccb5a5-1f99-4a17-8271-85f1fd5cc9cd", "fitness": -Infinity, "name": "ImprovedHybridMetaheuristic", "description": "Improved HybridMetaheuristic integrates adaptive mutation strategies and dynamic population size adjustments for enhanced convergence in Differential Evolution.", "code": "import numpy as np\n\nclass ImprovedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = max(4, int(0.1 * self.budget / self.dim))\n        F_base = 0.6\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_base + np.random.rand() * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    adaptive_factor = 1 - (evals / self.budget)\n                    candidate = best_ind + adaptive_factor * neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 19, "feedback": "An exception occurred: NameError(\"name 'evals' is not defined\").", "error": "NameError(\"name 'evals' is not defined\")", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {}}
{"id": "41ffb9b6-4f81-4dd6-b518-59c4c922bdd7", "fitness": -Infinity, "name": "RefinedHybridMetaheuristic", "description": "A dynamically-balanced hybrid metaheuristic combining Differential Evolution with stochastic local search to enhance global and local exploration.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget * 0.7:  # Allocate 70% of budget to DE\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))  # Dynamic scaling factor\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget * 0.7:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n\n        def stochastic_local_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget: # Use remaining budget for local search\n                candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                f_candidate = func(candidate)\n                evals += 1\n                if f_candidate < fitness[best_idx]:\n                    best_ind = candidate\n                    fitness[best_idx] = f_candidate\n                if evals >= self.budget:\n                    break\n\n        stochastic_local_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 20, "feedback": "An exception occurred: NameError(\"name 'evals' is not defined\").", "error": "NameError(\"name 'evals' is not defined\")", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {}}
{"id": "5f0497e9-61c0-4283-9e9e-836556744431", "fitness": 0.12421010070096822, "name": "EnhancedHybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with dynamic population size and elitist strategy for improved convergence in Differential Evolution.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        initial_pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop_size = initial_pop_size\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n                # Elitism: Maintain the best solution found so far\n                best_idx = np.argmin(fitness)\n                best_solution = pop[best_idx]\n                best_fitness = fitness[best_idx]\n\n                # Dynamic population size adjustment\n                if evals < self.budget * 0.5:\n                    pop_size = min(pop_size + 1, self.budget - evals)\n                    new_individual = np.random.rand(1, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                    pop = np.vstack((pop, new_individual))\n                    fitness = np.append(fitness, func(new_individual[0]))\n                    evals += 1\n\n            return best_solution, best_fitness, evals\n\n        best_solution, best_fitness, evals = differential_evolution()\n\n        def adaptive_neighborhood_search(best_solution, best_fitness, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                candidate = best_solution + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                f_candidate = func(candidate)\n                evals += 1\n                if f_candidate < best_fitness:\n                    best_solution = candidate\n                    best_fitness = f_candidate\n                if evals >= self.budget:\n                    break\n\n        adaptive_neighborhood_search(best_solution, best_fitness, evals)\n\n        return best_solution", "configspace": "", "generation": 21, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12421 with standard deviation 0.22276.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.7392390066178306, 0.12618086085388436, 0.13651069295021678, 0.03756346558760604, 0.03832340859185368, 0.039406805040655746, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "f2d71839-f850-422d-a278-670030c334b5", "fitness": -Infinity, "name": "EnhancedHybridMetaheuristic", "description": "An enhanced hybrid metaheuristic combining dynamic population sizing and adaptive parameter tuning for robust and efficient exploration.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size_init = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop_size = pop_size_init\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n                # Dynamic population resizing\n                if evals < self.budget / 2 and pop_size < 20:\n                    pop_size += 1\n                    new_ind = np.random.rand(self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                    pop = np.vstack([pop, new_ind])\n                    fitness = np.append(fitness, func(new_ind))\n                    evals += 1\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n                neighborhood_size *= 0.9  # Adaptive reduction of neighborhood size\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 22, "feedback": "An exception occurred: NameError(\"name 'pop_size' is not defined\").", "error": "NameError(\"name 'pop_size' is not defined\")", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {}}
{"id": "6bf339a0-21b0-46d7-9e9f-03ef06986529", "fitness": 0.15376447883953126, "name": "HybridMetaheuristic", "description": "Enhancing the algorithm by dynamically adjusting the crossover rate (CR) based on the evaluation progress to balance exploration and exploitation.", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget)) # Change 1\n                    CR_dynamic = CR * (1 - (evals / self.budget)) # Change 4\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1]) # Change 2\n                    cross_points = np.random.rand(self.dim) < CR_dynamic\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness) # Change 3\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 23, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.15376 with standard deviation 0.30294.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [1.0, 0.13399323226454296, 0.1251833437683313, 0.04031256578121589, 0.044657080240355196, 0.03906742083466941, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "cf373c77-3a12-4acd-b1d4-9bf197b06936", "fitness": -Infinity, "name": "EnhancedHybridMetaheuristic", "description": "Introducing a dynamic population size and adaptive crossover rate in a hybrid metaheuristic for balanced exploration-exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        initial_pop_size = 10\n        F = 0.8\n\n        def dynamic_population():\n            # Dynamic population size scaling with budget consumption\n            pop_size = max(4, int(initial_pop_size * (1 - (evals / self.budget))))\n            CR = 0.5 + 0.5 * (evals / self.budget)  # Dynamic CR\n        \n            # Initialization\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = dynamic_population()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(initial_pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 24, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'evals' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'evals' referenced before assignment\")", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {}}
{"id": "537f71d5-f42e-4b1f-82cc-b8387c05258c", "fitness": 0.05823378439393637, "name": "EnhancedHybridMetaheuristic", "description": "An enhanced hybrid metaheuristic integrating adaptive differential evolution and dynamic neighborhood search to improve performance with diminishing returns.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def dynamic_neighborhood_search(best_ind, evals):\n            initial_neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            neighborhood_size_decay = 0.95\n            while evals < self.budget:\n                for i in range(pop_size):\n                    neighborhood_size = initial_neighborhood_size * (neighborhood_size_decay ** (evals / pop_size))\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        dynamic_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 25, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05823 with standard deviation 0.05514.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13504438642134498, 0.12693355886894675, 0.13484568806191388, 0.04439040932837657, 0.04118800694350422, 0.041035343254674284, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "fc51a6c9-e31d-4e51-872f-b954cb7bfe3a", "fitness": 0.05549748574308157, "name": "EnhancedHybridMetaheuristic", "description": "Enhanced Exploitation and Adaptation within Differential Evolution Using Dynamic Crossover and Mutation Scaling.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 20\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget)) * np.random.rand() # Dynamic F with randomness\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    CR_dynamic = CR * (0.5 + 0.5 * np.random.rand()) # Dynamic CR adjustment\n                    cross_points = np.random.rand(self.dim) < CR_dynamic\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_scale = 0.1\n            while evals < self.budget:\n                for i in range(pop_size):\n                    neighborhood_size = neighborhood_scale * (bounds[:, 1] - bounds[:, 0])\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                        neighborhood_scale *= 0.9 # Reduce neighborhood size upon improvement\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 26, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05550 with standard deviation 0.05356.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.1252835588635437, 0.1300367908032315, 0.12855184097187744, 0.03682711889420398, 0.03867820500310437, 0.039433190485106406, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "24205fdb-c1e5-4fcf-8eb4-e0d66c11586e", "fitness": 0.05823378439393637, "name": "EnhancedHybridMetaheuristic", "description": "An enhanced hybrid metaheuristic integrating covariance matrix adaptation with dynamic parameter tuning and local search in Differential Evolution.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            cov_matrix = np.eye(self.dim) * (0.1 * (bounds[:, 1] - bounds[:, 0]))**2\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = np.random.multivariate_normal(best_ind, cov_matrix)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                        cov_matrix = np.cov((pop - best_ind).T)  # Update covariance matrix\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 27, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05823 with standard deviation 0.05514.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13504438642134498, 0.12693355886894675, 0.13484568806191388, 0.04439040932837657, 0.04118800694350422, 0.041035343254674284, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "f7b16920-3924-4b5d-859b-fe9c8a69c73e", "fitness": 0.05823378439393637, "name": "EnhancedMetaheuristic", "description": "An enhanced hybrid metaheuristic exploiting adaptive parameter tuning and local search intensification via covariance matrix adaptation in Differential Evolution.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def covariance_matrix_adaptive_search(best_ind, evals):\n            sigma = 0.3\n            C = np.eye(self.dim)\n            while evals < self.budget:\n                z = np.random.randn(self.dim)\n                step = np.dot(C, z)\n                candidate = best_ind + sigma * step\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                f_candidate = func(candidate)\n                evals += 1\n                if f_candidate < fitness[best_idx]:\n                    best_idx = np.argmin(fitness)\n                    best_ind = candidate\n                    fitness[best_idx] = f_candidate\n                    C = (1 - 0.1) * C + 0.1 * np.outer(z, z)\n                if evals >= self.budget:\n                    break\n\n        covariance_matrix_adaptive_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 28, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05823 with standard deviation 0.05514.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13504438642134498, 0.12693355886894675, 0.13484568806191388, 0.04439040932837657, 0.04118800694350422, 0.041035343254674284, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "3291ca1c-27b2-4860-842c-355554a7e233", "fitness": 0.05823378439393637, "name": "HybridMetaheuristicRefined", "description": "A hybrid metaheuristic algorithm combining dynamic adaptation strategies with elite exploitation to improve convergence speed and accuracy.", "code": "import numpy as np\n\nclass HybridMetaheuristicRefined:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = max(4, 10)  # Ensure population is at least 4 for DE\n        F = 0.8\n        CR = 0.9\n        elite_ratio = 0.2\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            elite_count = max(1, int(elite_ratio * pop_size))\n            elite_indices = np.argsort(fitness)[:elite_count]\n\n            return pop, fitness, elite_indices\n\n        pop, fitness, elite_indices = differential_evolution()\n        best_idx = elite_indices[0]\n        best_ind = pop[best_idx]\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in elite_indices:\n                    candidate = pop[i] + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[i]:\n                        pop[i] = candidate\n                        fitness[i] = f_candidate\n                        if f_candidate < fitness[best_idx]:\n                            best_idx = i\n                            best_ind = candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, len(fitness))\n\n        return best_ind", "configspace": "", "generation": 29, "feedback": "The algorithm HybridMetaheuristicRefined got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05823 with standard deviation 0.05514.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13504438642134498, 0.12693355886894675, 0.13484568806191388, 0.04439040932837657, 0.04118800694350422, 0.041035343254674284, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "6f1c78ca-80e5-4a96-a1f9-191ef466de4c", "fitness": 0.060954431839779076, "name": "HybridMetaheuristic", "description": "Enhance the mutation strategy by adding a weighted average of two vectors in the Differential Evolution phase for better exploration and convergence.", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * ((b + c) / 2 - c), bounds[:, 0], bounds[:, 1])  # Change here\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 30, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06095 with standard deviation 0.05277.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.12833925108227118, 0.13085066183996863, 0.12651765892995692, 0.044894184987403496, 0.0596352266481901, 0.057686236403554636, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "0fb03cbb-3600-49f2-b8fc-e10c66408153", "fitness": 0.05823378439393637, "name": "EnhancedHybridMetaheuristic", "description": "Enhanced HybridMetaheuristic with adaptive parameter tuning and Lvy flight-inspired exploration to improve convergence and robustness.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def levy_flight(Lambda):\n            sigma1 = np.power((np.math.gamma(1 + Lambda) * np.sin(np.pi * Lambda / 2)) / \n                              (np.math.gamma((1 + Lambda) / 2) * Lambda * \n                               np.power(2, (Lambda - 1) / 2)), 1 / Lambda)\n            u = np.random.normal(0, sigma1, size=self.dim)\n            v = np.random.normal(0, 1, size=self.dim)\n            step = u / np.power(np.fabs(v), 1 / Lambda)\n            return step\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget)) \n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n                # Lvy flight-inspired exploration\n                if evals < self.budget:\n                    step = levy_flight(1.5) * (bounds[:, 1] - bounds[:, 0]) * 0.01\n                    candidate = best_ind + step\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 31, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05823 with standard deviation 0.05514.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13504438642134498, 0.12693355886894675, 0.13484568806191388, 0.04439040932837657, 0.04118800694350422, 0.041035343254674284, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "c4d57307-9ab2-496e-9bce-88c1ecd856b3", "fitness": 0.056232045119776246, "name": "EnhancedHybridMetaheuristic", "description": "An enhanced hybrid metaheuristic utilizing dynamic population sizing and adaptive crossover for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        initial_pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop_size = initial_pop_size\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    CR_dynamic = CR * (0.5 + 0.5 * (evals / self.budget))\n                    cross_points = np.random.rand(self.dim) < CR_dynamic\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n                if evals >= self.budget:\n                    break\n\n                # Dynamic population resizing\n                if evals % (self.budget // 4) == 0:\n                    new_pop_size = min(pop_size * 2, 50)  # Cap the growth to avoid excessive size\n                    if new_pop_size > pop_size:\n                        additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                        pop = np.vstack((pop, additional_pop))\n                        pop_size = new_pop_size\n                        fitness = np.append(fitness, [func(ind) for ind in additional_pop])\n                        evals += new_pop_size - pop_size\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness) \n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(initial_pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05623 with standard deviation 0.05291.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13096247188017196, 0.12492048579147152, 0.12544736122796774, 0.04547298114783693, 0.03985421800467526, 0.038764221359196105, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "273b5223-1e02-4155-a973-a4fe5681df0a", "fitness": 0.05656459232077763, "name": "EnhancedHybridMetaheuristic", "description": "An enhanced hybrid metaheuristic that integrates dynamic crossover rate adjustment and local-best-guided perturbation to boost convergence in Differential Evolution.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    CR_dynamic = 0.5 + 0.4 * np.sin(np.pi * evals / self.budget)  # Dynamic CR based on budget usage\n                    cross_points = np.random.rand(self.dim) < CR_dynamic\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    direction = np.random.rand(self.dim) - 0.5\n                    direction /= np.linalg.norm(direction)\n                    candidate = best_ind + 0.05 * neighborhood_size * direction\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 33, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05656 with standard deviation 0.05496.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13724270973340158, 0.13209435654527601, 0.12338420689493768, 0.036592075748847264, 0.03867854740488108, 0.040422767892988376, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "984a5d05-e53e-48da-89e5-183c37b12aec", "fitness": 0.05691061024770264, "name": "EnhancedHybridMetaheuristic", "description": "A dynamic ensemble of Differential Evolution and Adaptive Neighborhood Search, optimizing parameter control for better convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = max(10, self.dim * 5)\n        F_base = 0.8\n        CR_base = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F = F_base * (1 - (evals / self.budget)) + np.random.randn() * 0.05\n                    F = np.clip(F, 0, 1)\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < (CR_base + np.random.randn() * 0.05)\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = pop_size\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 34, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05691 with standard deviation 0.05360.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.12587211035764134, 0.13047392254893075, 0.1300301596709137, 0.04151330305024603, 0.03950196991549715, 0.04413736001942803, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "edd165b2-bf45-4e76-a98a-8111cbdaa955", "fitness": 0.05707294646246838, "name": "AdvancedHybridMetaheuristic", "description": "An advanced adaptive hybrid metaheuristic that incorporates self-adaptive differential evolution and dynamic neighborhood reduction to efficiently balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        \n        # Self-adaptive parameters\n        F_min, F_max = 0.5, 1.0\n        CR_min, CR_max = 0.1, 0.9\n\n        def adaptive_differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            F = np.random.uniform(F_min, F_max, pop_size)\n            CR = np.random.uniform(CR_min, CR_max, pop_size)\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    \n                    mutant = np.clip(a + F[i] * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR[i]\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                        \n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i], fitness[i] = trial, f_trial\n                        F[i] = np.random.uniform(F_min, F_max) # Adaptive mutation factor\n                        CR[i] = np.random.uniform(CR_min, CR_max) # Adaptive crossover rate\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = adaptive_differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def dynamic_neighborhood_reduction(best_ind, evals):\n            initial_neighborhood = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            neighborhood_size = initial_neighborhood\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                        neighborhood_size *= 0.9 # Reduce neighborhood size progressively\n                    else:\n                        neighborhood_size = initial_neighborhood # Reset if not improved\n                    if evals >= self.budget:\n                        break\n\n        dynamic_neighborhood_reduction(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 35, "feedback": "The algorithm AdvancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05707 with standard deviation 0.05434.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.14250498851333837, 0.12263035881227713, 0.12430297571008275, 0.04120236902074226, 0.03957865063207888, 0.04277050880702937, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "0e061c32-c182-4769-860d-d59605752e09", "fitness": 0.05659641785661088, "name": "EnhancedHybridMetaheuristic", "description": "An enhanced hybrid metaheuristic combining adaptive differential evolution with dynamic population size and local search refinement.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        initial_pop_size = 10\n        F = 0.8\n        CR = 0.9\n        dynamic_pop_factor = 0.05\n\n        def differential_evolution():\n            pop_size = initial_pop_size\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                if evals % (self.budget // 5) == 0:\n                    pop_size = int(initial_pop_size + dynamic_pop_factor * (self.budget - evals))\n                    pop = np.vstack((pop, np.random.rand(pop_size - len(pop), self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]))\n                    fitness = np.hstack((fitness, [func(ind) for ind in pop[len(fitness):]]))\n                    evals += pop_size - len(fitness)\n\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(10):  # Fixed number of trials\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = np.argmin(fitness)\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 36, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05660 with standard deviation 0.05490.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.131539452422049, 0.12645523907954082, 0.13484568806191388, 0.03754773051997273, 0.0404681914361954, 0.03784479252315942, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "7ffff39e-d29b-4a31-a7ec-118ae5c773b7", "fitness": 0.05692984826726391, "name": "EnhancedHybridMetaheuristic", "description": "Enhanced HybridMetaheuristic optimizing exploration and exploitation balance with dynamic CR and adaptive neighborhood scaling.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n\n        def dynamic_CR(evals):\n            return 0.9 - 0.5 * (evals / self.budget)\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    CR_dynamic = dynamic_CR(evals)\n                    cross_points = np.random.rand(self.dim) < CR_dynamic\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = pop_size\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            initial_neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            neighborhood_size = initial_neighborhood_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                        neighborhood_size = initial_neighborhood_size  # Reset neighborhood size\n                    else:\n                        neighborhood_size *= 0.95  # Reduce neighborhood size\n                   \n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 37, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05693 with standard deviation 0.05518.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.1363387857523053, 0.13308214572471921, 0.12532511351586895, 0.03710239126065895, 0.040912578739488614, 0.038940952745667445, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "9ba86fb1-750f-491d-b355-55b5b7ef43dc", "fitness": 0.05615512916049205, "name": "HybridMetaheuristic", "description": "Enhanced HybridMetaheuristic with crossover probability adaptation based on fitness diversity for improved exploration.", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget)) # Change 1\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1]) # Change 2\n                    fitness_diversity = np.std(fitness) / (np.mean(fitness) + 1e-10) # Change\n                    CR_dynamic = CR * (1 + fitness_diversity) # Change 3\n                    cross_points = np.random.rand(self.dim) < CR_dynamic\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness) # Change 3\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 38, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05616 with standard deviation 0.05262.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.12793699216051413, 0.12462200444532734, 0.1272726607332073, 0.04336048248725677, 0.041744906899888745, 0.039792449051567425, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "72775cb3-74c1-4d3f-a8c1-c5e90ab3fcec", "fitness": 0.05823378439393637, "name": "HybridMetaheuristic", "description": "A refined hybrid metaheuristic enhancing exploration with adaptive parameter tuning in Differential Evolution and incorporating elitism for improved convergence.", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            best_idx = np.argmin(fitness)\n            best_ind = pop[best_idx]  # Elitism: Ensure the best individual is always kept\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 39, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05823 with standard deviation 0.05514.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13504438642134498, 0.12693355886894675, 0.13484568806191388, 0.04439040932837657, 0.04118800694350422, 0.041035343254674284, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "892e0c5e-1c96-4d47-adde-d1225555a1c3", "fitness": 0.05682825031305198, "name": "EnhancedHybridMetaheuristic", "description": "An enhanced metaheuristic integrating a dynamic mutation strategy and fitness diversity preservation to improve convergence in Differential Evolution.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            def compute_diversity():\n                return np.mean(np.std(pop, axis=0))\n\n            while evals < self.budget:\n                diversity = compute_diversity()\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget)) * (1 + diversity)  # Adjusted dynamic factor with diversity\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05683 with standard deviation 0.05371.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.1311458216334549, 0.124959468982352, 0.13065393434928596, 0.03990188634782699, 0.044171842378357584, 0.03995463245952369, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "78de08a8-cb83-4428-92e6-5e0c39947f91", "fitness": 0.05737353050922789, "name": "HybridMetaheuristic", "description": "Introduce an adaptive crossover rate decay to further refine exploration and exploitation balance in Differential Evolution.", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    CR_dynamic = CR * (1 - (evals / self.budget)) # Change 1\n                    cross_points = np.random.rand(self.dim) < CR_dynamic\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 41, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05737 with standard deviation 0.05446.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13248146502726954, 0.13399323226454296, 0.1251833437683313, 0.04031256578121589, 0.044657080240355196, 0.03906742083466941, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "b9f6488c-f097-4b11-bacf-3319b5114d99", "fitness": 0.05592511715570124, "name": "EnhancedHybridMetaheuristic", "description": "Enhanced HybridMetaheuristic using stochastic ranking for constraint handling and dual-adaptation in mutation and crossover rates.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F_base = 0.8\n        CR_base = 0.9\n\n        def stochastic_ranking(pop, fitness, penalty_func):\n            return np.argsort(fitness + penalty_func(pop))\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_base * (1 - (evals / self.budget))\n                    CR_dynamic = CR_base * (1 - ((fitness[i] - min(fitness)) / (max(fitness) - min(fitness) + 1e-9)))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR_dynamic\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            initial_neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            neighborhood_size = initial_neighborhood_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                        neighborhood_size = initial_neighborhood_size\n                    else:\n                        neighborhood_size *= 0.9\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05593 with standard deviation 0.05318.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.1325597209540631, 0.12563919282571812, 0.12416132528377455, 0.04191301687908988, 0.03822522825917141, 0.04016090353282742, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "27f8c03b-82d0-41fd-ac78-d5a8ad407bd1", "fitness": 0.055406464095861446, "name": "HybridMetaheuristic", "description": "An enhanced hybrid metaheuristic using Lvy flights and adaptive learning rates in Differential Evolution to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    # Introduce Lvy flight mechanism\n                    levy_factor = np.random.standard_cauchy(size=self.dim)\n                    F_dynamic = F * (1 - levy_factor * (evals / self.budget)) # Change 1\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1]) # Change 2\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                # Adaptive learning rate based on success rate\n                success_rate = np.mean(fitness < np.median(fitness))\n                learning_rate = success_rate if success_rate > 0 else 0.1 # Change 3\n                for i in range(pop_size):\n                    candidate = best_ind + learning_rate * neighborhood_size * (2 * np.random.rand(self.dim) - 1) # Change 4\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 43, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05541 with standard deviation 0.05294.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.12683853461896444, 0.1260062315985997, 0.12758073129543757, 0.03851903276826896, 0.04109411846497868, 0.037952861449837005, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "4db43d66-6503-4608-a871-6f6524d5b750", "fitness": 0.05737353050922789, "name": "HybridMetaheuristic", "description": "Enhanced exploration by incorporating adaptive CR and dynamic neighborhood scaling in Differential Evolution.", "code": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    CR_dynamic = CR * (1 - (evals / self.budget)) # Change 1\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR_dynamic # Change 2\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - (evals / self.budget)) # Change 3\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 44, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05737 with standard deviation 0.05446.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [0.13248146502726954, 0.13399323226454296, 0.1251833437683313, 0.04031256578121589, 0.044657080240355196, 0.03906742083466941, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "c0d413e5-c8c6-4e69-a941-e86b824f7e57", "fitness": -Infinity, "name": "EnhancedHybridMetaheuristic", "description": "An enhanced hybrid metaheuristic incorporating dynamic population size and adaptive crossover probability to improve convergence in Differential Evolution.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        initial_pop_size = 20\n        min_pop_size = 4\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop_size = initial_pop_size\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    CR_dynamic = CR * (1 - (evals / self.budget))\n                    cross_points = np.random.rand(self.dim) < CR_dynamic\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget or pop_size <= min_pop_size:\n                        break\n\n                # Reduce population size adaptively\n                if pop_size > min_pop_size:\n                    pop_size = max(min_pop_size, int(initial_pop_size * (1 - evals / self.budget)))\n                    pop = pop[:pop_size]\n                    fitness = fitness[:pop_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 45, "feedback": "An exception occurred: NameError(\"name 'pop_size' is not defined\").", "error": "NameError(\"name 'pop_size' is not defined\")", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {}}
{"id": "7c3c06df-b9ec-4eb3-b545-e985479814d1", "fitness": 0.15794481565442572, "name": "RefinedHybridMetaheuristic", "description": "A dynamic hybrid metaheuristic combining self-adapting differential evolution with neighborhood search and adaptive population resizing for improved convergence.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 46, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.15794 with standard deviation 0.30192.", "error": "", "parent_ids": ["9912ede8-5042-47c2-bdcc-932a3fac6b87"], "operator": null, "metadata": {"aucs": [1.0, 0.14321168804677764, 0.1313866396307133, 0.04949817881472074, 0.04729013211916311, 0.049450035611790066, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "3a8a8463-0740-4e2b-9a9d-0c387ce2c6eb", "fitness": 0.059507341317239004, "name": "ImprovedHybridMetaheuristic", "description": "A novel self-tuning hybrid metaheuristic combining adaptive differential evolution and localized random walks to enhance exploration and exploitation in black box optimization.", "code": "import numpy as np\n\nclass ImprovedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing and self-tuning\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n                    F_adapt *= 1.05  # Increase F adaptively to encourage exploration\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def localized_random_walk(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for _ in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        localized_random_walk(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 47, "feedback": "The algorithm ImprovedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05951 with standard deviation 0.05568.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12961392952472228, 0.14321168804677764, 0.12845201468647183, 0.04166976393602395, 0.0467753396295395, 0.04517666936494913, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "b07c2e45-33f8-4c39-a403-e2a690cb502c", "fitness": 0.056166782593355676, "name": "EnhancedDynamicAdaptiveMetaheuristic", "description": "EnhancedDynamicAdaptiveMetaheuristic: An improved hybrid approach integrating dynamic self-adapting differential evolution with probabilistic neighborhood exploration and variable population resizing for superior performance.", "code": "import numpy as np\n\nclass EnhancedDynamicAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 15  # Increased initial population size\n        F_initial = 0.8\n        CR_initial = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n\n                    # Dynamic F and CR adjustment\n                    F = F_initial * np.exp(-(evals / self.budget))\n                    CR = CR_initial * (1 - (evals / self.budget))\n                    \n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n                # Enhanced adaptive population resizing\n                if evals < self.budget and evals % int(0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * (0.9 + 0.1 * np.random.rand())))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.05 * (bounds[:, 1] - bounds[:, 0])  # Smaller initial neighborhood\n            decay_rate = 0.98\n\n            while evals < self.budget:\n                for _ in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = np.argmin(fitness)\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n                neighborhood_size *= decay_rate\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 48, "feedback": "The algorithm EnhancedDynamicAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05617 with standard deviation 0.05294.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12623547909915345, 0.12946061154191368, 0.1258681684150753, 0.04047227483661198, 0.04352466895218987, 0.03927317382859008, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "5c470d65-cc80-4db5-a3aa-e504005948f5", "fitness": 0.05710012935322574, "name": "RefinedHybridMetaheuristic", "description": "An improved hybrid metaheuristic that refines mutation strategy by dynamically adjusting the crossover rate based on the evaluation budget for enhanced exploration capabilities.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < (CR * (1 - (evals / self.budget)))  # Changed line\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 49, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05710 with standard deviation 0.05349.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.1293862588220972, 0.12629792817039032, 0.13017124409903935, 0.03960417775882685, 0.03955010250578539, 0.04822478615622583, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "cbed536b-81d6-4f8b-940f-8bd2b4f5614b", "fitness": -Infinity, "name": "EnhancedHybridMetaheuristic", "description": "A refined hybrid metaheuristic that integrates enhanced differential evolution with a novel adaptive neighborhood exploration and dynamic parameter tuning for improved efficiency and convergence on diverse optimization landscapes.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                    if evals >= self.budget:\n                        break\n\n                # Dynamic CR adjustment\n                if evals % (0.1 * self.budget) == 0:\n                    CR = np.clip(CR + (np.random.rand() - 0.5) * 0.2, 0.1, 0.9)\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_factor = 0.05  # Smaller factor for finer local search\n            while evals < self.budget:\n                for _ in range(pop_size):\n                    neighborhood_size = neighborhood_factor * (bounds[:, 1] - bounds[:, 0]) * (1 - evals / self.budget)\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = np.argmin(fitness)\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 50, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'CR' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'CR' referenced before assignment\")", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {}}
{"id": "9f72f7a5-d85d-48ed-a4f8-7bd49919e28f", "fitness": 0.059887376855650154, "name": "RefinedHybridMetaheuristic", "description": "Enhanced dynamic adjustment of F and CR for improved convergence and robustness in diverse landscapes.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            CR_adapt = CR  # Added adaptive CR\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR_adapt  # Use adaptive CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                        CR_adapt = min(CR_adapt + 0.05, 0.95)  # Dynamic CR adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 51, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05989 with standard deviation 0.05237.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.1284159780455848, 0.1291721214879623, 0.12532757175697373, 0.05612046902626788, 0.049287959614607524, 0.04999562510278843, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "da927f6d-3a46-40fc-9ac0-2c7fbe6e0e5e", "fitness": -Infinity, "name": "EnhancedHybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with adaptive mutation strategies and dynamic neighborhood exploration to improve convergence and solution quality.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F_base = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                F_dynamic = F_base * (1 - (evals / self.budget))\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_base = max(F_base - 0.05, 0.4)  # Smoother F adjustment\n                    if evals >= self.budget:\n                        break\n\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    dynamic_factor = (self.budget - evals) / self.budget\n                    candidate = best_ind + dynamic_factor * neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 52, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'F_base' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'F_base' referenced before assignment\")", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {}}
{"id": "0b8e9ccf-be79-437f-ab2e-b0494c7c94f3", "fitness": 0.05993107198178544, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic using dynamic scaling factors, multi-phase neighborhood search, and adaptive elitism for superior performance across diverse optimization landscapes.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 15\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.05, 0.3)  # More gradual F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing with elitism\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    elite_count = max(1, int(0.1 * pop_adapt_size))\n                    elite_indices = np.argsort(fitness)[:elite_count]\n                    elites = pop[elite_indices]\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = np.concatenate((elites, pop[:pop_adapt_size]))\n                    fitness = np.array([func(ind) for ind in pop])\n                    evals = len(fitness)\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def multi_phase_neighborhood_search(best_ind, evals):\n            initial_neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            phase_count = 3  # Number of phases with decreasing neighborhood size\n            for phase in range(phase_count):\n                neighborhood_size = initial_neighborhood_size * (0.5 ** phase)\n                while evals < self.budget:\n                    for i in range(pop_size):\n                        candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                        candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                        f_candidate = func(candidate)\n                        evals += 1\n                        if f_candidate < fitness[best_idx]:\n                            best_idx = i\n                            best_ind = candidate\n                            fitness[best_idx] = f_candidate\n                        if evals >= self.budget:\n                            break\n\n        multi_phase_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 53, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05993 with standard deviation 0.05463.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.13063182816015684, 0.13742893416827806, 0.12813791788258655, 0.05216014661541035, 0.0426169585574917, 0.04773719578547875, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "f2b25290-6dc4-4a98-b14d-40055db750f0", "fitness": 0.05942297895601783, "name": "RefinedHybridMetaheuristic", "description": "Improved adaptive strategies within the hybrid metaheuristic for enhanced convergence and exploitation.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.05, 0.4)  # Dynamic F adjustment improved\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.05 * (bounds[:, 1] - bounds[:, 0])  # Reduced neighborhood size\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 54, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05942 with standard deviation 0.05377.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.13161068846072266, 0.130668593583527, 0.12834087911339864, 0.0438804625091026, 0.0543884167238442, 0.04525110354689865, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "9a0ab177-a546-4208-8805-274713c73be3", "fitness": 0.058532854272737786, "name": "EnhancedAdaptiveHybridMetaheuristic", "description": "Enhanced adaptive hybrid metaheuristic integrating adaptive crossover and mutation dynamics with expanded neighborhood search for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        initial_pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def adaptive_differential_evolution():\n            pop = np.random.rand(initial_pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = initial_pop_size\n            F_adapt = F\n            CR_adapt = CR\n            pop_adapt_size = initial_pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR_adapt\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.05, 0.3)  # Fine-tuned F adjustment\n                        CR_adapt = min(CR_adapt + 0.05, 0.95)  # Adaptive CR adjustment\n                    if evals >= self.budget:\n                        break\n\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.85))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = adaptive_differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def expanded_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.15 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(initial_pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n                neighborhood_size *= 0.95  # Dynamically shrink neighborhood\n\n        expanded_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 55, "feedback": "The algorithm EnhancedAdaptiveHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05853 with standard deviation 0.05156.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12485935174645213, 0.12592886588968977, 0.1256992615231558, 0.04841439130294345, 0.055572074241028946, 0.04565507708470329, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "d45cd5d2-8775-40a6-9a34-43063510a739", "fitness": 0.058831627719769145, "name": "EnhancedRefinedHybridMetaheuristic", "description": "An enhanced adaptive hybrid metaheuristic integrating self-adaptive differential evolution with dynamic neighborhood search and adaptive crossover for superior exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedRefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n        alpha = 0.1\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            CR_adapt = CR\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR_adapt\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.05, 0.4)\n                        CR_adapt = min(CR_adapt + 0.05, 0.99)\n                    else:\n                        CR_adapt = max(CR_adapt - 0.05, 0.1)\n                    if evals >= self.budget:\n                        break\n\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = alpha * (bounds[:, 1] - bounds[:, 0]) * (1 - evals / self.budget)\n            while evals < self.budget:\n                for _ in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = np.argmin(fitness)\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                        neighborhood_size *= 0.9\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 56, "feedback": "The algorithm EnhancedRefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05883 with standard deviation 0.05385.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.1322228990136055, 0.12342889124390521, 0.1345716271791556, 0.04421318656686002, 0.044653675156002604, 0.04972770365172663, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "55070355-992e-4774-abf9-7b7d544cd634", "fitness": 0.05632942995030337, "name": "EnhancedHybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic with adaptive mutation and crossover strategies, integrating elitism and dynamic scaling for superior convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 20\n        F = 0.8\n        CR = 0.9\n        elite_rate = 0.1\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            CR_adapt = CR\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                elite_size = max(1, int(elite_rate * pop_adapt_size))\n                elite_idx = np.argsort(fitness)[:elite_size]\n                elite_pop = pop[elite_idx]\n                for i in range(pop_adapt_size):\n                    if i in elite_idx:\n                        continue\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    CR_dynamic = CR_adapt * (1 - (evals / self.budget))\n                    cross_points = np.random.rand(self.dim) < CR_dynamic\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = min(F_adapt + 0.1, 1.0)\n                        CR_adapt = max(CR_adapt - 0.05, 0.5)\n                    else:\n                        F_adapt = max(F_adapt - 0.05, 0.4)\n                        CR_adapt = min(CR_adapt + 0.1, 0.9)\n                    if evals >= self.budget:\n                        break\n\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = np.concatenate((elite_pop, pop[:pop_adapt_size - elite_size]))\n                    fitness = np.array([func(ind) for ind in pop])\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 57, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05633 with standard deviation 0.05410.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.1320432234631267, 0.12915055990876256, 0.12695707328273054, 0.04209181022827291, 0.03860705972843326, 0.037448476274737685, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "a8d8b6a1-b812-42b7-afff-ba1cf89c0ebe", "fitness": -Infinity, "name": "RefinedHybridMetaheuristic", "description": "Enhanced RefinedHybridMetaheuristic integrating adaptive mutation strategies and elitism to improve both convergence speed and solution quality.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                best_idx = np.argmin(fitness)\n                best_individual = pop[best_idx]  # Elitism: retain best individual\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    adaptive_factor = 0.6 + 0.4 * (fitness[best_idx] - fitness[i]) / np.ptp(fitness)\n                    mutant = np.clip(a + F_dynamic * adaptive_factor * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n                pop[best_idx] = best_individual  # Ensure best individual survives\n\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 58, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'best_idx' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'best_idx' referenced before assignment\")", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {}}
{"id": "285b0b32-a5ef-4755-8ae3-51d2f0ffc414", "fitness": 0.15562573975013144, "name": "RefinedHybridMetaheuristic", "description": "Introduced a fitness-based dynamic crossover rate to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    \n                    # Dynamic crossover rate based on fitness\n                    CR_dynamic = CR * (1 - (fitness[i] / np.max(fitness)))\n                    cross_points = np.random.rand(self.dim) < CR_dynamic\n                    \n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 59, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.15563 with standard deviation 0.30036.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.994465375941419, 0.12940894485591914, 0.13108900834122617, 0.04726188856678404, 0.05173872118441858, 0.04600105219474948, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "e3486805-c32f-427e-89b6-a1b41670505a", "fitness": 0.057094939217431966, "name": "EnhancedHybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with adaptive crossover rate and dynamic local search, improving robustness and convergence speed.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            CR_adapt = CR\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR_adapt\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)\n                        CR_adapt = min(CR_adapt + 0.05, 1.0)  # Dynamic CR adjustment\n                    else:\n                        CR_adapt = max(CR_adapt - 0.05, 0.5)  # Penalize CR if not improved\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def dynamic_local_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for _ in range(pop_size):\n                    candidate = best_ind + np.random.normal(0, neighborhood_size, self.dim)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = np.argmin(fitness)\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n                neighborhood_size *= 0.9  # Decrease neighborhood size dynamically\n\n        dynamic_local_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 60, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05709 with standard deviation 0.05538.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.13771687319394554, 0.12737644999519226, 0.13110995751583798, 0.039410007060791785, 0.038138348173796355, 0.03943615035065706, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "eb6b7ba0-250c-4545-88e8-04dfa8d53089", "fitness": 0.0612396776932779, "name": "RefinedHybridMetaheuristic", "description": "A dynamic hybrid metaheuristic combining self-adapting differential evolution with neighborhood search and adaptive population resizing for improved convergence and diversity maintenance.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 61, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06124 with standard deviation 0.05574.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12965375834966952, 0.14321168804677764, 0.1313866396307133, 0.04949817881472074, 0.04729013211916311, 0.049450035611790066, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "2c319f44-4c27-4cc0-9cbc-cd0297cd03cb", "fitness": 0.06076680032831643, "name": "RefinedHybridMetaheuristic", "description": "A dynamic hybrid metaheuristic with an enhanced mutation strategy in self-adapting differential evolution and neighborhood search for improved convergence.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c, d = pop[np.random.choice(indices, 4, replace=False)]  # Added a fourth vector d for enhanced mutation\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * ((b - c) + (c - d)), bounds[:, 0], bounds[:, 1])  # Enhanced mutation strategy\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 62, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06077 with standard deviation 0.05582.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.13508611098955292, 0.1368362083123773, 0.13265386226156717, 0.0479530682282332, 0.044030093445134755, 0.04967519305131585, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "c63d7e08-2d8a-4e07-9a02-1450674ebcc1", "fitness": 0.05669134445798422, "name": "RefinedHybridMetaheuristic", "description": "Introduce dynamic crossover probability adaptation to enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            CR_adapt = CR  # Introduce adaptation for CR\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR_adapt  # Use adapted CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                        CR_adapt = min(CR_adapt + 0.05, 0.95)  # Increase CR when successful\n                    else:\n                        CR_adapt = max(CR_adapt - 0.05, 0.6)  # Decrease CR when not successful\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 63, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05669 with standard deviation 0.05468.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.137471377088207, 0.1287192194011394, 0.12529053299158688, 0.04255061869048893, 0.03555281661509391, 0.039970868668675164, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "3dc34b55-f7f4-464c-92b5-2f0383f56c4c", "fitness": -Infinity, "name": "EnhancedHybridMetaheuristic", "description": "An enhanced hybrid metaheuristic implementing adaptive parameter control and elitism in differential evolution for robust convergence across diverse optimization landscapes.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 15\n        F_base = 0.7\n        CR_base = 0.8\n        elitism_rate = 0.2\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F_base\n            CR_adapt = CR_base\n\n            while evals < self.budget:\n                sorted_indices = np.argsort(fitness)\n                elite_pop = pop[sorted_indices[:int(elitism_rate * pop_size)]]\n                \n                for i in range(pop_size):\n                    if np.random.rand() < elitism_rate:\n                        a, b = np.random.choice(elite_pop, 2, replace=False)\n                    else:\n                        indices = list(range(pop_size))\n                        indices.remove(i)\n                        a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    \n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR_adapt\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.05, 0.4)  # Dynamic F adjustment\n                        CR_adapt = min(CR_adapt + 0.05, 0.95)  # Dynamic CR adjustment\n                    if evals >= self.budget:\n                        break\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 64, "feedback": "An exception occurred: ValueError('a must be 1-dimensional').", "error": "ValueError('a must be 1-dimensional')", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {}}
{"id": "8273ff5f-26a0-4f1d-8627-77471a4af7fb", "fitness": 0.06034861241516219, "name": "EnhancedHybridMetaheuristic", "description": "An enhanced hybrid metaheuristic combining dynamic differential evolution with an adaptive memory-based mutation strategy and precision-tuning neighborhood search for robust optimization.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            memory_F = np.full(pop_size, F)\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    F_i = memory_F[i]\n                    mutant = np.clip(a + F_i * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)\n                        memory_F[i] = F_dynamic\n                    if evals >= self.budget:\n                        break\n\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n                    memory_F = memory_F[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n                neighborhood_size *= 0.95\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 65, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06035 with standard deviation 0.05529.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.13996693262557125, 0.13082838946147457, 0.1299076301836033, 0.048565581438785865, 0.04754761787839035, 0.04565469348196771, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "91c1574c-3041-4a29-a168-b5a838f7ec2f", "fitness": 0.05986700649844782, "name": "RefinedHybridMetaheuristic", "description": "A refined hybrid metaheuristic with a slightly more aggressive differential strategy to enhance exploration and convergence.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c) + 0.1 * (a - c), bounds[:, 0], bounds[:, 1]) # Change made here\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 66, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05987 with standard deviation 0.05680.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12702791717112338, 0.137091688761399, 0.14388764662867448, 0.04556080832732767, 0.044545595374660296, 0.04002273555617886, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "a9088142-2d15-4452-8de4-d2288c4ed8bc", "fitness": 0.056467828565258976, "name": "RefinedHybridMetaheuristic", "description": "Enhance the mutation strategy by incorporating a decay factor to the crossover rate for better diversity and convergence.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n            CR_decay = CR\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR_decay  # Modified line\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                        CR_decay = max(CR_decay - 0.01, 0.3)  # Decay factor for CR\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 67, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05647 with standard deviation 0.05341.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12913626126113176, 0.13061516386862604, 0.12484914971059147, 0.04192756477997461, 0.03928763735762353, 0.041728013442716705, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "764d3487-8abb-4f8c-b0f3-3146eb84bb83", "fitness": 0.05910678071666628, "name": "RefinedHybridMetaheuristic", "description": "Enhanced dynamic population resizing to improve convergence efficiency by adjusting resize factor based on evaluation progress.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * (0.8 + 0.2 * (evals / self.budget))))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 68, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05911 with standard deviation 0.05522.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12839294687276193, 0.14321168804677764, 0.1263262232803708, 0.04328873553488566, 0.044701423338369906, 0.04537334271016391, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "781e25fa-e832-4792-926f-090c80bb264c", "fitness": -Infinity, "name": "EnhancedHybridMetaheuristic", "description": "An enhanced hybrid metaheuristic integrating adaptive differential evolution with a restart mechanism and local search intensification for superior optimization performance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n            best_idx = np.argmin(fitness)\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        if f_trial < fitness[best_idx]:\n                            best_idx = i\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness, best_idx\n\n        pop, fitness, best_idx = differential_evolution()\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n            return best_ind\n\n        best_ind = adaptive_neighborhood_search(best_ind, evals)\n\n        # Restart mechanism\n        while evals < self.budget:\n            pop, fitness, best_idx = differential_evolution()\n            best_ind = adaptive_neighborhood_search(pop[best_idx], evals)\n            evals += pop_size\n\n        return best_ind", "configspace": "", "generation": 69, "feedback": "An exception occurred: IndexError('index 9 is out of bounds for axis 0 with size 9').", "error": "IndexError('index 9 is out of bounds for axis 0 with size 9')", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {}}
{"id": "77c542d0-1bbc-491a-a230-e1896da7325c", "fitness": 0.07224754037594464, "name": "RefinedHybridMetaheuristic", "description": "Enhanced adaptive hybrid metaheuristic with improved mutation strategy and dynamic crossover rate adjustment for superior optimization performance.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c) + 0.1 * (pop[np.random.randint(pop_adapt_size)] - pop[i]), bounds[:, 0], bounds[:, 1])  # Enhanced mutation \n                    CR_dynamic = 0.9 - 0.5 * (evals / self.budget)  # Dynamic crossover rate adjustment\n                    cross_points = np.random.rand(self.dim) < CR_dynamic  # Use dynamic CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  \n                    if evals >= self.budget:\n                        break\n\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 70, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07225 with standard deviation 0.08446.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.2724733520400461, 0.12646389110930167, 0.12536153782079684, 0.04178388659762433, 0.043405151901462435, 0.040073377247603625, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "0cf7f7d6-a416-4175-a65d-eeb97853adff", "fitness": 0.0612396776932779, "name": "EnhancedHybridMetaheuristic", "description": "A refined hybrid metaheuristic that integrates adaptive Levy flight for exploration and enhanced exploitation strategies, balancing exploration and exploitation dynamically for better convergence.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def enhanced_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    # Levy flight step\n                    step_size = levy.rvs(size=self.dim)\n                    candidate = best_ind + step_size * neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        enhanced_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 71, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06124 with standard deviation 0.05574.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12965375834966952, 0.14321168804677764, 0.1313866396307133, 0.04949817881472074, 0.04729013211916311, 0.049450035611790066, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "0d33f488-20dc-49a3-bcfd-193468e1f2fb", "fitness": 0.05939192934760348, "name": "AdvancedHybridMetaheuristic", "description": "An advanced hybrid metaheuristic integrating adaptive differential evolution with a novel perturbation-based local search and dynamic population sizing for enhanced performance.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    else:\n                        F_adapt = min(F_adapt + 0.05, 0.9)  # Recovery if not improved\n                    if evals >= self.budget:\n                        break\n\n                # Dynamic population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def perturbation_local_search(best_ind, evals):\n            perturbation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    perturbation = perturbation_strength * np.random.randn(self.dim)\n                    candidate = best_ind + perturbation\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                        perturbation_strength *= 0.9  # Reduce strength if improvement is found\n                    else:\n                        perturbation_strength *= 1.1  # Increase strength if no improvement\n                    if evals >= self.budget:\n                        break\n\n        perturbation_local_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 72, "feedback": "The algorithm AdvancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05939 with standard deviation 0.05382.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12509269604524442, 0.134424722943298, 0.13009248276357865, 0.044502315096141776, 0.03945775913880445, 0.06029072147469727, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "f4c2c02c-4e5d-4311-89dc-546729bf7205", "fitness": 0.0612396776932779, "name": "RefinedHybridMetaheuristic", "description": "Enhance adaptive parameters by adjusting crossover rate to improve solution diversity and convergence.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 73, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06124 with standard deviation 0.05574.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12965375834966952, 0.14321168804677764, 0.1313866396307133, 0.04949817881472074, 0.04729013211916311, 0.049450035611790066, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "6174c1aa-69a6-4f22-b766-18bc04ab4a04", "fitness": 0.060017003857875374, "name": "EnhancedMemeticMetaheuristic", "description": "EnhancedMemeticMetaheuristic: An enhanced memetic algorithm integrating adaptive differential evolution with local search leveraging dynamic step size and iterative improvement strategies for robust optimization.", "code": "import numpy as np\n\nclass EnhancedMemeticMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 15\n        F = 0.7\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.3)  # More aggressive dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_local_search(best_ind, evals):\n            initial_step_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            iteration = 0\n            while evals < self.budget:\n                neighborhood_size = initial_step_size * (0.95 ** iteration)\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n                iteration += 1\n\n        adaptive_local_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedMemeticMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06002 with standard deviation 0.05542.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12639086905489005, 0.14470781428130097, 0.12913335785787816, 0.04906902693130499, 0.04536759423159331, 0.04481770569724419, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "e6385f41-639d-4597-bb0b-638cf87052cd", "fitness": 0.058147459415068666, "name": "RefinedHybridMetaheuristic", "description": "Enhanced population diversity by introducing a mutation step using a random vector within bounds.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    random_vector = np.random.rand(self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                    mutant = np.clip(a + F_dynamic * (b - c) + 0.1 * (random_vector - pop[i]), bounds[:, 0], bounds[:, 1])  # Modified line\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 75, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05815 with standard deviation 0.05567.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.13409554528833756, 0.13328297238486264, 0.13248833222015144, 0.041472012799404157, 0.04105908690336402, 0.040262518472831466, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "cd61d4ad-91c7-4d02-ab56-f73e0041dd7b", "fitness": 0.05999434719571755, "name": "RefinedHybridMetaheuristic", "description": "Enhanced adaptive differential evolution with dynamic neighborhood search and stochastic ranking for improved convergence and robustness.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n                    \n                # Stochastic ranking to balance convergence/exploration\n                stoch_order = np.argsort(np.random.rand(pop_adapt_size) + fitness)\n\n            return pop[stoch_order], fitness[stoch_order]\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 76, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05999 with standard deviation 0.05225.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12262662890201836, 0.1328017887318843, 0.12657625634891212, 0.05324947873865005, 0.05542238077155226, 0.04860592460177415, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "7bfe96e6-0d34-46f0-a3cb-138968a0d989", "fitness": 0.0612396776932779, "name": "RefinedHybridMetaheuristic", "description": "Improved local search by dynamic neighborhood size adjustment based on evaluation progress.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - evals / self.budget)\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 77, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06124 with standard deviation 0.05574.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12965375834966952, 0.14321168804677764, 0.1313866396307133, 0.04949817881472074, 0.04729013211916311, 0.049450035611790066, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "05c7e39b-6a69-4e8e-b5ec-01ab0c33e812", "fitness": 0.059674612610691055, "name": "RefinedHybridMetaheuristic", "description": "A refined hybrid metaheuristic with an adaptive crossover rate starting lower initially and increasing over time for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.5  # Start with a lower crossover rate\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    CR_dynamic = CR + (0.4 * (evals / self.budget))  # Increase CR over time\n                    cross_points = np.random.rand(self.dim) < CR_dynamic\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 78, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05967 with standard deviation 0.05669.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.13441934772059694, 0.13286536199817656, 0.14037451223178965, 0.04247840547542869, 0.046421561657930654, 0.039845657745630314, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "c0ebc144-5a91-4e0b-b6a8-2907b1c54450", "fitness": 0.0612396776932779, "name": "RefinedHybridMetaheuristic", "description": "A refined hybrid metaheuristic with enhanced neighborhood search strategy for robust black box optimization.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.05 * (bounds[:, 1] - bounds[:, 0]) # Reduced neighborhood size for finer exploration\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 79, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06124 with standard deviation 0.05574.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12965375834966952, 0.14321168804677764, 0.1313866396307133, 0.04949817881472074, 0.04729013211916311, 0.049450035611790066, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "cfad47de-5c8c-4cbb-b887-db85d9703f75", "fitness": 0.0614196313096492, "name": "RefinedHybridMetaheuristic", "description": "Improved mutation strategy by introducing rotation and scaling to enhance exploration capabilities in differential evolution.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    # Introduce rotation and scaling\n                    rotation_matrix = np.random.randn(self.dim, self.dim)\n                    rotation_matrix = np.linalg.qr(rotation_matrix)[0]  # QR decomposition for orthogonal matrix\n                    scaled_vector = F_dynamic * (b - c)\n                    rotated_vector = np.dot(rotation_matrix, scaled_vector)\n                    mutant = np.clip(a + rotated_vector, bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 80, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06142 with standard deviation 0.05367.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12932153047677353, 0.13357922718081705, 0.12863408429714807, 0.05230285176807692, 0.04495304571658931, 0.06331927568077123, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "c289693b-d3d1-416b-ac5e-0346753e302b", "fitness": 0.05710012935322574, "name": "RefinedHybridMetaheuristic", "description": "Improved exploration by variability in crossover probability in differential evolution for better convergence.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR * (1 - evals / self.budget)  # Modified crossover probability\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 81, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05710 with standard deviation 0.05349.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.1293862588220972, 0.12629792817039032, 0.13017124409903935, 0.03960417775882685, 0.03955010250578539, 0.04822478615622583, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "592025a5-24a8-4229-a65f-38ba2273bf28", "fitness": 0.0612396776932779, "name": "RefinedHybridMetaheuristic", "description": "RefinedHybridMetaheuristicPlus utilizes a pivot-based neighborhood search to enhance global exploration and convergence.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            pivot = np.median(pop, axis=0)  # Added line for pivot-based search\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = pivot + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 82, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06124 with standard deviation 0.05574.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12965375834966952, 0.14321168804677764, 0.1313866396307133, 0.04949817881472074, 0.04729013211916311, 0.049450035611790066, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "cf500aa2-5073-4dcf-a2d5-308e8559ebd5", "fitness": 0.05787866852146448, "name": "RefinedHybridMetaheuristic", "description": "Enhanced exploration by modifying crossover rate dynamically based on evaluations.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    CR_dynamic = CR * (evals / self.budget)  # Adaptive crossover\n                    cross_points = np.random.rand(self.dim) < CR_dynamic\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 83, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05788 with standard deviation 0.05398.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.1281248252131012, 0.1339537847684561, 0.1277759539920036, 0.04389235223722188, 0.04032570955155257, 0.046168724264178285, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "b6105c13-69c9-43b6-af7f-09acba7e5bf4", "fitness": 0.05606763000971228, "name": "ImprovedHybridMetaheuristic", "description": "A multi-strategy metaheuristic that couples enhanced differential evolution with adaptive local search and dynamic population control, improving convergence through intelligent parameter tuning and strategic restarts.", "code": "import numpy as np\n\nclass ImprovedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution_restart():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n            best_overall = np.inf\n            restart_threshold = 0.2 * self.budget\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt * 1.1, 0.2)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n                best_current = np.min(fitness)\n                if best_current < best_overall:\n                    best_overall = best_current\n                else:\n                    if evals % restart_threshold == 0:\n                        pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                        fitness = np.array([func(ind) for ind in pop])\n                        evals += pop_size\n                        if evals >= self.budget:\n                            break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution_restart()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n                neighborhood_size *= 0.9  # Dynamic neighborhood size reduction\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 84, "feedback": "The algorithm ImprovedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05607 with standard deviation 0.05377.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12832537603662308, 0.12761581683513956, 0.1300596499295663, 0.038818041577390106, 0.037315911111367894, 0.04180720793065684, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "e1c07ebc-ac12-4d15-9f78-5dd7c44dc997", "fitness": -Infinity, "name": "EnhancedHybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic integrates adaptive differential evolution with periodic local search and dynamic parameter tuning to boost performance across diverse optimization landscapes.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 15\n        F = 0.7\n        CR = 0.8\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.05, 0.3)  # More gradual F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Dynamic population resizing and periodic local optimization\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.85))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n                if evals < self.budget and evals % (0.2 * self.budget) == 0:\n                    local_search(pop, fitness)\n\n            return pop, fitness\n\n        def local_search(pop, fitness):\n            for i in range(len(pop)):\n                candidate = pop[i] + 0.01 * (bounds[:, 1] - bounds[:, 0]) * (2 * np.random.rand(self.dim) - 1)\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                f_candidate = func(candidate)\n                nonlocal evals\n                evals += 1\n                if f_candidate < fitness[i]:\n                    pop[i] = candidate\n                    fitness[i] = f_candidate\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 85, "feedback": "An exception occurred: SyntaxError(\"no binding for nonlocal 'evals' found\").", "error": "SyntaxError(\"no binding for nonlocal 'evals' found\")", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {}}
{"id": "bbafefee-8965-4284-881d-3a00cbfa4a1e", "fitness": -Infinity, "name": "EnhancedHybridMetaheuristic", "description": "A novel adaptive hybrid algorithm using self-adaptive differential evolution with dynamic archive-based learning and stochastic neighbor perturbation for enhanced exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n        archive = []\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            archive.extend(pop.copy())\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        archive.append(trial.copy())\n                        F_adapt = max(F_adapt - 0.1, 0.4)\n                    if evals >= self.budget:\n                        break\n\n                if len(archive) > 2 * pop_size:\n                    archive = archive[-2 * pop_size:]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    if evals >= self.budget:\n                        break\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n\n                if evals < self.budget:\n                    # Stochastic neighbor perturbation\n                    archive_sample = archive[np.random.randint(len(archive))]\n                    candidate = best_ind + np.random.uniform(-neighborhood_size, neighborhood_size, self.dim) + 0.5 * (archive_sample - best_ind)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 86, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'archive' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'archive' referenced before assignment\")", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {}}
{"id": "40366071-8c69-41a5-bb80-bb129230db8a", "fitness": 0.05929048116080604, "name": "EnhancedHybridMetaheuristic", "description": "An enhanced dynamic hybrid metaheuristic incorporating self-adaptive F-weighted differential evolution, variable neighborhood search, and periodic population rejuvenation for robust convergence across diverse landscapes.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        initial_F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = initial_F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget) ** 0.5)  # exponential decay for smoother adjustment\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt * 0.95, 0.4)  # Smoothly adapt F\n                    if evals >= self.budget:\n                        break\n\n                # Periodic population rejuvenation\n                if evals < self.budget and evals % (0.15 * self.budget) == 0:\n                    rejuvenate_size = max(5, int(pop_adapt_size * 1.2))\n                    new_individuals = np.random.rand(rejuvenate_size - pop_adapt_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                    new_fitness = np.array([func(ind) for ind in new_individuals])\n                    pop = np.vstack((pop, new_individuals))\n                    fitness = np.hstack((fitness, new_fitness))\n                    pop_adapt_size = rejuvenate_size\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    perturbation = np.exp(-evals / self.budget)  # shrinking perturbation\n                    candidate = best_ind + perturbation * neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05929 with standard deviation 0.05286.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12933148993889976, 0.1286818400398425, 0.1268952049812465, 0.05692237617720519, 0.047532893611938576, 0.04358385903145512, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "34c49cdf-2a9a-4d50-9945-6d392210ef55", "fitness": 0.0612396776932779, "name": "RefinedHybridMetaheuristic", "description": "Integrating a Lvy flight mechanism with a refined neighborhood strategy for enhanced exploration and exploitation balance.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def levy_flight(Lambda):\n            sigma1 = (np.math.gamma(1 + Lambda) * np.sin(np.pi * Lambda / 2) /\n                      (np.math.gamma((1 + Lambda) / 2) * Lambda * 2**((Lambda - 1) / 2)))**(1 / Lambda)\n            sigma2 = 1\n            u = np.random.normal(0, sigma1, size=self.dim)\n            v = np.random.normal(0, sigma2, size=self.dim)\n            step = u / np.abs(v)**(1 / Lambda)\n            return step\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            Lambda = 1.5  # Levy flight parameter\n            while evals < self.budget:\n                for i in range(pop_size):\n                    step = levy_flight(Lambda)\n                    candidate = best_ind + step * neighborhood_size\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 88, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06124 with standard deviation 0.05574.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12965375834966952, 0.14321168804677764, 0.1313866396307133, 0.04949817881472074, 0.04729013211916311, 0.049450035611790066, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "e1d5d08b-0934-4659-be51-89881b49cd6a", "fitness": 0.05788641329680755, "name": "EnhancedHybridMetaheuristic", "description": "An enhanced hybrid metaheuristic integrating adaptive differential evolution with dynamic local search and flexible population control for robust exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 15\n        F = 0.7\n        CR = 0.8\n        stagnation_limit = 20  # New parameter to control stagnation\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            stagnation_counter = 0\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    indices = list(range(pop_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.05, 0.3)  # More gradual dynamic F adjustment\n                        stagnation_counter = 0  # Reset stagnation counter\n                    else:\n                        stagnation_counter += 1\n\n                    if evals >= self.budget or stagnation_counter >= stagnation_limit:\n                        break\n\n                if stagnation_counter >= stagnation_limit:\n                    # Trigger reinitialization if stagnation limit reached\n                    for j in range(int(0.2 * pop_size)):\n                        pop[j] = np.random.rand(self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                        fitness[j] = func(pop[j])\n                        evals += 1\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def dynamic_local_search(best_ind, evals):\n            initial_neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            neighborhood_size = initial_neighborhood_size\n\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                        neighborhood_size = initial_neighborhood_size  # Reset neighborhood size on improvement\n                    else:\n                        neighborhood_size *= 0.9  # Gradually reduce neighborhood size\n\n                    if evals >= self.budget:\n                        break\n\n        dynamic_local_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 89, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05789 with standard deviation 0.05492.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.13404990028208974, 0.12773105513277572, 0.13312288098675673, 0.04330275557438834, 0.04601306737971511, 0.03609139364887559, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "cb380e03-c9d8-498d-84e8-66f726d1b8c7", "fitness": 0.06138005404576844, "name": "RefinedHybridMetaheuristic", "description": "A dynamic hybrid metaheuristic combining self-adapting differential evolution with neighborhood search and adaptive population resizing for improved convergence; now includes periodic reinitialization to enhance exploration.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Periodic reinitialization\n                if evals < self.budget and evals % int(0.2 * self.budget) == 0:\n                    pop = np.random.rand(pop_adapt_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                    fitness = np.array([func(ind) for ind in pop])\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 90, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06138 with standard deviation 0.05480.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12911412295372848, 0.14321168804677764, 0.12549425882076182, 0.04290473854744692, 0.05667399767986636, 0.054355013696668064, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "b05943ec-8ea4-48aa-ba3f-fe28dea42b7e", "fitness": 0.058956903476519815, "name": "RefinedHybridMetaheuristic", "description": "Enhanced dynamic hybrid metaheuristic with adaptive crossover probability for better exploration and convergence.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            CR_adapt = CR  # Initialize adaptive crossover probability\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR_adapt\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                        CR_adapt = min(CR_adapt + 0.05, 1.0)  # Increase CR if successful\n                    else:\n                        CR_adapt = max(CR_adapt - 0.05, 0.1)  # Decrease CR if not successful\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 91, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05896 with standard deviation 0.05414.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.1293299592701409, 0.13000731106860852, 0.1331410147595613, 0.042776536778356467, 0.047223648876935864, 0.04746699386840858, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "dd9553db-86d0-4b21-936b-a9a1ba2e455a", "fitness": 0.05846986355682155, "name": "RefinedHybridMetaheuristic", "description": "Enhancing the hybrid metaheuristic by integrating selective pressure with elitism and improved adaptive parameters for F and CR.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            CR_adapt = CR  # Added adaptive CR\n            pop_adapt_size = pop_size\n            elite_keep = max(1, int(0.1 * pop_size))  # Number of elites kept\n\n            while evals < self.budget:\n                # Sort population by fitness to keep elite individuals\n                sorted_indices = np.argsort(fitness)\n                pop = pop[sorted_indices]\n                fitness = fitness[sorted_indices]\n                \n                for i in range(pop_adapt_size): # Apply selective pressure\n                    if i < elite_keep:\n                        continue  # Skip elites\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR_adapt  # Use adaptive CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                        CR_adapt = min(CR_adapt + 0.05, 1.0)  # Increase CR adaptively\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 92, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05847 with standard deviation 0.05489.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.13144611410802176, 0.12631498339810243, 0.13644637879922938, 0.036910219835531555, 0.057593399604190854, 0.03685100959965126, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "8ac904db-5333-433d-b870-e1bb4fe3fe56", "fitness": 0.061823689027411324, "name": "RefinedHybridMetaheuristic", "description": "A refined hybrid metaheuristic with dynamic scaling of differential mutation factor based on fitness improvement rate for enhanced convergence.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget) * (fitness[i] / np.mean(fitness)))  # Changed line for dynamic scaling based on fitness\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 93, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06182 with standard deviation 0.05595.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.13184081301058803, 0.12909545472015993, 0.14500340329763362, 0.05173989546194813, 0.04611954427890064, 0.05194742381080486, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "eb7d6eca-a6cf-4c88-851f-f25f31842118", "fitness": 0.05966940093713245, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with Dynamic Neighborhood and Population Diversity Maintenance for Improved AOCC.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 12\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.05, 0.4)  # More gradual F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Enhanced population diversity maintenance\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_variance = np.var(pop, axis=0)\n                    if np.any(pop_variance < 1e-5):\n                        # Re-initialize part of the population\n                        num_reinit = max(2, int(0.2 * pop_adapt_size))\n                        reinit_indices = np.random.choice(pop_adapt_size, num_reinit, replace=False)\n                        pop[reinit_indices] = np.random.rand(num_reinit, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                        fitness[reinit_indices] = [func(ind) for ind in pop[reinit_indices]]\n                        evals += num_reinit\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 94, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05967 with standard deviation 0.05392.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12580218514888863, 0.132435561031489, 0.13355366566380522, 0.05150357708479214, 0.04304268806626599, 0.050020264772284384, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "1bf0808c-d72e-4487-ae8c-b8af522cd6de", "fitness": 0.05986088016060837, "name": "EnhancedHybridMetaheuristic", "description": "Hybrid strategy with dynamic learning reinforcement for self-adaptive differential evolution and stochastic neighborhood refinement for robust convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            CR_adapt = CR\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR_adapt\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.05, 0.4)  # Smaller dynamic F adjustment\n                        CR_adapt = min(CR_adapt + 0.05, 1.0)  # Dynamic CR adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def stochastic_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n\n                # Decrease neighborhood size as we converge\n                neighborhood_size *= 0.98  # Gradually refine search space\n                if evals >= self.budget:\n                    break\n\n        stochastic_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 95, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05986 with standard deviation 0.05380.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.13078995097148471, 0.13501592895419734, 0.12548711507103427, 0.04952903587449875, 0.04453141918569925, 0.05272780472189431, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "bfc4513b-7418-4e85-93bc-9dc644ea7572", "fitness": 0.060097903813750304, "name": "AdvancedHybridMetaheuristic", "description": "A refined dynamic hybrid metaheuristic incorporating adaptive mutation control, differential evolution, and neighborhood search with adaptive boundary scaling for enhanced convergence.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.05, 0.4)  # More gradual F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.85))  # Adjusted resizing rate\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            shrink_factor = 0.95  # Boundary scaling factor\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                        neighborhood_size *= shrink_factor  # Dynamically shrink neighborhood\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 96, "feedback": "The algorithm AdvancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06010 with standard deviation 0.05252.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.1301837971166231, 0.12700665904202346, 0.12684714884965964, 0.048073207336668755, 0.05254671500713526, 0.05555694030497582, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "b86eeacb-59be-45c8-a6d4-837dee3ae096", "fitness": 0.0612396776932779, "name": "RefinedHybridMetaheuristic", "description": "Modified the adaptive neighborhood search to dynamically adjust the neighborhood size, enhancing exploration capabilities.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - (evals / self.budget))  # Dynamic adjustment\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 97, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06124 with standard deviation 0.05574.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12965375834966952, 0.14321168804677764, 0.1313866396307133, 0.04949817881472074, 0.04729013211916311, 0.049450035611790066, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "7c6680b8-52ca-4d87-b4e8-06360d1b9aa8", "fitness": 0.05710012935322574, "name": "RefinedHybridMetaheuristic", "description": "Enhances convergence by adjusting the crossover probability dynamically based on the evaluation ratio.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * (1 - (evals / self.budget))\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < (CR * (1 - evals / self.budget))  # Modified line\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 98, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05710 with standard deviation 0.05349.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.1293862588220972, 0.12629792817039032, 0.13017124409903935, 0.03960417775882685, 0.03955010250578539, 0.04822478615622583, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
{"id": "b93fb894-fda2-4a81-804a-24ef5082bcda", "fitness": 0.057649158376186754, "name": "RefinedHybridMetaheuristic", "description": "Enhanced Adaptive Mutation: Improved mutation strategy by dynamically adjusting the mutation factor in differential evolution for better exploration.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10\n        F = 0.8\n        CR = 0.9\n\n        def differential_evolution():\n            pop = np.random.rand(pop_size, self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            fitness = np.array([func(ind) for ind in pop])\n            evals = pop_size\n            F_adapt = F\n            pop_adapt_size = pop_size\n\n            while evals < self.budget:\n                for i in range(pop_adapt_size):\n                    indices = list(range(pop_adapt_size))\n                    indices.remove(i)\n                    a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                    F_dynamic = F_adapt * np.exp(-evals/self.budget)  # Changed line for enhanced mutation\n                    mutant = np.clip(a + F_dynamic * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    evals += 1\n                    if f_trial < fitness[i]:\n                        pop[i] = trial\n                        fitness[i] = f_trial\n                        F_adapt = max(F_adapt - 0.1, 0.4)  # Dynamic F adjustment\n                    if evals >= self.budget:\n                        break\n\n                # Adaptive population resizing\n                if evals < self.budget and evals % (0.1 * self.budget) == 0:\n                    pop_adapt_size = max(5, int(pop_adapt_size * 0.9))\n                    pop = pop[:pop_adapt_size]\n                    fitness = fitness[:pop_adapt_size]\n\n            return pop, fitness\n\n        pop, fitness = differential_evolution()\n        best_idx = np.argmin(fitness)\n        best_ind = pop[best_idx]\n        evals = len(fitness)\n\n        def adaptive_neighborhood_search(best_ind, evals):\n            neighborhood_size = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            while evals < self.budget:\n                for i in range(pop_size):\n                    candidate = best_ind + neighborhood_size * (2 * np.random.rand(self.dim) - 1)\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    f_candidate = func(candidate)\n                    evals += 1\n                    if f_candidate < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = candidate\n                        fitness[best_idx] = f_candidate\n                    if evals >= self.budget:\n                        break\n\n        adaptive_neighborhood_search(best_ind, evals)\n\n        return best_ind", "configspace": "", "generation": 99, "feedback": "The algorithm RefinedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05765 with standard deviation 0.05312.", "error": "", "parent_ids": ["7c3c06df-b9ec-4eb3-b545-e985479814d1"], "operator": null, "metadata": {"aucs": [0.12734422105121523, 0.12563193962050145, 0.13117302549481424, 0.042126772161546255, 0.03868975727038626, 0.053210043120550665, 0.00022222222222223476, 0.00022222222222223476, 0.00022222222222223476]}}
