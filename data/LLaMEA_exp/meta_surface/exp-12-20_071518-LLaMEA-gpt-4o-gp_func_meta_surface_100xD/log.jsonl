{"id": "c0e9127a-a6a4-496d-9410-ab609eb28b43", "fitness": 0.062476849847734925, "name": "HybridDEPSO", "description": "A hybrid evolutionary algorithm combining differential evolution and particle swarm optimization for robust black-box optimization across diverse problem landscapes.", "code": "import numpy as np\n\nclass HybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F = 0.5        # Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w = 0.5        # Inertia weight for PSO\n        self.c1 = 1.5       # Cognitive weight\n        self.c2 = 1.5       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution step\n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i]))\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.06247338899532795, 0.06247292011508743, 0.06248423685912696, 0.06247339255981088, 0.062472923679745995, 0.06248424042435097, 0.06247338900416899, 0.06247292012405481, 0.06248423686794036]}}
{"id": "b0c3d1dd-2761-4cdd-915f-f27badfdec16", "fitness": 0.06247699949066584, "name": "HybridDEPSO", "description": "Enhanced the exploration capability of HybridDEPSO by adjusting the velocity update in PSO with dynamic weights to improve global search efficiency.", "code": "import numpy as np\n\nclass HybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F = 0.5        # Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w = 0.5        # Inertia weight for PSO\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution step\n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocity[i] = (0.7 * self.w * velocity[i] +  # Changed line\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i]))\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 1, "feedback": "The algorithm HybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["c0e9127a-a6a4-496d-9410-ab609eb28b43"], "operator": null, "metadata": {"aucs": [0.06247435978957383, 0.06247267759275743, 0.06248395751599389, 0.06247436335414791, 0.06247268115743265, 0.06248396108113874, 0.062474359798410095, 0.0624726776016139, 0.06248395752492408]}}
{"id": "30ab163c-b749-4c98-b394-d468e14a2f9d", "fitness": 0.0624765388274401, "name": "HybridDEPSO", "description": "Improved HybridDEPSO by implementing adaptive mutation factor in DE for better exploration-exploitation balance.", "code": "import numpy as np\n\nclass HybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F = 0.5        # Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w = 0.5        # Inertia weight for PSO\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution step\n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                adaptive_F = 0.5 + 0.1 * np.random.rand()  # Changed line\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)  # Changed line\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocity[i] = (0.7 * self.w * velocity[i] +  # Changed line\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i]))\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 2, "feedback": "The algorithm HybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["b0c3d1dd-2761-4cdd-915f-f27badfdec16"], "operator": null, "metadata": {"aucs": [0.06247278889335761, 0.06247255784050887, 0.06248426617460301, 0.062472792457986204, 0.06247256140519353, 0.06248426973996779, 0.06247278890232644, 0.06247255784942818, 0.062484266183589265]}}
{"id": "4477d5e2-10ca-46d8-8835-efa79ebac187", "fitness": 0.06247700482472075, "name": "HybridDEPSO", "description": "Refined HybridDEPSO by dynamically adjusting the inertia weight `w` to enhance convergence speed while maintaining exploration capability.", "code": "import numpy as np\n\nclass HybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F = 0.5        # Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w = 0.5        # Inertia weight for PSO\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution step\n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.9 - ((0.9 - 0.4) * (self.eval_count / self.budget))  # Dynamically adjust inertia weight\n                velocity[i] = (self.w * velocity[i] +  # Adjusted line\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i]))\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 3, "feedback": "The algorithm HybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["b0c3d1dd-2761-4cdd-915f-f27badfdec16"], "operator": null, "metadata": {"aucs": [0.06247432935824726, 0.062472360795367, 0.06248432074670096, 0.06247433292285898, 0.062472364360028676, 0.06248432431212736, 0.06247432936715125, 0.06247236080432106, 0.06248432075568422]}}
{"id": "558c6431-1471-440e-972b-1c33f1060fcb", "fitness": 0.06247638702162393, "name": "AdaptiveHybridDEPSO", "description": "Enhance HybridDEPSO with adaptive parameter control for F and CR based on population diversity to balance exploration and exploitation dynamically.", "code": "import numpy as np\n\nclass AdaptiveHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F_min, self.F_max = 0.4, 0.9  # Adaptive Differential evolution scale factor range\n        self.CR_min, self.CR_max = 0.1, 0.9  # Adaptive Crossover probability range\n        self.w_min, self.w_max = 0.4, 0.9  # Inertia weight range\n        self.c1 = 2.0  # Cognitive weight\n        self.c2 = 2.0  # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Evaluate population diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            adapt_ratio = (diversity - np.min(diversity)) / (np.max(diversity) - np.min(diversity) + 1e-9)\n            \n            # Adaptive parameters\n            F = self.F_min + adapt_ratio * (self.F_max - self.F_min)\n            CR = self.CR_min + adapt_ratio * (self.CR_max - self.CR_min)\n            w = self.w_max - adapt_ratio * (self.w_max - self.w_min)\n            \n            # Differential Evolution step\n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n            \n            # Particle Swarm Optimization step\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocity[i] = (w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i]))\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["4477d5e2-10ca-46d8-8835-efa79ebac187"], "operator": null, "metadata": {"aucs": [0.062475524681192995, 0.062469500987247906, 0.06248413182276147, 0.06247552824580638, 0.06246950455170308, 0.062484135388130135, 0.062475524689955764, 0.0624695009960714, 0.06248413183174628]}}
{"id": "07e39d85-cb39-49a3-989c-3889238629e8", "fitness": 0.06247613538759514, "name": "EnhancedHybridDEPSO", "description": "Enhanced HybridDEPSO by introducing adaptive mutation strategies and chaotic maps to improve exploration and convergence in dynamic environments.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  \n        self.F = 0.5        \n        self.CR = 0.9       \n        self.w = 0.5        \n        self.c1 = 2.0       \n        self.c2 = 2.0       \n        self.eval_count = 0\n\n    def chaotic_map(self, x):\n        return 4 * x * (1 - x)  # Logistic map\n\n    def adaptive_mutation(self, F, budget_ratio):\n        return F * (1 + 0.5 * np.sin(np.pi * budget_ratio))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        current_chaos = np.random.rand()\n\n        while self.eval_count < self.budget:\n            budget_ratio = self.eval_count / self.budget\n            self.F = self.adaptive_mutation(self.F, budget_ratio)\n            current_chaos = self.chaotic_map(current_chaos)\n            \n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.9 - ((0.9 - 0.4) * budget_ratio) * current_chaos\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i]))\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["4477d5e2-10ca-46d8-8835-efa79ebac187"], "operator": null, "metadata": {"aucs": [0.062472463542514456, 0.062471720893109595, 0.0624842181535088, 0.0624724671069119, 0.06247172445765414, 0.06248422171884482, 0.06247246355150793, 0.062471720901869254, 0.06248421816243532]}}
{"id": "400f1c9a-d32b-4a4a-a1f6-71af6497e82f", "fitness": 0.0624752966716893, "name": "HybridDEPSO", "description": "Introduce adaptive parameter control for the crossover probability `CR` and differential evolution scale factor `F` in HybridDEPSO, improving exploration-exploitation balance through the optimization process.", "code": "import numpy as np\n\nclass HybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.w = 0.5\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Adjust CR and F dynamically based on progress\n            progress_ratio = self.eval_count / self.budget\n            self.CR = 0.9 - 0.5 * progress_ratio\n            self.F = 0.5 + 0.5 * progress_ratio\n            \n            # Differential Evolution step\n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.9 - ((0.9 - 0.4) * progress_ratio)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i]))\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 6, "feedback": "The algorithm HybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["4477d5e2-10ca-46d8-8835-efa79ebac187"], "operator": null, "metadata": {"aucs": [0.06247146360196287, 0.06247021505399686, 0.06248420778555697, 0.06247146716628882, 0.06247021861855828, 0.0624842113507077, 0.06247146361083222, 0.062470215062978895, 0.06248420779432107]}}
{"id": "86533899-df93-4d5c-a805-0fc39251a113", "fitness": 0.06247697388247978, "name": "HybridDEPSO", "description": "Enhanced HybridDEPSO by introducing adaptive crossover probability to balance exploration and exploitation effectively.", "code": "import numpy as np\n\nclass HybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F = 0.5        # Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w = 0.5        # Inertia weight for PSO\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution step\n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.9 - ((0.9 - 0.4) * (self.eval_count / self.budget))  # Dynamically adjust inertia weight\n                self.CR = 0.9 - ((0.9 - 0.4) * (self.eval_count / self.budget))  # Adapt crossover probability\n                velocity[i] = (self.w * velocity[i] +  # Adjusted line\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i]))\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 7, "feedback": "The algorithm HybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["4477d5e2-10ca-46d8-8835-efa79ebac187"], "operator": null, "metadata": {"aucs": [0.06247426347555729, 0.06247263472661835, 0.062484019871494545, 0.06247426704004666, 0.062472638291447447, 0.062484023436815805, 0.06247426348437346, 0.0624726347355985, 0.062484019880366004]}}
{"id": "778b08d3-bfd5-47a3-829b-24a69bec4c82", "fitness": 0.062476453782334386, "name": "HybridDEPSO", "description": "Enhanced HybridDEPSO by integrating adaptive crossover probability to further balance exploration and exploitation.", "code": "import numpy as np\n\nclass HybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F = 0.5        # Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w = 0.5        # Inertia weight for PSO\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution step\n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                self.CR = 0.9 - (0.5 * (self.eval_count / self.budget))  # Dynamically adjust CR\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.9 - ((0.9 - 0.4) * (self.eval_count / self.budget))  # Dynamically adjust inertia weight\n                velocity[i] = (self.w * velocity[i] + \n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i]))\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 8, "feedback": "The algorithm HybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["4477d5e2-10ca-46d8-8835-efa79ebac187"], "operator": null, "metadata": {"aucs": [0.062473740102103936, 0.062471263641003416, 0.0624843540301212, 0.06247374366673708, 0.06247126720578444, 0.06248435759533055, 0.06247374011106033, 0.06247126364998323, 0.062484354038885304]}}
{"id": "8d1bf2b2-8bd9-497c-a66d-ed84b3bd3384", "fitness": -Infinity, "name": "HybridDEPSO", "description": "Incorporating a dynamic population size adjustment mechanism in HybridDEPSO to enhance exploration and exploitation balance throughout optimization.", "code": "import numpy as np\n\nclass HybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = 50  # Initial population size\n        self.F = 0.5        # Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w = 0.5        # Inertia weight for PSO\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        current_pop_size = self.initial_pop_size\n        pop = np.random.uniform(lb, ub, (current_pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += current_pop_size\n\n        velocity = np.random.uniform(-1, 1, (current_pop_size, self.dim))\n\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            new_pop_size = self.initial_pop_size + int((self.budget - self.eval_count) / self.budget * self.initial_pop_size)\n            if new_pop_size != current_pop_size:\n                pop = np.resize(pop, (new_pop_size, self.dim))\n                velocity = np.resize(velocity, (new_pop_size, self.dim))\n                personal_best = np.resize(personal_best, (new_pop_size, self.dim))\n                personal_best_fitness = np.resize(personal_best_fitness, new_pop_size)\n                current_pop_size = new_pop_size\n\n            for i in range(current_pop_size):\n                idxs = np.random.choice(np.delete(np.arange(current_pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n\n            for i in range(current_pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.9 - ((0.9 - 0.4) * (self.eval_count / self.budget))\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i]))\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n\n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 9, "feedback": "An exception occurred: IndexError('index 50 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 50 is out of bounds for axis 0 with size 50')", "parent_ids": ["4477d5e2-10ca-46d8-8835-efa79ebac187"], "operator": null, "metadata": {}}
{"id": "5aa5bfb7-206f-45ab-91bd-5bcb7ff20b6e", "fitness": 0.06247697414654579, "name": "EnhancedHybridDEPSO", "description": "Enhanced HybridDEPSO by integrating adaptive mutation scaling based on convergence progress to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F = 0.5        # Initial Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w = 0.5        # Inertia weight for PSO\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution step with adaptive F\n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                F = 0.5 + 0.5 * (1 - self.eval_count / self.budget)  # Adaptive mutation scaling\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.9 - ((0.9 - 0.4) * (self.eval_count / self.budget))  # Dynamically adjust inertia weight\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i]))\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 10, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["4477d5e2-10ca-46d8-8835-efa79ebac187"], "operator": null, "metadata": {"aucs": [0.062475209606982385, 0.06247149119989892, 0.06248421805917048, 0.0624752131715931, 0.06247149476445302, 0.06248422162431899, 0.06247520961579678, 0.062471491208671015, 0.0624842180680274]}}
{"id": "c9eb1a7b-b8de-417d-a635-715dc753933f", "fitness": 0.062475685967403286, "name": "EnhancedHybridDEPSO", "description": "Enhanced HybridDEPSO by implementing adaptive mutation strategies and adaptive crossover rates to boost exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F = 0.5        # Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w = 0.5        # Inertia weight for PSO\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Adaptive Differential Evolution step\n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                adaptive_F = self.F + np.random.normal(0, 0.1)  # Adaptive mutation strategy\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                adaptive_CR = self.CR * (1 - self.eval_count / self.budget)  # Adaptive crossover rate\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.9 - ((0.9 - 0.4) * (self.eval_count / self.budget))\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i]))\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 11, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["4477d5e2-10ca-46d8-8835-efa79ebac187"], "operator": null, "metadata": {"aucs": [0.062473864233586696, 0.062469115325576086, 0.06248407476949458, 0.062473867798085614, 0.062469118890020714, 0.062484078334704485, 0.06247386424237389, 0.062469115334344516, 0.06248407477844298]}}
{"id": "44092814-4fd6-4282-b698-be3b09000df0", "fitness": 0.062476325309073606, "name": "EnhancedHybridDEPSO", "description": "Enhanced HybridDEPSO by introducing an adaptive mutation strategy in DE and utilizing a convergence-enhancing local search operator in PSO.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F = 0.5        # Initial DE scale factor\n        self.CR = 0.9       # Crossover probability\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Adaptive Differential Evolution step\n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                F_adaptive = 0.5 + 0.3 * (1 - (self.eval_count / self.budget))  # Adaptive scale factor\n                mutant = np.clip(a + F_adaptive * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization with local search step\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                w = 0.9 - ((0.9 - 0.4) * (self.eval_count / self.budget))  # Dynamically adjust inertia weight\n                velocity[i] = (w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i]))\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                # Local search operator\n                local_step = np.random.uniform(-0.1, 0.1, self.dim)\n                pop[i] = np.clip(pop[i] + local_step, lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["4477d5e2-10ca-46d8-8835-efa79ebac187"], "operator": null, "metadata": {"aucs": [0.06247484987529828, 0.06247010588120494, 0.06248401659702418, 0.062474853439869804, 0.06247010944585718, 0.06248402016217369, 0.062474849884254224, 0.06247010589019186, 0.062484016605788284]}}
{"id": "005b28ef-3e26-4757-a4e7-fc72c4e946e3", "fitness": 0.062474448462371664, "name": "EnhancedHybridDEPSO", "description": "Enhanced HybridDEPSO by incorporating adaptive differential evolution parameters and a local search mechanism to improve exploitation abilities.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F_min = 0.4    # Min DE scale factor\n        self.F_max = 0.9    # Max DE scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w = 0.5        # Inertia weight for PSO\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                adaptive_F = self.F_min + (self.F_max - self.F_min) * (self.eval_count / self.budget)\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.9 - ((0.9 - 0.4) * (self.eval_count / self.budget))\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i]))\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                local_search = pop[i] + 0.01 * np.random.randn(self.dim)\n                local_search = np.clip(local_search, lb, ub)\n                current_fitness = func(local_search)\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop[i] = local_search\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 13, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06247 with standard deviation 0.00001.", "error": "", "parent_ids": ["4477d5e2-10ca-46d8-8835-efa79ebac187"], "operator": null, "metadata": {"aucs": [0.06246767185644908, 0.062471392719745666, 0.062484277237402575, 0.06246767542068299, 0.06247139628431164, 0.062484280802632464, 0.06246767186540336, 0.06247139272855362, 0.062484277246163566]}}
{"id": "e5de2b3e-4650-44f2-9fa6-83cba213e083", "fitness": 0.06247703646506023, "name": "EnhancedHybridDEPSO", "description": "Enhanced HybridDEPSO by incorporating adaptive mutation scaling in DE and learning-based inertia weight adjustments in PSO for balanced exploration-exploitation trade-off.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F_min = 0.3    # Minimum Differential evolution scale factor\n        self.F_max = 0.8    # Maximum Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w_max = 0.9    # Maximum inertia weight\n        self.w_min = 0.4    # Minimum inertia weight\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution step with adaptive scaling\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step with learning-based inertia\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - ((self.w_max - self.w_min) * (self.eval_count / self.budget))  # Learning-based adjustment\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i]))\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 14, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["4477d5e2-10ca-46d8-8835-efa79ebac187"], "operator": null, "metadata": {"aucs": [0.0624739410331816, 0.06247289931688549, 0.062484265471335676, 0.062473944597682074, 0.06247290288161278, 0.06248426903675697, 0.06247394104200654, 0.06247289932574962, 0.06248426548033137]}}
{"id": "8466c7c4-e41b-4cbc-b9ec-2fdc8761b049", "fitness": 0.06247723663847698, "name": "EnhancedHybridDEPSO", "description": "Introduce diversity by adding a random perturbation in the PSO velocity update to escape local optima.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F_min = 0.3    # Minimum Differential evolution scale factor\n        self.F_max = 0.8    # Maximum Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w_max = 0.9    # Maximum inertia weight\n        self.w_min = 0.4    # Minimum inertia weight\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution step with adaptive scaling\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step with learning-based inertia\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - ((self.w_max - self.w_min) * (self.eval_count / self.budget))  # Learning-based adjustment\n                random_perturbation = np.random.normal(0, 0.1, self.dim)  # New line for diversity\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation  # Modified line with perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 15, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["e5de2b3e-4650-44f2-9fa6-83cba213e083"], "operator": null, "metadata": {"aucs": [0.06247413851533867, 0.06247317964291199, 0.06248438818340318, 0.062474142080027995, 0.06247318320760742, 0.06248439174861453, 0.062474138524277856, 0.06247317965172661, 0.062484388192384555]}}
{"id": "a58fc0d7-9b42-437b-b84d-22042539f375", "fitness": 0.06247718076710981, "name": "EnhancedHybridDEPSO", "description": "Add a dynamic random perturbation factor to enhance exploration as the budget progresses.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F_min = 0.3    # Minimum Differential evolution scale factor\n        self.F_max = 0.8    # Maximum Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w_max = 0.9    # Maximum inertia weight\n        self.w_min = 0.4    # Minimum inertia weight\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution step with adaptive scaling\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step with learning-based inertia\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - ((self.w_max - self.w_min) * (self.eval_count / self.budget))  # Learning-based adjustment\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget), self.dim)  # Dynamic perturbation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation  # Modified line with perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 16, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["8466c7c4-e41b-4cbc-b9ec-2fdc8761b049"], "operator": null, "metadata": {"aucs": [0.06247434288938769, 0.06247311666512667, 0.06248407917303944, 0.06247434645391492, 0.062473120229984525, 0.06248408273823225, 0.06247434289816056, 0.06247311667410582, 0.06248407918203647]}}
{"id": "e31e452e-1f9a-4622-bce4-60c898d70eba", "fitness": 0.062476109650622585, "name": "DynamicEnhancedHybridDEPSO", "description": "Enhance convergence by integrating a dynamic population size and adaptive mutation step in the hybrid DE-PSO algorithm to balance exploration and exploitation.", "code": "import numpy as np\n\nclass DynamicEnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 50  # Initial population size\n        self.F_min = 0.3    # Minimum Differential evolution scale factor\n        self.F_max = 0.9    # Maximum Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w_max = 0.9    # Maximum inertia weight\n        self.w_min = 0.4    # Minimum inertia weight\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop_size = self.init_pop_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Dynamic population adjustment based on convergence\n            if self.eval_count > self.budget / 2:\n                pop_size = self.init_pop_size // 2\n                pop = pop[:pop_size]\n                velocity = velocity[:pop_size]\n                personal_best = personal_best[:pop_size]\n                personal_best_fitness = personal_best_fitness[:pop_size]\n            \n            # Differential Evolution step with adaptive scaling\n            F = self.F_min + (self.F_max - self.F_min) * np.exp(-self.eval_count / self.budget)\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step with learning-based inertia\n            for i in range(pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - ((self.w_max - self.w_min) * (self.eval_count / self.budget))  # Learning-based adjustment\n                random_perturbation = np.random.normal(0, 0.1, self.dim)  # Diversity\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation  # Modified line with perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 17, "feedback": "The algorithm DynamicEnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["8466c7c4-e41b-4cbc-b9ec-2fdc8761b049"], "operator": null, "metadata": {"aucs": [0.062472037004384307, 0.06247195254478255, 0.062484335828933646, 0.0624720405687218, 0.062471956109572124, 0.06248433939436571, 0.06247203701315185, 0.06247195255377602, 0.06248433583791524]}}
{"id": "6657c4b5-c02e-4fb7-94c5-0ec324afe208", "fitness": 0.06247730057092815, "name": "RefinedHybridDEPSO", "description": "Introduce adaptive mutation strategies in DE and integrate an elite preservation mechanism to enhance exploration and exploitation balance in DEPSO.", "code": "import numpy as np\n\nclass RefinedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F_min = 0.3    # Minimum Differential evolution scale factor\n        self.F_max = 0.9    # Maximum Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w_max = 0.9    # Maximum inertia weight\n        self.w_min = 0.4    # Minimum inertia weight\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution step with adaptive amplitude scaling\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_index = np.argmin(pop_fitness)\n            elite = np.copy(pop[elite_index])\n            \n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c) + 0.5 * (elite - a), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step with learning-based inertia\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - ((self.w_max - self.w_min) * (self.eval_count / self.budget))\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 18, "feedback": "The algorithm RefinedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["8466c7c4-e41b-4cbc-b9ec-2fdc8761b049"], "operator": null, "metadata": {"aucs": [0.06247543354910201, 0.0624720596941285, 0.062484404895829404, 0.062475437113867716, 0.06247206325871735, 0.06248440846099945, 0.06247543355808194, 0.06247205970303693, 0.06248440490459006]}}
{"id": "3882820a-ddaf-497a-83b9-fd6d22421cb9", "fitness": 0.062476017935988636, "name": "EnhancedHybridDEPSO", "description": "Incorporate a dynamic balance between exploration and exploitation by adjusting crossover probability and cognitive-social weights based on population diversity in RefinedHybridDEPSO.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR_min = 0.5\n        self.CR_max = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_min = 1.5\n        self.c1_max = 2.5\n        self.c2_min = 1.5\n        self.c2_max = 2.5\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Dynamic adaptation of parameters\n            diversity = np.std(pop, axis=0).mean()\n            CR = self.CR_min + (self.CR_max - self.CR_min) * (1 - (diversity / (ub - lb).mean()))\n            c1 = self.c1_min + (self.c1_max - self.c1_min) * (diversity / (ub - lb).mean())\n            c2 = self.c2_min + (self.c2_max - self.c2_min) * (diversity / (ub - lb).mean())\n            \n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_index = np.argmin(pop_fitness)\n            elite = np.copy(pop[elite_index])\n            \n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c) + 0.5 * (elite - a), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - ((self.w_max - self.w_min) * (self.eval_count / self.budget))\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               c1 * r1 * (personal_best[i] - pop[i]) +\n                               c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 19, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["6657c4b5-c02e-4fb7-94c5-0ec324afe208"], "operator": null, "metadata": {"aucs": [0.06247460672930494, 0.06246906228772742, 0.062484381217201124, 0.06247461029402035, 0.062469065852094996, 0.062484384782577784, 0.0624746067382983, 0.06246906229648974, 0.06248438122618305]}}
{"id": "6ab521ad-8c8c-4a6b-8b9f-93d05b9b6e30", "fitness": 0.06247671026384871, "name": "RefinedHybridDEPSO", "description": "Improve stability by adding a small perturbation to the global best update.", "code": "import numpy as np\n\nclass RefinedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F_min = 0.3    # Minimum Differential evolution scale factor\n        self.F_max = 0.9    # Maximum Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w_max = 0.9    # Maximum inertia weight\n        self.w_min = 0.4    # Minimum inertia weight\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution step with adaptive amplitude scaling\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_index = np.argmin(pop_fitness)\n            elite = np.copy(pop[elite_index])\n            \n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c) + 0.5 * (elite - a), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial + np.random.normal(0, 1e-5, global_best.shape)\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step with learning-based inertia\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - ((self.w_max - self.w_min) * (self.eval_count / self.budget))\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 20, "feedback": "The algorithm RefinedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["6657c4b5-c02e-4fb7-94c5-0ec324afe208"], "operator": null, "metadata": {"aucs": [0.06247543354910201, 0.06247028877295657, 0.062484404895829404, 0.062475437113867716, 0.06247029233744894, 0.06248440846099945, 0.06247543355808194, 0.062470288781762306, 0.06248440490459006]}}
{"id": "faa996de-3a51-48fa-8446-4a243f04383b", "fitness": 0.062477132601537616, "name": "RefinedHybridDEPSO", "description": "Refine the velocity update in PSO by introducing a random perturbation scaled by the current inertia weight.", "code": "import numpy as np\n\nclass RefinedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F_min = 0.3    # Minimum Differential evolution scale factor\n        self.F_max = 0.9    # Maximum Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w_max = 0.9    # Maximum inertia weight\n        self.w_min = 0.4    # Minimum inertia weight\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution step with adaptive amplitude scaling\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_index = np.argmin(pop_fitness)\n            elite = np.copy(pop[elite_index])\n            \n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c) + 0.5 * (elite - a), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step with learning-based inertia\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - ((self.w_max - self.w_min) * (self.eval_count / self.budget))\n                random_perturbation = np.random.normal(0, self.w, self.dim)  # Refined line\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 21, "feedback": "The algorithm RefinedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["6657c4b5-c02e-4fb7-94c5-0ec324afe208"], "operator": null, "metadata": {"aucs": [0.06247543354910201, 0.06247177178509811, 0.06248418889669305, 0.062475437113867716, 0.06247177534966597, 0.06248419246188863, 0.06247543355808194, 0.06247177179385832, 0.06248418890558283]}}
{"id": "850c3250-4920-494e-a638-5d3fb816763f", "fitness": 0.06247733644786825, "name": "RefinedHybridDEPSO", "description": "Introduce non-linear scaling of inertia weight to improve convergence speed and balance exploration and exploitation.", "code": "import numpy as np\n\nclass RefinedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F_min = 0.3    # Minimum Differential evolution scale factor\n        self.F_max = 0.9    # Maximum Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w_max = 0.9    # Maximum inertia weight\n        self.w_min = 0.4    # Minimum inertia weight\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution step with adaptive amplitude scaling\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_index = np.argmin(pop_fitness)\n            elite = np.copy(pop[elite_index])\n            \n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c) + 0.5 * (elite - a), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step with learning-based inertia\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = ((self.w_max - self.w_min) * ((self.budget - self.eval_count) / self.budget) ** 2 + self.w_min) # Non-linear scaling of inertia weight\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 22, "feedback": "The algorithm RefinedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["6657c4b5-c02e-4fb7-94c5-0ec324afe208"], "operator": null, "metadata": {"aucs": [0.06247543354910201, 0.06247238544008926, 0.06248418678056866, 0.062475437113867716, 0.062472389004755935, 0.06248419034590347, 0.06247543355808194, 0.06247238544890388, 0.062484186789541374]}}
{"id": "0d9b82ee-586a-4c3b-a1f9-fd812f7ff4a3", "fitness": 0.06247677516310232, "name": "RefinedHybridDEPSO", "description": "Introduce dynamic crossover probability adjustment to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass RefinedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F_min = 0.3    # Minimum Differential evolution scale factor\n        self.F_max = 0.9    # Maximum Differential evolution scale factor\n        self.CR_max = 0.9   # Maximum Crossover probability\n        self.w_max = 0.9    # Maximum inertia weight\n        self.w_min = 0.4    # Minimum inertia weight\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution step with adaptive amplitude scaling\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            CR = self.CR_max * (1 - (self.eval_count / self.budget))  # Dynamic crossover probability\n            elite_index = np.argmin(pop_fitness)\n            elite = np.copy(pop[elite_index])\n            \n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c) + 0.5 * (elite - a), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR  # Adjusted crossover points\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step with learning-based inertia\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = ((self.w_max - self.w_min) * ((self.budget - self.eval_count) / self.budget) ** 2 + self.w_min) # Non-linear scaling of inertia weight\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 23, "feedback": "The algorithm RefinedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["850c3250-4920-494e-a638-5d3fb816763f"], "operator": null, "metadata": {"aucs": [0.06247543354910201, 0.06247055459157069, 0.06248433377498486, 0.062475437113867716, 0.06247055815605662, 0.06248433734016501, 0.06247543355808194, 0.062470554600332684, 0.062484333783759394]}}
{"id": "f216c860-b775-4758-b1e4-5d64b6d557bf", "fitness": 0.06247661259640295, "name": "EnhancedAdaptiveDEPSO", "description": "Integrate an adaptive learning strategy in PSO for dynamic adjustment of cognitive and social weights to enhance convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F_min = 0.3    # Minimum Differential evolution scale factor\n        self.F_max = 0.9    # Maximum Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w_max = 0.9    # Maximum inertia weight\n        self.w_min = 0.4    # Minimum inertia weight\n        self.c1_min = 1.5   # Minimum cognitive weight\n        self.c1_max = 2.5   # Maximum cognitive weight\n        self.c2_min = 1.5   # Minimum social weight\n        self.c2_max = 2.5   # Maximum social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution step with adaptive amplitude scaling\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_index = np.argmin(pop_fitness)\n            elite = np.copy(pop[elite_index])\n            \n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c) + 0.5 * (elite - a), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Adaptive PSO step with dynamic cognitive and social weights\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = ((self.w_max - self.w_min) * ((self.budget - self.eval_count) / self.budget) ** 2 + self.w_min)\n                c1 = self.c1_min + (self.c1_max - self.c1_min) * (global_best_fitness - personal_best_fitness[i]) / (global_best_fitness + 1e-10)\n                c2 = self.c2_min + (self.c2_max - self.c2_min) * (global_best_fitness - pop_fitness[i]) / (global_best_fitness + 1e-10)\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               c1 * r1 * (personal_best[i] - pop[i]) +\n                               c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 24, "feedback": "The algorithm EnhancedAdaptiveDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["850c3250-4920-494e-a638-5d3fb816763f"], "operator": null, "metadata": {"aucs": [0.06247543354910201, 0.06246996293145879, 0.06248441699141827, 0.06247543711780901, 0.06247002846992589, 0.06248442053533776, 0.06247543355808194, 0.062469963214150215, 0.062484417000342685]}}
{"id": "8d62c981-6c64-4f0b-aa86-3a4f87000dfb", "fitness": 0.062476252129099574, "name": "EnhancedStochasticHybridDEPSO", "description": "Introduce stochastic parameter adaptation to further enhance exploration and exploitation balance in hybrid DE-PSO.", "code": "import numpy as np\n\nclass EnhancedStochasticHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.2\n        self.F_max = 0.8\n        self.CR_base = 0.8\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + np.random.rand() * (self.F_max - self.F_min)\n            CR = self.CR_base + np.random.normal(0, 0.1) * (1 - (self.eval_count / self.budget))\n            \n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_min + (self.w_max - self.w_min) * np.exp(-5 * self.eval_count / self.budget)\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 25, "feedback": "The algorithm EnhancedStochasticHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["850c3250-4920-494e-a638-5d3fb816763f"], "operator": null, "metadata": {"aucs": [0.06247205121922217, 0.06247286250236028, 0.06248383909206756, 0.062472054783615394, 0.06247286606703728, 0.06248384265720053, 0.06247205122820543, 0.06247286251135398, 0.06248383910083355]}}
{"id": "a65a0612-ab16-4953-8efc-f3f4c0ffb9d6", "fitness": 0.06247713877716816, "name": "EnhancedHybridDEPSO", "description": "Introducing adaptive learning rates for Differential Evolution and PSO to dynamically adjust exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F_min = 0.3    # Minimum DE scale factor\n        self.F_max = 0.9    # Maximum DE scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w_max = 0.9    # Maximum inertia weight\n        self.w_min = 0.4    # Minimum inertia weight\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Adaptive Differential Evolution step\n            F = self.F_min + (self.F_max - self.F_min) * np.exp(-(self.eval_count / self.budget))\n            elite_index = np.argmin(pop_fitness)\n            elite = np.copy(pop[elite_index])\n            \n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c) + 0.5 * (elite - a), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Adaptive PSO step with learning-based inertia\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = (self.w_min + (self.w_max - self.w_min) * np.cos(np.pi * self.eval_count / self.budget)) # Adaptive inertia weight\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 26, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["850c3250-4920-494e-a638-5d3fb816763f"], "operator": null, "metadata": {"aucs": [0.0624754335239498, 0.062471659005790325, 0.06248432022806161, 0.06247543708855319, 0.06247166257038028, 0.06248432379326918, 0.0624754335329063, 0.0624716590145854, 0.06248432023701733]}}
{"id": "fed5ee3b-5abf-43a0-9886-b18d7d185379", "fitness": 0.062476783135741405, "name": "EnhancedHybridDEPSO", "description": "Introduce a convergence-driven adaptive mutation strategy to enhance both local exploitation and global exploration.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F_min = 0.3    # Minimum Differential evolution scale factor\n        self.F_max = 0.9    # Maximum Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w_max = 0.9    # Maximum inertia weight\n        self.w_min = 0.4    # Minimum inertia weight\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eps = 1e-8     # Small constant to avoid division by zero\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution step with convergence-driven adaptive mutation\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (global_best_fitness / (np.min(pop_fitness) + self.eps)))\n            elite_index = np.argmin(pop_fitness)\n            elite = np.copy(pop[elite_index])\n            \n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c) + 0.5 * (elite - a), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step with learning-based inertia\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = ((self.w_max - self.w_min) * ((self.budget - self.eval_count) / self.budget) ** 2 + self.w_min) # Non-linear scaling of inertia weight\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 27, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["850c3250-4920-494e-a638-5d3fb816763f"], "operator": null, "metadata": {"aucs": [0.06247385890611801, 0.062472421854568694, 0.06248406507274129, 0.0624738624707869, 0.06247242541925013, 0.06248406863807621, 0.06247385891510848, 0.062472421863315586, 0.06248406508170734]}}
{"id": "016f86e7-7c56-453f-b9b9-18e0aa890d32", "fitness": 0.06247677471292317, "name": "RefinedHybridDEPSO", "description": "Introduce random initial inertia weights to further balance exploration and exploitation in the early stages.", "code": "import numpy as np\n\nclass RefinedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50  # Population size\n        self.F_min = 0.3    # Minimum Differential evolution scale factor\n        self.F_max = 0.9    # Maximum Differential evolution scale factor\n        self.CR = 0.9       # Crossover probability\n        self.w_max = 0.9    # Maximum inertia weight\n        self.w_min = 0.4    # Minimum inertia weight\n        self.c1 = 2.0       # Cognitive weight\n        self.c2 = 2.0       # Social weight\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        # Initialize velocity for PSO\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        \n        # Track personal bests for PSO\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        # Track global best\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution step with adaptive amplitude scaling\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_index = np.argmin(pop_fitness)\n            elite = np.copy(pop[elite_index])\n            \n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c) + 0.5 * (elite - a), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            # Particle Swarm Optimization step with learning-based inertia\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = (np.random.uniform(self.w_min, self.w_max) * ((self.budget - self.eval_count) / self.budget) ** 2 + self.w_min) # Random initial inertia weight\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 28, "feedback": "The algorithm RefinedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["850c3250-4920-494e-a638-5d3fb816763f"], "operator": null, "metadata": {"aucs": [0.06247543354910201, 0.06247058807023553, 0.062484298945778116, 0.062475437113867716, 0.06247059163472213, 0.06248430251098358, 0.06247543355808194, 0.06247058807899819, 0.06248429895453933]}}
{"id": "7f0097de-ff26-477a-ac3a-2febf0bfcd5f", "fitness": 0.06247717508668165, "name": "EnhancedHybridDEPSO", "description": "Introduce a population diversity measure and adaptive mutation strategy to enhance exploration and prevent premature convergence.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        def population_diversity():\n            mean_pop = np.mean(pop, axis=0)\n            return np.mean(np.sqrt(np.sum((pop - mean_pop) ** 2, axis=1)))\n        \n        while self.eval_count < self.budget:\n            diversity = population_diversity()\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget)) * (1 + diversity)\n            elite_index = np.argmin(pop_fitness)\n            elite = np.copy(pop[elite_index])\n            \n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c) + 0.5 * (elite - a), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = ((self.w_max - self.w_min) * ((self.budget - self.eval_count) / self.budget) ** 2 + self.w_min)\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 29, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["850c3250-4920-494e-a638-5d3fb816763f"], "operator": null, "metadata": {"aucs": [0.06247456004915919, 0.06247268982152243, 0.0624842718155908, 0.06247456361372461, 0.06247269338635553, 0.06248427538077439, 0.06247456005797358, 0.06247268983048504, 0.0624842718245493]}}
{"id": "91c13d20-2fde-4290-b481-b12bd2043fb6", "fitness": 0.0624772535792365, "name": "AdaptiveHybridDEPSO", "description": "Introduce adaptive learning rates based on feedback from convergence trends to dynamically balance exploration and exploitation in the optimization process.", "code": "import numpy as np\n\nclass AdaptiveHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.eval_count = 0\n        self.previous_best_fitness = None\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_index = np.argmin(pop_fitness)\n            elite = np.copy(pop[elite_index])\n            \n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c) + 0.5 * (elite - a), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = ((self.w_max - self.w_min) * ((self.budget - self.eval_count) / self.budget) ** 2 + self.w_min)\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n            \n            # Adaptive learning rate adjustment\n            if self.previous_best_fitness is not None:\n                fitness_improvement = self.previous_best_fitness - global_best_fitness\n                if fitness_improvement < 1e-6:  # Threshold to detect stagnation\n                    self.c1 *= 0.95  # Decrease cognitive component\n                    self.c2 *= 1.05  # Increase social component\n                else:\n                    self.c1 *= 1.05  # Increase cognitive component\n                    self.c2 *= 0.95  # Decrease social component\n            \n            self.previous_best_fitness = global_best_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 30, "feedback": "The algorithm AdaptiveHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["850c3250-4920-494e-a638-5d3fb816763f"], "operator": null, "metadata": {"aucs": [0.06247543354910201, 0.06247238544008926, 0.06248393817466924, 0.062475437113867716, 0.062472389004755935, 0.06248394174002958, 0.06247543355808194, 0.06247238544890388, 0.06248393818362896]}}
{"id": "8aeddd81-450b-4eec-a569-5b4f4b428e93", "fitness": 0.062477428960369515, "name": "EnhancedHybridDEPSO", "description": "Introduce adaptive random search and elitism in RefinedHybridDEPSO to enhance exploration and maintain diversity while improving convergence speed.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = ((self.w_max - self.w_min) * ((self.budget - self.eval_count) / self.budget) ** 2 + self.w_min)\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 31, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["850c3250-4920-494e-a638-5d3fb816763f"], "operator": null, "metadata": {"aucs": [0.062475365669532046, 0.06247272673469606, 0.062484190903251524, 0.06247536923407604, 0.062472730299360735, 0.06248419446843356, 0.062475365678295924, 0.06247272674365256, 0.06248419091202717]}}
{"id": "4efcb957-8afb-410b-bf99-898a73215312", "fitness": 0.062477036455287176, "name": "EnhancedAdaptiveDEPSO", "description": "Utilize adaptive inertia weight and mutation factor tuning alongside a dynamic elite mechanism to improve convergence and diversity in HybridDEPSO.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.2\n        self.F_max = 0.8\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * np.random.rand()\n            self.elite_fraction = 0.05 + 0.15 * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - (self.w_max - self.w_min) * (self.eval_count / self.budget)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i]))\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedAdaptiveDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["8aeddd81-450b-4eec-a569-5b4f4b428e93"], "operator": null, "metadata": {"aucs": [0.06247422825036386, 0.06247295070147463, 0.06248392684035953, 0.06247423181487832, 0.06247295426621979, 0.06248393040558098, 0.062474228259126297, 0.062472950710355635, 0.062483926849225546]}}
{"id": "ae570bf6-963d-49af-b734-2102557af24d", "fitness": 0.062477164567671876, "name": "EnhancedHybridDEPSO", "description": "Introduce adaptive inertia weights and dynamic crossover rates to further enhance exploration and exploitation balance in EnhancedHybridDEPSO.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR_min = 0.5  # Changed from static to dynamic\n        self.CR_max = 0.9  # Added for dynamic crossover rate\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            CR = self.CR_min + (self.CR_max - self.CR_min) * (self.eval_count / self.budget)  # Dynamic CR\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = ((self.w_max - self.w_min) * ((self.budget - self.eval_count) / self.budget) ** 2 + self.w_min)\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 33, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["8aeddd81-450b-4eec-a569-5b4f4b428e93"], "operator": null, "metadata": {"aucs": [0.06247530168630244, 0.0624720838878956, 0.0624841045551463, 0.06247530525101219, 0.06247208745255983, 0.062484108120302695, 0.06247530169515225, 0.06247208389675751, 0.062484104563918064]}}
{"id": "2ed1deab-8be5-4ea8-911f-3b554377f753", "fitness": 0.06247629266181823, "name": "EnhancedHybridDEPSO", "description": "Integrate adaptive inertia weight and self-adaptive DE mutation strategies in EnhancedHybridDEPSO to improve convergence and robustness.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        F = np.full(self.pop_size, self.F_min)\n        \n        while self.eval_count < self.budget:\n            self.w = self.w_min + (self.w_max - self.w_min) * ((self.budget - self.eval_count) / self.budget)\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    F[i] = self.F_min + (self.F_max - self.F_min) * np.random.rand()\n                    mutant = np.clip(a + F[i] * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < np.random.rand()\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 34, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["8aeddd81-450b-4eec-a569-5b4f4b428e93"], "operator": null, "metadata": {"aucs": [0.06247285070930697, 0.06247173594165689, 0.06248428776077308, 0.06247285427381999, 0.06247173950624452, 0.06248429132613309, 0.06247285071821351, 0.06247173595045796, 0.06248428776975801]}}
{"id": "d5f7f35c-bea7-4cc2-ba3d-f16991b37b30", "fitness": 0.0624770739155749, "name": "EnhancedHybridDEPSO", "description": "Enhance the exploration-exploitation balance in EnhancedHybridDEPSO by integrating a dynamic population size and a self-adaptive crossover strategy.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = 50\n        self.min_pop_size = 20\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR_max = 0.9\n        self.CR_min = 0.1\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_pop_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            CR = self.CR_min + (self.CR_max - self.CR_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            for i in range(pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            for i in range(pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = ((self.w_max - self.w_min) * ((self.budget - self.eval_count) / self.budget) ** 2 + self.w_min)\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n\n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n            if pop_size > self.min_pop_size and self.eval_count % (self.budget // 10) == 0:\n                pop_size = max(self.min_pop_size, int(pop_size * 0.9))\n                pop = pop[:pop_size]\n                pop_fitness = pop_fitness[:pop_size]\n                velocity = velocity[:pop_size]\n                personal_best = personal_best[:pop_size]\n                personal_best_fitness = personal_best_fitness[:pop_size]\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 35, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["8aeddd81-450b-4eec-a569-5b4f4b428e93"], "operator": null, "metadata": {"aucs": [0.06247450657145337, 0.06247259426994867, 0.062484117331599864, 0.06247451013600547, 0.06247259783458847, 0.06248412089694544, 0.0624745065804192, 0.06247259427871843, 0.06248411734049519]}}
{"id": "43065f59-c128-4f85-b6c6-cd34867ba9c6", "fitness": 0.062477428960369515, "name": "EnhancedHybridDEPSO", "description": "Introduce adaptive inertia weight adjustment to balance exploration and exploitation in EnhancedHybridDEPSO.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = ((self.w_max - self.w_min) * ((1 - self.eval_count / self.budget) ** 2) + self.w_min)  # Changed the inertia weight adjustment\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 36, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["8aeddd81-450b-4eec-a569-5b4f4b428e93"], "operator": null, "metadata": {"aucs": [0.062475365669532046, 0.06247272673469606, 0.062484190903251524, 0.06247536923407604, 0.062472730299360735, 0.06248419446843356, 0.062475365678295924, 0.06247272674365256, 0.06248419091202717]}}
{"id": "1848a3e2-5f83-484b-8a70-4051c9c710af", "fitness": 0.062475975915004485, "name": "RefinedEnhancedHybridDEPSO", "description": "Introduce adaptive differential mutation and crowding distance sorting to EnhancedHybridDEPSO for better balance between exploration and exploitation.", "code": "import numpy as np\n\nclass RefinedEnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            \n            sorted_indices = np.argsort(pop_fitness)\n            sorted_pop = pop[sorted_indices]\n            sorted_pop_fitness = pop_fitness[sorted_indices]\n            \n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = ((self.w_max - self.w_min) * ((self.budget - self.eval_count) / self.budget) + self.w_min)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i]))\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n            \n            # Crowding Distance Calculation and Replacement\n            distances = np.zeros(self.pop_size)\n            for i in range(self.dim):\n                sorted_indices = np.argsort(pop[:, i])\n                distances[sorted_indices[0]] = distances[sorted_indices[-1]] = np.inf\n                for j in range(1, self.pop_size - 1):\n                    distances[sorted_indices[j]] += (pop[sorted_indices[j + 1], i] - pop[sorted_indices[j - 1], i])\n\n            sorted_indices = np.argsort(distances)[::-1]\n            pop = pop[sorted_indices[:self.pop_size]]\n            pop_fitness = pop_fitness[sorted_indices[:self.pop_size]]\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 37, "feedback": "The algorithm RefinedEnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["8aeddd81-450b-4eec-a569-5b4f4b428e93"], "operator": null, "metadata": {"aucs": [0.0624729918012652, 0.062471242994046605, 0.06248368937600024, 0.06247299536570161, 0.06247124655858294, 0.06248369294127887, 0.0624729918102187, 0.0624712430030363, 0.06248368938490989]}}
{"id": "d87af5ac-b70e-445d-9c88-872a80c5ea54", "fitness": 0.062477298368606604, "name": "RefinedEnhancedHybridDEPSO", "description": "Incorporate dynamic local search intensification and adaptive mutation scaling to boost exploitation and accelerate convergence in EnhancedHybridDEPSO.", "code": "import numpy as np\n\nclass RefinedEnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    dynamic_mutation_factor = F * (0.5 + np.linalg.norm(global_best - pop[i]) / (ub - lb))\n                    mutant = np.clip(a + dynamic_mutation_factor * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = ((self.w_max - self.w_min) * ((self.budget - self.eval_count) / self.budget) ** 2 + self.w_min)\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n\n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 38, "feedback": "The algorithm RefinedEnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["8aeddd81-450b-4eec-a569-5b4f4b428e93"], "operator": null, "metadata": {"aucs": [0.06247537512925094, 0.0624723169049074, 0.062484199497750526, 0.062475378693907846, 0.062472320469761033, 0.06248420306311664, 0.062475375138109746, 0.06247231691391997, 0.06248419950673534]}}
{"id": "6bb5515e-8bae-4e0e-a62a-f36fcd7f66ab", "fitness": 0.062477424154597734, "name": "RefinedHybridDEPSO", "description": "Integrate a dynamic learning factor and opposition-based learning in EnhancedHybridDEPSO to enhance adaptability and improve convergence precision.", "code": "import numpy as np\n\nclass RefinedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_max = 2.5\n        self.c1_min = 1.0\n        self.c2_max = 2.5\n        self.c2_min = 1.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n\n    def opposition_based_learning(self, pop, lb, ub):\n        return lb + ub - pop\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = ((self.w_max - self.w_min) * ((self.budget - self.eval_count) / self.budget) ** 2 + self.w_min)\n                c1 = self.c1_min + (self.c1_max - self.c1_min) * (self.eval_count / self.budget)\n                c2 = self.c2_min + (self.c2_max - self.c2_min) * (self.eval_count / self.budget)\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               c1 * r1 * (personal_best[i] - pop[i]) +\n                               c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n\n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n            # Apply opposition-based learning\n            opposite_pop = self.opposition_based_learning(pop, lb, ub)\n            opposite_fitness = np.array([func(ind) for ind in opposite_pop])\n            self.eval_count += self.pop_size\n            for i in range(self.pop_size):\n                if opposite_fitness[i] < pop_fitness[i]:\n                    pop[i] = opposite_pop[i]\n                    pop_fitness[i] = opposite_fitness[i]\n                    if opposite_fitness[i] < personal_best_fitness[i]:\n                        personal_best[i] = opposite_pop[i]\n                        personal_best_fitness[i] = opposite_fitness[i]\n                        if opposite_fitness[i] < global_best_fitness:\n                            global_best = opposite_pop[i]\n                            global_best_fitness = opposite_fitness[i]\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 39, "feedback": "The algorithm RefinedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["8aeddd81-450b-4eec-a569-5b4f4b428e93"], "operator": null, "metadata": {"aucs": [0.0624753567521521, 0.06247263574094786, 0.06248427639696519, 0.062475360316773476, 0.06247263930557634, 0.062484279962332745, 0.062475356760966716, 0.06247263574971518, 0.06248427640595]}}
{"id": "0e0f4f50-7b01-44a7-bc7e-e879ac488651", "fitness": 0.062477478240769595, "name": "EnhancedHybridDEPSO", "description": "Introduce exponential decay to inertia weight for improved balance between exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Changed to exponential decay\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["8aeddd81-450b-4eec-a569-5b4f4b428e93"], "operator": null, "metadata": {"aucs": [0.0624753567521521, 0.062472797632896926, 0.06248427676348345, 0.062475360316773476, 0.06247280119773391, 0.06248428032868869, 0.062475356760966716, 0.062472797641880295, 0.0624842767723508]}}
{"id": "ff4d2a69-a5ae-480e-a38e-5260120e0438", "fitness": 0.06247684825035411, "name": "EnhancedHybridDEPSO_V2", "description": "Adaptive elite selection and diversity preservation for enhanced exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO_V2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - np.exp(-5 * self.eval_count / self.budget))\n            elite_size = max(1, int(self.pop_size * self.elite_fraction * np.cos(np.pi * self.eval_count / self.budget)))\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.95 ** (self.eval_count / self.budget))  # Adjusted exponential decay\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 41, "feedback": "The algorithm EnhancedHybridDEPSO_V2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["0e0f4f50-7b01-44a7-bc7e-e879ac488651"], "operator": null, "metadata": {"aucs": [0.0624745284832221, 0.06247191634319749, 0.06248409635087415, 0.06247453204782405, 0.062471919908028584, 0.062484099916018554, 0.06247452849220303, 0.062471916352180856, 0.06248409635963814]}}
{"id": "1fe40b42-1100-4e34-a0c8-610a96f56560", "fitness": 0.06247731844921853, "name": "EnhancedHybridDEPSO", "description": "Incorporate adaptive mutation scaling and dynamic population size to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = 50\n        self.final_pop_size = 20\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_size = self.initial_pop_size\n        pop = np.random.uniform(lb, ub, (pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * np.sin(0.5 * np.pi * (1 - (self.eval_count / self.budget)))\n            elite_size = int(pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            for i in range(pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Exponential decay\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n\n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n            # Dynamically adjust population size\n            new_pop_size = int(self.initial_pop_size - ((self.initial_pop_size - self.final_pop_size) * (self.eval_count / self.budget)))\n            if new_pop_size < pop_size:\n                sorted_indices = np.argsort(pop_fitness)\n                pop = pop[sorted_indices[:new_pop_size]]\n                pop_fitness = pop_fitness[sorted_indices[:new_pop_size]]\n                personal_best = personal_best[sorted_indices[:new_pop_size]]\n                personal_best_fitness = personal_best_fitness[sorted_indices[:new_pop_size]]\n                velocity = velocity[sorted_indices[:new_pop_size]]\n                pop_size = new_pop_size\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["0e0f4f50-7b01-44a7-bc7e-e879ac488651"], "operator": null, "metadata": {"aucs": [0.062475360543272074, 0.06247248018253859, 0.06248411104817386, 0.062475364107867026, 0.0624724837471925, 0.062484114613373665, 0.06247536055208147, 0.06247248019129925, 0.062484111057168334]}}
{"id": "c021124d-afe2-4a8d-aad7-753ee8a30d2b", "fitness": 0.062477290811111676, "name": "EnhancedAdaptiveDEPSO", "description": "Incorporate adaptive mutation scaling and neighborhood-based selection to enhance exploitation and exploration balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * np.exp(-5 * (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    neighborhood_indices = np.random.choice(self.pop_size, 5, replace=False)\n                    best_neighbor_idx = neighborhood_indices[np.argmin(pop_fitness[neighborhood_indices])]\n                    a, b, c = pop[best_neighbor_idx], pop[np.random.choice(neighborhood_indices)], pop[np.random.choice(neighborhood_indices)]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - ((self.w_max - self.w_min) * (self.eval_count / self.budget))  # Adaptive inertia weight\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n\n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 43, "feedback": "The algorithm EnhancedAdaptiveDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["0e0f4f50-7b01-44a7-bc7e-e879ac488651"], "operator": null, "metadata": {"aucs": [0.062475321977051634, 0.06247222482440218, 0.06248432205805243, 0.06247532554164781, 0.06247222838920785, 0.06248432562342976, 0.06247532198581085, 0.062472224833368784, 0.062484322067033804]}}
{"id": "8603e5fb-1ccd-49ce-b1a9-f5b4b03c481b", "fitness": 0.062476824420716434, "name": "EnhancedHybridDEPSO", "description": "Incorporate adaptive mutation based on population diversity to enhance exploration in EnhancedHybridDEPSO.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            diversity_factor = np.std(pop, axis=0).mean() / (ub - lb)  # Line 1: Calculate diversity factor\n            F *= (1 + diversity_factor)  # Line 2: Adjust F by diversity factor\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):  # Line 3-5: Modify lines related to velocity and exploration\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  \n                random_perturbation = np.random.normal(0, 0.1, self.dim) * diversity_factor  # Line 6\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["0e0f4f50-7b01-44a7-bc7e-e879ac488651"], "operator": null, "metadata": {"aucs": [0.06247389366571687, 0.0624727656862516, 0.06248381033656958, 0.06247389723027719, 0.062472769250892735, 0.06248381390175828, 0.06247389367459655, 0.06247276569502214, 0.06248381034536299]}}
{"id": "c7e3f824-905f-43ca-8bcd-f0daacbbb373", "fitness": 0.06247614055356505, "name": "EnhancedHybridDEPSO", "description": "Introduce a non-uniform mutation mechanism based on function evaluation progress to enhance global search capability.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    T = self.eval_count / self.budget\n                    non_uniform_mutation_factor = np.exp(-5 * T)  # Added line for non-uniform mutation\n                    mutant = np.clip(a + F * (b - c) * non_uniform_mutation_factor + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Changed to exponential decay\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 45, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["0e0f4f50-7b01-44a7-bc7e-e879ac488651"], "operator": null, "metadata": {"aucs": [0.06247192346120012, 0.06247230155142092, 0.06248419307452768, 0.06247192702563675, 0.06247230511606583, 0.06248419663967075, 0.06247192347007591, 0.0624723015601919, 0.06248419308329556]}}
{"id": "2b65d952-ed31-4b60-8680-078988c52631", "fitness": 0.06247727512812608, "name": "EnhancedAdaptiveDEPSO", "description": "Introduce adaptive inertia weight and differential mutation scaling for enhanced exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            progress_ratio = self.eval_count / self.budget\n            F = self.F_min + (self.F_max - self.F_min) * np.exp(-progress_ratio)  # Adaptive mutation scaling\n            self.w = self.w_max - (self.w_max - self.w_min) * progress_ratio  # Adaptive inertia weight\n\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n\n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 46, "feedback": "The algorithm EnhancedAdaptiveDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["0e0f4f50-7b01-44a7-bc7e-e879ac488651"], "operator": null, "metadata": {"aucs": [0.0624753605515731, 0.06247218847915326, 0.06248427278004698, 0.06247536411616983, 0.06247219204376642, 0.062484276345235124, 0.06247536056033598, 0.06247218848800673, 0.06248427278884727]}}
{"id": "a8359fc3-6df8-4957-bf11-8f00741a62b7", "fitness": 0.062477176981753896, "name": "EnhancedHybridDEPSO", "description": "Integrate a piecewise linear decay for inertia weight to enhance convergence dynamics.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - ((self.w_max - self.w_min) * (self.eval_count / self.budget))  # Changed to linear decay\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 47, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["0e0f4f50-7b01-44a7-bc7e-e879ac488651"], "operator": null, "metadata": {"aucs": [0.0624753567521521, 0.06247220332697445, 0.0624839672925146, 0.062475360316773476, 0.062472206891596604, 0.06248397085766699, 0.062475356760966716, 0.06247220333573589, 0.062483967301404264]}}
{"id": "f4b47387-ea51-4eb5-9108-447c05f02990", "fitness": 0.062477267481767434, "name": "EnhancedHybridDEPSO", "description": "Adjusted elite fraction to enhance the algorithm's focus on top solutions.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.15  # Adjusted from 0.1 to 0.15\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Changed to exponential decay\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 48, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["0e0f4f50-7b01-44a7-bc7e-e879ac488651"], "operator": null, "metadata": {"aucs": [0.062475093663028836, 0.06247275525661178, 0.06248394995199469, 0.06247509722760691, 0.06247275882125136, 0.06248395351717595, 0.06247509367206694, 0.06247275526537466, 0.06248394996079576]}}
{"id": "75f1751e-de64-407b-ba87-0d6b82a11b36", "fitness": 0.0624766092237749, "name": "EnhancedHybridDEPSOAdaptive", "description": "Introduce adaptive mutation factor and neighborhood learning mechanism to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSOAdaptive:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            # Adaptive mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget)) * (diversity / (ub - lb).mean())\n            \n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Exponential decay\n                random_perturbation = np.random.normal(0, 0.1, self.dim)\n                # Enhanced velocity update using neighborhood learning\n                neighbors_idx = np.random.choice(np.delete(np.arange(self.pop_size), i), 2, replace=False)\n                neighbor_best = pop[np.argmin(pop_fitness[neighbors_idx])]\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i]) +\n                               0.5 * (neighbor_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 49, "feedback": "The algorithm EnhancedHybridDEPSOAdaptive got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["0e0f4f50-7b01-44a7-bc7e-e879ac488651"], "operator": null, "metadata": {"aucs": [0.062475296138570724, 0.0624701493433949, 0.062484378615753955, 0.062475299703120823, 0.062470152907904586, 0.062484382180976294, 0.06247529614733338, 0.062470149352196636, 0.06248437862472278]}}
{"id": "237a79f8-a2c6-4f8f-8737-bf3e44f6f78e", "fitness": 0.062477489695164236, "name": "EnhancedHybridDEPSO", "description": "Adjust perturbation spread by modifying standard deviation based on convergence progress.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Changed to exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget), self.dim)  # Adjusted standard deviation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 50, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["0e0f4f50-7b01-44a7-bc7e-e879ac488651"], "operator": null, "metadata": {"aucs": [0.06247535683425243, 0.06247273751731641, 0.06248437116020733, 0.06247536039884738, 0.062472741082158945, 0.06248437472536328, 0.06247535684306704, 0.062472737526293676, 0.06248437116897165]}}
{"id": "844218e2-95df-4b09-babe-043136d54859", "fitness": 0.06247714118950191, "name": "EnhancedHybridDEPSO", "description": "Improved weight decay strategy for better convergence control in EnhancedHybridDEPSO.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - (self.w_max - self.w_min) * (self.eval_count / self.budget)  # Changed to linear decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget), self.dim)  # Adjusted standard deviation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 51, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["237a79f8-a2c6-4f8f-8737-bf3e44f6f78e"], "operator": null, "metadata": {"aucs": [0.0624753567521521, 0.06247190095225941, 0.06248416229037923, 0.062475360316773476, 0.06247190451704232, 0.06248416585553851, 0.062475356760966716, 0.062471900961222016, 0.06248416229918341]}}
{"id": "d7242076-1d72-4d33-9eb8-5f5eb54d14ff", "fitness": 0.062476309196505805, "name": "EnhancedHybridDEPSO", "description": "Introduce adaptive crossover probability to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < (self.CR * (1 - self.eval_count / self.budget))  # Adaptive CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Changed to exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget), self.dim)  # Adjusted standard deviation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 52, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["237a79f8-a2c6-4f8f-8737-bf3e44f6f78e"], "operator": null, "metadata": {"aucs": [0.06247253993595847, 0.06247231600940417, 0.06248406807053186, 0.0624725435003608, 0.062472319574074064, 0.06248407163573699, 0.062472539944774086, 0.062472316018204466, 0.06248406807950735]}}
{"id": "7aaaf356-7a66-429f-905f-de4906fd84ab", "fitness": 0.062475601402741394, "name": "EnhancedHybridDEPSO", "description": "Introduce adaptive crossover rate by decreasing CR dynamically as the evaluation progresses to maintain diversity.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            self.CR = 0.9 * (1 - (self.eval_count / self.budget))  # Adaptive crossover rate\n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Changed to exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget), self.dim)  # Adjusted standard deviation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 53, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["237a79f8-a2c6-4f8f-8737-bf3e44f6f78e"], "operator": null, "metadata": {"aucs": [0.062471588814975054, 0.062471012322317, 0.06248419949743589, 0.0624715923792577, 0.062471015886819914, 0.06248420306260127, 0.06247158882384474, 0.062471012331202, 0.062484199506218974]}}
{"id": "5d454166-0ab8-4915-9a29-61bbe6c568e9", "fitness": 0.062476973741785104, "name": "EnhancedHybridDEPSO", "description": "Incorporating adaptive inertia weight strategy and improved selection using crowding distance-based diversity preservation.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - (self.eval_count / self.budget) * (self.w_max - self.w_min)  # Adaptive inertia weight\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 54, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["237a79f8-a2c6-4f8f-8737-bf3e44f6f78e"], "operator": null, "metadata": {"aucs": [0.06247404581704141, 0.06247282207913951, 0.06248404975560373, 0.06247404938149925, 0.062472825643777874, 0.06248405332081308, 0.062474045825888114, 0.06247282208788518, 0.06248404976441779]}}
{"id": "d1f1b386-3cc5-43f6-aa74-29a7a5d8cf54", "fitness": 0.06247714573426314, "name": "EnhancedHybridDEPSO", "description": "Implement a dynamic neighborhood search strategy to enhance exploration and exploitation phases.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - ((self.w_max - self.w_min) * (self.eval_count / self.budget))  # Linear decay\n                dynamic_perturbation = np.random.normal(0, 0.1 * np.linalg.norm(global_best - pop[i]) / np.sqrt(self.dim), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + dynamic_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 55, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["237a79f8-a2c6-4f8f-8737-bf3e44f6f78e"], "operator": null, "metadata": {"aucs": [0.062475358580528106, 0.062471909355445554, 0.062484165692992755, 0.06247536214512728, 0.06247191292021692, 0.06248416925818834, 0.06247535858948605, 0.06247190936443159, 0.0624841657019517]}}
{"id": "d58d71bb-d77c-4f76-a042-aefdcc6a8e31", "fitness": 0.062477516958397844, "name": "EnhancedHybridDEPSO", "description": "Improved convergence by enhancing velocity update with adaptive perturbation based on current best fitness.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Changed to exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)  # Adjusted perturbation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 56, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["237a79f8-a2c6-4f8f-8737-bf3e44f6f78e"], "operator": null, "metadata": {"aucs": [0.0624753567521521, 0.06247281928682724, 0.06248437127216433, 0.062475360316773476, 0.0624728228346173, 0.06248437482551683, 0.062475356760966716, 0.062472819295634974, 0.06248437128092765]}}
{"id": "16bc2b6f-7c45-4588-be67-0a161643bd3c", "fitness": 0.06247663277058096, "name": "EnhancedHybridDEPSO", "description": "Enhanced convergence by dynamically adjusting crossover probability based on population diversity.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    diversity_measure = np.var(pop, axis=0).mean()  # New line\n                    cross_points = np.random.rand(self.dim) < (self.CR * (1 - diversity_measure))  # Adjusted CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Changed to exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)  # Adjusted perturbation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 57, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.06247417282569545, 0.06247130353364916, 0.06248441836626162, 0.062474176393047975, 0.062471307133006304, 0.062484421931393475, 0.062474172834463215, 0.062471303542685486, 0.06248441837502594]}}
{"id": "659c069b-e895-45a3-b419-63a7dcc5a36e", "fitness": 0.06247707202172307, "name": "EnhancedHybridDEPSO", "description": "EnhancedHybridDEPSO with dynamic control of CR based on evaluation progress for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    \n                    self.CR = 0.6 + 0.3 * (self.eval_count / self.budget)  # Dynamic control of CR\n                    \n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 58, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.062474437009811146, 0.06247245441443672, 0.06248432106703816, 0.06247444057407381, 0.06247245798891765, 0.06248432462350684, 0.06247443701860156, 0.06247245442330296, 0.062484321075818805]}}
{"id": "0990ba96-147e-486a-ad55-10827ef26b96", "fitness": 0.06247719754872717, "name": "EnhancedHybridDEPSO", "description": "Refined convergence by increasing the elite fraction for better exploration.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.15  # Increased elite_fraction from 0.1 to 0.15\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Changed to exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)  # Adjusted perturbation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 59, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.06247503581158753, 0.06247269967334579, 0.062483853580923676, 0.06247503938889487, 0.06247270324371246, 0.06248385714767368, 0.06247503582060543, 0.06247269968210867, 0.06248385358969244]}}
{"id": "60d2ce3c-e259-4669-9a73-e9d659acda37", "fitness": -Infinity, "name": "EnhancedHybridDEPSO", "description": "Introduce dynamic adaptive inertia weight and perturbation scaling based on convergence rate and search space diversity to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        previous_best_fitness = np.inf\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            # Dynamic adjustment of inertia weight\n            if previous_best_fitness - global_best_fitness < 1e-6:\n                self.w = max(self.w_min, self.w_max * 0.95**(self.eval_count/self.budget))\n            else:\n                self.w = min(self.w_max, self.w * 1.05)\n                \n            previous_best_fitness = global_best_fitness\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                diversity = np.mean(np.std(pop, axis=0))\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-diversity), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 60, "feedback": "An exception occurred: AttributeError(\"'EnhancedHybridDEPSO' object has no attribute 'w'\").", "error": "AttributeError(\"'EnhancedHybridDEPSO' object has no attribute 'w'\")", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {}}
{"id": "6b7ae5e1-a4cb-43cf-9162-d7b5a92e93c2", "fitness": 0.062477331130110954, "name": "EnhancedHybridDEPSO", "description": "Improved perturbation mechanism by modifying the scaling factor to enhance exploration.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Changed to exponential decay\n                random_perturbation = np.random.normal(0, 0.2 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)  # Adjusted perturbation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.0624753567521521, 0.062472522241806994, 0.062484110832212725, 0.062475360316773476, 0.06247252577754303, 0.062484114397714285, 0.062475356760966716, 0.062472522250576534, 0.062484110841252716]}}
{"id": "0100a9c6-8311-4c49-ade4-dc0025952c9e", "fitness": 0.06247712392484059, "name": "EnhancedHybridDEPSO", "description": "Enhanced search capability by introducing chaotic random perturbation to improve exploration.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Changed to exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)  # Adjusted perturbation\n                chaotic_perturbation = 0.1 * (np.sin(self.eval_count) - 0.5)  # Added chaotic perturbation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation + chaotic_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 62, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.0624753567521521, 0.062471707739289006, 0.0624843037350058, 0.062475360316773476, 0.062471711230101534, 0.06248430729730414, 0.062475356760966716, 0.06247170774804123, 0.062484303743931324]}}
{"id": "906d7949-dfa1-43c1-8104-3d0c2e7d0c79", "fitness": 0.06247683443066349, "name": "EnhancedHybridDEPSO", "description": "Refinement of velocity update using dynamically adapted inertia weight based on iteration count.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - ((self.w_max - self.w_min) * (self.eval_count / self.budget))  # Changed inertia weight adaptation\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 63, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.0624753567521521, 0.062470778234385405, 0.06248436475963148, 0.062475360316773476, 0.062470781712333, 0.062484368327927475, 0.062475356760966716, 0.06247077824313374, 0.06248436476866803]}}
{"id": "48862137-014f-4dd9-b657-bf781512ef0d", "fitness": 0.06247643151202147, "name": "EnhancedHybridDEPSO", "description": "Improved convergence by adjusting the mutation strategy to enhance exploration.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    # Changed mutation strategy\n                    mutant = np.clip(a + F * (b - c) + F * (c - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Changed to exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)  # Adjusted perturbation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 64, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.062472529550548495, 0.06247257207927237, 0.06248418932097166, 0.06247253311898848, 0.06247257564343267, 0.0624841929176051, 0.06247252955955118, 0.06247257208801238, 0.062484189329810924]}}
{"id": "a1c5eb21-8667-46be-bc58-5a14be058f0a", "fitness": 0.06247638374101775, "name": "RefinedHybridDEPSO", "description": "Incorporate adaptive learning rates and diversity preservation to improve global search capabilities and convergence stability.", "code": "import numpy as np\n\nclass RefinedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        # Dynamically adjustable learning rates\n        learning_rates = np.random.uniform(0.5, 1.5, self.pop_size)\n        \n        while self.eval_count < self.budget:\n            # Diversity preservation\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < (self.CR * learning_rates[i])  # Modifying CR by learning rate\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)  # Adjusted perturbation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                               \n                pop[i] = np.clip(pop[i] + velocity[i] * learning_rates[i], lb, ub)  # Adjust velocity by learning rate\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 65, "feedback": "The algorithm RefinedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.06247280364325081, 0.06247210166415551, 0.06248424237302952, 0.06247280709560776, 0.06247210526282898, 0.06248424592348223, 0.06247280365190533, 0.06247210167314465, 0.062484242381754984]}}
{"id": "5c01e1e8-7ebb-4adb-8c8b-cdc46f066722", "fitness": 0.06247709040870301, "name": "AdaptiveEliteDEPSO", "description": "Adaptive Elite Strategy and Diversity Enhancement for Improved Convergence by dynamically balancing exploration and exploitation phases.", "code": "import numpy as np\n\nclass AdaptiveEliteDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)\n                diversity_factor = np.std(pop, axis=0).mean() / np.abs(ub - lb).mean()\n                adjusted_perturbation = np.random.normal(0, 0.1 * diversity_factor * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + adjusted_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n\n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 66, "feedback": "The algorithm AdaptiveEliteDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.06247535927632819, 0.06247159312386352, 0.06248431525175957, 0.062475362841394544, 0.0624715966890641, 0.06248431881739536, 0.06247535928528203, 0.062471593132629843, 0.062484315260609935]}}
{"id": "f8c99f97-f18f-4378-bbf6-0901cb7baf2a", "fitness": 0.06247684822049382, "name": "EnhancedHybridDEPSO", "description": "Introduced adaptive learning rates for DE mutation and PSO inertia, enhancing exploration-exploitation balance and convergence.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    adaptive_F = F * np.exp(-(personal_best_fitness[i] - global_best_fitness) / max(1e-10, global_best_fitness))\n                    mutant = np.clip(a + adaptive_F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (1 - self.eval_count / self.budget)\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.06247550111809319, 0.0624711688369215, 0.06248387109488429, 0.06247550478841413, 0.0624711723247342, 0.06248387474442396, 0.06247550112725653, 0.062471168845852576, 0.062483871103864]}}
{"id": "899f8829-d918-4cbd-a9b3-50273b484a24", "fitness": 0.062476697336057985, "name": "DynamicStrategyDEPSO", "description": "Incorporate dynamic strategy selection using fitness diversity measures to adaptively balance exploration and exploitation for improved convergence.", "code": "import numpy as np\n\nclass DynamicStrategyDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        diversity_threshold = 0.1  # Threshold to switch strategies\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            fitness_diversity = np.std(pop_fitness) / np.mean(pop_fitness)\n            \n            for i in range(self.pop_size):\n                if fitness_diversity > diversity_threshold:\n                    # Exploration strategy\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c), lb, ub)\n                else:\n                    # Exploitation strategy\n                    if i not in elite_indices:\n                        idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                        a, b, c = pop[idxs]\n                        mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)  # Adjusted perturbation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 68, "feedback": "The algorithm DynamicStrategyDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.06247353068563821, 0.06247237767018243, 0.0624841800708994, 0.06247353425050606, 0.06247238125373911, 0.062484183640506896, 0.062473530694410195, 0.062472377678976065, 0.0624841800796635]}}
{"id": "32d04bea-9309-41d4-a748-1452ddccecbe", "fitness": 0.06247712689447894, "name": "EnhancedHybridDEPSO", "description": "Improved local search by refining mutation strategy using adaptive F scaling based on population diversity.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            diversity = np.std(pop, axis=0).mean()  # New line for diversity measure\n            F = self.F_min + (self.F_max - self.F_min) * diversity  # Updated mutation factor based on diversity\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Changed to exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)  # Adjusted perturbation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 69, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.062474646701025316, 0.06247259655956683, 0.062484133869619374, 0.062474650220197714, 0.06247260010871425, 0.06248413743465364, 0.06247464670983194, 0.06247259656830262, 0.062484133878398795]}}
{"id": "524c9a3d-8dd2-41a0-9d79-4e7e5ba009bb", "fitness": 0.06247732430729696, "name": "RefinedHybridDEPSO", "description": "Introduces adaptive learning rates and dynamic population resizing to enhance convergence speed and solution quality.", "code": "import numpy as np\n\nclass RefinedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = 50\n        self.pop_size = self.initial_pop_size\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        self.learning_rate_decay = 0.99\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * self.learning_rate_decay ** (self.eval_count / self.pop_size)\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n            \n            if self.eval_count % (self.budget // 10) == 0:\n                self.pop_size = max(10, self.pop_size - 5) # Dynamically resize population\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 70, "feedback": "The algorithm RefinedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.06247535805848037, 0.062472528539962435, 0.062484082873268565, 0.06247536162363376, 0.06247253207355097, 0.06248408609928746, 0.06247535806744564, 0.06247252854871255, 0.062484082881330893]}}
{"id": "c584a461-c09e-46df-af54-86f6cf43cab5", "fitness": 0.06247712230007333, "name": "ImprovedEnhancedHybridDEPSO", "description": "Further improved convergence by introducing adaptive inertia weight and dynamic elitism in a hybrid DE/PSO framework.", "code": "import numpy as np\n\nclass ImprovedEnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            dynamic_elite_fraction = min(self.elite_fraction + (0.5 * (self.eval_count / self.budget)), 0.5)\n            elite_size = int(self.pop_size * dynamic_elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - ((self.w_max - self.w_min) * (self.eval_count / self.budget))\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 71, "feedback": "The algorithm ImprovedEnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.06247535959235073, 0.06247160419525288, 0.0624843995230161, 0.06247536316314495, 0.06247160779322758, 0.062484403095949825, 0.06247535960131345, 0.06247160420436482, 0.06248439953203966]}}
{"id": "ef459fa7-7eac-44a9-8a99-aef696872ca2", "fitness": 0.06247733290016963, "name": "EnhancedHybridDEPSO", "description": "Enhanced adaptability by dynamic weight strategy and refined selection mechanism to improve convergence.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_min + (self.w_max - self.w_min) * (1 - self.eval_count / self.budget)  # Line 1 changed\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n                else:  # Line 2 changed\n                    if np.random.rand() < 0.1:  # Line 3 changed\n                        global_best = pop[i]  # Line 3 changed\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 72, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.0624753567521521, 0.06247227567263269, 0.06248436269083668, 0.062475360316773476, 0.062472279294758515, 0.062484366232248956, 0.062475356760966716, 0.06247227568156466, 0.0624843626995929]}}
{"id": "9ba0fab6-dbb9-4208-ab41-db4b5fe3da89", "fitness": 0.06247750902353509, "name": "RefinedHybridDEPSO", "description": "Hybrid DEPSO with adaptive exploration-exploitation balance and dynamic parameter adjustment for improved convergence.", "code": "import numpy as np\n\nclass RefinedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.4\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            progress_ratio = self.eval_count / self.budget\n            F = self.F_min + (self.F_max - self.F_min) * (1 - progress_ratio)\n            w = self.w_max - (self.w_max - self.w_min) * progress_ratio\n            \n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                random_perturbation = np.random.normal(0, 0.1 * (1 - progress_ratio) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 73, "feedback": "The algorithm RefinedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.06247535237726298, 0.062472823616645745, 0.062484347533850126, 0.06247535593036935, 0.06247282715394853, 0.06248435104531802, 0.062475352386255456, 0.06247282362553963, 0.062484347542625995]}}
{"id": "323c3365-51d2-4d33-92ac-8f0e200d9e67", "fitness": 0.06247652078164345, "name": "EnhancedHybridDEPSO", "description": "Improved convergence by adjusting F value based on elite individuals' diversity.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            elite_diversity = np.mean(np.std(elites, axis=0))  # Calculate diversity of elites\n            F = self.F_min + (self.F_max - self.F_min) * elite_diversity  # Adjust F based on diversity\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.06247339744786784, 0.062471817660069706, 0.062484343660926034, 0.062473401007850704, 0.06247182123770023, 0.06248434722448559, 0.06247339745685132, 0.0624718176690916, 0.06248434366994804]}}
{"id": "4610abef-5a94-4371-99b2-25f8eb355323", "fitness": 0.062477516958397844, "name": "EnhancedHybridDEPSO", "description": "Enhanced convergence through dynamic crossover rate adjustment.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Changed to exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)  # Adjusted perturbation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 75, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.0624753567521521, 0.06247281928682724, 0.06248437127216433, 0.062475360316773476, 0.0624728228346173, 0.06248437482551683, 0.062475356760966716, 0.062472819295634974, 0.06248437128092765]}}
{"id": "0ba8f4fe-28f0-4123-b511-fc30342f2369", "fitness": 0.062476536954806754, "name": "RefinedHybridDEPSO", "description": "Refined convergence by integrating elite learning with adaptive inertia and differential perturbation strategies.", "code": "import numpy as np\n\nclass RefinedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    elite_perturb = F * (elites[np.random.randint(elite_size)] - a)\n                    mutant = np.clip(a + F * (b - c) + elite_perturb, lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - (self.w_max - self.w_min) * (self.eval_count / self.budget)\n                adaptive_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + adaptive_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 76, "feedback": "The algorithm RefinedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.062474141894936164, 0.06247131155397878, 0.06248415372006555, 0.06247414589134637, 0.06247131502065595, 0.0624841573155851, 0.06247414190506084, 0.06247131156257779, 0.06248415372905425]}}
{"id": "a8330f74-92b2-41eb-828d-f1eadd5707a1", "fitness": 0.06247679359546681, "name": "EnhancedHybridDEPSO", "description": "Improved convergence by introducing adaptive control over elite fraction based on fitness variance.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            # Adjust elite fraction based on fitness variance\n            fitness_variance = np.var(pop_fitness)\n            self.elite_fraction = 0.1 + 0.4 * (1 - fitness_variance / (fitness_variance + 1))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Changed to exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)  # Adjusted perturbation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 77, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.062474771162564946, 0.0624712147678812, 0.062484391297352615, 0.062474774730070015, 0.06247121828491131, 0.062484394862293735, 0.06247477117132949, 0.062471214776626316, 0.06248439130617167]}}
{"id": "360dc494-41ef-4ea5-a5a4-5f6494412ff5", "fitness": 0.06247686880083921, "name": "EnhancedHybridDEPSO_V2", "description": "Enhanced convergence through dynamic learning rates and adaptive mutation based on diversity and fitness landscape analysis.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO_V2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            # Dynamic learning rate for acceleration coefficients\n            c1_dynamic = self.c1 * (1 - self.eval_count / self.budget)\n            c2_dynamic = self.c2 * (self.eval_count / self.budget)\n\n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    diversity_factor = np.std(pop, axis=0)  # Measure population diversity\n                    adaptive_mutation = np.random.uniform(0, 1, self.dim) * diversity_factor\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a) + adaptive_mutation, lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - (self.w_max - self.w_min) * (self.eval_count / self.budget)  # Linear decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               c1_dynamic * r1 * (personal_best[i] - pop[i]) +\n                               c2_dynamic * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n\n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedHybridDEPSO_V2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.06247471315649633, 0.06247158455632129, 0.06248430509871283, 0.06247471674482197, 0.062471588133712785, 0.062484308679057854, 0.06247471316536335, 0.06247158456536861, 0.06248430510769787]}}
{"id": "7235157a-617f-45ec-878d-88c106b8ca27", "fitness": 0.06247689067689223, "name": "EnhancedHybridDEPSO", "description": "Dynamic Leader Selection and Adaptive Mutation in Hybrid DEPSO to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                a, b, c = pop[idxs]\n                dynamic_leader = elites[np.random.randint(elite_size)] if np.random.rand() < 0.5 else global_best\n                mutant = np.clip(a + F * (b - c) + 0.5 * (dynamic_leader - a), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < pop_fitness[i]:\n                    pop[i] = trial\n                    pop_fitness[i] = trial_fitness\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.95 ** self.eval_count)  # Faster decay for inertia weight\n                adaptive_perturbation = np.random.normal(0, 0.1 * np.exp(-0.1 * (pop_fitness[i] - global_best_fitness)), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + adaptive_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n\n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 79, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.06247408667087806, 0.06247238989215842, 0.0624841918939496, 0.06247409023535688, 0.06247239345677935, 0.06248419545925088, 0.06247408667964782, 0.062472389901136904, 0.06248419190287213]}}
{"id": "b98b4e65-c5e1-477d-8f55-0199e5aa5fdd", "fitness": 0.0624771016528802, "name": "EnhancedHybridDEPSO", "description": "Enhance convergence by integrating Lvy flights to escape local optima and dynamic parameter tuning for adaptive search balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def levy_flight(self, L):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1 / beta)\n        u = np.random.normal(0, sigma, size=L)\n        v = np.random.normal(0, 1, size=L)\n        step = u / np.abs(v)**(1 / beta)\n        return 0.01 * step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                if np.random.rand() < 0.1:\n                    pop[i] = np.clip(pop[i] + self.levy_flight(self.dim), lb, ub)\n                else:\n                    pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 80, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.0624753567521521, 0.06247196736469418, 0.06248397726741339, 0.062475360316773476, 0.06247197093489598, 0.062483980829021624, 0.062475356760966716, 0.06247196737363225, 0.062483977276372116]}}
{"id": "1faeba01-1243-484a-9675-f872d3ab0658", "fitness": 0.06247734274702602, "name": "EnhancedHybridDEPSO", "description": "Enhanced convergence through elite-guided mutation and adaptive scaling in DE-PSO hybrid.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    elite_guide = elites[np.random.randint(elite_size)]\n                    mutant = np.clip(a + F * (b - c) + 0.6 * (elite_guide - a), lb, ub)  # Modify elite influence\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - (self.w_max - self.w_min) * (self.eval_count / self.budget)  # Linear decay\n                random_perturbation = np.random.normal(0, 0.1 * np.exp(-global_best_fitness), self.dim)  # Adjusted perturbation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.06247487840525778, 0.062472961284376405, 0.06248418496928676, 0.062474881986967934, 0.06247296484130793, 0.062484188550468, 0.06247487841413568, 0.06247296129332913, 0.062484184978104595]}}
{"id": "7c61272b-0cd0-4d9b-8cd6-37ccf3a9fa15", "fitness": 0.062477269310440474, "name": "RefinedEnhancedHybridDEPSO", "description": "Improved convergence by integrating adaptive inertia weights and local search intensification, promoting diverse exploration and fine-tuned exploitation.", "code": "import numpy as np\n\nclass RefinedEnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                inertia_weight = self.w_min + (self.w_max - self.w_min) * np.exp(-global_best_fitness)\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (inertia_weight * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 82, "feedback": "The algorithm RefinedEnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.0624753567521521, 0.06247220397491482, 0.062484243655254734, 0.062475360316773476, 0.062472207519434164, 0.06248424716646772, 0.062475356760966716, 0.06247220398383713, 0.062484243664163386]}}
{"id": "1a8bfefa-eb8d-4f2b-bd91-85f8eed1c211", "fitness": 0.062476279009215124, "name": "EnhancedHybridDEPSO", "description": "Introduced diversity by adding Gaussian noise to the mutant vector for improved exploration.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub) + np.random.normal(0, 0.1, self.dim)  # Added Gaussian noise\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Changed to exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)  # Adjusted perturbation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.06247256169590498, 0.06247192687352643, 0.062484344888219856, 0.06247256525420042, 0.06247193043410881, 0.062484348452345406, 0.06247256170490756, 0.062471926882518014, 0.06248434489720467]}}
{"id": "f2a94102-187c-4363-a90b-ed805da8787f", "fitness": 0.06247718210283378, "name": "EnhancedHybridDEPSO", "description": "Introduced adaptive weight decay for velocity update to enhance exploitation near optimal solutions.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count) * (global_best_fitness / (global_best_fitness + 1))  # Changed to adaptive weight decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)  # Adjusted perturbation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 84, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.0624753567521521, 0.062471861737339096, 0.06248432423530137, 0.062475360316773476, 0.062471865331239274, 0.06248432780110302, 0.062475356760966716, 0.062471861746369095, 0.06248432424425987]}}
{"id": "8c36a6bf-9ce3-4b69-bb86-63cc92371092", "fitness": 0.06247695795082612, "name": "EnhancedHybridDEPSO_v2", "description": "EnhancedHybridDEPSO_v2 improves convergence by dynamically adjusting mutation strategies and elite exploration for diverse solution space exploration.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO_v2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    if self.eval_count / self.budget < 0.5:\n                        mutant = np.clip(a + F * (b - c), lb, ub)  # Early exploration\n                    else:\n                        mutant = np.clip(a + F * (b - c) + 0.3 * (elites[np.random.randint(elite_size)] - a), lb, ub)  # Late exploitation\n                    \n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - (self.w_max - self.w_min) * (self.eval_count / self.budget)  # Linear inertia weight decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget), self.dim)  # Adjusted perturbation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedHybridDEPSO_v2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.062474048895430734, 0.06247282082569272, 0.06248400055768577, 0.06247405245989146, 0.06247282439033108, 0.06248400412286703, 0.062474048904409885, 0.0624728208344556, 0.062484000566670805]}}
{"id": "c10a228d-5775-44eb-af2a-b081f1b53a3f", "fitness": -Infinity, "name": "EnhancedHybridDEPSO", "description": "Enhanced convergence through dynamic adaptation of differential evolution and random perturbations calibrated with fitness landscape insights.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * np.sin(np.pi * (1 - (self.eval_count / self.budget)))  # Sinusoidal adaptation of F\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - (self.w_max - self.w_min) * (self.eval_count / self.budget)  # Linear decay of inertia\n                fitness_dependent_scale = np.tanh(1 - global_best_fitness)  # Fitness landscape insight\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * fitness_dependent_scale, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 86, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {}}
{"id": "cc5e4256-7c28-4780-b23f-19c651d835b1", "fitness": 0.06247714515392396, "name": "RefinedHybridDEPSO", "description": "Adaptive memory integration with diversity-driven selection to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass RefinedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        self.adaptive_memory = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            diversity = np.std(pop_fitness)\n            self.adaptive_memory.append(diversity)\n            if len(self.adaptive_memory) > 5:\n                self.adaptive_memory.pop(0)\n            \n            average_diversity = np.mean(self.adaptive_memory)\n            diversity_factor = np.exp(-average_diversity)\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max - (self.w_max - self.w_min) * (self.eval_count / self.budget)\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness) * diversity_factor, self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n\n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 87, "feedback": "The algorithm RefinedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.0624753567521521, 0.06247229232583518, 0.06248378279327682, 0.062475360316773476, 0.06247229591720316, 0.0624837863819957, 0.062475356760966716, 0.06247229233483975, 0.062483782802272736]}}
{"id": "a58c1c8d-ee5f-402c-aecf-3c44e22ab647", "fitness": 0.06247560922259668, "name": "EnhancedHybridDEPSO", "description": "Improved mutation strategy by integrating elite influence for enhanced exploration.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    # Changed line: Integrating elite influence more effectively in the mutation step\n                    mutant = np.clip(a + F * (b - c) + 0.3 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Changed to exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)  # Adjusted perturbation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 88, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.06246971933336232, 0.06247276395811907, 0.062484340792374304, 0.06246972289999997, 0.06247276752765296, 0.0624843443810128, 0.06246971934235601, 0.06247276396711676, 0.06248434080137588]}}
{"id": "279bdea2-a62b-46a5-9a33-8a437cd3d712", "fitness": 0.062477516958397844, "name": "EnhancedHybridDEPSO", "description": "Improved convergence through dynamic elitism adjustment based on current best fitness.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n        \n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n        \n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n        \n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * (self.elite_fraction + 0.05 * np.exp(-global_best_fitness)))  # Adjusted\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n            \n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    \n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n            \n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Changed to exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)  # Adjusted perturbation\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n        \n        return global_best, global_best_fitness", "configspace": "", "generation": 89, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.0624753567521521, 0.06247281928682724, 0.06248437127216433, 0.062475360316773476, 0.0624728228346173, 0.06248437482551683, 0.062475356760966716, 0.062472819295634974, 0.06248437128092765]}}
{"id": "f2fdf326-33bc-40d4-995e-fa74afc4e1d5", "fitness": 0.062477519274156935, "name": "EnhancedHybridDEPSO", "description": "Improved multi-strategy algorithm by dynamically balancing exploration and exploitation stages based on the convergence rate.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        self.convergence_rate = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            # Dynamically adjust exploration and exploitation balance\n            if len(self.convergence_rate) > 1:\n                rate_change = self.convergence_rate[-1] - self.convergence_rate[-2]\n                if rate_change > 0:\n                    self.w_max *= 0.99  # Increase focus on exploitation\n                else:\n                    self.w_max *= 1.01  # Increase focus on exploration\n\n            self.convergence_rate.append(global_best_fitness)\n\n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 90, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["d58d71bb-d77c-4f76-a042-aefdcc6a8e31"], "operator": null, "metadata": {"aucs": [0.0624753567521521, 0.06247282225221651, 0.06248437525118866, 0.062475360316773476, 0.06247282581381253, 0.062484378799181894, 0.062475356760966716, 0.06247282226116757, 0.062484375259952984]}}
{"id": "a8bfd37c-df50-4e06-909a-f156d96fee3b", "fitness": 0.062477519274156935, "name": "EnhancedHybridDEPSO", "description": "Enhanced exploration-exploitation balance by incorporating adaptive mutation scaling based on convergence behavior.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        self.convergence_rate = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            if len(self.convergence_rate) > 1:\n                rate_change = self.convergence_rate[-1] - self.convergence_rate[-2]\n                if rate_change > 0:\n                    self.w_max *= 0.99\n                else:\n                    self.w_max *= 1.01\n\n            self.convergence_rate.append(global_best_fitness)\n\n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 91, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["f2fdf326-33bc-40d4-995e-fa74afc4e1d5"], "operator": null, "metadata": {"aucs": [0.0624753567521521, 0.06247282225221651, 0.06248437525118866, 0.062475360316773476, 0.06247282581381253, 0.062484378799181894, 0.062475356760966716, 0.06247282226116757, 0.062484375259952984]}}
{"id": "f32631c7-bd7f-4916-b11a-e077bdfa387c", "fitness": 0.0624764517582823, "name": "EnhancedMemoryAdaptiveDEPSO", "description": "Enhanced memory and adaptive strategies with learning-based mutation for improved convergence in uncertain environments.  ", "code": "import numpy as np\n\nclass EnhancedMemoryAdaptiveDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        self.convergence_rate = []\n        self.memory = np.full(self.pop_size, np.inf)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            # Adaptive exploration and exploitation strategy\n            if len(self.convergence_rate) > 1:\n                rate_change = self.convergence_rate[-1] - self.convergence_rate[-2]\n                if rate_change > 0:\n                    self.w_max = max(self.w_min, self.w_max * 0.98)\n                else:\n                    self.w_max = min(0.95, self.w_max * 1.02)\n\n            self.convergence_rate.append(global_best_fitness)\n\n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    memory_factor = np.random.rand() < 0.5\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    if memory_factor and self.memory[i] != np.inf:\n                        mutant = np.clip(mutant + 0.5 * (self.memory[i] - mutant), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        if trial_fitness < self.memory[i]:\n                            self.memory[i] = trial_fitness\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n\n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedMemoryAdaptiveDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["f2fdf326-33bc-40d4-995e-fa74afc4e1d5"], "operator": null, "metadata": {"aucs": [0.06247339219595782, 0.06247162117485239, 0.062484352092173734, 0.06247348880764936, 0.062471612725393366, 0.06248423333260167, 0.06247339220466752, 0.062471621186556914, 0.062484352104687946]}}
{"id": "003bcd0a-c451-43a1-bd1e-77851ceeba0f", "fitness": 0.06247482873856954, "name": "EnhancedHybridDEPSO", "description": "Enhance the exploration-exploitation balance by adjusting the mutation factor more dynamically based on convergence rate.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        self.convergence_rate = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - np.std(pop_fitness) / np.mean(pop_fitness))  # Change 1\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            # Dynamically adjust exploration and exploitation balance\n            if len(self.convergence_rate) > 1:\n                rate_change = self.convergence_rate[-1] - self.convergence_rate[-2]\n                if rate_change > 0:\n                    self.w_max *= 0.99  # Increase focus on exploitation\n                else:\n                    self.w_max *= 1.01  # Increase focus on exploration\n\n            self.convergence_rate.append(global_best_fitness)\n\n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 93, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06247 with standard deviation 0.00001.", "error": "", "parent_ids": ["f2fdf326-33bc-40d4-995e-fa74afc4e1d5"], "operator": null, "metadata": {"aucs": [0.06247009921141555, 0.06247034107908478, 0.06248404236164118, 0.06247010280360399, 0.062470344585934034, 0.06248404592642898, 0.062470099220404585, 0.06247034108798655, 0.062484042370626214]}}
{"id": "420c2f1c-fbd4-4f9d-b103-c5ae47ca7d6e", "fitness": 0.062477370974102234, "name": "OptimizedEliteDEPSO", "description": "OptimizedEliteDEPSO: A refined hybrid algorithm enhancing elite diversity with adaptive random perturbations and probabilistic parameter tuning for improved convergence.", "code": "import numpy as np\n\nclass OptimizedEliteDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        self.convergence_rate = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            # Dynamically adjust exploration and exploitation balance\n            if len(self.convergence_rate) > 1:\n                rate_change = self.convergence_rate[-1] - self.convergence_rate[-2]\n                self.w_max *= 0.99 if rate_change > 0 else 1.01\n\n            self.convergence_rate.append(global_best_fitness)\n\n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget), self.dim)\n                if np.random.rand() < 0.1:  # Introduce probabilistic perturbation\n                    random_perturbation *= np.random.uniform(0.5, 1.5)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 94, "feedback": "The algorithm OptimizedEliteDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["f2fdf326-33bc-40d4-995e-fa74afc4e1d5"], "operator": null, "metadata": {"aucs": [0.062475358099992384, 0.062472453588342525, 0.062484297660159616, 0.06247536166462431, 0.06247245715300431, 0.062484301225543604, 0.06247535810897942, 0.06247245359711695, 0.062484297669156974]}}
{"id": "2085eb34-7059-49b5-bd0a-94f142a97bc7", "fitness": 0.06247647846486537, "name": "EnhancedHybridDEPSO", "description": "Enhance exploration by introducing Lvy flights for greater diversity in candidate solutions.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        self.convergence_rate = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            if len(self.convergence_rate) > 1:\n                rate_change = self.convergence_rate[-1] - self.convergence_rate[-2]\n                if rate_change > 0:\n                    self.w_max *= 0.99  # Increase focus on exploitation\n                else:\n                    self.w_max *= 1.01  # Increase focus on exploration\n\n            self.convergence_rate.append(global_best_fitness)\n\n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    # Introduce Lvy flight for mutation\n                    levy_flight = levy.rvs(size=self.dim)\n                    trial += levy_flight\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 95, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["f2fdf326-33bc-40d4-995e-fa74afc4e1d5"], "operator": null, "metadata": {"aucs": [0.062474985620378565, 0.06247000716171225, 0.06248443904175027, 0.06247498918608818, 0.06247001071654534, 0.06248444260703723, 0.062474985629189184, 0.062470007170520425, 0.06248443905056689]}}
{"id": "0f322327-526f-42e1-ac30-e4e662e41765", "fitness": 0.062476776227197814, "name": "EnhancedHybridDEPSO", "description": "Enhanced exploration by adding Gaussian perturbation to the mutant vector during differential evolution.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        self.convergence_rate = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            # Dynamically adjust exploration and exploitation balance\n            if len(self.convergence_rate) > 1:\n                rate_change = self.convergence_rate[-1] - self.convergence_rate[-2]\n                if rate_change > 0:\n                    self.w_max *= 0.99  # Increase focus on exploitation\n                else:\n                    self.w_max *= 1.01  # Increase focus on exploration\n\n            self.convergence_rate.append(global_best_fitness)\n\n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutation_noise = np.random.normal(0, 0.1 * (ub - lb), self.dim)  # Added Gaussian perturbation\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a) + mutation_noise, lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 96, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["f2fdf326-33bc-40d4-995e-fa74afc4e1d5"], "operator": null, "metadata": {"aucs": [0.062474691256746984, 0.06247166749737798, 0.06248396638504261, 0.06247469482145529, 0.06247167100495288, 0.06248396991364502, 0.062474691265508864, 0.062471667506070916, 0.062483966393979795]}}
{"id": "c2359ae4-38b0-4970-85d6-700d6bfe4bdc", "fitness": -Infinity, "name": "AdaptiveHybridDEPSO", "description": "Enhanced dynamic balancing of exploration and exploitation using adaptive population size and crossover rate adjustment.", "code": "import numpy as np\n\nclass AdaptiveHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = 50\n        self.pop_size = self.initial_pop_size\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR_min = 0.5\n        self.CR_max = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        self.convergence_rate = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            CR = self.CR_min + (self.CR_max - self.CR_min) * (1 - global_best_fitness / np.max(pop_fitness))\n\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            # Dynamically adjust population size\n            self.pop_size = self.initial_pop_size - int(self.initial_pop_size * (self.eval_count / self.budget))\n\n            self.convergence_rate.append(global_best_fitness)\n            if len(self.convergence_rate) > 1:\n                rate_change = self.convergence_rate[-1] - self.convergence_rate[-2]\n                if rate_change > 0:\n                    self.w_max *= 0.99  # Enhance exploitation\n                else:\n                    self.w_max *= 1.01  # Enhance exploration\n\n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 97, "feedback": "An exception occurred: ValueError('high <= 0').", "error": "ValueError('high <= 0')", "parent_ids": ["f2fdf326-33bc-40d4-995e-fa74afc4e1d5"], "operator": null, "metadata": {}}
{"id": "c5aa0956-2e9f-44c2-a97c-57235f08b69e", "fitness": 0.062476510377394066, "name": "EnhancedHybridDEPSO", "description": "Enhanced self-adaptive strategy by fine-tuning mutation factor and crossover rate based on fitness improvement.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        self.convergence_rate = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            CR = self.CR * (1 - np.exp(-0.1 * self.eval_count / self.budget))  # Adapt CR based on evaluations\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            # Dynamically adjust exploration and exploitation balance\n            if len(self.convergence_rate) > 1:\n                rate_change = self.convergence_rate[-1] - self.convergence_rate[-2]\n                if rate_change > 0:\n                    self.w_max *= 0.99  # Increase focus on exploitation\n                else:\n                    self.w_max *= 1.01  # Increase focus on exploration\n\n            self.convergence_rate.append(global_best_fitness)\n\n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 98, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00001.", "error": "", "parent_ids": ["f2fdf326-33bc-40d4-995e-fa74afc4e1d5"], "operator": null, "metadata": {"aucs": [0.06247398408857485, 0.06247134326349224, 0.062484200202863605, 0.06247398765893475, 0.062471346831104224, 0.06248420376998143, 0.06247398409735072, 0.0624713432725017, 0.06248420021174306]}}
{"id": "b7ed66a9-ef22-4407-90b9-4231b8c35455", "fitness": 0.062477519274156935, "name": "EnhancedHybridDEPSO", "description": "Enhanced convergence through adaptive elite fraction adjustment based on convergence rate.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F_min = 0.3\n        self.F_max = 0.9\n        self.CR = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.elite_fraction = 0.1\n        self.eval_count = 0\n        self.convergence_rate = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += self.pop_size\n\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best = np.copy(pop)\n        personal_best_fitness = np.copy(pop_fitness)\n\n        global_best_idx = np.argmin(pop_fitness)\n        global_best = np.copy(pop[global_best_idx])\n        global_best_fitness = pop_fitness[global_best_idx]\n\n        while self.eval_count < self.budget:\n            F = self.F_min + (self.F_max - self.F_min) * (1 - (self.eval_count / self.budget))\n            self.elite_fraction = 0.1 + 0.1 * (self.convergence_rate[-1] - global_best_fitness) if len(self.convergence_rate) > 0 else 0.1\n            elite_size = int(self.pop_size * self.elite_fraction)\n            elite_indices = np.argsort(pop_fitness)[:elite_size]\n            elites = pop[elite_indices]\n\n            # Dynamically adjust exploration and exploitation balance\n            if len(self.convergence_rate) > 1:\n                rate_change = self.convergence_rate[-1] - self.convergence_rate[-2]\n                if rate_change > 0:\n                    self.w_max *= 0.99  # Increase focus on exploitation\n                else:\n                    self.w_max *= 1.01  # Increase focus on exploration\n\n            self.convergence_rate.append(global_best_fitness)\n\n            for i in range(self.pop_size):\n                if i not in elite_indices:\n                    idxs = np.random.choice(np.delete(np.arange(self.pop_size), i), 3, replace=False)\n                    a, b, c = pop[idxs]\n                    mutant = np.clip(a + F * (b - c) + 0.5 * (elites[np.random.randint(elite_size)] - a), lb, ub)\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n\n                    trial_fitness = func(trial)\n                    self.eval_count += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n                        if trial_fitness < personal_best_fitness[i]:\n                            personal_best[i] = trial\n                            personal_best_fitness[i] = trial_fitness\n                            if trial_fitness < global_best_fitness:\n                                global_best = trial\n                                global_best_fitness = trial_fitness\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = self.w_max * (0.99 ** self.eval_count)  # Exponential decay\n                random_perturbation = np.random.normal(0, 0.1 * (1 - self.eval_count / self.budget) * np.exp(-global_best_fitness), self.dim)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best[i] - pop[i]) +\n                               self.c2 * r2 * (global_best - pop[i])) + random_perturbation\n                pop[i] = np.clip(pop[i] + velocity[i], lb, ub)\n                \n                current_fitness = func(pop[i])\n                self.eval_count += 1\n                if current_fitness < pop_fitness[i]:\n                    pop_fitness[i] = current_fitness\n                    if current_fitness < personal_best_fitness[i]:\n                        personal_best[i] = pop[i]\n                        personal_best_fitness[i] = current_fitness\n                        if current_fitness < global_best_fitness:\n                            global_best = pop[i]\n                            global_best_fitness = current_fitness\n\n        return global_best, global_best_fitness", "configspace": "", "generation": 99, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06248 with standard deviation 0.00000.", "error": "", "parent_ids": ["f2fdf326-33bc-40d4-995e-fa74afc4e1d5"], "operator": null, "metadata": {"aucs": [0.0624753567521521, 0.06247282225221651, 0.06248437525118866, 0.062475360316773476, 0.06247282581381253, 0.062484378799181894, 0.062475356760966716, 0.06247282226116757, 0.062484375259952984]}}
