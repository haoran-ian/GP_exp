{"id": "9689bb0e-41c5-442f-9afb-89bace3fdc19", "fitness": 0.06412636216845469, "name": "HybridOptimizer", "description": "A hybrid optimization algorithm combining differential evolution and adaptive local search to efficiently explore and exploit the search space.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        \n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n        \n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Adaptive Local Search\n                if eval_count < self.budget and np.random.rand() < 0.1:  # 10% chance for local search\n                    new_trial = trial + np.random.normal(0, 0.1, self.dim)\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 0, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06413 with standard deviation 0.05680.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13840725198197212, 0.13840725198197212, 0.13840725198197212, 0.05347183452339199, 0.05347183452339199, 0.05347183452339199, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "492cc14d-b736-490e-b61f-112b5203ed61", "fitness": 0.060767837740955365, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid algorithm integrating differential evolution with adaptive local search and dynamic parameter tuning to optimize exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F = 0.8  # Initial differential weight\n        CR = 0.9  # Initial crossover probability\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n        \n        # Track best fitness for dynamic parameter adjustment\n        best_fitness = np.min(fitness)\n        \n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Dynamic adjustment of F and CR\n                F = 0.7 + 0.3 * np.random.rand()  # Randomize F for diversity\n                CR = 0.8 + 0.2 * np.random.rand()  # Randomize CR for diversity\n                \n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Update best fitness\n                    if f_trial < best_fitness:\n                        best_fitness = f_trial\n                \n                # Adaptive Local Search\n                if eval_count < self.budget and np.random.rand() < 0.2:  # 20% chance for local search\n                    new_trial = trial + np.random.normal(0, 0.1, self.dim)\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if f_new_trial < best_fitness:\n                            best_fitness = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 1, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06077 with standard deviation 0.05582.", "error": "", "parent_ids": ["9689bb0e-41c5-442f-9afb-89bace3fdc19"], "operator": null, "metadata": {"aucs": [0.13505727997786232, 0.13505727997786232, 0.13505727997786232, 0.04674623324500382, 0.04674623324500382, 0.04674623324500382, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "38b1f928-1041-455f-98d3-fdeb84e493c0", "fitness": 0.06579640116684778, "name": "HybridOptimizer", "description": "A refined hybrid optimizer that combines adaptive differential evolution with stochastic local search, introducing dynamic parameter adaptation and selective amplification to enhance search efficiency.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.5  # Base Differential weight\n        CR_base = 0.7  # Base Crossover probability\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Dynamic parameter adaptation\n                F = F_base + np.random.rand() * 0.3\n                CR = CR_base + np.random.rand() * 0.2\n\n                # Mutation and crossover with selective amplification\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Stochastic Local Search with increased chance and perturbation size\n                if eval_count < self.budget and np.random.rand() < 0.15:  # 15% chance for local search\n                    perturbation = np.random.normal(0, 0.2, self.dim)\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 2, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06580 with standard deviation 0.05785.", "error": "", "parent_ids": ["9689bb0e-41c5-442f-9afb-89bace3fdc19"], "operator": null, "metadata": {"aucs": [0.14113474761231293, 0.14113474761231293, 0.14113474761231293, 0.05575445588823047, 0.05575445588823047, 0.05575445588823047, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "d21bca7d-c755-41dd-a967-5f7eefb8a974", "fitness": 0.06368256249839639, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer leveraging adaptive differential evolution blended with a progressive stochastic local search, incorporating a dynamic inertia control mechanism for improved convergence.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.5  # Base Differential weight\n        CR_base = 0.7  # Base Crossover probability\n        inertia_base = 0.9  # Initial inertia factor\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            inertia = inertia_base * (1 - eval_count / self.budget) + 0.4  # Dynamic inertia\n            for i in range(population_size):\n                # Dynamic parameter adaptation\n                F = F_base + np.random.rand() * 0.3\n                CR = CR_base + np.random.rand() * 0.2\n\n                # Mutation and crossover with selective amplification\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i]) * inertia + population[i] * (1 - inertia)\n\n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Progressive stochastic local search\n                if eval_count < self.budget and np.random.rand() < 0.2:  # 20% chance for local search\n                    perturbation = np.random.normal(0, 0.1 * inertia, self.dim)  # Scaled by inertia\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06368 with standard deviation 0.05638.", "error": "", "parent_ids": ["38b1f928-1041-455f-98d3-fdeb84e493c0"], "operator": null, "metadata": {"aucs": [0.13740112763027135, 0.13740112763027135, 0.13740112763027135, 0.0531465598649179, 0.0531465598649179, 0.0531465598649179, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "c867704d-c122-45cb-8042-7f76b8bf203d", "fitness": 0.06230845078198118, "name": "HybridOptimizer", "description": "Hybrid optimizer enhancement with improved local search and dynamic population size adjustment.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.5  # Base Differential weight\n        CR_base = 0.7  # Base Crossover probability\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Dynamic parameter adaptation\n                F = F_base + np.random.rand() * 0.3\n                CR = CR_base + np.random.rand() * 0.2\n\n                # Mutation and crossover with selective amplification\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Stochastic Local Search with increased chance and perturbation size\n                if eval_count < self.budget and np.random.rand() < 0.2:  # 20% chance for local search\n                    perturbation = np.random.normal(0, 0.1, self.dim)\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 4, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06231 with standard deviation 0.05654.", "error": "", "parent_ids": ["38b1f928-1041-455f-98d3-fdeb84e493c0"], "operator": null, "metadata": {"aucs": [0.1371438911488413, 0.1371438911488413, 0.1371438911488413, 0.049281461197102305, 0.049281461197102305, 0.049281461197102305, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "c28a0f10-1fcc-495a-8fd2-147e111c4123", "fitness": 0.06270740889935689, "name": "HybridOptimizer", "description": "Enhanced hybrid optimizer with adaptive population size and stochastic local search prioritization to improve convergence efficiency and accuracy.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n\n        # Adjusted adaptive population size based on dimension\n        population_size = max(10 * self.dim, 100)  # Ensure a minimum size for diverse exploration\n        F_base = 0.5  # Base Differential weight\n        CR_base = 0.7  # Base Crossover probability\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n\n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Dynamic parameter adaptation\n                F = F_base + np.random.rand() * 0.3\n                CR = CR_base + np.random.rand() * 0.2\n\n                # Mutation and crossover with selective amplification\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                # Stochastic Local Search with enhanced prioritization\n                if eval_count < self.budget and np.random.rand() < 0.25:  # Increased chance for local search\n                    perturbation = np.random.normal(0, 0.2, self.dim)\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 5, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06271 with standard deviation 0.05694.", "error": "", "parent_ids": ["38b1f928-1041-455f-98d3-fdeb84e493c0"], "operator": null, "metadata": {"aucs": [0.1380867195428852, 0.1380867195428852, 0.1380867195428852, 0.04953550715518551, 0.04953550715518551, 0.04953550715518551, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "44535575-28b8-462a-8be0-08d02315aa75", "fitness": 0.0638438062124032, "name": "HybridOptimizer", "description": "Enhance mutation strategy by incorporating best solution guidance and adjust stochastic search probability for improved convergence.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.5  # Base Differential weight\n        CR_base = 0.7  # Base Crossover probability\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            best_idx = np.argmin(fitness)  # Keep track of the best solution\n            for i in range(population_size):\n                # Dynamic parameter adaptation\n                F = F_base + np.random.rand() * 0.3\n                CR = CR_base + np.random.rand() * 0.2\n\n                # Mutation and crossover with selective amplification\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c) + F * (population[best_idx] - a), bounds[:, 0], bounds[:, 1])  # Adjust mutation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Stochastic Local Search with increased chance and perturbation size\n                if eval_count < self.budget and np.random.rand() < 0.10:  # Adjusted local search probability to 10%\n                    perturbation = np.random.normal(0, 0.2, self.dim)\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 6, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06384 with standard deviation 0.05735.", "error": "", "parent_ids": ["38b1f928-1041-455f-98d3-fdeb84e493c0"], "operator": null, "metadata": {"aucs": [0.13938452138097757, 0.13938452138097757, 0.13938452138097757, 0.05164689725623206, 0.05164689725623206, 0.05164689725623206, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "405004df-4ab6-4af9-bf91-5750e56d1a3a", "fitness": 0.06385121055174776, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer incorporating adaptive differential evolution, stochastic local search, and a novel elitism strategy that emphasizes diversity maintenance and convergence balance to improve optimization performance.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.5  # Base Differential weight\n        CR_base = 0.7  # Base Crossover probability\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Elitism parameters\n        elite_fraction = 0.1\n        num_elites = max(1, int(elite_fraction * population_size))\n        \n        while eval_count < self.budget:\n            elites_idx = np.argsort(fitness)[:num_elites]\n            elites = population[elites_idx]\n\n            for i in range(population_size):\n                # Dynamic parameter adaptation\n                F = F_base + np.random.rand() * 0.3\n                CR = CR_base + np.random.rand() * 0.2\n                \n                # Mutation and crossover with selective amplification\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Stochastic Local Search with increased chance and perturbation size\n                if eval_count < self.budget and np.random.rand() < 0.15:  # 15% chance for local search\n                    perturbation = np.random.normal(0, 0.2, self.dim)\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n            \n            # Replace worst individuals with elites to maintain diversity\n            worst_idx = np.argsort(fitness)[-num_elites:]\n            population[worst_idx] = elites\n            fitness[worst_idx] = np.array([func(ind) for ind in elites])\n            eval_count += num_elites\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06385 with standard deviation 0.05790.", "error": "", "parent_ids": ["38b1f928-1041-455f-98d3-fdeb84e493c0"], "operator": null, "metadata": {"aucs": [0.14045822552163478, 0.14045822552163478, 0.14045822552163478, 0.05059540613360858, 0.05059540613360858, 0.05059540613360858, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "638e4d85-6b0c-4050-a793-dd51765ec402", "fitness": 0.06624732289336206, "name": "HybridOptimizer", "description": "Enhanced hybrid optimizer using adaptive local search intensity and adjusted mutation strategy for improved convergence.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.5  # Base Differential weight\n        CR_base = 0.7  # Base Crossover probability\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Dynamic parameter adaptation\n                F = F_base + np.random.randn() * 0.1  # Changed randomness multiplier\n                CR = CR_base + np.random.rand() * 0.2\n\n                # Mutation and crossover with selective amplification\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Stochastic Local Search with increased chance and perturbation size\n                if eval_count < self.budget and np.random.rand() < 0.25:  # Increased chance for local search\n                    perturbation = np.random.normal(0, 0.15, self.dim)  # Adjusted perturbation size\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 8, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06625 with standard deviation 0.06341.", "error": "", "parent_ids": ["38b1f928-1041-455f-98d3-fdeb84e493c0"], "operator": null, "metadata": {"aucs": [0.15192920014257005, 0.15192920014257005, 0.15192920014257005, 0.0463127685375162, 0.0463127685375162, 0.0463127685375162, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "7b6163ef-323a-4307-a645-62eb7f920dab", "fitness": 0.06665392505964285, "name": "ImprovedHybridOptimizer", "description": "Multi-strategy optimizer with enhanced dynamic parameter adaptation and adaptive local search to boost convergence and solution quality.", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.5\n        CR_base = 0.7\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Improved dynamic parameter adaptation\n                F = F_base + np.random.randn() * 0.1\n                CR = CR_base + np.random.rand() * 0.3  # Adjusted crossover range\n\n                # Mutation and crossover with diversity preservation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Adaptive local search with dynamic intensity\n                if eval_count < self.budget and np.random.rand() < 0.3:  # Increased local search probability\n                    perturbation_scale = 0.1 if f_trial < np.min(fitness) else 0.15  # Dynamic perturbation size\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 9, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06665 with standard deviation 0.05962.", "error": "", "parent_ids": ["638e4d85-6b0c-4050-a793-dd51765ec402"], "operator": null, "metadata": {"aucs": [0.14500832323458757, 0.14500832323458757, 0.14500832323458757, 0.054453451944341036, 0.054453451944341036, 0.054453451944341036, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "7c0c36cf-c686-4b84-afb5-742bc331ac3a", "fitness": 0.06434302950266368, "name": "ImprovedHybridOptimizer", "description": "Introduced adaptive scaling in local search based on fitness trends to enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.5\n        CR_base = 0.7\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Improved dynamic parameter adaptation\n                F = F_base + np.random.randn() * 0.1\n                CR = CR_base + np.random.rand() * 0.3  # Adjusted crossover range\n\n                # Mutation and crossover with diversity preservation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Adaptive local search with dynamic intensity\n                if eval_count < self.budget and np.random.rand() < 0.3:  # Increased local search probability\n                    perturbation_scale = 0.05 if f_trial < np.mean(fitness) else 0.2  # Adaptive perturbation based on fitness trend\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 10, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06434 with standard deviation 0.05701.", "error": "", "parent_ids": ["7b6163ef-323a-4307-a645-62eb7f920dab"], "operator": null, "metadata": {"aucs": [0.1389063218712372, 0.1389063218712372, 0.1389063218712372, 0.053622766636753916, 0.053622766636753916, 0.053622766636753916, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "68d24be9-11e6-4400-aa4d-fcc958e4b2d7", "fitness": 0.06609141784815238, "name": "EnhancedHybridOptimizer", "description": "Incorporate adaptive population resizing and fitness diversity maintenance to enhance exploration-exploitation balance and avoid premature convergence.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        base_population_size = 10 * self.dim\n        F_base = 0.5\n        CR_base = 0.7\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(base_population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = base_population_size\n\n        while eval_count < self.budget:\n            # Adaptive population resizing\n            population_size = max(4, int(base_population_size * (1 - eval_count / self.budget)))\n            \n            for i in range(population_size):\n                # Improved dynamic parameter adaptation\n                F = F_base + np.random.rand() * 0.1\n                CR = CR_base + np.random.rand() * 0.2\n\n                # Mutation and crossover with diversity preservation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Adaptive local search with dynamic intensity\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.1 if f_trial < np.min(fitness) else 0.15\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n            # Maintain diversity by re-initializing part of the population\n            if eval_count < self.budget and np.random.rand() < 0.1:\n                worst_idx = np.argsort(fitness)[-population_size//5:]\n                for idx in worst_idx:\n                    population[idx] = bounds[:, 0] + np.random.rand(self.dim) * (bounds[:, 1] - bounds[:, 0])\n                    fitness[idx] = func(population[idx])\n                    eval_count += 1\n                    if eval_count >= self.budget:\n                        break\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 11, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06609 with standard deviation 0.05793.", "error": "", "parent_ids": ["7b6163ef-323a-4307-a645-62eb7f920dab"], "operator": null, "metadata": {"aucs": [0.14139039786064578, 0.14139039786064578, 0.14139039786064578, 0.056383855683811435, 0.056383855683811435, 0.056383855683811435, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "69f188cf-df48-4333-91f8-80f445293e98", "fitness": 0.06945076362739229, "name": "EnhancedHybridOptimizer", "description": "Enhanced Multi-strategy optimizer with adaptive parameter tuning and dynamic local search prioritization for accelerated convergence and solution refinement.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 12 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Enhanced dynamic parameter adaptation\n                F = F_base + np.random.uniform(-0.1, 0.3)\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover with increased diversity\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Dynamic local search with intensified focus on potential solutions\n                if eval_count < self.budget and np.random.rand() < 0.4:  # Increased local search probability\n                    perturbation_scale = 0.08 if f_trial < np.min(fitness) else 0.12  # Dynamic perturbation size\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06945 with standard deviation 0.06289.", "error": "", "parent_ids": ["7b6163ef-323a-4307-a645-62eb7f920dab"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05527023508362505, 0.05527023508362505, 0.05527023508362505, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "7898ce2e-2739-4f13-b7d4-e2783267d5d2", "fitness": 0.0707449590767439, "name": "EnhancedHybridOptimizerV2", "description": "EnhancedHybridOptimizerV2: Integrates adaptive differential evolution with memory-based local search and opposition-based learning for robust global exploration and fast convergence.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Enhanced dynamic parameter adaptation\n                F = F_base + np.random.uniform(-0.1, 0.3)\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover with opposition-based learning\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                oppo_mutant = bounds[:, 0] + bounds[:, 1] - mutant\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                oppo_trial = np.where(cross_points, oppo_mutant, population[i])\n                \n                # Evaluate new candidates\n                f_trial = func(trial)\n                f_oppo_trial = func(oppo_trial)\n                eval_count += 2\n                \n                # Select the better trial\n                if f_oppo_trial < f_trial:\n                    trial = oppo_trial\n                    f_trial = f_oppo_trial\n\n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.5:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 13, "feedback": "The algorithm EnhancedHybridOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07074 with standard deviation 0.06263.", "error": "", "parent_ids": ["69f188cf-df48-4333-91f8-80f445293e98"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05915282143167988, 0.05915282143167988, 0.05915282143167988, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "2ad4bd5a-b5f1-4d4b-9dbd-67dc1ab8177f", "fitness": 0.0707449590767439, "name": "EnhancedHybridOptimizerV3", "description": "EnhancedHybridOptimizerV3: Introduces adaptive population size and elite retention strategy for improved exploration and convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n\n        while eval_count < self.budget:\n            # Adaptive population size\n            if eval_count % (self.budget // 10) == 0 and population_size > 5:\n                population_size = max(5, population_size // 2)\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n            \n            for i in range(population_size):\n                # Enhanced dynamic parameter adaptation\n                F = F_base + np.random.uniform(-0.1, 0.3)\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover with opposition-based learning\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                oppo_mutant = bounds[:, 0] + bounds[:, 1] - mutant\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                oppo_trial = np.where(cross_points, oppo_mutant, population[i])\n                \n                # Evaluate new candidates\n                f_trial = func(trial)\n                f_oppo_trial = func(oppo_trial)\n                eval_count += 2\n                \n                # Select the better trial\n                if f_oppo_trial < f_trial:\n                    trial = oppo_trial\n                    f_trial = f_oppo_trial\n\n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.5:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Elite retention strategy for best solution\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 14, "feedback": "The algorithm EnhancedHybridOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07074 with standard deviation 0.06263.", "error": "", "parent_ids": ["7898ce2e-2739-4f13-b7d4-e2783267d5d2"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05915282143167988, 0.05915282143167988, 0.05915282143167988, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "85f4391a-0316-4997-ac8e-43bbf64e258a", "fitness": 0.06873855817898437, "name": "EnhancedHybridOptimizerV3", "description": "An advanced adaptive hybrid optimizer incorporating a memory-based search strategy with adaptive mutation and crossover rates for enhanced global exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = [np.inf] * 5\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Enhanced dynamic parameter adaptation\n                adapt_factor = (1 - eval_count / self.budget) ** 0.5\n                F = F_base + np.random.uniform(-0.1, 0.3) * adapt_factor\n                CR = CR_base + np.random.uniform(-0.2, 0.2) * adapt_factor\n\n                # Mutation and crossover with enhanced memory-based local search\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                oppo_mutant = bounds[:, 0] + bounds[:, 1] - mutant\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                oppo_trial = np.where(cross_points, oppo_mutant, population[i])\n                \n                # Evaluate new candidates\n                f_trial = func(trial)\n                f_oppo_trial = func(oppo_trial)\n                eval_count += 2\n                \n                # Select the better trial\n                if f_oppo_trial < f_trial:\n                    trial = oppo_trial\n                    f_trial = f_oppo_trial\n\n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem.remove(max(best_mem))\n\n                # Memory-based local search with adaptive perturbation\n                if eval_count < self.budget and np.random.rand() < 0.5:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5) * adapt_factor\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if f_new_trial < max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem.remove(max(best_mem))\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 15, "feedback": "The algorithm EnhancedHybridOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06874 with standard deviation 0.06306.", "error": "", "parent_ids": ["7898ce2e-2739-4f13-b7d4-e2783267d5d2"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05313361873840128, 0.05313361873840128, 0.05313361873840128, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "691f6ac4-fe47-47d5-8583-b1f31d8a4ddc", "fitness": 0.0707449590767439, "name": "EnhancedHybridOptimizerV3", "description": "Implements adaptive population size and a restart mechanism to escape local optima and enhance exploration.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n\n        while eval_count < self.budget:\n            # Adaptive population size\n            if eval_count % 100 == 0 and len(best_mem) > 0: \n                population_size = max(10, int(population_size * 0.9))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n\n            for i in range(population_size):\n                # Enhanced dynamic parameter adaptation\n                F = F_base + np.random.uniform(-0.1, 0.3)\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover with opposition-based learning\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                oppo_mutant = bounds[:, 0] + bounds[:, 1] - mutant\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                oppo_trial = np.where(cross_points, oppo_mutant, population[i])\n                \n                # Evaluate new candidates\n                f_trial = func(trial)\n                f_oppo_trial = func(oppo_trial)\n                eval_count += 2\n                \n                # Select the better trial\n                if f_oppo_trial < f_trial:\n                    trial = oppo_trial\n                    f_trial = f_oppo_trial\n\n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.5:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n            \n            # Restart mechanism to escape local optima\n            if eval_count < self.budget and eval_count % 200 == 0 and np.min(fitness) > np.min(best_mem):\n                population = np.random.rand(population_size, self.dim)\n                population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n                fitness = np.array([func(ind) for ind in population])\n                eval_count += population_size\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 16, "feedback": "The algorithm EnhancedHybridOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07074 with standard deviation 0.06263.", "error": "", "parent_ids": ["7898ce2e-2739-4f13-b7d4-e2783267d5d2"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05915282143167988, 0.05915282143167988, 0.05915282143167988, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "ace9da3b-0900-4ea4-a916-a1c78727a03c", "fitness": 0.06904688217209483, "name": "EnhancedHybridOptimizerV2", "description": "Integrates adaptive differential evolution with elite local search and strategy diversification for improved global exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Enhanced dynamic parameter adaptation\n                F = F_base + np.random.uniform(-0.1, 0.3)\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover with opposition-based learning\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                oppo_mutant = bounds[:, 0] + bounds[:, 1] - mutant\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                oppo_trial = np.where(cross_points, oppo_mutant, population[i])\n                \n                # Evaluate new candidates\n                f_trial = func(trial)\n                f_oppo_trial = func(oppo_trial)\n                eval_count += 2\n                \n                # Select the better trial\n                if f_oppo_trial < f_trial:\n                    trial = oppo_trial\n                    f_trial = f_oppo_trial\n\n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Elite local search strategy\n                if eval_count < self.budget and np.random.rand() < 0.7:\n                    perturbation_scale = 0.03 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedHybridOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06905 with standard deviation 0.06299.", "error": "", "parent_ids": ["7898ce2e-2739-4f13-b7d4-e2783267d5d2"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05405859071773267, 0.05405859071773267, 0.05405859071773267, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "1829c2d8-5dbd-4c1a-b681-bd5974fc9447", "fitness": 0.07144133915579487, "name": "EnhancedHybridOptimizerV2", "description": "Introduces a restart mechanism to escape local optima, enhancing exploration without sacrificing convergence speed.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Enhanced dynamic parameter adaptation\n                F = F_base + np.random.uniform(-0.1, 0.3)\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover with opposition-based learning\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                oppo_mutant = bounds[:, 0] + bounds[:, 1] - mutant\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                oppo_trial = np.where(cross_points, oppo_mutant, population[i])\n                \n                # Evaluate new candidates\n                f_trial = func(trial)\n                f_oppo_trial = func(oppo_trial)\n                eval_count += 2\n                \n                # Select the better trial\n                if f_oppo_trial < f_trial:\n                    trial = oppo_trial\n                    f_trial = f_oppo_trial\n\n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Forced restart mechanism\n                if eval_count < self.budget and np.random.rand() < 0.01:\n                    population[i] = np.random.rand(self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                    fitness[i] = func(population[i])\n                    eval_count += 1\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.5:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedHybridOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07144 with standard deviation 0.06260.", "error": "", "parent_ids": ["7898ce2e-2739-4f13-b7d4-e2783267d5d2"], "operator": null, "metadata": {"aucs": [0.1527787938461973, 0.1527787938461973, 0.1527787938461973, 0.06104522362118736, 0.06104522362118736, 0.06104522362118736, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "5ddb25ef-6915-45de-822d-dc0abe6f7a6d", "fitness": 0.06749312098570863, "name": "AdaptiveMultiPopOptimizer", "description": "Implements adaptive learning rate adjustment and multi-population strategy to balance exploration and exploitation dynamically.", "code": "import numpy as np\n\nclass AdaptiveMultiPopOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        num_populations = 3\n        population_size = 10 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        max_eval_per_pop = self.budget // num_populations\n\n        # Create multiple populations\n        populations = [np.random.rand(population_size, self.dim) for _ in range(num_populations)]\n        populations = [bounds[:, 0] + pop * (bounds[:, 1] - bounds[:, 0]) for pop in populations]\n        fitness = [np.array([func(ind) for ind in pop]) for pop in populations]\n        eval_count = population_size * num_populations\n\n        # Initialize adaptive learning rate\n        learning_rate = 0.1\n\n        while eval_count < self.budget:\n            for p in range(num_populations):\n                for i in range(population_size):\n                    # Adaptive F and CR based on performance\n                    F = F_base + np.random.uniform(-0.1, 0.3) * (1 - fitness[p][i] / np.max(fitness[p]))\n                    CR = CR_base + np.random.uniform(-0.2, 0.2) * (1 - fitness[p][i] / np.max(fitness[p]))\n\n                    # Mutation and crossover\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = populations[p][indices]\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n\n                    trial = np.where(cross_points, mutant, populations[p][i])\n                    \n                    # Evaluate new candidate\n                    f_trial = func(trial)\n                    eval_count += 1\n\n                    # Selection with adaptive learning\n                    if f_trial < fitness[p][i]:\n                        populations[p][i] = trial\n                        fitness[p][i] = f_trial\n                        learning_rate = max(0.01, learning_rate * 0.9)\n                    else:\n                        learning_rate = min(0.2, learning_rate * 1.1)\n\n                # Inter-population migration\n                if p < num_populations - 1 and eval_count < self.budget:\n                    top_individuals = np.argsort(fitness[p])[:population_size // 5]\n                    for idx in top_individuals:\n                        target_pop = populations[p+1]\n                        target_fitness = fitness[p+1]\n                        if eval_count < self.budget:\n                            target_pop[np.argmax(target_fitness)] = populations[p][idx]\n                            target_fitness[np.argmax(target_fitness)] = fitness[p][idx]\n                            eval_count += 1\n\n        # Return the best solution found across all populations\n        best_idx, best_fit = None, float('inf')\n        for p in range(num_populations):\n            idx = np.argmin(fitness[p])\n            if fitness[p][idx] < best_fit:\n                best_fit = fitness[p][idx]\n                best_idx = (p, idx)\n        return populations[best_idx[0]][best_idx[1]]", "configspace": "", "generation": 19, "feedback": "The algorithm AdaptiveMultiPopOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06749 with standard deviation 0.06339.", "error": "", "parent_ids": ["1829c2d8-5dbd-4c1a-b681-bd5974fc9447"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.049397307158574044, 0.049397307158574044, 0.049397307158574044, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "77a7ac18-58b1-48e4-8165-ddc492571bf2", "fitness": 0.06876083664906285, "name": "RefinedEnhancedHybridOptimizer", "description": "Combines dynamic parameter adjustment with adaptive elitism and stochastic restarts to enhance convergence and exploration in complex landscapes.", "code": "import numpy as np\n\nclass RefinedEnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.5\n        CR_base = 0.9\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Enhanced dynamic parameter adaptation\n                F = F_base + np.random.uniform(-0.1, 0.2)\n                CR = CR_base + np.random.uniform(-0.15, 0.15)\n\n                # Mutation and crossover with opposition-based learning\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                oppo_mutant = bounds[:, 0] + bounds[:, 1] - mutant\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                oppo_trial = np.where(cross_points, oppo_mutant, population[i])\n                \n                # Evaluate new candidates\n                f_trial = func(trial)\n                f_oppo_trial = func(oppo_trial)\n                eval_count += 2\n                \n                # Select the better trial\n                if f_oppo_trial < f_trial:\n                    trial = oppo_trial\n                    f_trial = f_oppo_trial\n\n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if len(best_mem) < 10 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:10]\n\n                # Stochastic restart mechanism\n                if eval_count < self.budget and np.random.rand() < 0.02:\n                    population[i] = np.random.rand(self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                    fitness[i] = func(population[i])\n                    eval_count += 1\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.4:\n                    perturbation_scale = 0.1 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 10 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:10]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 20, "feedback": "The algorithm RefinedEnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06876 with standard deviation 0.06305.", "error": "", "parent_ids": ["1829c2d8-5dbd-4c1a-b681-bd5974fc9447"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05320045414863672, 0.05320045414863672, 0.05320045414863672, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "435b66f6-e0af-4f79-ab91-ec15d5c84271", "fitness": -Infinity, "name": "AdaptiveMemoryHybridOptimizer", "description": "Introduces adaptive memory and self-adaptive parameter tuning for dynamic exploration-exploitation balance, enhancing convergence robustness.", "code": "import numpy as np\n\nclass AdaptiveMemoryHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        F_adapt = 0.6\n        CR_adapt = 0.8\n\n        while eval_count < self.budget:\n            sorted_indices = np.argsort(fitness)\n            top_25_percent = sorted_indices[:population_size // 4]\n            \n            for i in range(population_size):\n                # Self-adaptive parameter tuning\n                F = F_adapt + np.random.uniform(-0.1, 0.1)\n                CR = CR_adapt + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover with memory-based enhancement\n                indices = np.random.choice(top_25_percent, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                \n                # Adaptive memory update\n                if eval_count < self.budget and np.random.rand() < 0.1:\n                    memory_candidate = np.random.rand(self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                    f_memory_candidate = func(memory_candidate)\n                    eval_count += 1\n                    if f_memory_candidate < np.max(best_mem):\n                        best_mem.append(f_memory_candidate)\n                        best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 21, "feedback": "An exception occurred: ValueError('zero-size array to reduction operation maximum which has no identity').", "error": "ValueError('zero-size array to reduction operation maximum which has no identity')", "parent_ids": ["1829c2d8-5dbd-4c1a-b681-bd5974fc9447"], "operator": null, "metadata": {}}
{"id": "25c3e898-9a8d-4a6e-b706-3e37d1cccbd8", "fitness": 0.08564999146632939, "name": "AdaptiveHybridOptimizer", "description": "Enhances exploration by implementing adaptive mutation strategies using successful solutions and dynamic population resizing based on diversity.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 22, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08565 with standard deviation 0.07435.", "error": "", "parent_ids": ["1829c2d8-5dbd-4c1a-b681-bd5974fc9447"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.19894280552721244, 0.05633047513704825, 0.05633047513704825, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "cff72225-cc7c-44c2-8f1c-44e63d61d2e0", "fitness": 0.0636042531147543, "name": "EnhancedAdaptiveHybridOptimizer", "description": "Introduce adaptive learning rates and a feedback mechanism from past successes to control exploration and exploitation balance for improved convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.7\n        CR_base = 0.9\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.8 * population_size))\n                else:\n                    population_size = min(25 * self.dim, int(1.2 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.2), 0.3, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.2, 0.4)\n\n                CR = CR_base + np.random.uniform(-0.3, 0.3)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 10:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search with feedback mechanism\n                if eval_count < self.budget and np.random.rand() < 0.2:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 23, "feedback": "The algorithm EnhancedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06360 with standard deviation 0.05785.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.1402423018619351, 0.1402423018619351, 0.1402423018619351, 0.05007045748232786, 0.05007045748232786, 0.05007045748232786, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "d664ae18-a6ec-4bcf-86b2-d04f364b59c0", "fitness": 0.06980417697853336, "name": "EnhancedAdaptiveHybridOptimizer", "description": "Introduce dynamic learning rates and adaptive restart mechanisms to enhance global search and escape local optima.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n        dynamic_restart_threshold = 50  # Threshold for restart based on memory diversity\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05: \n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n                # Dynamic restart if memory diversity is low\n                if len(best_mem) >= dynamic_restart_threshold and np.std(best_mem) < 0.001:\n                    population = np.random.rand(population_size, self.dim)\n                    population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n                    fitness = np.array([func(ind) for ind in population])\n                    eval_count += population_size\n                    best_mem.clear()\n                    success_mem.clear()\n                    continue\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < dynamic_restart_threshold or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:dynamic_restart_threshold]\n\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < dynamic_restart_threshold or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:dynamic_restart_threshold]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 24, "feedback": "The algorithm EnhancedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06980 with standard deviation 0.06281.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05633047513704825, 0.05633047513704825, 0.05633047513704825, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "14e5fc16-7519-4708-9b5b-dafc8e5f2b45", "fitness": 0.06826055959742865, "name": "AdvancedHybridOptimizer", "description": "Integrates a multi-strategy approach combining adaptive mutation, diversity preservation, and elite learning to enhance convergence and exploration in black box optimization.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n\n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions and success history\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = 0.6 + np.random.uniform(-0.1, 0.3)\n\n                CR = 0.8 + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 10:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search with elite learning\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n            # Elite learning phase based on best memory\n            if best_mem and eval_count < self.budget:\n                elite_candidate = np.random.choice(best_mem)\n                perturbation = np.random.normal(0, 0.02, self.dim)\n                elite_trial = elite_candidate + perturbation\n                elite_trial = np.clip(elite_trial, bounds[:, 0], bounds[:, 1])\n                f_elite_trial = func(elite_trial)\n                eval_count += 1\n                if f_elite_trial < np.min(best_mem):\n                    best_mem.append(f_elite_trial)\n                    best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 25, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06826 with standard deviation 0.06318.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.051699622993734096, 0.051699622993734096, 0.051699622993734096, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "70b3a601-662c-40f5-8bb4-e60f1efc7df3", "fitness": 0.07776974500509326, "name": "EnhancedAdaptiveHybridOptimizer", "description": "Introduces a diversity-driven dynamic scaling mechanism and strategic exploration using best-so-far solutions to enhance convergence and robustness.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 20 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            diversity = np.mean(np.std(population, axis=0))\n            if eval_count % (2 * population_size) == 0:\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.8 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.2 * population_size))\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.5, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                if eval_count < self.budget and np.random.rand() < 0.3 and best_mem:\n                    best_candidate = population[np.argmin(fitness)]\n                    perturbation_scale = 0.02 * np.std(population, axis=0)\n                    new_trial = best_candidate + np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < np.min(fitness):\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 26, "feedback": "The algorithm EnhancedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07777 with standard deviation 0.06211.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.08022717921672795, 0.08022717921672795, 0.08022717921672795, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "e05eb570-2342-41cf-8544-2b3345624d63", "fitness": 0.06980417697853336, "name": "AdaptiveHybridOptimizer", "description": "Enhances exploration and exploitation by incorporating adaptive learning from the best solutions and adjusting population size based on recent improvements.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n        recent_improvements = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                improvement_rate = np.mean(recent_improvements[-5:]) if recent_improvements else 0.1\n                if diversity < 0.05 or improvement_rate < 0.01:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n                \n                recent_improvements.clear()\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    recent_improvements.append(fitness[i] - f_trial)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 27, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06980 with standard deviation 0.06281.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05633047513704825, 0.05633047513704825, 0.05633047513704825, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "209a280a-7f16-4309-9aaf-ddd3c2689680", "fitness": 0.06851664934842087, "name": "AdaptiveHybridOptimizer", "description": "Augments local search by integrating a covariance-based perturbation for enhanced convergence in diverse landscapes.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search with covariance perturbation\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    cov = np.cov(population.T)\n                    perturbation = np.random.multivariate_normal(np.zeros(self.dim), cov)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 28, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06852 with standard deviation 0.06312.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05246789224671078, 0.05246789224671078, 0.05246789224671078, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "63979492-21ff-4dce-840c-b4ffb2b913f9", "fitness": 0.06885884265954441, "name": "AdaptiveHybridOptimizer", "description": "Utilize memory-based local search more effectively by refining perturbation conditions.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.1:  # Reduced the probability from 0.3 to 0.1\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 29, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06886 with standard deviation 0.06303.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05349447218008141, 0.05349447218008141, 0.05349447218008141, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "f6407410-a7d9-40a8-9ba8-b6ac98c0a216", "fitness": 0.06741755054526644, "name": "AdaptiveHybridOptimizer", "description": "Enhances exploration and exploitation by introducing a tournament selection mechanism and refining memory-based local search strategies.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                indices = np.random.choice(population_size, 4, replace=False)\n                a, b, c, d = population[indices]\n                if func(b) < func(c):  # Tournament selection\n                    c = b\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.03 * np.random.uniform(0.5, 1.5)  # Refined local search\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 30, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06742 with standard deviation 0.06341.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.04917059583724748, 0.04917059583724748, 0.04917059583724748, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "23a95230-97bf-472a-8c20-a7026007e8e2", "fitness": 0.0699254376017376, "name": "AdaptiveHybridOptimizer", "description": "Enhances exploration by implementing adaptive mutation strategies using successful solutions and dynamic population resizing based on diversity, with improved local search exploration.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Improved memory-based local search exploration\n                if eval_count < self.budget and np.random.rand() < 0.35:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 31, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06993 with standard deviation 0.06279.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.056694257006660975, 0.056694257006660975, 0.056694257006660975, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "bd328d1d-1ee8-4af0-aa39-b8ceba0048e7", "fitness": 0.06980417697853336, "name": "AdaptiveHybridOptimizer", "description": "Introduces a memory-based strategy for dynamic mutation scale adjustment to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n            \n            # Dynamic mutation scale adjustment\n            if best_mem:\n                F_base = max(0.4, np.mean(best_mem) / np.min(best_mem) * 0.5)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 32, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06980 with standard deviation 0.06281.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05633047513704825, 0.05633047513704825, 0.05633047513704825, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "af9fb5a1-0bc8-4946-8874-e1ece281afba", "fitness": 0.06799613809060419, "name": "EnhancedAdaptiveOptimizer", "description": "Incorporates dynamic learning rate scaling and elite opposition-based learning to enhance diversity and convergence speed in adaptive mutation strategies.", "code": "import numpy as np\n\nclass EnhancedAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection with dynamic learning rate scaling\n                F = F_base + np.random.uniform(-0.1, 0.3)\n                F *= 1 + np.random.normal(0, 0.1) * (np.std(fitness) / (np.mean(fitness) + 1e-9))\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Elite opposition-based learning\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    opp_trial = bounds[:, 1] + bounds[:, 0] - trial\n                    opp_trial = np.clip(opp_trial, bounds[:, 0], bounds[:, 1])\n                    f_opp_trial = func(opp_trial)\n                    eval_count += 1\n                    if f_opp_trial < f_trial:\n                        population[i] = opp_trial\n                        fitness[i] = f_opp_trial\n                        if len(best_mem) < 5 or f_opp_trial < np.max(best_mem):\n                            best_mem.append(f_opp_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 33, "feedback": "The algorithm EnhancedAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06800 with standard deviation 0.06325.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05090635847326075, 0.05090635847326075, 0.05090635847326075, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "21947b77-9b3c-4074-9f4c-41ca3bdb8e13", "fitness": 0.0711748452508502, "name": "AdaptiveHybridOptimizer", "description": "Introduces a stochastic adaptive memory-based search strategy with diversity boosting.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n            # Diversity boosting\n            if np.random.rand() < 0.1:\n                diversity_perturbation = np.random.normal(0, 0.01, (population_size, self.dim))\n                population += diversity_perturbation\n                population = np.clip(population, bounds[:, 0], bounds[:, 1])\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 34, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07117 with standard deviation 0.06255.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.06044247995399876, 0.06044247995399876, 0.06044247995399876, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "3f8d2ddf-d457-4a66-8387-7fb27ebea65a", "fitness": 0.06857922778119778, "name": "AdaptiveHybridOptimizer", "description": "Enhances exploration and exploitation by introducing adaptive learning rates for mutation and leveraging historical success to guide search.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.15, 0.25)  # Adjusted range for CR\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c + np.random.normal(0, 0.1, self.dim)), bounds[:, 0], bounds[:, 1])  # Added noise\n\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 35, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06858 with standard deviation 0.06310.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.052655627545041495, 0.052655627545041495, 0.052655627545041495, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "080dd883-2852-4639-94bf-47ce410a4e55", "fitness": -Infinity, "name": "EnhancedAdaptiveOptimizer", "description": "Improve convergence by incorporating adaptive learning rates and a rotationally invariant local search to enhance diversity and fine-tuning.", "code": "import numpy as np\n\nclass EnhancedAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    perturbation = np.dot(func.rotation, perturbation)  # Applying rotation for invariance\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 36, "feedback": "An exception occurred: AttributeError(\"'ioh.iohcpp.problem.Katsuura' object has no attribute 'rotation'\").", "error": "AttributeError(\"'ioh.iohcpp.problem.Katsuura' object has no attribute 'rotation'\")", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {}}
{"id": "7e64d60e-dc93-4191-82c7-87b3a3cf2032", "fitness": -Infinity, "name": "MultiStrategyAdaptiveOptimizer", "description": "Introduce multi-strategy adaptation by combining diverse mutation tactics and adaptive parameter tuning for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass MultiStrategyAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 20 * self.dim\n        F_base = 0.7\n        CR_base = 0.9\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (3 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.85 * population_size))\n                else:\n                    population_size = min(25 * self.dim, int(1.15 * population_size))\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.5, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.3, 0.3)\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (best_mem[np.random.randint(len(best_mem))] - a) + F * (b - c), bounds[:, 0], bounds[:, 1])\n\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.1 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 37, "feedback": "An exception occurred: ValueError('high <= 0').", "error": "ValueError('high <= 0')", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {}}
{"id": "3fd234d0-74f3-4d77-a7f6-cc198b1f9f78", "fitness": 0.10099214978279297, "name": "AdaptiveHybridOptimizer", "description": "Incorporates a success-based parameter adaptation and an elitist archive mechanism to enhance convergence and solution quality.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0) # Decreased F variation\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1) # Reduced CR variation\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 38, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10099 with standard deviation 0.13595.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.4516802105252712, 0.15258205579855189, 0.15258205579855189, 0.050195008640920635, 0.050195008640920635, 0.050195008640920635, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "f4b0d28c-e0a2-4fe1-b456-6bacf6335ea8", "fitness": -Infinity, "name": "EnhancedAdaptiveHybridOptimizer", "description": "Introduces dynamic scaling of parameters based on diversity and a memory-based perturbation mechanism to balance exploration and exploitation effectively.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            diversity = np.mean(np.std(population, axis=0))\n            if diversity < 0.05:\n                population_size = max(self.dim, int(0.9 * population_size))\n            else:\n                population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial)\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * (1.0 - diversity) * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 39, "feedback": "An exception occurred: IndexError('index 306 is out of bounds for axis 0 with size 300').", "error": "IndexError('index 306 is out of bounds for axis 0 with size 300')", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {}}
{"id": "ec9ec25b-84d2-4fba-a239-6dae1aec6ad9", "fitness": 0.08286843967121499, "name": "EnhancedAdaptiveOptimizer", "description": "Introduces a multilevel adaptive mechanism with feedback-driven diversity control to enhance robustness and convergence speed.", "code": "import numpy as np\n\nclass EnhancedAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 20 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n        \n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.85 * population_size))\n                else:\n                    population_size = min(25 * self.dim, int(1.15 * population_size))\n\n                # Refresh archive when diversity is low\n                if diversity < 0.02 and len(archive) > 0:\n                    random_archive_idx = np.random.choice(len(archive), size=5, replace=False)\n                    for idx in random_archive_idx:\n                        population[np.random.randint(0, population_size)] = archive[idx]\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.3, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 10:\n                        success_mem.pop(0)\n                    if len(best_mem) < 10 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:10]\n                        archive.append(trial)\n\n                if eval_count < self.budget and np.random.rand() < 0.2:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 2.0)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 10 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:10]\n                            archive.append(new_trial)\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08287 with standard deviation 0.09032.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.28397377260426404, 0.15258205579855189, 0.15258205579855189, 0.05172602427985573, 0.05172602427985573, 0.05172602427985573, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "400d270c-5bdc-4027-b82c-fbe8cb44e675", "fitness": 0.06939827396541627, "name": "AdaptiveHybridOptimizer", "description": "Enhanced solution diversity by incorporating a novel random restart mechanism for global exploration.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial)\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial)\n                \n            # Random restart mechanism\n            if eval_count < self.budget and np.random.rand() < 0.01:\n                population = np.random.rand(population_size, self.dim)\n                population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n                fitness = np.array([func(ind) for ind in population])\n                eval_count += population_size\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 41, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06940 with standard deviation 0.06290.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.055112766097697, 0.055112766097697, 0.055112766097697, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "cd3d8e18-299f-4fab-87eb-60f42ccdb40e", "fitness": 0.0688309559271375, "name": "EnhancedAdaptiveHybridOptimizer", "description": "Introduces adaptive memory-based mutation and dynamic population scaling to enhance exploration and convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions and success\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.85 * population_size))\n                else:\n                    population_size = min(25 * self.dim, int(1.15 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 10 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:10]\n                        archive.append(trial)\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.25:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 10 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:10]\n                            archive.append(new_trial)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06883 with standard deviation 0.06304.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.053410811982860684, 0.053410811982860684, 0.053410811982860684, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "b89a6513-ed38-4be5-ae06-2e3d84cb4625", "fitness": 0.06775902147982416, "name": "AdaptiveHybridOptimizer", "description": "Introduced successful trial probability adjustment for better exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0) # Decreased F variation\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1) # Reduced CR variation\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and (np.random.rand() < 0.3 or len(success_mem) / eval_count > 0.1): # Adjusted success probability\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 43, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06776 with standard deviation 0.06332.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.050195008640920635, 0.050195008640920635, 0.050195008640920635, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "5195bbc5-be49-46bf-a446-802693feb38d", "fitness": 0.0680443682860985, "name": "EnhancedHybridOptimizer", "description": "Enhances solution diversity and convergence speed by incorporating adaptive parameter tuning and stochastic ranking within a dynamic archive-based hybrid optimizer.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters\n        population_size = 15 * self.dim\n        F_base = 0.5\n        CR_base = 0.9\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.1 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n            # Adaptive archive influence on population\n            if len(archive) > 5:\n                archive.sort(key=lambda ind: func(ind))\n                best_archive = archive[:5]\n                for idx in range(min(5, len(best_archive))):\n                    if func(best_archive[idx]) < fitness[idx]:\n                        population[idx] = best_archive[idx]\n                        fitness[idx] = func(best_archive[idx])\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06804 with standard deviation 0.06324.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05105104905974367, 0.05105104905974367, 0.05105104905974367, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "50e1bb11-2913-4fac-9989-d68a52069f88", "fitness": 0.06806923715666253, "name": "EnhancedHybridOptimizer", "description": "Integrates adaptive differential evolution with a memory-based perturbation strategy and dynamic population resizing for enhanced convergence and diversity maintenance.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0) # Decreased F variation\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1) # Reduced CR variation\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 10: # Increased memory size for success rate\n                        success_mem.pop(0)\n                    if len(best_mem) < 10 or f_trial < np.max(best_mem): # Expanded best memory\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:10] # Keep top 10\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search with improved perturbations\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.1 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 10 or f_new_trial < np.max(best_mem): # Expanded best memory\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:10] # Keep top 10\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 45, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06807 with standard deviation 0.06323.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05112565567143579, 0.05112565567143579, 0.05112565567143579, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "3a91d9b0-9dc0-4175-ade3-db095381bf29", "fitness": 0.07297973375571772, "name": "AdaptiveHybridOptimizer", "description": "Introduces a dynamic scaling of mutation rates based on population diversity to enhance exploration and convergence.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            diversity = np.mean(np.std(population, axis=0))  # Modify: Calculate population diversity dynamically\n            if eval_count % (2 * population_size) == 0:  # Modify: Adjust mutation scaling based on diversity\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                F_scaled = F * (1 + diversity)  # Modify: Scale F with diversity\n                mutant = np.clip(a + F_scaled * (b - c), bounds[:, 0], bounds[:, 1])  # Modify: Apply scaled F\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial)\n\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial)\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 46, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07298 with standard deviation 0.06229.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.06585714546860133, 0.06585714546860133, 0.06585714546860133, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "54f8cb93-59d1-4520-885f-f353d0e279f3", "fitness": 0.06781295260805476, "name": "AdaptiveHybridOptimizer", "description": "AdaptiveHybridOptimizer with enhanced dynamic population scaling and improved mutation strategy for better exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.7  # Slightly increased base F for more exploration\n        CR_base = 0.9  # Slightly increased base CR for more crossover\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.85 * population_size))  # Slightly more aggressive scaling down\n                else:\n                    population_size = min(25 * self.dim, int(1.15 * population_size))  # Slightly more aggressive scaling up\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)  # Increased F variation slightly\n                else:\n                    F = F_base + np.random.uniform(-0.2, 0.3)  # Updated variation range\n\n                CR = CR_base + np.random.uniform(-0.1, 0.2)  # Updated CR variation range\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 47, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06781 with standard deviation 0.06330.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.050356802025612435, 0.050356802025612435, 0.050356802025612435, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "8305c0dc-00f4-490b-ad35-2a8d092c8c9e", "fitness": 0.0679545843034357, "name": "AdaptiveHybridOptimizer", "description": "Enhances the adaptive strategy by introducing a dynamic archive utilization and a refined refinement local search for improved convergence.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0) # Decreased F variation\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1) # Reduced CR variation\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search with improved exploration\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.3, 1.7)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 48, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06795 with standard deviation 0.06326.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.050781697111755286, 0.050781697111755286, 0.050781697111755286, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "5aac6346-14d0-4a14-b669-64a7d59dce7b", "fitness": 0.06816053829275082, "name": "AdaptiveHybridOptimizer", "description": "Enhances convergence by introducing historical information for each individual in the population, focusing on a dynamic mutation strategy and adaptive crossover rate.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n        historical_mutation = np.zeros((population_size, self.dim))\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = np.clip(CR_base + np.random.normal(0, 0.05), 0.7, 0.9)  # Adjusted CR adaptation\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                historical_mutation[i] = F * (b - c)  # Store mutation history\n                mutant = np.clip(a + historical_mutation[i], bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial)\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 49, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06816 with standard deviation 0.06321.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05139955907970062, 0.05139955907970062, 0.05139955907970062, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "9dbb5f91-b9a3-42cc-aaed-64ecee03a3fb", "fitness": 0.06817827890890256, "name": "EnhancedAdaptiveOptimizer", "description": "EnhancedAdaptiveOptimizer with a dynamic memory-based local search, improved parameter adaptation, and a probabilistic elitist archive to balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 20 * self.dim\n        F_base = 0.6\n        CR_base = 0.9\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.8 * population_size))\n                else:\n                    population_size = min(25 * self.dim, int(1.2 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.05, 0.25)\n\n                CR = CR_base + np.random.uniform(-0.05, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 7:\n                        success_mem.pop(0)\n                    if len(best_mem) < 7 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:7]\n                        if np.random.rand() < 0.5:\n                            archive.append(trial)\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.5:\n                    perturbation_scale = 0.1 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 7 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:7]\n                            if np.random.rand() < 0.5:\n                                archive.append(new_trial)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 50, "feedback": "The algorithm EnhancedAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06818 with standard deviation 0.06320.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05145278092815586, 0.05145278092815586, 0.05145278092815586, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "fadddc7e-e12b-444b-bf51-692d38f7a906", "fitness": 0.06775902147982416, "name": "AdaptiveHybridOptimizer", "description": "Introduces a dynamic population resizing based on performance history and a two-phase exploration strategy to improve convergence rates.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        # Dynamic resizing variables\n        improvement_threshold = 0.01\n        no_improvement_counter = 0\n        max_no_improvement_epochs = 5\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n                # Reset success memory and check for no improvement\n                if len(best_mem) > 0 and (np.max(best_mem) - np.min(fitness)) < improvement_threshold:\n                    no_improvement_counter += 1\n                    if no_improvement_counter >= max_no_improvement_epochs:\n                        # Trigger global exploration\n                        population = bounds[:, 0] + np.random.rand(population_size, self.dim) * (bounds[:, 1] - bounds[:, 0])\n                        fitness = np.array([func(ind) for ind in population])\n                        eval_count += population_size\n                        no_improvement_counter = 0\n                else:\n                    no_improvement_counter = 0\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial)\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 51, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06776 with standard deviation 0.06332.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.050195008640920635, 0.050195008640920635, 0.050195008640920635, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "c54bf8fb-e253-490d-b454-0904d6b31d7d", "fitness": 0.06890914082851529, "name": "AdvancedHybridOptimizer", "description": "Enhances convergence by integrating adaptive learning rates and diversity preservation mechanisms into the differential evolution framework.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing and diversity check\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection with learning rate\n                F = F_base + 0.1 * np.tanh(np.std(fitness) / np.mean(fitness)) \n                CR = CR_base * (1 - np.mean(success_mem)) if success_mem else CR_base\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(1)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial)  # Add to archive\n                else:\n                    success_mem.append(0)\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial)  # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 52, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06891 with standard deviation 0.06302.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.053645366686994045, 0.053645366686994045, 0.053645366686994045, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "bee07d6b-cb9f-41b2-a50c-26e2fb2b6238", "fitness": -Infinity, "name": "AdaptiveHybridOptimizer", "description": "Introduce a dynamic crossover rate adaptation and a focused local search leveraging top solutions for improved convergence and exploration.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            # Dynamic population adjustment based on diversity\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR_mean = np.mean(success_mem) if success_mem else CR_base\n                CR = CR_mean + np.random.uniform(-0.05, 0.05) # Dynamic CR\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Enhanced memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    top_solutions = np.array(sorted(archive, key=func)[:3])\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation + np.mean(top_solutions - population[i], axis=0)\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 53, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (0,) (20,) ').", "error": "ValueError('operands could not be broadcast together with shapes (0,) (20,) ')", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {}}
{"id": "f5882cbf-1dba-4eb7-a140-0816e1174fb9", "fitness": -Infinity, "name": "EnhancedAdaptiveOptimizer", "description": "Introduce a self-adaptive mutation strategy and dynamic crossover probability adjustment for improved exploration and convergence balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n\n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            diversity = np.mean(np.std(population, axis=0))\n            if eval_count % (2 * population_size) == 0:\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem) + np.random.normal(0, 0.1 * (1 - diversity))\n                    F = np.clip(F, 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base * (0.9 + 0.1 * diversity)  # Dynamic CR adjustment\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial)  # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * (1 - diversity) * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial)  # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 54, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {}}
{"id": "2a43f6d8-957d-4f2e-a5c1-470642e40359", "fitness": -Infinity, "name": "EnhancedAdaptiveOptimizer", "description": "Leveraging adaptive memory-based strategies and dynamic scaling of mutation parameters, enhanced with global search through a diverse pool, for robust convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist and diverse archive\n        archive = []\n\n        while eval_count < self.budget:\n            # Adaptive population size adjustment\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(archive + list(population), 3, replace=False)\n                a, b, c = indices\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        if trial not in archive:\n                            archive.append(trial)  # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            if new_trial not in archive:\n                                archive.append(new_trial)  # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 55, "feedback": "An exception occurred: ValueError('a must be 1-dimensional').", "error": "ValueError('a must be 1-dimensional')", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {}}
{"id": "5457368f-5fa2-4743-90c2-d1f869c461a4", "fitness": 0.11040465778984962, "name": "RefinedHybridOptimizer", "description": "Utilizes adaptive parameters with archive-guided mutation and dynamic population control to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 56, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11040 with standard deviation 0.16067.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.5365895153668894, 0.15258205579855189, 0.15258205579855189, 0.050129431048217876, 0.050129431048217876, 0.050129431048217876, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "bced5a08-a8eb-44ba-99f9-95681e15ad16", "fitness": 0.0697766410143958, "name": "RefinedHybridOptimizer", "description": "Implements a slightly adjusted perturbation scale distribution to enhance local search efficiency.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.2, 1.8)  # Adjusted distribution range\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 57, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06978 with standard deviation 0.06282.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.056247867244635597, 0.056247867244635597, 0.056247867244635597, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "7efffd18-b894-4c52-beb4-cf6ae7e93b0e", "fitness": 0.06976105783349489, "name": "RefinedHybridOptimizer", "description": "Enhances exploration by increasing archive usage probability in mutation.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.3:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 58, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06976 with standard deviation 0.06282.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05620111770193281, 0.05620111770193281, 0.05620111770193281, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "02a7256b-ae94-4ed4-b4f3-88f5146ea169", "fitness": 0.06804859781463268, "name": "SynergisticSwarmOptimizer", "description": "Introduces self-adaptive parameter control and swarm synergy with archive-based guided mutation to enhance exploration-exploitation balance in optimization.", "code": "import numpy as np\n\nclass SynergisticSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 20 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = np.array([])\n        \n        # Initialize archive for guided mutation\n        archive = []\n\n        while eval_count < self.budget:\n            success_mem = np.append(success_mem, fitness)\n            if len(success_mem) > 10 * population_size:\n                success_mem = success_mem[-10 * population_size:]\n\n            # Calculate adaptive parameters\n            F = 0.5 + 0.3 * np.random.rand(population_size)\n            CR = np.clip(0.3 + 0.6 * np.random.randn(population_size), 0.1, 0.9)\n\n            for i in range(population_size):\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 5, replace=False)\n                a, b, c, d, e = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.4:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = a + F[i] * (b - c + archive[archive_indx] - d)\n                else:\n                    mutant = a + F[i] * (b - c + d - e)\n                mutant = np.clip(mutant, bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR[i]\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    archive.append(trial)  # Add successful candidate to archive\n                    archive = archive[-population_size:]  # Limit archive size\n\n                    # Update success memory\n                    best_mem.append(f_trial)\n                    best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 59, "feedback": "The algorithm SynergisticSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06805 with standard deviation 0.06324.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05106373764534622, 0.05106373764534622, 0.05106373764534622, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "6cb53708-d49f-4df0-aa74-98e4648c5001", "fitness": 0.06799097126529936, "name": "AdvancedHybridOptimizer", "description": "Introduces multi-phase adaptive exploration with dynamic parameter and archive allocation to enhance the search efficiency.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.3:  # Increased probability for archive mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 10:\n                        success_mem.pop(0)\n                    if len(best_mem) < 10 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:10]\n                        archive.append(trial)  # Add to archive\n\n                # Memory-based local search with dynamic perturbation\n                if eval_count < self.budget and np.random.rand() < 0.5:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 2)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 10 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:10]\n                            archive.append(new_trial)  # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 60, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06799 with standard deviation 0.06325.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05089085799734627, 0.05089085799734627, 0.05089085799734627, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "f60cd0f1-29a1-4d42-89c4-5ae11c38fbc1", "fitness": 0.0702854384745798, "name": "EnhancedHybridOptimizer", "description": "Introduce a multi-parent crossover and adaptive memory-based mutation inspired by successful solutions to enhance convergence speed and diversity.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 20 * self.dim\n        F_base = 0.5\n        CR_base = 0.7\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.8 * population_size))\n                else:\n                    population_size = min(25 * self.dim, int(1.2 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                F = np.mean(success_mem) if success_mem else F_base + np.random.uniform(-0.1, 0.3)\n                F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Multi-parent crossover selection\n                indices = np.random.choice(population_size, 5, replace=False)\n                parents = population[indices]\n                mutant = np.mean(parents[:3], axis=0) + F * (parents[3] - parents[4])\n                mutant = np.clip(mutant, bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 10:\n                        success_mem.pop(0)\n                    archive.append(trial)\n\n                # Memory-based local search with adaptive mutation\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.1 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        archive.append(new_trial)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07029 with standard deviation 0.06271.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05777425962518756, 0.05777425962518756, 0.05777425962518756, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "09bb3c28-4d1c-46af-a1a6-a14e9c87ce2d", "fitness": 0.06923515483446148, "name": "RefinedHybridOptimizer", "description": "Enhances adaptive parameters and archive mutation with improved elitist selection and additional diversity control for robust exploration.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 6:  # Changed from 5 to 6\n                        success_mem.pop(0)\n                    if len(best_mem) < 7 or f_trial < np.max(best_mem):  # Changed from 5 to 7\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:7]  # Changed from 5 to 7\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 7 or f_new_trial < np.max(best_mem):  # Changed from 5 to 7\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:7]  # Changed from 5 to 7\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 62, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06924 with standard deviation 0.06294.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05462340870483262, 0.05462340870483262, 0.05462340870483262, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "f47d08ef-6cd4-4578-839b-f0b67184a21c", "fitness": 0.06864231780159617, "name": "RefinedHybridOptimizer", "description": "A minimally refined mutation strategy introducing noise for increased diversity in solutions.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a + np.random.normal(0, 0.01, self.dim)), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 63, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06864 with standard deviation 0.06308.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.052844897606236674, 0.052844897606236674, 0.052844897606236674, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "67d695c1-b85a-4249-8493-6c2152d8fbc4", "fitness": 0.0683185751940107, "name": "EnhancedHybridOptimizer", "description": "Incorporate a self-adaptive crossover strategy and introduce a crowding mechanism to maintain diversity while refining parameter adjustments based on recent success trends.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions and parameter success history\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing with crowding mechanism\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                    # Remove similar individuals\n                    unique_population = np.unique(population, axis=0)\n                    population = unique_population[:population_size]\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Self-adaptive parameter selection\n                if success_mem:\n                    F = np.clip(np.random.normal(np.mean(success_mem), 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = np.clip(CR_base + np.random.normal(0, 0.1), 0.1, 1.0)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 64, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06832 with standard deviation 0.06317.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05187366978348029, 0.05187366978348029, 0.05187366978348029, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "f453a5dd-c497-4f5f-9fed-28f47bbed4b2", "fitness": 0.06773716228225657, "name": "RefinedHybridOptimizer", "description": "Introduced slight randomness to dynamic population resizing to enhance diversity and adaptation.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                randomness_factor = np.random.uniform(0.95, 1.05)  # Introduce randomness\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size * randomness_factor))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size * randomness_factor))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 65, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06774 with standard deviation 0.06332.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.050129431048217876, 0.050129431048217876, 0.050129431048217876, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "db3a16fa-679c-4d39-b726-ea3b0ce1fef9", "fitness": 0.06973540890314937, "name": "RefinedHybridOptimizer", "description": "Utilizes adaptive parameters with enhanced archive-based mutation and dynamic population control to improve exploration and exploitation.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.25:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 66, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06974 with standard deviation 0.06283.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.056124170910896276, 0.056124170910896276, 0.056124170910896276, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "d1528ce4-26e4-4779-869c-e496242ad2c5", "fitness": 0.06745971886413453, "name": "EnhancedHybridOptimizer", "description": "Enhances adaptive differential evolution with probabilistic crossover tuning and archive-driven mutation for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions and success rates\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            # Dynamic resizing with probabilistic criteria\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                resize_prob = np.tanh(1.5 * (0.1 - diversity))\n                if np.random.rand() < resize_prob:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection with probabilistic crossover tuning\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1) * np.random.rand()\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial)  # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial)  # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06746 with standard deviation 0.06340.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.049297100793851745, 0.049297100793851745, 0.049297100793851745, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "db0dedac-056a-4ad2-97ac-658a487cc5c1", "fitness": 0.07264148305045189, "name": "AdvancedHybridOptimizer", "description": "Combines adaptive parameters, elite archive mutation, and a diversity-preserving local search to enhance exploration and exploitation efficiency.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 20 * self.dim\n        F_base = 0.7\n        CR_base = 0.9\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.1:\n                    population_size = max(self.dim, int(0.8 * population_size))\n                else:\n                    population_size = min(25 * self.dim, int(1.2 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.5, 1.2)\n                else:\n                    F = F_base + np.random.uniform(-0.2, 0.4)\n\n                CR = CR_base + np.random.uniform(-0.15, 0.15)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if np.random.rand() < 0.3 and len(archive) > 0:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 10:\n                        success_mem.pop(0)\n                    if len(best_mem) < 10 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:10]\n                        archive.append(trial)  # Add to archive\n\n                # Diversity-preserving local search\n                if eval_count < self.budget and np.random.rand() < 0.2:\n                    perturbation_scale = 0.1 * np.random.uniform(0.5, 2.0)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 10 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:10]\n                            archive.append(new_trial)  # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 68, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07264 with standard deviation 0.06349.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.1550033293176003, 0.1550033293176003, 0.1550033293176003, 0.06242111983375542, 0.06242111983375542, 0.06242111983375542, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "8ace1ebb-a709-43f6-ba25-88b079dd742a", "fitness": 0.06804322652442434, "name": "LearningElitistOptimizer", "description": "Introduces a learning-based component to adapt step sizes dynamically and incorporates an elitist strategy for improved convergence.", "code": "import numpy as np\n\nclass LearningElitistOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n        archive = []\n        learning_rate = 0.1\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                if success_mem:\n                    F = (1 - learning_rate) * np.mean(success_mem) + learning_rate * np.random.rand()\n                    F = np.clip(F, 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial)\n\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial)\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 69, "feedback": "The algorithm LearningElitistOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06804 with standard deviation 0.06324.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05104762377472116, 0.05104762377472116, 0.05104762377472116, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "cf323f87-20e2-42f2-9dc8-2a344295d53c", "fitness": 0.07002886030899773, "name": "EnhancedSwarmOptimizer", "description": "Integrates adaptive swarm intelligence with multi-strategy recombination and elite memorization to enhance convergence and robustness.", "code": "import numpy as np\n\nclass EnhancedSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n\n        # Initialize parameters for Particle Swarm Optimization Enhanced with Differential Evolution strategies\n        population_size = 20 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n\n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        velocity = np.zeros((population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Initialize personal best and global best\n        p_best = population.copy()\n        p_best_fitness = fitness.copy()\n        g_best_idx = np.argmin(fitness)\n        g_best = population[g_best_idx].copy()\n\n        # Adaptive parameters\n        inertia_weight = 0.7\n        cognitive_coeff = 1.5\n        social_coeff = 1.5\n\n        # Memory for adaptive DE\n        success_mem = []\n        archive = []\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Update velocity\n                inertia = inertia_weight * velocity[i]\n                cognitive = cognitive_coeff * np.random.rand(self.dim) * (p_best[i] - population[i])\n                social = social_coeff * np.random.rand(self.dim) * (g_best - population[i])\n                velocity[i] = inertia + cognitive + social\n\n                # Update position\n                population[i] += velocity[i]\n                population[i] = np.clip(population[i], bounds[:, 0], bounds[:, 1])\n                \n                # Evaluate particle\n                f_value = func(population[i])\n                eval_count += 1\n\n                # Update personal best\n                if f_value < p_best_fitness[i]:\n                    p_best[i] = population[i].copy()\n                    p_best_fitness[i] = f_value\n\n                    # Adaptive DE based on success history\n                    if success_mem:\n                        F = np.mean(success_mem)\n                        F = np.clip(F + np.random.normal(0, 0.05), 0.5, 1.2)\n                    else:\n                        F = 0.7 + np.random.uniform(-0.1, 0.3)\n\n                    CR = 0.9 + np.random.uniform(-0.1, 0.1)\n\n                    # Mutation and crossover (DE variation)\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, population[i])\n                    f_trial = func(trial)\n                    eval_count += 1\n\n                    if f_trial < f_value:\n                        population[i] = trial\n                        fitness[i] = f_trial\n                        success_mem.append(F)\n                        if len(success_mem) > 10:\n                            success_mem.pop(0)\n                        archive.append(trial)\n\n                # Update global best\n                if f_value < fitness[g_best_idx]:\n                    g_best_idx = i\n                    g_best = population[i].copy()\n\n                # Archive-assisted exploration\n                if eval_count < self.budget and np.random.rand() < 0.3 and len(archive) > 0:\n                    archive_idx = np.random.randint(len(archive))\n                    perturbed = archive[archive_idx] + np.random.normal(0, 0.1, self.dim)\n                    perturbed = np.clip(perturbed, bounds[:, 0], bounds[:, 1])\n                    f_perturbed = func(perturbed)\n                    eval_count += 1\n                    if f_perturbed < fitness[g_best_idx]:\n                        g_best = perturbed\n                        g_best_idx = np.argmin([f_perturbed, fitness[g_best_idx]])\n\n        return g_best", "configspace": "", "generation": 70, "feedback": "The algorithm EnhancedSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07003 with standard deviation 0.06378.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15454603949123635, 0.15454603949123635, 0.15454603949123635, 0.05504054143575687, 0.05504054143575687, 0.05504054143575687, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "1f2f5c12-1c0d-4023-a9b7-9be07125d6c4", "fitness": 0.06970380771546163, "name": "RefinedHybridOptimizer", "description": "Introduces an elite archive memory replacement and a refined mutation rate adaptation to enhance convergence.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.03), 0.3, 1.0)  # Changed from 0.05\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.25)  # Changed from 0.3\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial)  # Consider replacing the worst in archive\n                        if len(archive) > 20:  # Archive size limit\n                            archive.pop(np.argmax([func(ind) for ind in archive]))\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 71, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06970 with standard deviation 0.06284.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05602936734783304, 0.05602936734783304, 0.05602936734783304, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "76f86c0a-0793-4c21-9ea5-f5356a1063a7", "fitness": -Infinity, "name": "RefinedHybridOptimizer", "description": "Implements adaptive crossover rate based on diversity and history of successful mutations to enhance exploration.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                # Modified CR adaptation based on diversity and history\n                CR = CR_base + np.random.uniform(-0.1, 0.1) + 0.05 * (diversity - np.mean(success_mem))\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 72, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'diversity' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'diversity' referenced before assignment\")", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {}}
{"id": "1c56f64c-2557-415b-870c-b878bdc33c71", "fitness": 0.08275369872416222, "name": "EnhancedMultiPopulationOptimizer", "description": "Introduces a synergy of multi-population framework with adaptive learning and stochastic ranking to balance exploration-exploitation, enhancing convergence diversity.", "code": "import numpy as np\n\nclass EnhancedMultiPopulationOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters\n        num_populations = 3\n        population_size = 10 * self.dim\n        F_base = 0.7\n        CR_base = 0.9\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Create multiple populations\n        populations = [np.random.rand(population_size, self.dim) for _ in range(num_populations)]\n        populations = [bounds[:, 0] + pop * (bounds[:, 1] - bounds[:, 0]) for pop in populations]\n        fitnesses = [np.array([func(ind) for ind in pop]) for pop in populations]\n        eval_count = sum(population_size for _ in populations)\n        \n        # Memory for best solutions\n        best_mem = []\n\n        while eval_count < self.budget:\n            for p_idx in range(num_populations):\n                population = populations[p_idx]\n                fitness = fitnesses[p_idx]\n                \n                for i in range(population_size):\n                    # Adaptive parameter selection\n                    F = F_base + np.random.uniform(-0.1, 0.1)\n                    CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                    # Mutation and crossover with stochastic ranking\n                    if np.random.rand() < 0.5:\n                        # DE/rand/1 scheme\n                        indices = np.random.choice(population_size, 3, replace=False)\n                        a, b, c = population[indices]\n                        mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                    else:\n                        # Rank-based selection\n                        ranked_indices = np.argsort(fitness)\n                        a, b, c = population[ranked_indices[:3]]\n                        mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                    \n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    \n                    trial = np.where(cross_points, mutant, population[i])\n                    \n                    # Evaluate the trial candidate\n                    f_trial = func(trial)\n                    eval_count += 1\n\n                    # Selection\n                    if f_trial < fitness[i]:\n                        population[i] = trial\n                        fitness[i] = f_trial\n                        if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                            best_mem.append(f_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n                # Cooperation between populations\n                if p_idx > 0 and np.random.rand() < 0.2:\n                    top_idx = np.argmin(fitness)\n                    populations[p_idx - 1][np.random.randint(0, population_size)] = population[top_idx]\n\n        # Return the best solution found\n        all_fitness = np.concatenate(fitnesses)\n        all_population = np.concatenate(populations)\n        best_idx = np.argmin(all_fitness)\n        return all_population[best_idx]", "configspace": "", "generation": 73, "feedback": "The algorithm EnhancedMultiPopulationOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08275 with standard deviation 0.08988.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.28228403519448053, 0.15258205579855189, 0.15258205579855189, 0.051945047241958586, 0.051945047241958586, 0.051945047241958586, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "63846084-5352-466b-a3b4-fb4a16151d20", "fitness": 0.06981164873173873, "name": "EnhancedHybridOptimizer", "description": "Incorporates a self-adaptive learning mechanism with exploration-exploitation balance using archived elite solutions and dynamic parameter tuning to enhance convergence.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Archive and memory for best solutions\n        archive = []\n        best_mem = []\n        success_mem = []\n        \n        while eval_count < self.budget:\n            # Dynamic population resizing based on diversity\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.1:\n                    population_size = max(self.dim, int(0.8 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.2 * population_size))\n\n            for i in range(population_size):\n                # Self-adaptive parameter adjustment\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 0.9)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.25:  # Enhanced archive utilization\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.35:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial)  # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06981 with standard deviation 0.06281.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05635289039666436, 0.05635289039666436, 0.05635289039666436, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "916e855f-c874-46e8-b2d6-79255e5ea992", "fitness": 0.06773716228225657, "name": "RefinedHybridOptimizerV2", "description": "Implements enhanced exploration by integrating a self-adaptive learning strategy to adjust mutation and crossover rates dynamically based on population diversity and convergence trends.", "code": "import numpy as np\n\nclass RefinedHybridOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions and a success history\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            # Adjust population size based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            if eval_count % (2 * population_size) == 0:\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Self-adaptive strategy for F and CR\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 75, "feedback": "The algorithm RefinedHybridOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06774 with standard deviation 0.06332.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.050129431048217876, 0.050129431048217876, 0.050129431048217876, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "de8f7668-4e1d-41f6-a226-e422a2874293", "fitness": 0.06773716228225657, "name": "RefinedHybridOptimizer", "description": "Introduced a more aggressive dynamic population control for better adaptation to problem landscapes.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.85 * population_size)) # Slightly more aggressive shrinkage\n                else:\n                    population_size = min(20 * self.dim, int(1.15 * population_size)) # Slightly more aggressive expansion\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 76, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06774 with standard deviation 0.06332.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.050129431048217876, 0.050129431048217876, 0.050129431048217876, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "ba0b5d5e-f202-41f1-977f-11fbce086fdc", "fitness": 0.06885200074593416, "name": "RefinedHybridOptimizer", "description": "Enhances adaptive parameter control by incorporating a slight momentum effect in the success memory, using weighted average to better capture recent trends.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Introduce momentum in success memory\n                    success_mem.append(0.1 * F + 0.9 * (success_mem[-1] if success_mem else F))\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 77, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06885 with standard deviation 0.06303.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05347394643925063, 0.05347394643925063, 0.05347394643925063, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "90d58ce5-f850-46d6-a189-d7960bec858e", "fitness": 0.06773716228225657, "name": "RefinedHybridOptimizer", "description": "Refines archive-guided mutation and implements adaptive scaling to enhance convergence dynamics.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n            # Adaptively adjust CR based on population diversity\n            if eval_count % (5 * population_size) == 0:\n                CR_base = np.clip(CR_base + 0.1 * (0.5 - diversity), 0.1, 0.9)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 78, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06774 with standard deviation 0.06332.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.050129431048217876, 0.050129431048217876, 0.050129431048217876, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "d851dc90-5013-4880-9969-353c9a732035", "fitness": 0.06928684398596989, "name": "RefinedHybridOptimizer", "description": "Introduce random elitism to boost exploration by occasionally replacing the worst individual with a random one.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n                # Introduce random elitism by occasionally replacing the worst individual\n                if np.random.rand() < 0.05:\n                    worst_idx = np.argmax(fitness)\n                    random_individual = bounds[:, 0] + np.random.rand(self.dim) * (bounds[:, 1] - bounds[:, 0])\n                    f_random = func(random_individual)\n                    eval_count += 1\n                    if f_random < fitness[worst_idx]:\n                        population[worst_idx] = random_individual\n                        fitness[worst_idx] = f_random\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 79, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06929 with standard deviation 0.06293.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.054778476159357825, 0.054778476159357825, 0.054778476159357825, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "39c489d8-6de4-4192-8993-3b816a14870f", "fitness": 0.0694429499318805, "name": "AdvancedHybridOptimizer", "description": "Introduces variable neighborhood search and adaptive archive management to further enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.3:  # Enhanced archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 8:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        if len(archive) < 2 * population_size:\n                            archive.append(trial)  # Add to archive with adaptive control\n\n                # Variable Neighborhood Search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            if len(archive) < 2 * population_size:\n                                archive.append(new_trial)  # Add to archive with adaptive control\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 80, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06944 with standard deviation 0.06290.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15259752019705652, 0.15259752019705652, 0.15259752019705652, 0.05523132959858501, 0.05523132959858501, 0.05523132959858501, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "84c534a6-cf56-4ecd-a743-20d85c646cf5", "fitness": 0.06791704039066164, "name": "EnhancedHybridOptimizer", "description": "Combines adaptive parameters with a hybrid strategy incorporating opposition-based learning and restart mechanisms for enhanced diversity and convergence in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n        restart_threshold = 0.05 * (bounds[:, 1] - bounds[:, 0])\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                    # Restart mechanism\n                    if diversity < 0.01:\n                        population = np.random.rand(population_size, self.dim)\n                        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n                        fitness = np.array([func(ind) for ind in population])\n                        eval_count += population_size\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search with opposition-based learning\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    opposite = bounds[:, 1] + bounds[:, 0] - new_trial\n                    opposite = np.clip(opposite, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    f_opposite = func(opposite)\n                    eval_count += 2\n\n                    if f_new_trial < f_trial or f_opposite < f_trial:\n                        if f_new_trial < f_opposite:\n                            population[i] = new_trial\n                            fitness[i] = f_new_trial\n                            if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                                best_mem.append(f_new_trial)\n                                best_mem = sorted(best_mem)[:5]\n                                archive.append(new_trial)\n                        else:\n                            population[i] = opposite\n                            fitness[i] = f_opposite\n                            if len(best_mem) < 5 or f_opposite < np.max(best_mem):\n                                best_mem.append(f_opposite)\n                                best_mem = sorted(best_mem)[:5]\n                                archive.append(opposite)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06792 with standard deviation 0.06327.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.050669065373433075, 0.050669065373433075, 0.050669065373433075, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "68b333ba-5886-4b32-b055-4a05422d785a", "fitness": 0.06976105783349489, "name": "RefinedHybridOptimizer", "description": "Improved archive utilization by adjusting the probability of archive-guided mutation to enhance exploration in diverse regions.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.3:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 82, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06976 with standard deviation 0.06282.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05620111770193281, 0.05620111770193281, 0.05620111770193281, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "232004e3-392a-40b8-b40a-1fdab5e180e8", "fitness": 0.070766499793901, "name": "RefinedHybridOptimizer", "description": "Enhances exploration by incorporating Lvy flight perturbations during local search, providing more diverse candidate solutions.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search with Lvy flight perturbation\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    levy_flight = np.random.standard_cauchy(self.dim) * 0.01\n                    new_trial = population[i] + levy_flight  # Changed from Gaussian perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 83, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07077 with standard deviation 0.06262.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.059217443583151175, 0.059217443583151175, 0.059217443583151175, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "ddaf3e56-8d7d-45fb-8c2a-05c9fcbe28e0", "fitness": 0.07153711142925985, "name": "RefinedHybridOptimizer", "description": "Introduced adaptive archive usage and enhanced mutation strategies to improve convergence speed and solution quality.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.4:  # Increased archive usage\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.1 * np.random.uniform(0.5, 1.5)  # Enhanced local search\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 84, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07154 with standard deviation 0.06249.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.061529278489227734, 0.061529278489227734, 0.061529278489227734, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "c1d249ad-7d45-416a-b159-56599678d219", "fitness": 0.06759928957495098, "name": "RefinedHybridOptimizer", "description": "Introduces a strategic hypermutation mechanism based on dynamic fitness thresholds to enhance exploration.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n                # Strategic hypermutation mechanism\n                if eval_count < self.budget and np.random.rand() < 0.1: \n                    hypermutation = np.random.normal(0, 0.2, self.dim)\n                    hyper_trial = population[i] + hypermutation\n                    hyper_trial = np.clip(hyper_trial, bounds[:, 0], bounds[:, 1])\n                    f_hyper_trial = func(hyper_trial)\n                    eval_count += 1\n                    if f_hyper_trial < fitness[i]:\n                        population[i] = hyper_trial\n                        fitness[i] = f_hyper_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 85, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06760 with standard deviation 0.06336.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.04971581292630112, 0.04971581292630112, 0.04971581292630112, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "08093272-363b-4cab-90fa-ea606d274732", "fitness": 0.06773716228225657, "name": "EnhancedClusteredHybridOptimizer", "description": "Introduce hierarchical clustering-based population organization and enhanced archive update rule for improved exploitation and exploration balance in black box optimization.", "code": "import numpy as np\nfrom scipy.spatial.distance import pdist, squareform\nfrom sklearn.cluster import AgglomerativeClustering\n\nclass EnhancedClusteredHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n        \n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n        \n        def update_archive(candidate):\n            archive.append(candidate)\n            if len(archive) > 10 * self.dim:  # Limit the size of the archive\n                archive.pop(0)  # Remove the oldest entry\n\n        while eval_count < self.budget:\n            # Hierarchical clustering for population reorganization\n            if eval_count % (3 * population_size) == 0:\n                distance_matrix = squareform(pdist(population))\n                clustering = AgglomerativeClustering(n_clusters=5, affinity='precomputed', linkage='complete')\n                labels = clustering.fit_predict(distance_matrix)\n                \n                # Balance clusters by moving individuals if necessary\n                for cluster_id in range(5):\n                    cluster_indices = np.where(labels == cluster_id)[0]\n                    if len(cluster_indices) < 2:  # Ensure minimum cluster size\n                        continue\n                    sub_population = population[cluster_indices]\n                    sub_fitness = fitness[cluster_indices]\n                    \n                    best_sub_idx = np.argmin(sub_fitness)\n                    best_sub_candidate = sub_population[best_sub_idx]\n                    \n                    if len(archive) > 0:\n                        archive_candidate = archive[np.random.randint(len(archive))]\n                        if func(archive_candidate) < func(best_sub_candidate):\n                            best_sub_candidate = archive_candidate\n                    \n                    for idx in cluster_indices:\n                        if idx != cluster_indices[best_sub_idx]:\n                            population[idx] = best_sub_candidate + np.random.normal(0, 0.1, self.dim)\n                            population[idx] = np.clip(population[idx], bounds[:, 0], bounds[:, 1])\n                            fitness[idx] = func(population[idx])\n                            eval_count += 1\n                    \n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        update_archive(trial)\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            update_archive(new_trial)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 86, "feedback": "The algorithm EnhancedClusteredHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06774 with standard deviation 0.06332.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.050129431048217876, 0.050129431048217876, 0.050129431048217876, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "44de34a2-fbfd-44cb-a1b4-17fab62fba3a", "fitness": 0.06806840849428297, "name": "RefinedHybridOptimizer", "description": "Utilizes adaptive parameters with archive-guided mutation and dynamic population control to enhance exploration and exploitation, with improved success memory management.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 10: # Change from 5 to 10\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 87, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06807 with standard deviation 0.06323.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05112316968429709, 0.05112316968429709, 0.05112316968429709, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "8f052d3f-e687-46a3-aeb1-bbba5cc0ab1d", "fitness": 0.06383168996714872, "name": "AdvancedHybridOptimizer", "description": "Incorporate competitive-cooperative mechanisms with a self-adaptive learning strategy to balance global and local searches dynamically.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.5\n        CR_base = 0.9\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions and strategy adaptation\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive and learning strategy\n        archive = []\n        learning_rate = 0.1\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(15 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on competitive-cooperative strategy\n                if success_mem:\n                    F = np.mean(success_mem) + learning_rate * (np.random.rand() - 0.5)\n                    F = np.clip(F, 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover using competitive-cooperative mechanism\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial)  # Add to archive\n\n                # Memory-based local search with adaptive perturbation\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial)  # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 88, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06383 with standard deviation 0.05897.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.14249373511085806, 0.14249373511085806, 0.14249373511085806, 0.04850133479058816, 0.04850133479058816, 0.04850133479058816, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "2a53b566-d729-4e88-a203-c570c3aa1156", "fitness": 0.06780476557047814, "name": "RefinedHybridOptimizer", "description": "Enhances exploitation by increasing the probability of memory-based local search.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.4:  # Increased probability from 0.3 to 0.4\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 89, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06780 with standard deviation 0.06330.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.050332240912882575, 0.050332240912882575, 0.050332240912882575, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "ee43001c-d34b-4c83-9a7a-1136d17d04b0", "fitness": 0.06961948368807336, "name": "RefinedHybridOptimizer", "description": "Enhanced mutation strategy by incorporating global best guidance for improved exploration.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n        global_best = population[np.argmin(fitness)]\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c + global_best - a), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n                    if f_trial < func(global_best):\n                        global_best = trial\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 90, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06962 with standard deviation 0.06285.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.055776395265668244, 0.055776395265668244, 0.055776395265668244, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "8b95c474-a9a6-4990-a3a6-cf1f252ce5d5", "fitness": 0.06829791984238147, "name": "RefinedHybridOptimizer", "description": "Enhances exploration and convergence by incorporating chaotic maps into parameter adaptation and selection processes.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Incorporate a chaotic map for mutation scaling\n                chaotic_factor = 0.7 * np.sin(eval_count) + 0.3\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * chaotic_factor * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * chaotic_factor * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 91, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06830 with standard deviation 0.06317.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05181170372859256, 0.05181170372859256, 0.05181170372859256, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "ec9362a8-e36e-4ea5-bf4c-0db8ba615913", "fitness": 0.06741755054526644, "name": "EnhancedHybridOptimizer", "description": "Incorporates adaptive differential evolution with success-history-based parameter adaptation and a competitive elitist strategy to enhance both convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n        archive = []\n        \n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.03), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 10:\n                        success_mem.pop(0)\n                    if len(best_mem) < 10 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:10]\n                        archive.append(trial)\n\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 10 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:10]\n                            archive.append(new_trial)\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06742 with standard deviation 0.06341.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.04917059583724748, 0.04917059583724748, 0.04917059583724748, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "0b269069-55b1-4c92-80e9-e1127f3d7ffb", "fitness": -Infinity, "name": "EnhancedHybridOptimizer", "description": "Introduces a learning strategy to dynamically adjust mutation and crossover rates based on population performance and integrates ensemble strategies for enhanced adaptability.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions and past success rate\n        best_mem = []\n        success_mem = []\n        past_success_rate = 0\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            # Dynamic resizing based on performance\n            if eval_count % (2 * population_size) == 0:  \n                diversity = np.mean(np.std(population, axis=0))\n                success_rate = len(success_mem) / (population_size * 2)\n                if success_rate < past_success_rate:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n                past_success_rate = success_rate\n\n            for i in range(population_size):\n                # Adaptive parameter selection with learning strategy\n                F = F_base + np.random.normal(0, 0.1 * (1 - success_rate))\n                CR = CR_base + np.random.normal(0, 0.1 * success_rate)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if archive and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial)  # Add to archive\n\n                # Ensemble-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    gaussian_perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    uniform_perturbation = np.random.uniform(-perturbation_scale, perturbation_scale, self.dim)\n                    ensemble_perturbation = (gaussian_perturbation + uniform_perturbation) / 2\n                    new_trial = population[i] + ensemble_perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial)  # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 93, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'success_rate' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'success_rate' referenced before assignment\")", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {}}
{"id": "a08e321d-351b-4014-8c26-1b872730314a", "fitness": -Infinity, "name": "EnhancedHybridOptimizer", "description": "Implements adaptive memory and variable neighborhood search with dynamic mutation strategies for improved exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 20 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(30 * self.dim, int(1.2 * population_size))\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem) + np.random.normal(0, 0.1)\n                    F = np.clip(F, 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.3:\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 10:\n                        success_mem.pop(0)\n                    if len(best_mem) < 10 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:10]\n                        archive.append(trial)\n\n                if eval_count < self.budget and np.random.rand() < 0.4:\n                    perturbation_scale = 0.1 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 10 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:10]\n                            archive.append(new_trial)\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 94, "feedback": "An exception occurred: IndexError('index 402 is out of bounds for axis 0 with size 400').", "error": "IndexError('index 402 is out of bounds for axis 0 with size 400')", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {}}
{"id": "85c09c89-6e89-42af-9b71-0554c17cd1ea", "fitness": 0.10434466655126595, "name": "EnhancedHybridOptimizer", "description": "Combines adaptive differential evolution with probabilistic local search and elitist archive, modulating exploration and exploitation dynamically using success-based memory and diversity metrics.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 20 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions and successful F values\n        best_mem = []\n        success_mem = []\n        \n        # Elitist archive for diversity\n        archive = []\n\n        while eval_count < self.budget:\n            # Adjust population size based on diversity\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.04:\n                    population_size = max(self.dim, int(0.85 * population_size))\n                else:\n                    population_size = min(25 * self.dim, int(1.15 * population_size))\n\n            for i in range(population_size):\n                # Adaptive F and CR based on success history\n                if success_mem:\n                    F = np.mean(success_mem) + np.random.normal(0, 0.05)\n                    F = np.clip(F, 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.2)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.25:\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial)\n\n                # Probabilistic local search\n                if eval_count < self.budget and np.random.rand() < 0.35:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 95, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10434 with standard deviation 0.13559.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.45421481663994634, 0.15258205579855189, 0.15258205579855189, 0.059407690241447875, 0.059407690241447875, 0.059407690241447875, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "67f8554a-ce1e-49ef-b9c4-1085775041ad", "fitness": -Infinity, "name": "EnhancedAdaptiveOptimizer", "description": "Introduce a self-adaptive learning mechanism to adjust mutation and crossover rates dynamically based on historical trial success rates and incorporate niche-based clustering to maintain diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions and success statistics\n        best_mem = []\n        success_rates = np.zeros(population_size)\n        \n        # Initialize elitist archive\n        archive = []\n        \n        # Clustering parameters\n        niche_threshold = 0.1 * (bounds[:, 1] - bounds[:, 0])\n\n        while eval_count < self.budget:\n            # Dynamic clustering to maintain diversity\n            clusters = self.cluster_population(population, niche_threshold)\n\n            for i in range(population_size):\n                # Dynamic parameter adjustment based on success rate\n                F = F_base + np.tanh(success_rates[i] - 0.5)\n                CR = CR_base + np.tanh(success_rates[i] - 0.5)\n\n                # Mutation and crossover\n                cluster_indices = clusters[i]\n                indices = np.random.choice(cluster_indices, 3, replace=False)\n                a, b, c = population[indices]\n                if archive and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success rate update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_rates[i] += 1 / max(1, np.sum(success_rates))\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial)  # Add to archive\n                else:\n                    success_rates[i] *= 0.9  # Decay unsuccessful attempts\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]\n\n    def cluster_population(self, population, niche_threshold):\n        num_individuals = len(population)\n        clusters = [[] for _ in range(num_individuals)]\n        for i in range(num_individuals):\n            for j in range(num_individuals):\n                if i != j and np.linalg.norm(population[i] - population[j]) < niche_threshold:\n                    clusters[i].append(j)\n            if not clusters[i]:  # Ensure at least self presence\n                clusters[i].append(i)\n        return clusters", "configspace": "", "generation": 96, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {}}
{"id": "ea14ed3a-76b4-4ab3-904b-57aa80df1798", "fitness": 0.08797737104855613, "name": "AdvancedHybridOptimizer", "description": "Introduce a historical memory of explorative and exploitative steps for adaptive parameter tuning, diverging escape strategies, and enhanced local search with archive-driven learning.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n\n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n\n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions and adaptive parameters\n        best_mem = []\n        success_mem = []\n        history_mem = []\n\n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success and historical memory\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover with historical balance\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.3:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + archive[archive_indx] - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update with historical influence\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    history_mem.append((F, CR))\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(history_mem) > 10:\n                        history_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial)  # Add to archive\n\n                # Enhanced local search with archive-driven learning\n                if eval_count < self.budget and np.random.rand() < 0.4:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial)  # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 97, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08798 with standard deviation 0.08838.", "error": "", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {"aucs": [0.28228403519448053, 0.15258205579855189, 0.15258205579855189, 0.06761606421514033, 0.06761606421514033, 0.06761606421514033, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "f49ee190-a609-424e-8627-d949535d3dbe", "fitness": -Infinity, "name": "RefinedHybridOptimizer", "description": "Enhances mutation strategy by introducing a more diverse archive influence for robust exploration.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if len(archive) > 0 and np.random.rand() < 0.2:  # Use archive-guided mutation\n                    archive_indx = np.random.randint(len(archive))\n                    mutant = np.clip(a + F * (b - c + np.random.choice(archive) - a), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 98, "feedback": "An exception occurred: ValueError('a must be 1-dimensional').", "error": "ValueError('a must be 1-dimensional')", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {}}
{"id": "917be193-a20e-4690-8f17-c95a0f72c115", "fitness": -Infinity, "name": "QuantumCovarianceOptimizer", "description": "Introduces a synergy of adaptive covariance-based mutation and differential elitism within a dynamically evolving quantum-inspired population framework to enhance convergence.", "code": "import numpy as np\n\nclass QuantumCovarianceOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for the algorithm\n        population_size = 20 * self.dim\n        F_base = 0.5\n        CR_base = 0.9\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        covariance_matrices = [np.eye(self.dim) for _ in range(population_size)]\n        \n        # Quantum-inspired dynamics\n        quantum_population = population.copy()\n        quantum_fitness = fitness.copy()\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(25 * self.dim, int(1.1 * population_size))\n                quantum_population = np.copy(population)\n                quantum_fitness = np.copy(fitness)\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on historical population\n                F = F_base + np.random.normal(0, 0.1)\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Covariance-based mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                cov_matrix = covariance_matrices[i]\n                mutation_vector = F * np.dot(np.random.randn(self.dim), cov_matrix)\n                mutant = np.clip(a + mutation_vector, bounds[:, 0], bounds[:, 1])\n                \n                # Quantum-inspired population crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and update covariance strategy\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    covariance_matrices[i] = np.cov(population.T)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Dynamic quantum-inspired update\n                if eval_count < self.budget and quantum_fitness[i] > f_trial:\n                    quantum_population[i] = trial\n                    quantum_fitness[i] = f_trial\n\n                # Differential elitism adjustment\n                if eval_count % population_size == 0:\n                    elite_index = np.argmin(fitness)\n                    for j in range(population_size):\n                        if j != elite_index:\n                            population[j] += 0.1 * (population[elite_index] - population[j])\n                            population[j] = np.clip(population[j], bounds[:, 0], bounds[:, 1])\n                            covariance_matrices[j] = np.cov(population.T)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 99, "feedback": "An exception occurred: IndexError('index 411 is out of bounds for axis 0 with size 400').", "error": "IndexError('index 411 is out of bounds for axis 0 with size 400')", "parent_ids": ["5457368f-5fa2-4743-90c2-d1f869c461a4"], "operator": null, "metadata": {}}
