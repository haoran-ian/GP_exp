{"id": "c7fd80e5-4600-4029-a675-c8dee20e1ef3", "fitness": 0.09400692790620852, "name": "HybridOptimizer", "description": "A hybrid metaheuristic combining Differential Evolution and Random Search to efficiently explore and exploit the search space within the budget constraints.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 0, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09401 with standard deviation 0.01651.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11392272460429431, 0.1139179416938062, 0.11391600884523279, 0.0734831458791716, 0.07348139106178542, 0.07348075315969615, 0.0946224874862499, 0.09461951399222235, 0.09461838443341797]}}
{"id": "87331a4b-d2cc-42ef-9f10-027df09ce9ba", "fitness": 0.09400271835882369, "name": "EnhancedHybridOptimizer", "description": "Enhanced HybridOptimizer using Adaptive Differential Evolution with a Local Search phase to dynamically balance exploration and exploitation for improved convergence within budget constraints.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive Differential Evolution parameters\n        F_min, F_max = 0.5, 1.0  # Range for differential weight\n        CR_min, CR_max = 0.4, 0.9  # Range for crossover probability\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptive parameters\n                F = F_min + (F_max - F_min) * evaluations / self.budget\n                CR = CR_max - (CR_max - CR_min) * evaluations / self.budget\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Local Search phase\n            if evaluations < self.budget and np.random.rand() < 0.2:  # 20% chance of local search\n                best_idx = np.argmin(fitness)\n                local_candidate = population[best_idx] + np.random.normal(0, 0.1, self.dim)\n                local_candidate = np.clip(local_candidate, func.bounds.lb, func.bounds.ub)\n                local_fitness = func(local_candidate)\n                evaluations += 1\n\n                if local_fitness < fitness[best_idx]:\n                    population[best_idx] = local_candidate\n                    fitness[best_idx] = local_fitness\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 1, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11390841560532261, 0.11390886915695142, 0.113920145853602, 0.07347803838450728, 0.07347814953547283, 0.0734822322179316, 0.09461373559051278, 0.09461396164570057, 0.09462091723941213]}}
{"id": "57f3662f-d015-4531-932b-32b8880d5fc9", "fitness": 0.09399515638042022, "name": "EnhancedHybridOptimizer", "description": "A synergistic metaheuristic that enhances the balance between exploration and exploitation by integrating adaptive Differential Evolution with Stochastic Hill Climbing.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive Differential Evolution parameters\n        F = 0.5  # Initial differential weight\n        CR = 0.7  # Initial crossover probability\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adapt F and CR\n                F = 0.5 + 0.3 * np.random.rand()\n                CR = 0.6 + 0.4 * np.random.rand()\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Stochastic Hill Climbing\n            if evaluations < self.budget:\n                step_size = 0.1  # Hill climbing step size\n                for i in range(population_size):\n                    if evaluations >= self.budget:\n                        break\n                    current = population[i]\n                    neighbor = np.clip(current + step_size * np.random.normal(size=self.dim), func.bounds.lb, func.bounds.ub)\n                    neighbor_fitness = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fitness < fitness[i]:\n                        population[i] = neighbor\n                        fitness[i] = neighbor_fitness\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 2, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11390718797122923, 0.11390147049173638, 0.11389420903752512, 0.07347758784868763, 0.07347551088789828, 0.07347297569247324, 0.09461297197155027, 0.09460943868700555, 0.09460505483567627]}}
{"id": "6989482f-1131-41d5-bb17-32c0d2466493", "fitness": 0.09400278180419958, "name": "HybridOptimizer", "description": "An enhanced hybrid optimizer incorporating a dynamic population size adjustment for better exploration and exploitation balance.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = min(20, max(5, int(self.budget / 10)))  # Adjusted dynamic population size\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 3, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11391623375642068, 0.11391341467621452, 0.11390807746651077, 0.07348082582275772, 0.07347977212065981, 0.0734779225384623, 0.09461851415009648, 0.09461674194802983, 0.09461353375864401]}}
{"id": "979603f1-4b11-4bc3-af8b-0ce8c6f56516", "fitness": 0.09399536385362245, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid metaheuristic integrating Simulated Annealing with Differential Evolution and Random Search for improved convergence and exploration within budget constraints.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        # Simulated Annealing parameters\n        initial_temp = 100\n        cooling_rate = 0.99\n\n        while evaluations < self.budget:\n            temp = initial_temp * (cooling_rate ** (evaluations // population_size))\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i] or np.random.rand() < np.exp((fitness[i] - trial_fitness) / temp):\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11390847898404122, 0.11390033647848863, 0.11389500285425558, 0.07347806227801645, 0.07347510222383369, 0.07347324710396463, 0.09461377566701701, 0.09460874106973993, 0.09460552802324484]}}
{"id": "7ddb0bb9-0739-4f4b-9581-4e4ef5ffc748", "fitness": 0.09400333795861228, "name": "HybridOptimizer", "description": "An improved hybrid metaheuristic combining Differential Evolution and Adaptive Crossover Strategy to enhance search diversity and convergence efficiency within budget constraints.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR_base = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Adaptive Crossover\n                CR = CR_base * (1 - evaluations / self.budget)  # Adaptive crossover probability\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 5, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11390478200348397, 0.11391683092815197, 0.11391864890686942, 0.07347674198882126, 0.07348099466661928, 0.0734816954994797, 0.09461151376052135, 0.0946188346956679, 0.09461999917789565]}}
{"id": "a036e5a7-9faa-4898-9cd5-b13179f29b81", "fitness": 0.0939840392422034, "name": "HybridOptimizer", "description": "Introducing Adaptive Differential Evolution with Periodic Re-initialization to diversify search and accelerate convergence.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n        adaptive_factor = 0.7\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                else:\n                    F *= adaptive_factor  # Adaptively reduce F if no improvement\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Periodic Re-initialization\n            if evaluations % (self.budget // 4) == 0:\n                new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size // 2, self.dim))\n                new_fitness = np.array([func(ind) for ind in new_population])\n                evaluations += population_size // 2\n\n                for j in range(population_size // 2):\n                    if new_fitness[j] < fitness[np.argmax(fitness)]:\n                        worst_idx = np.argmax(fitness)\n                        fitness[worst_idx] = new_fitness[j]\n                        population[worst_idx] = new_population[j]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 6, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09398 with standard deviation 0.01650.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11388622856344677, 0.11391849758965766, 0.11384732608477877, 0.07347011455956598, 0.07348158593459864, 0.07345622576680133, 0.09460016073573851, 0.09461985032879261, 0.09457636361645039]}}
{"id": "d602347a-3795-486f-9915-ae708039c529", "fitness": 0.09400664275229728, "name": "HybridOptimizer", "description": "A hybrid metaheuristic combining Differential Evolution and Adaptive Random Search to efficiently explore and exploit the search space within the budget constraints.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Adaptive Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n                elif np.random.rand() < 0.1:  # Add adaptive strategy with a small probability\n                    alternative_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                    alternative_fitness = func(alternative_ind)\n                    evaluations += 1\n                    if alternative_fitness < fitness[np.argmax(fitness)]:\n                        worst_idx = np.argmax(fitness)\n                        fitness[worst_idx] = alternative_fitness\n                        population[worst_idx] = alternative_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 7, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09401 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11392272460429431, 0.11391632660089668, 0.11391632013332587, 0.0734831458791716, 0.07348081469398571, 0.07348086427009914, 0.0946224874862499, 0.09461852627595813, 0.09461857482669422]}}
{"id": "68450027-0d07-4412-9fb0-628bf79e2e0b", "fitness": 0.09400208649926357, "name": "AdvancedHybridOptimizer", "description": "An advanced hybrid optimizer that integrates Adaptive Differential Evolution with Simulated Annealing to dynamically balance exploration and exploitation across the search space.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F_base = 0.5  # Starting differential weight\n        CR_base = 0.9  # Starting crossover probability\n\n        # Simulated Annealing parameters\n        T_initial = 100.0  # Initial temperature\n        cooling_rate = 0.99\n\n        while evaluations < self.budget:\n            T = T_initial * cooling_rate ** (evaluations // population_size)\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptively update F and CR\n                F = F_base + np.random.rand() * 0.3\n                CR = CR_base + np.random.rand() * 0.1\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection with Simulated Annealing\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i] or np.random.rand() < np.exp((fitness[i] - trial_fitness) / T):\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Random Search with adaptive probability\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 8, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11391958401473146, 0.11390447024193473, 0.1139104972692484, 0.07348202631613165, 0.07347657316600509, 0.07347878363084981, 0.09462056808565289, 0.09461126470038228, 0.09461501106843584]}}
{"id": "ce3f5c9b-5ea5-4a93-9dff-9eea7b76f2c4", "fitness": 0.09399686890159507, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer that integrates Particle Swarm Optimization and Adaptive Differential Evolution to balance exploration and exploitation in black-box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 20\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        # Particle Swarm Optimization parameters\n        velocities = np.random.uniform(-1, 1, (population_size, self.dim))\n        pbest_positions = population.copy()\n        pbest_fitness = fitness.copy()\n        gbest_position = population[np.argmin(fitness)].copy()\n        gbest_fitness = np.min(fitness)\n\n        w = 0.5  # Inertia weight\n        c1 = 1.5  # Cognitive coefficient\n        c2 = 1.5  # Social coefficient\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Particle Swarm Optimization Update\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                velocities[i] = w * velocities[i] + c1 * r1 * (pbest_positions[i] - population[i]) + c2 * r2 * (gbest_position - population[i])\n                population[i] += velocities[i]\n                population[i] = np.clip(population[i], func.bounds.lb, func.bounds.ub)\n\n                # Evaluate new fitness\n                current_fitness = func(population[i])\n                evaluations += 1\n\n                # Update personal best\n                if current_fitness < pbest_fitness[i]:\n                    pbest_positions[i] = population[i]\n                    pbest_fitness[i] = current_fitness\n\n                # Update global best\n                if current_fitness < gbest_fitness:\n                    gbest_position = population[i]\n                    gbest_fitness = current_fitness\n\n                # Differential Evolution Mutation and Crossover\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Adaptive parameter adjustment\n            if evaluations < self.budget:\n                F = 0.5 + 0.5 * (gbest_fitness - np.min(fitness)) / (np.max(fitness) - np.min(fitness) + 1e-10)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 9, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11391248624412653, 0.11388259919086008, 0.11391852972856298, 0.07347949067564563, 0.07346365617405826, 0.07348102390439115, 0.09461622462208763, 0.09459788140767167, 0.09461992816695175]}}
{"id": "3c8924c4-4503-49e6-b94b-fb4dae6b0a78", "fitness": 0.09400453238546846, "name": "RefinedHybridOptimizer", "description": "A refined hybrid metaheuristic integrating Adaptive Differential Evolution and Guided Random Search to adaptively balance exploration and exploitation under budget constraints.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive Differential Evolution parameters\n        F_min, F_max = 0.5, 1.0  # Differential weight bounds\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            # Adjust F dynamically based on current evaluations\n            F = F_min + (F_max - F_min) * (self.budget - evaluations) / self.budget\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Guided Random Search\n            if evaluations < self.budget:\n                best_ind = population[np.argmin(fitness)]\n                rand_vector = np.random.uniform(-1, 1, self.dim)\n                guidance = best_ind + rand_vector * (func.bounds.ub - func.bounds.lb) * 0.1\n                guidance = np.clip(guidance, func.bounds.lb, func.bounds.ub)\n                \n                rand_fitness = func(guidance)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = guidance\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 10, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11391921960983187, 0.11390900507386004, 0.1139175002576771, 0.07348189433197927, 0.07347820013687167, 0.07348128538883847, 0.0946203432246111, 0.09461404693584397, 0.09461929650970269]}}
{"id": "8502eea8-3bef-40aa-ab62-c97f3eaf7cb2", "fitness": -Infinity, "name": "HybridOptimizer", "description": "Enhanced HybridOptimizer by introducing adaptive crossover probability to balance exploration and exploitation dynamically.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Initial crossover probability\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Adaptive Crossover: Adjust CR based on success ratio\n                successful_trials = fitness[fitness < trial_fitness].size / float(population_size)\n                CR = np.clip(0.5 * (1 + successful_trials), 0.1, 0.9)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 11, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'trial_fitness' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'trial_fitness' referenced before assignment\")", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {}}
{"id": "4120c623-7216-451b-a959-408be913e5f4", "fitness": 0.09400137847100684, "name": "HybridOptimizer", "description": "A hybrid metaheuristic combining Differential Evolution and Random Search, with an adaptive mutation strategy for better exploration and exploitation in constrained budgets.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * np.random.rand()  # Adaptive mutation strategy\n                mutant = np.clip(a + adaptive_F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 12, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11391495962494602, 0.11390039130230478, 0.11391596461743736, 0.07348036985536543, 0.07347511855953792, 0.0734807386707318, 0.09461773362895187, 0.09460877125648581, 0.0946183587233006]}}
{"id": "3214e75b-a2e6-4582-adaa-697cb8d9598a", "fitness": 0.0939986497706433, "name": "EnhancedHybridOptimizer", "description": "Enhanced Hybrid Metaheuristic with Adaptive Differential Evolution and Stochastic Local Search for Dynamic Exploration-Exploitation Balance.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive Differential Evolution parameters\n        F_min, F_max = 0.5, 1.0  # Adaptive differential weight range\n        CR_min, CR_max = 0.1, 0.9  # Adaptive crossover probability range\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptive parameters based on success history\n                success_rate = np.mean(fitness < np.median(fitness))\n                F = F_min + (F_max - F_min) * success_rate\n                CR = CR_max - (CR_max - CR_min) * success_rate\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Stochastic Local Search\n            if evaluations < self.budget:\n                selected_idx = np.random.choice(population_size)\n                local_ind = population[selected_idx] + np.random.normal(0, 0.1, self.dim)\n                local_ind = np.clip(local_ind, func.bounds.lb, func.bounds.ub)\n                local_fitness = func(local_ind)\n                evaluations += 1\n\n                if local_fitness < fitness[selected_idx]:\n                    fitness[selected_idx] = local_fitness\n                    population[selected_idx] = local_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 13, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.1139146965750869, 0.11389339695032918, 0.1139107451977529, 0.0734802793657997, 0.07347261958654905, 0.07347887624217286, 0.09461757625699074, 0.09460449079536626, 0.09461516696574213]}}
{"id": "4fc47972-54d5-45b3-a1e9-c00761707637", "fitness": 0.09400056856685972, "name": "AdaptiveHybridOptimizer", "description": "Adaptive Hybrid Optimization Algorithm with Self-Adaptive Parameters, combining Differential Evolution and Stochastic Hill Climbing to dynamically tune parameter settings for improved exploitation and exploration.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.5  # Initial differential weight\n        CR = 0.5  # Initial crossover probability\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptive parameter adjustment\n                F_adaptive = np.clip(F + 0.1 * (np.random.rand() - 0.5), 0.1, 0.9)\n                CR_adaptive = np.clip(CR + 0.1 * (np.random.rand() - 0.5), 0.1, 0.9)\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F_adaptive * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Stochastic Hill Climbing\n            if evaluations < self.budget:\n                rand_ind = population[np.random.randint(population_size)] + np.random.normal(0, 0.1, self.dim)\n                rand_ind = np.clip(rand_ind, func.bounds.lb, func.bounds.ub)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 14, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.1139157593532697, 0.11390286343575029, 0.1139089876501137, 0.07348065797556991, 0.07347599913734626, 0.07347824974951511, 0.09461822552345789, 0.09461028141032302, 0.09461409286639155]}}
{"id": "7b709e79-277a-4f02-a1f0-c05baa1e8578", "fitness": 0.09400031357382212, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid metaheuristic algorithm combining adaptive Differential Evolution and pattern-based local search to improve the balance of exploration and exploitation within given budget constraints.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        def adaptive_FCR(evals):\n            # Adapt F and CR based on remaining budget\n            return 0.5 + 0.3 * (1 - evals / self.budget), 0.8 + 0.1 * (evals / self.budget)\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                F, CR = adaptive_FCR(evaluations)\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Pattern-based Local Search\n            if evaluations < self.budget:\n                base = population[np.argmin(fitness)]\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim) * (func.bounds.ub - func.bounds.lb)\n                trial = np.clip(base + perturbation, func.bounds.lb, func.bounds.ub)\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = trial_fitness\n                    population[worst_idx] = trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 15, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11390060498485477, 0.1139085649063416, 0.11391727847469435, 0.07347524374116232, 0.07347803779694528, 0.07348120693652727, 0.09460895141192571, 0.09461377231640067, 0.09461916159554706]}}
{"id": "0fa4cf4e-4223-4762-a6a2-f2479bf033cb", "fitness": 0.09400298114706468, "name": "AdaptiveHybridOptimizer", "description": "An adaptive hybrid optimizer combining Differential Evolution and Local Search to dynamically balance exploration and exploitation in black-box optimization under budget constraints.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n        adaptive_threshold = 0.1\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Adaptive Local Search\n            if evaluations < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate = population[best_idx]\n\n                # Localized search around the best candidate\n                localized_points = np.random.uniform(\n                    best_candidate - adaptive_threshold * (func.bounds.ub - func.bounds.lb),\n                    best_candidate + adaptive_threshold * (func.bounds.ub - func.bounds.lb),\n                    (population_size, self.dim)\n                )\n                localized_points = np.clip(localized_points, func.bounds.lb, func.bounds.ub)\n\n                for local_point in localized_points:\n                    if evaluations >= self.budget:\n                        break\n                    local_fitness = func(local_point)\n                    evaluations += 1\n\n                    if local_fitness < fitness[np.argmax(fitness)]:\n                        worst_idx = np.argmax(fitness)\n                        fitness[worst_idx] = local_fitness\n                        population[worst_idx] = local_point\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 16, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11390357739586576, 0.1139200362327728, 0.1139150162779109, 0.07347631189695736, 0.07348213835040407, 0.07348040003232614, 0.09461077685909658, 0.0946207947335218, 0.09461777854472664]}}
{"id": "1f0aa0e0-a071-4529-9513-eaf1d1aae489", "fitness": 0.09400330351139397, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid metaheuristic blending Differential Evolution with Adaptive Random Search, dynamically adjusting strategies based on convergence progress to optimize exploration and exploitation efficiently.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        # Adaptivity factor\n        adapt_factor = 0.1\n\n        while evaluations < self.budget:\n            improved = False\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    improved = True\n\n            # Adaptive Random Search\n            if not improved and evaluations < self.budget:\n                rand_step_size = adapt_factor * (func.bounds.ub - func.bounds.lb)\n                rand_ind = population[np.random.randint(0, population_size)] + np.random.uniform(-rand_step_size, rand_step_size, self.dim)\n                rand_ind = np.clip(rand_ind, func.bounds.lb, func.bounds.ub)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11390951662189563, 0.11391984019476631, 0.11391075294659914, 0.07347842970456742, 0.0734820684255596, 0.07347887450642054, 0.09461440727239112, 0.09462067488077086, 0.09461516704957507]}}
{"id": "f717b511-df62-469c-93f0-eb455b665202", "fitness": 0.09400099933141953, "name": "HybridOptimizer", "description": "An enhanced hybrid metaheuristic incorporating adaptive parameters to balance exploration and exploitation efficiently within budget constraints.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                else:\n                    F = np.clip(F * 0.95, 0.5, 1.0)  # Adaptive adjustment of F\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 18, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11390716391015476, 0.11391084347560632, 0.11391157805551344, 0.07347758358435308, 0.07347885388247077, 0.0734791689172507, 0.09461296172330891, 0.09461516882767707, 0.09461567160644069]}}
{"id": "6100e436-d08c-4da3-bb2f-a4042b5fb4a4", "fitness": 0.09399978515124717, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid algorithm integrating Adaptive Differential Evolution and Dynamic Random Search to efficiently balance global exploration and local exploitation within budget constraints.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive Differential Evolution parameters\n        F_min, F_max = 0.5, 1.0\n        CR_min, CR_max = 0.1, 0.9\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptive parameters\n                F = F_min + np.random.rand() * (F_max - F_min)\n                CR = CR_min + np.random.rand() * (CR_max - CR_min)\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Dynamic Random Search\n            if evaluations < self.budget:\n                rand_scale = 1 - (evaluations / self.budget)  # scale decreases over time\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim) * rand_scale\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 19, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11391216351332478, 0.11389802313793518, 0.11391383561653412, 0.07347937309288022, 0.07347427915571936, 0.07347998016247792, 0.0946160247617237, 0.09460732889920487, 0.0946170580214244]}}
{"id": "fcc95187-496e-4577-886f-18e6fcd8a441", "fitness": 0.09400328953634635, "name": "AdaptiveHybridOptimizer", "description": "A novel hybrid optimizer integrating Differential Evolution, Random Search, and Adaptive Parameter Tuning to dynamically balance exploration and exploitation in black box optimization.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            # Adaptive parameter tuning\n            F = 0.5 + 0.3 * np.random.rand()\n            CR = 0.5 + 0.4 * np.random.rand()\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Random Search with adaptive frequency\n            if evaluations < self.budget:\n                if np.random.rand() < 0.1:  # Adaptive frequency\n                    rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                    rand_fitness = func(rand_ind)\n                    evaluations += 1\n\n                    if rand_fitness < fitness[np.argmax(fitness)]:\n                        worst_idx = np.argmax(fitness)\n                        fitness[worst_idx] = rand_fitness\n                        population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 20, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11391465956452718, 0.11390700930936049, 0.11391838150774192, 0.07348026274217301, 0.07347748250971442, 0.07348160181928975, 0.09461755009517425, 0.094612820826388, 0.09461983745274805]}}
{"id": "4ce34849-2d46-4edf-8f29-4f1723c37d49", "fitness": 0.09400319265655953, "name": "RefinedHybridOptimizer", "description": "A refined hybrid optimizer combining Differential Evolution with adaptive parameters and enhanced Random Search using elite-based initialization to better balance exploration and exploitation within budget constraints.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F_base = 0.6  # Base differential weight\n        CR_base = 0.7  # Base crossover probability\n\n        while evaluations < self.budget:\n            # Adaptation of F and CR based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            F = F_base * (1 + (diversity / self.dim))\n            CR = CR_base * (1 - (diversity / self.dim))\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Enhanced Random Search\n            if evaluations < self.budget:\n                elite_idx = np.argmin(fitness)\n                elite = population[elite_idx]\n                rand_ind = np.random.normal(elite, 0.1 * (func.bounds.ub - func.bounds.lb))\n                rand_ind = np.clip(rand_ind, func.bounds.lb, func.bounds.ub)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 21, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11391148422194763, 0.1139153511911617, 0.11391278491478152, 0.07347912382938926, 0.07348045921061819, 0.07347959807421534, 0.09461560225903076, 0.09461792211980058, 0.0946164080880908]}}
{"id": "9e482b11-498a-45c5-b8ed-53efa349cc36", "fitness": 0.09400160753398722, "name": "AdaptiveDifferentialSearch", "description": "Adaptive Differential Search (ADS): An enhanced hybrid algorithm that dynamically adapts mutation strategies and incorporates chaotic local search to improve convergence speed and accuracy within budget constraints.", "code": "import numpy as np\n\nclass AdaptiveDifferentialSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F_base = 0.5  # Base differential weight\n        CR = 0.9  # Crossover probability\n\n        def chaotic_local_search(individual):\n            # Apply a simple chaotic local search for fine-tuning\n            tau = 0.1  # Learning rate\n            perturbation = np.random.randn(self.dim) * tau\n            return np.clip(individual + perturbation, func.bounds.lb, func.bounds.ub)\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Adaptive mutation strategy\n                F = F_base + np.random.rand() * 0.2\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Chaotic local search on the best individual\n            if evaluations < self.budget:\n                best_idx = np.argmin(fitness)\n                refined_candidate = chaotic_local_search(population[best_idx])\n                refined_fitness = func(refined_candidate)\n                evaluations += 1\n\n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx] = refined_candidate\n                    fitness[best_idx] = refined_fitness\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 22, "feedback": "The algorithm AdaptiveDifferentialSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11390433380904263, 0.11390714078574138, 0.11392089334152988, 0.07347657189217438, 0.07347752994492973, 0.07348249589816602, 0.09461122917353892, 0.09461290176780224, 0.09462137119295977]}}
{"id": "119c9150-73a2-4c93-800b-dcbf8ab25455", "fitness": 0.09400064184468557, "name": "EnhancedHybridOptimizer", "description": "Enhanced HybridOptimizer incorporates adaptive mutation strategies and elitism to dynamically balance exploration and exploitation, improving convergence rates within budget constraints.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F_base = 0.5  # Base differential weight\n        CR = 0.9  # Crossover probability\n        elitism_rate = 0.1\n\n        while evaluations < self.budget:\n            # Sort population by fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n\n            # Apply elitism\n            elite_size = max(1, int(elitism_rate * population_size))\n            elites = population[:elite_size]\n            elites_fitness = fitness[:elite_size]\n\n            for i in range(elite_size, population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptive mutation strategy\n                F = F_base + np.random.rand() * (1.0 - F_base)\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[-1]:\n                    fitness[-1] = rand_fitness\n                    population[-1] = rand_ind\n\n            # Reinforce elitism\n            population[:elite_size] = elites\n            fitness[:elite_size] = elites_fitness\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 23, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11391421375846522, 0.11390223125894439, 0.11391150017228735, 0.0734801082783213, 0.07347577347287504, 0.07347914485228901, 0.09461728224189692, 0.09460989473521719, 0.09461562783187372]}}
{"id": "66c020fc-f242-46bd-8163-864998013899", "fitness": 0.0940007205512133, "name": "EnhancedHybridOptimizer", "description": "A hybrid metaheuristic enhancing Differential Evolution with Gaussian Mutation and Adaptive Crossover to effectively balance exploration and exploitation within budget constraints.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR_initial = 0.9  # Initial Crossover probability\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptive Crossover\n                CR = CR_initial * (1 - (evaluations / self.budget))\n\n                # Mutation with Gaussian Noise\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                noise = np.random.normal(0, 0.1, self.dim)\n                mutant = np.clip(a + F * (b - c) + noise, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Random Search with Gaussian Mutation\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_ind += np.random.normal(0, 0.1, self.dim)\n                rand_ind = np.clip(rand_ind, func.bounds.lb, func.bounds.ub)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 24, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11391159118259708, 0.11389591519745945, 0.1139207978684349, 0.07347916688731193, 0.07347352445967026, 0.07348246417059379, 0.09461567271917881, 0.09460603724745609, 0.09462131522821737]}}
{"id": "ab5cfb65-2db7-4b2a-add0-58567bd265a4", "fitness": 0.09399533978117981, "name": "EnhancedHybridOptimizer", "description": "An adaptive hybrid metaheuristic blending Differential Evolution with Guided Random Search, dynamically adjusting exploration-exploitation balance based on performance feedback to optimize the search space efficiently within budget constraints.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        # Adaptive parameters\n        rs_factor = 0.2  # Initial probability of performing random search\n        improvement_threshold = 0.01  # Threshold for performance improvement\n        last_best_fitness = np.min(fitness)\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Guided Random Search based on adaptive probability\n            if evaluations < self.budget and np.random.rand() < rs_factor:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Adapt rs_factor based on the improvement of the best solution\n            current_best_fitness = np.min(fitness)\n            if current_best_fitness < last_best_fitness - improvement_threshold:\n                rs_factor *= 0.9  # Reduce random search probability if improving\n            else:\n                rs_factor = min(0.5, rs_factor * 1.1)  # Increase it if not improving\n\n            last_best_fitness = current_best_fitness\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 25, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11390125450723054, 0.11390981374592613, 0.11389263341269396, 0.07347548282058458, 0.07347849080175262, 0.07347240273286038, 0.09460935616152355, 0.09461454363339006, 0.0946040802146565]}}
{"id": "4f63faed-d8e2-4afe-a8d1-aaf4360e7b14", "fitness": 0.09352317790488306, "name": "HybridOptimizer", "description": "An enhanced hybrid optimizer that adaptively adjusts the Differential Evolution parameters based on the diversity of the population to improve convergence.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            # Adaptive parameter update based on population diversity\n            population_std = np.std(population, axis=0)\n            F = 0.5 + 0.3 * (1 - np.mean(population_std) / np.std(func.bounds.ub - func.bounds.lb))\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 26, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09352 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11342189860444818, 0.1134389839687524, 0.11342645274428853, 0.07299910184832326, 0.0730051580193588, 0.0730007352278419, 0.09413435661378389, 0.0941447639824855, 0.09413715013466506]}}
{"id": "595e066c-f9c0-4b5b-9bb9-c82b9a403010", "fitness": 0.09400116678652266, "name": "HybridOptimizer", "description": "Enhancing the HybridOptimizer by introducing a greedy exploitation strategy to improve search efficiency.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Greedy Exploitation\n            if evaluations < self.budget:\n                best_idx = np.argmin(fitness)\n                local_search_point = population[best_idx] + np.random.normal(0, 0.1, self.dim)\n                local_search_point = np.clip(local_search_point, func.bounds.lb, func.bounds.ub)\n                local_fitness = func(local_search_point)\n                evaluations += 1\n                if local_fitness < fitness[best_idx]:\n                    population[best_idx] = local_search_point\n                    fitness[best_idx] = local_fitness\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 27, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11390942164056406, 0.11391419333449526, 0.11390672321762518, 0.07347839680586565, 0.07348005353494724, 0.07347743714681776, 0.0946143502171427, 0.0946172217989274, 0.09461270338231875]}}
{"id": "18ccc1a0-acbd-47e0-b978-83ce934f4ed6", "fitness": 0.09400316915018367, "name": "EnhancedHybridOptimizer", "description": "A hybrid metaheuristic integrating Adaptive Differential Evolution with Self-Adaptive Random Search, enhancing convergence through dynamically adjusted parameters and strategic exploration.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F_lower, F_upper = 0.5, 0.9  # Differential weight range\n        CR_lower, CR_upper = 0.1, 0.9  # Crossover probability range\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Adaptive strategy for DE parameters\n                F = np.random.uniform(F_lower, F_upper)\n                CR = np.random.uniform(CR_lower, CR_upper)\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Self-Adaptive Random Search\n            if evaluations < self.budget:\n                rand_scale = np.random.beta(a=2.0, b=5.0) * (func.bounds.ub - func.bounds.lb)\n                rand_ind = np.clip(np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim) + rand_scale, func.bounds.lb, func.bounds.ub)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 28, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11391550438413922, 0.1139143342138268, 0.1139096561693953, 0.07348057037438926, 0.0734801007536745, 0.07347848268481705, 0.09461807309874026, 0.09461730479612263, 0.09461449587654802]}}
{"id": "beef2f88-b420-4142-9fd2-74b208dca570", "fitness": 0.09399995952563184, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer that incorporates adaptive parameter tuning and local search strategies for improved convergence and efficiency within budget constraints.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive Differential Evolution parameters\n        F = 0.5 + np.random.rand() * 0.5  # Adaptive differential weight\n        CR = 0.5 + np.random.rand() * 0.5  # Adaptive crossover probability\n\n        def local_search(individual):\n            local_step_size = 0.1 * (func.bounds.ub - func.bounds.lb)\n            perturbation = np.random.uniform(-local_step_size, local_step_size, self.dim)\n            neighbor = np.clip(individual + perturbation, func.bounds.lb, func.bounds.ub)\n            return neighbor\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if evaluations < self.budget:\n                    neighbor = local_search(population[i])\n                    neighbor_fitness = func(neighbor)\n                    evaluations += 1\n\n                    if neighbor_fitness < fitness[i]:\n                        population[i] = neighbor\n                        fitness[i] = neighbor_fitness\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 29, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11389760823292805, 0.11391407125948605, 0.1139131379061864, 0.07347417824592428, 0.07348000969126112, 0.07347973030287103, 0.0946071228048927, 0.09461714685465572, 0.09461663043248114]}}
{"id": "068d95cf-2e6a-47bf-9fc8-d2e9c7cac4ec", "fitness": 0.093999350624372, "name": "EnhancedHybridOptimizer", "description": "A hybrid metaheuristic combining adaptive Differential Evolution and a focused Random Search to dynamically balance exploration and exploitation, improving convergence efficiency within budget constraints.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.5  # Initial differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation with adaptive F\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F + (1 - F) * (fitness[i] - min(fitness)) / (max(fitness) - min(fitness) + 1e-9)\n                mutant = np.clip(a + adaptive_F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Focused Random Search\n            if evaluations < self.budget:\n                best_idx = np.argmin(fitness)\n                rand_ind = population[best_idx] + np.random.normal(0, 0.1 * (func.bounds.ub - func.bounds.lb), self.dim)\n                rand_ind = np.clip(rand_ind, func.bounds.lb, func.bounds.ub)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 30, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11391386970066997, 0.11390266062804033, 0.11391219777665063, 0.07347229907512154, 0.07347430679604494, 0.07347704297691549, 0.09461706696171768, 0.09461015994208666, 0.09461455176210076]}}
{"id": "7b0a4199-5a3b-4a1d-9bd5-e6041a26c360", "fitness": 0.09400674489396786, "name": "HybridOptimizer", "description": "A refined hybrid optimizer combining Differential Evolution and Random Search with adaptive population size adjustment for improved exploration within budget constraints.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            # Adjust population size adaptively\n            population_size = min(10, max(4, int(20 * (1 - evaluations / self.budget))))\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 31, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09401 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11392272460429431, 0.11391770286736203, 0.1139154108461351, 0.0734831458791716, 0.07348130585189405, 0.0734805397920203, 0.0946224874862499, 0.09461936795619941, 0.09461801876238407]}}
{"id": "3e597473-f77a-4390-bf24-9ae1e8637f16", "fitness": 0.09400617944298005, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer integrating adaptive Differential Evolution with strategic random search and dynamic population scaling for improved convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population with dynamic scaling based on budget and dimension\n        initial_pop_scale = 0.1  # Initial population scaling factor\n        population_size = max(10, int(self.dim * initial_pop_scale))\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters with adaptive mutation factor\n        F_min, F_max = 0.5, 1.0  # Differential weight range\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            # Adaptive population size scaling\n            curr_pop_size = population_size + (self.budget - evaluations) // (self.dim * 2)\n            curr_pop_size = min(curr_pop_size, self.budget - evaluations)\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptive mutation factor\n                F = F_min + (F_max - F_min) * (1 - evaluations / self.budget)\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Strategic Random Search\n            if evaluations < self.budget:\n                rand_inds = np.random.uniform(func.bounds.lb, func.bounds.ub, (curr_pop_size, self.dim))\n                rand_fitnesses = np.array([func(ind) for ind in rand_inds])\n                evaluations += curr_pop_size\n\n                for j, rand_fitness in enumerate(rand_fitnesses):\n                    if rand_fitness < fitness[np.argmax(fitness)]:\n                        worst_idx = np.argmax(fitness)\n                        fitness[worst_idx] = rand_fitness\n                        population[worst_idx] = rand_inds[j]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09401 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11391286049906346, 0.11391627200458587, 0.1139241224339701, 0.07347962342714065, 0.07348079386575834, 0.07348365040450533, 0.09461645266356955, 0.09461849149747259, 0.09462334819075457]}}
{"id": "a321ad87-36f3-4f66-a03c-f7cd8ba24d0c", "fitness": 0.09400235591383159, "name": "EnhancedHybridOptimizer", "description": "Enhance exploration and exploitation by integrating Adaptive Differential Evolution with a Local Search for efficient optimization under budget constraints.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive Differential Evolution parameters\n        F_min, F_max = 0.5, 0.9  # Differential weight range\n        CR_min, CR_max = 0.1, 0.9  # Crossover probability range\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Dynamic parameter adaptation\n                F = F_min + np.random.rand() * (F_max - F_min)\n                CR = CR_min + np.random.rand() * (CR_max - CR_min)\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Local Search with Random Search\n            if evaluations < self.budget:\n                for _ in range(2):  # Two local perturbations\n                    rand_idx = np.random.randint(0, population_size)\n                    perturbation = np.random.normal(0, 0.1, self.dim)\n                    local_candidate = np.clip(population[rand_idx] + perturbation, func.bounds.lb, func.bounds.ub)\n                    local_fitness = func(local_candidate)\n                    evaluations += 1\n\n                    if local_fitness < fitness[rand_idx]:\n                        fitness[rand_idx] = local_fitness\n                        population[rand_idx] = local_candidate\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 33, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11391128063821854, 0.1139122821362647, 0.11391221295642917, 0.07347906272209259, 0.07347936720680848, 0.07347939742661569, 0.09461548968702849, 0.0946160485648655, 0.09461606188616112]}}
{"id": "05403edc-891d-408b-8a2f-c3eaf9a78d48", "fitness": 0.09399924960371749, "name": "ImprovedHybridOptimizer", "description": "A novel hybrid metaheuristic combining Adaptive Differential Evolution and Stochastic Local Search to dynamically balance exploration and exploitation in optimizing black box functions within budget constraints.", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive Differential Evolution parameters\n        F_min, F_max = 0.5, 0.9  # Differential weight range\n        CR_min, CR_max = 0.1, 0.9  # Crossover probability range\n\n        while evaluations < self.budget:\n            F = F_min + np.random.rand() * (F_max - F_min)\n            CR = CR_min + np.random.rand() * (CR_max - CR_min)\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Stochastic Local Search\n            if evaluations < self.budget:\n                rand_idx = np.random.choice(population_size)\n                rand_ind = population[rand_idx] + np.random.normal(0, 0.1, self.dim)\n                rand_ind = np.clip(rand_ind, func.bounds.lb, func.bounds.ub)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[rand_idx]:\n                    fitness[rand_idx] = rand_fitness\n                    population[rand_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 34, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.1139099252494441, 0.11389748044846149, 0.11391416643946806, 0.07347857917523493, 0.07347408456359683, 0.07348009576416636, 0.09461466094071103, 0.09460699605696721, 0.09461725779540742]}}
{"id": "b0618c6d-24ef-4225-8cf1-bb1d0db00764", "fitness": 0.09400296338808446, "name": "HybridOptimizer", "description": "A hybrid metaheuristic combining Differential Evolution and Random Search, with an improved mutation strategy to enhance search quality within budget constraints.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                \n                # Enhanced mutation strategy\n                mutant = np.clip(a + F * (b - c) + F * (population[np.argmin(fitness)] - a), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 35, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11391555614057847, 0.11391333989752461, 0.1139096591216956, 0.07348058591522877, 0.07347974677167879, 0.07347848467435103, 0.09461810172294416, 0.09461669759882885, 0.09461449864992988]}}
{"id": "8db6bb56-3a89-498e-bf00-b07d5c942992", "fitness": 0.09399936826470136, "name": "RefinedHybridOptimizer", "description": "A refined hybrid optimizer that enhances the exploration-exploitation balance by integrating Adaptive Differential Evolution and Stochastic Local Search for improved convergence within budget constraints.", "code": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive Differential Evolution parameters\n        F_min, F_max = 0.5, 0.9  # Adaptive differential weight range\n        CR_min, CR_max = 0.1, 0.9  # Adaptive crossover probability range\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation with adaptive F\n                F = F_min + np.random.rand() * (F_max - F_min)\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover with adaptive CR\n                CR = CR_min + np.random.rand() * (CR_max - CR_min)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Stochastic Local Search\n            if evaluations < self.budget:\n                best_idx = np.argmin(fitness)\n                local_search_point = population[best_idx] + np.random.normal(0, 0.1, self.dim)\n                local_search_point = np.clip(local_search_point, func.bounds.lb, func.bounds.ub)\n                local_fitness = func(local_search_point)\n                evaluations += 1\n\n                if local_fitness < fitness[best_idx]:\n                    fitness[best_idx] = local_fitness\n                    population[best_idx] = local_search_point\n        \n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 36, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.1139113572015007, 0.11389218136498291, 0.11391859111679092, 0.07347908579452644, 0.07347218241257403, 0.0734816757439527, 0.094615532114536, 0.09460374390107507, 0.09461996473237344]}}
{"id": "8942cf2b-8a7e-49a5-a80f-4bea5c3122a1", "fitness": 0.09400333795861228, "name": "HybridOptimizer", "description": "A hybrid metaheuristic integrating Differential Evolution and Random Search with a dynamic crossover probability to enhance search space exploration.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover with dynamic CR\n                CR_dynamic = CR * (1 - (evaluations / self.budget))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 37, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11390478200348397, 0.11391683092815197, 0.11391864890686942, 0.07347674198882126, 0.07348099466661928, 0.0734816954994797, 0.09461151376052135, 0.0946188346956679, 0.09461999917789565]}}
{"id": "3d1ff7fa-56f9-4c8f-9cac-b0798cb1c90c", "fitness": 0.09399524020421998, "name": "AdaptiveHybridOptimizer", "description": "A novel hybrid metaheuristic combining Adaptive Differential Evolution and Simulated Annealing to dynamically balance exploration and exploitation within budget constraints.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        F_min, F_max = 0.5, 1.0  # Adaptive Differential weight bounds\n        CR_min, CR_max = 0.1, 0.9  # Adaptive Crossover probability bounds\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptive Differential Evolution Parameters\n                F = F_min + np.random.rand() * (F_max - F_min)\n                CR = CR_min + np.random.rand() * (CR_max - CR_min)\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Simulated Annealing as Random Search Replacement\n            if evaluations < self.budget:\n                temperature = max(0.1, 1.0 - evaluations / self.budget)  # Decreases over time\n                current_idx = np.random.randint(0, population_size)\n                current_sol = population[current_idx]\n                current_fitness = fitness[current_idx]\n\n                neighbor = current_sol + np.random.normal(0, temperature, self.dim)\n                neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub)\n\n                neighbor_fitness = func(neighbor)\n                evaluations += 1\n\n                if neighbor_fitness < current_fitness or \\\n                   np.random.rand() < np.exp((current_fitness - neighbor_fitness) / temperature):\n                    population[current_idx] = neighbor\n                    fitness[current_idx] = neighbor_fitness\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 38, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11391446712770215, 0.11388957409549716, 0.11389921287902438, 0.07348019837200104, 0.07347125122194054, 0.07347475956180827, 0.09461743685644342, 0.09460214865185101, 0.09460811307171191]}}
{"id": "6c95c2ba-3825-4576-88d5-6072d4f49ce9", "fitness": 0.09400452683240824, "name": "HybridOptimizer", "description": "Enhanced HybridOptimizer by introducing adaptive crossover probability and differential weight for better exploration and exploitation balance.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        while evaluations < self.budget:\n            adaptive_F = 0.5 + 0.3 * np.random.rand()  # Adaptive Differential weight\n            adaptive_CR = 0.8 + 0.1 * np.random.rand()  # Adaptive Crossover probability\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + adaptive_F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 39, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.1139162936264837, 0.11391816581039726, 0.11391124648566897, 0.07348084740971017, 0.07348146740957784, 0.07347905208224659, 0.09461855099190386, 0.0946196472996802, 0.0946154703760056]}}
{"id": "f775666a-c57e-424c-80c1-bb60c82b7fb0", "fitness": 0.09399867861690012, "name": "HybridOptimizer", "description": "Improved HybridOptimizer by introducing adaptive control parameters in Differential Evolution to enhance convergence speed.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Adaptive F update\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    F = min(1.2, F * 1.1)  # Increase F\n                else:\n                    F = max(0.4, F * 0.9)  # Decrease F\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 40, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11391543890245337, 0.11390445841687458, 0.11389908200020393, 0.07348054273350513, 0.07347656925477886, 0.07347470495496, 0.09461802863721291, 0.09461125778754675, 0.09460802486456554]}}
{"id": "c46c2a55-65e1-4994-8959-8e955ca8c8e0", "fitness": 0.09399765116380122, "name": "HybridOptimizer", "description": "Enhanced HybridOptimizer using adaptive Differential Evolution parameters to improve convergence.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.5 + 0.3 * np.random.rand()  # Differential weight (adaptive)\n        CR = 0.4 + 0.5 * np.random.rand()  # Crossover probability (adaptive)\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Random Search\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 41, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11390242030153286, 0.11390195687125682, 0.11390990340563678, 0.07347588984549758, 0.07347567811128208, 0.07347857316321837, 0.09461005979913462, 0.09460972957680913, 0.09461464939984277]}}
{"id": "ae435c33-40b5-413f-9bd1-29743579a44a", "fitness": 0.09400705115708352, "name": "HybridOptimizer", "description": "Enhance the HybridOptimizer by adding elitism to preserve the best solution found so far, improving convergence reliability.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 42, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09401 with standard deviation 0.01651.", "error": "", "parent_ids": ["c7fd80e5-4600-4029-a675-c8dee20e1ef3"], "operator": null, "metadata": {"aucs": [0.11392272460429431, 0.11391411951903274, 0.11392039463881998, 0.0734831458791716, 0.07348002725225711, 0.07348231801619287, 0.0946224874862499, 0.09461717671824177, 0.09462106629949141]}}
{"id": "579e0ed8-4ca7-4525-9717-b9354f68f619", "fitness": 0.09400532884855319, "name": "HybridOptimizer", "description": "Enhance the HybridOptimizer by introducing adaptive crossover and mutation rates to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover with adaptive rate\n                CR = 0.9 - (0.5 * evaluations / self.budget)  # Adaptive crossover probability\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 43, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09401 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11390865120961435, 0.11391935396411135, 0.11392136025660049, 0.07347812224995132, 0.07348189494277246, 0.07348266288685279, 0.0946138794530782, 0.09462037756106978, 0.09462165711292791]}}
{"id": "2bd5ac54-dcc8-4061-b3af-610ddca7d069", "fitness": 0.0939989514079071, "name": "EnhancedHybridOptimizer", "description": "Introduce adaptive parameter tuning and multi-strategy integration to enhance exploration and exploitation balance in HybridOptimizer.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n\n            # Adaptive parameter tuning\n            F = 0.5 + 0.3 * np.random.rand()\n            CR = 0.7 + 0.2 * np.random.rand()\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Hybrid strategy: Random search injection\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Elitism: Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.1138963592410015, 0.11391187211536735, 0.11391198774520928, 0.07347372714182254, 0.0734792198177373, 0.07347931964888998, 0.09460635341382362, 0.09461579671075582, 0.0946159268365564]}}
{"id": "c2368cdd-41ae-4deb-8853-dac6b2f45fc5", "fitness": 0.09399708016404029, "name": "HybridOptimizer", "description": "Enhance mutation diversity by using adaptive differential weight and improve crossover by using dynamic crossover probability.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adaptive = np.random.uniform(0.5, 1.0)  # Adaptive F\n                mutant = np.clip(a + F_adaptive * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                CR_dynamic = 0.5 + 0.3 * (fitness[i] < np.mean(fitness))  # Dynamic CR\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 45, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11390799112072969, 0.1138991682534255, 0.11390449884938803, 0.07347788649625786, 0.07347468253873468, 0.07347664752014038, 0.09461347557353772, 0.0946080237530933, 0.09461134737105548]}}
{"id": "bc678207-4dc1-433c-bfab-84ec6b5e3e5a", "fitness": 0.09400061627416835, "name": "HybridOptimizer", "description": "Introduce a small random mutation to the best solution in the elitism step to enhance exploration.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained with slight mutation\n            population[best_idx] = np.clip(population[best_idx] + np.random.normal(0, 0.01, self.dim), func.bounds.lb, func.bounds.ub)\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 46, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.1139080419405919, 0.11390847313039698, 0.11391130631780255, 0.07347790475827676, 0.07347801239681062, 0.07347907193607117, 0.09461350678439229, 0.09461372378698385, 0.09461550541618902]}}
{"id": "5dbc7ac3-f06d-4808-8c07-ad3707e7f7ab", "fitness": 0.0940011630312449, "name": "HybridOptimizer", "description": "Introduce stochastic perturbation in mutation to increase exploration and avoid local optima.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                # Stochastic perturbation added\n                mutant = np.clip(a + F * (b - c) + np.random.normal(0, 0.1, self.dim), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 47, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11391367024132448, 0.11390678704396617, 0.11390987847612233, 0.07347990667971882, 0.07347739875449544, 0.0734785669156961, 0.0946169419599362, 0.0946126803185342, 0.09461463689141036]}}
{"id": "069440fc-7945-4bdd-81de-8c1335711ec4", "fitness": 0.09400096320665813, "name": "HybridOptimizer", "description": "Enhance the HybridOptimizer by introducing adaptive mutation factor adjustment to improve exploration-exploitation balance.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adaptive = F + np.random.uniform(-0.1, 0.1)  # Adaptive mutation factor\n                mutant = np.clip(a + F_adaptive * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 48, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11391401640512322, 0.11389252717064335, 0.11392287729922201, 0.07348003059994923, 0.07347230929892934, 0.07348320717407841, 0.09461715405768101, 0.09460395897938301, 0.09462258787491351]}}
{"id": "25065697-1ddf-4246-b7a1-b0bba9ec53a2", "fitness": 0.09400182677318196, "name": "HybridOptimizer", "description": "Integrate adaptive parameters into the HybridOptimizer to dynamically adjust mutation and crossover rates, enhancing exploration and exploitation balance for improved convergence.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F_initial = 0.8  # Initial differential weight\n        CR_initial = 0.9  # Initial crossover probability\n        F = F_initial\n        CR = CR_initial\n\n        # Adaptive control parameters\n        adaptation_rate = 0.05\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    # Adaptive adjustment\n                    F = min(1.0, F + adaptation_rate)\n                    CR = min(1.0, CR + adaptation_rate)\n                else:\n                    # Adaptive adjustment\n                    F = max(0.5, F - adaptation_rate)\n                    CR = max(0.3, CR - adaptation_rate)\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 49, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11391152042289976, 0.11391820439085054, 0.11390364261788943, 0.07347913934647776, 0.07348148399113674, 0.07347633390179487, 0.09461562708224147, 0.09461967379969827, 0.09461081540564875]}}
{"id": "c60c59af-7c8d-4349-95f3-52d1f1da86f9", "fitness": 0.09399837927845832, "name": "HybridOptimizer", "description": "Enhance the HybridOptimizer by incorporating adaptive differential weights and crossover rates to dynamically balance exploration and exploitation, improving convergence efficiency.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution adaptive parameters\n        F_min, F_max = 0.5, 1.0  # Adaptive differential weight range\n        CR_min, CR_max = 0.1, 1.0  # Adaptive crossover probability range\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptive parameters based on iterations\n                F = F_min + (F_max - F_min) * evaluations / self.budget\n                CR = CR_max - (CR_max - CR_min) * evaluations / self.budget\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 50, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11390893988928841, 0.11391325610606573, 0.11389541352860666, 0.07347822301175733, 0.07347971663684139, 0.07347338958838945, 0.09461405366402609, 0.09461664611560605, 0.09460577496554379]}}
{"id": "7a815d73-2785-4a31-ad4d-b4b7fc53397f", "fitness": 0.09400667010605858, "name": "AdaptiveHybridOptimizer", "description": "Introduce adaptive control of differential evolution parameters to balance exploration and exploitation dynamically, enhancing convergence speed and reliability.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Initial differential weight\n        CR = 0.9  # Initial crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n            # Adaptive control of F and CR\n            F = 0.5 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            CR = 0.5 + 0.4 * np.sin(np.pi * evaluations / self.budget)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 51, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09401 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11391570209827717, 0.11391810086806986, 0.11392169450019396, 0.07348063867014576, 0.07348144777560328, 0.0734827823817612, 0.09461819167394092, 0.09461961124298146, 0.09462186174355358]}}
{"id": "df2bf361-8d7c-47ed-ab87-62ea3362edcd", "fitness": 0.0939966814972914, "name": "HybridOptimizer", "description": "Refine the HybridOptimizer by incorporating adaptive control of the differential weight F, enhancing exploration and exploitation balance.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = 0.5 + 0.3 * np.random.rand()  # Change 1: Adaptive differential weight\n                mutant = np.clip(a + adaptive_F * (b - c), func.bounds.lb, func.bounds.ub)  # Change 2\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)  # Change 3\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 52, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.113896643622587, 0.11390028899755, 0.11391291702727979, 0.07347382537026681, 0.07347508331294372, 0.07347964870533474, 0.09460652395763669, 0.09460870999648474, 0.09461649248553916]}}
{"id": "c42ed1e5-ae71-4d01-833d-a578ecf58ebc", "fitness": 0.09400524198624964, "name": "HybridOptimizer", "description": "Introduce adaptive differential weight for mutation in HybridOptimizer to dynamically enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F = 0.5 + np.random.rand() * (0.9 - 0.5)  # Adaptive differential weight\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 53, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09401 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11391584378575681, 0.11391485465188167, 0.11391827286901046, 0.07348068820633213, 0.07348028660845307, 0.07348156168407094, 0.09461827726420113, 0.09461762320466194, 0.09461976960187857]}}
{"id": "f4e256bc-30f6-44cd-aa16-89e1d730028d", "fitness": 0.0940039088574373, "name": "HybridOptimizer", "description": "Enhance the HybridOptimizer by using adaptive crossover probability to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Initial crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                CR = 0.5 + 0.4 * (1 - evaluations / self.budget)  # Adaptive crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 54, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11390620155604636, 0.11391646166530078, 0.11392020952754622, 0.0734772480410274, 0.07348086294869505, 0.0734822522787355, 0.09461238134050864, 0.09461860893215968, 0.09462095342691601]}}
{"id": "3a793517-9230-4788-b748-79c4c3d965dd", "fitness": 0.09400164288190528, "name": "HybridOptimizer", "description": "Enhance the HybridOptimizer by introducing adaptive crossover probability and differential weight, further improving exploration and convergence.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                CR = 0.7 + 0.3 * np.random.rand()  # Adaptive CR\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                F = 0.5 + 0.5 * np.random.rand()  # Adaptive F\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 55, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11391517958249253, 0.11390902824201021, 0.11390830478596137, 0.07348045422643823, 0.07347820988342146, 0.07347800165773521, 0.09461787421909063, 0.09461406263376249, 0.09461367070623539]}}
{"id": "b3c37b2e-e463-4915-b1fb-5133e16758b4", "fitness": 0.09399989436195365, "name": "AdaptiveHybridOptimizer", "description": "Integrate adaptive parameter control into the HybridOptimizer to dynamically adjust mutation and crossover rates, enhancing exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Initial differential weight\n        CR = 0.9  # Initial crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptive mutation and crossover\n                F = 0.5 + np.random.rand() * 0.5  # Adjust F dynamically between [0.5, 1.0]\n                CR = 0.8 + np.random.rand() * 0.2  # Adjust CR dynamically between [0.8, 1.0]\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 56, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11389799650282995, 0.11390902824407734, 0.11391750752135588, 0.07347431511004954, 0.07347820174969932, 0.07348128722893299, 0.09460735849673929, 0.09461405422897795, 0.0946193001749206]}}
{"id": "1d730a6d-baf0-407a-9666-a2b61978ea92", "fitness": 0.09399903097401811, "name": "HybridOptimizer", "description": "Enhance mutation strategy and introduce dynamic parameters to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation with dynamic scaling\n                F_dynamic = F * (1 - evaluations / self.budget)\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 57, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11391151778457198, 0.11390278239031626, 0.11390627556501243, 0.07347914313353154, 0.07347597962953056, 0.0734772780432078, 0.09461563035812104, 0.09461024157603248, 0.09461243028583888]}}
{"id": "8ed3f668-2a1d-4698-bdc9-9d0e4f201ce2", "fitness": 0.09400216488178054, "name": "HybridOptimizer", "description": "Improve the HybridOptimizer by adjusting mutation and crossover strategies to better balance exploration and exploitation.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.9  # Differential weight (increased for better exploration)\n        CR = 0.8  # Crossover probability (slightly decreased for continuity)\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 58, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11390837619361871, 0.11391103445154938, 0.11391549272523627, 0.07347802088849709, 0.07347892262406475, 0.07348057148727005, 0.09461370793990065, 0.09461528623092652, 0.09461807139496137]}}
{"id": "c6806b98-579d-4938-b460-2a76b1736fda", "fitness": 0.09400087468534922, "name": "HybridOptimizer", "description": "Introduce adaptive mutation and crossover rates to the HybridOptimizer to dynamically balance exploration and exploitation, enhancing convergence speed and accuracy.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Initial Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        # Adaptive rates\n        min_F, max_F = 0.5, 1.0\n        min_CR, max_CR = 0.1, 0.9\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adapt mutation rate based on progress\n                F = min_F + (max_F - min_F) * (1 - evaluations / self.budget)\n                CR = min_CR + (max_CR - min_CR) * (evaluations / self.budget)\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 59, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11390445559566842, 0.1139090612064454, 0.11391549787703315, 0.07347661468310074, 0.07347821686037848, 0.07348057198207636, 0.09461130295962805, 0.09461407784725095, 0.09461807315656146]}}
{"id": "03a8d718-f2f7-4315-bf5e-4ced8c2129bf", "fitness": 0.09400261096845752, "name": "EnhancedHybridOptimizer", "description": "Integrate adaptive parameter control and local search within HybridOptimizer to improve exploration-exploitation balance and convergence speed.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive Differential Evolution parameters\n        F = 0.5  # Initial Differential weight\n        CR = 0.9  # Initial Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adapt parameters based on generation\n                gen_factor = evaluations / self.budget\n                F = 0.4 + 0.4 * gen_factor\n                CR = 0.9 - 0.4 * gen_factor\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n            # Local Search on the best solution to refine\n            if evaluations < self.budget:\n                local_search_candidate = population[best_idx] + np.random.normal(0, 0.1, self.dim)\n                local_search_candidate = np.clip(local_search_candidate, func.bounds.lb, func.bounds.ub)\n                local_fitness = func(local_search_candidate)\n                evaluations += 1\n\n                if local_fitness < fitness[best_idx]:\n                    population[best_idx] = local_search_candidate\n                    fitness[best_idx] = local_fitness\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 60, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11391746615240828, 0.11389632697084073, 0.11392314854808339, 0.07348126914381803, 0.0734736705547322, 0.07348330395351765, 0.09461927146699178, 0.09460628818752725, 0.0946227537381984]}}
{"id": "68dbc898-8a29-4028-8237-4244625bf2a9", "fitness": 0.09399663869681144, "name": "EnhancedHybridOptimizer", "description": "Introduce adaptive mutation and crossover rates in the HybridOptimizer to dynamically balance exploration and exploitation, aiming for better convergence and solution quality.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        best_idx = np.argmin(fitness)\n        best_fitness = fitness[best_idx]\n\n        # Adaptive DE parameters\n        F_min, F_max = 0.5, 1.0  # Differential weight range\n        CR_min, CR_max = 0.1, 0.9  # Crossover probability range\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adapt F and CR based on progress\n                F = F_min + (F_max - F_min) * (1 - evaluations / self.budget)\n                CR = CR_max - (CR_max - CR_min) * (1 - evaluations / self.budget)\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Update best solution found\n                if trial_fitness < best_fitness:\n                    best_fitness = trial_fitness\n                    best_idx = i\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        return population[best_idx]", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.1139054250785051, 0.11390068473678727, 0.11390354237303302, 0.07347696177375107, 0.0734752236047993, 0.07347630325273069, 0.09461189700507111, 0.0946089510481154, 0.09461075939850994]}}
{"id": "dc294692-e0d7-4718-9151-593a920bf9b9", "fitness": 0.09399693306315973, "name": "HybridOptimizer", "description": "Integrate adaptive differential and crossover rates in HybridOptimizer to improve scalability across diverse optimization landscapes.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.5 + 0.3 * np.random.rand()  # Adaptive differential weight\n        CR = 0.7 + 0.2 * np.random.rand()  # Adaptive crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 62, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11391175063404602, 0.11389464288379447, 0.1139046166159956, 0.07347921879089248, 0.07347305780027491, 0.07347668497981796, 0.09461576507189517, 0.09460524612397747, 0.09461141466774348]}}
{"id": "0fff220c-9434-4921-be33-23e6e0dd797f", "fitness": 0.09400099538044772, "name": "AdaptiveHybridOptimizer", "description": "Implement Adaptive Differential Evolution with Elitism to dynamically adjust parameters and preserve the best solutions for enhanced convergence. ", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive Differential Evolution parameters\n        F_min, F_max = 0.5, 1.0  # Differential weight range\n        CR_min, CR_max = 0.1, 0.9  # Crossover probability range\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            F = np.random.uniform(F_min, F_max)\n            CR = np.random.uniform(CR_min, CR_max)\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 63, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11391150082689427, 0.11391348385141731, 0.11390456521958425, 0.07347914051295579, 0.07347980096392226, 0.07347666914168915, 0.09461562353230413, 0.0946167885498177, 0.0946113858254446]}}
{"id": "294b9837-59b9-40c7-bec3-8c4ccf4c2238", "fitness": 0.09399537712780652, "name": "HybridOptimizer", "description": "Improve HybridOptimizer by tuning differential evolution parameters dynamically and enhancing exploration-exploitation balance.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8\n        CR = 0.9\n\n        while evaluations < self.budget:\n            # Dynamically adjust F\n            F = 0.5 + (0.9 - 0.5) * (self.budget - evaluations) / self.budget\n            best_idx = np.argmin(fitness)\n            \n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 64, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11390103565956977, 0.11389859069438035, 0.11390424671250376, 0.07347539986464324, 0.07347448197652373, 0.07347655505537698, 0.09460921730460636, 0.09460767627778754, 0.09461119060486689]}}
{"id": "25ce7680-f5b6-4330-a569-3c037900ee15", "fitness": 0.09400144888366768, "name": "HybridOptimizer", "description": "Enhance mutation strategy by introducing adaptive differential weight (F) to improve exploration capabilities.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F_min, F_max = 0.4, 0.9  # Adaptive differential weight range\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptive Mutation\n                F = F_min + (F_max - F_min) * (1 - evaluations / self.budget)\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 65, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11391449092035111, 0.11389975127497298, 0.11391738686798192, 0.07348020519320742, 0.07347489498592408, 0.07348124694138602, 0.09461744968069719, 0.09460838483683964, 0.09461922925164878]}}
{"id": "4685f459-fd6f-4b05-a20c-a770a8423607", "fitness": 0.09400089310920098, "name": "HybridOptimizer", "description": "Enhance HybridOptimizer using adaptive population size to improve exploration and convergence efficiency.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = max(int(self.budget / 20), 5)  # Adaptive population size based on budget\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 66, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.1139026866076207, 0.11391696339906077, 0.11390944702349626, 0.07347598968084157, 0.07348103975283504, 0.07347840531028593, 0.09461022762142401, 0.09461891344684692, 0.09461436514039767]}}
{"id": "5137d9a9-fbc9-4814-aa32-854984271057", "fitness": 0.09400127001681258, "name": "AdaptiveHybridOptimizer", "description": "Enhance the HybridOptimizer further by incorporating adaptive parameter tuning for differential evolution, adjusting F and CR dynamically based on population diversity to improve convergence.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Initial Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            # Adaptive parameter adjustment based on population diversity\n            diversity = np.std(population, axis=0).mean()\n            F = np.clip(0.5 + 0.5 * diversity, 0.5, 1.0)\n            CR = np.clip(0.5 + 0.5 * (1.0 - diversity), 0.1, 0.9)\n\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 67, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11390721090843303, 0.113908845606981, 0.11391476100594733, 0.07347760426762273, 0.07347813946888149, 0.073480307701006, 0.09461299450608818, 0.09461394552921609, 0.09461762115713734]}}
{"id": "c190015c-174d-4254-8265-ee182af9fcd4", "fitness": 0.09399590555881492, "name": "HybridOptimizer", "description": "Introduce adaptive mutation and crossover rates to enhance exploration and exploitation balance in HybridOptimizer.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.6  # Adaptive Differential weight initialization\n        CR = 0.7  # Adaptive Crossover probability initialization\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Adaptive Crossover\n                F = 0.5 + 0.2 * np.random.rand()  # Adjust F dynamically\n                CR = 0.5 + 0.4 * np.random.rand()  # Adjust CR dynamically\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 68, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11389633012004252, 0.11390839619337934, 0.1139015633746574, 0.07347371708030459, 0.07347798334624889, 0.07347559836098994, 0.09460633594714107, 0.09461367508521079, 0.09460955052135966]}}
{"id": "d590ffc8-dc9b-4492-a389-3f8a6ebe6750", "fitness": 0.09399342270353694, "name": "HybridOptimizer", "description": "Improve the diversification of the HybridOptimizer by incorporating a mutation strategy based on Levy flights to enhance exploration capabilities.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        def levy_flight(Lambda):\n            # Levy flight formula\n            sigma1 = np.power((np.math.gamma(1 + Lambda) * np.sin(np.pi * Lambda / 2)) /\n                              (np.math.gamma((1 + Lambda) / 2) * Lambda * np.power(2, ((Lambda - 1) / 2))), (1 / Lambda))\n            u = np.random.normal(0, sigma1, size=self.dim)\n            v = np.random.normal(0, 1, size=self.dim)\n            step = u / np.power(np.abs(v), (1 / Lambda))\n            return step\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Levy flight mutation\n                if np.random.rand() < 0.5:\n                    mutant += levy_flight(1.5)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 69, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09399 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11389234768314604, 0.11388600205410127, 0.11391660251138791, 0.07347229373987907, 0.07346997942109057, 0.07348096550074612, 0.09460389824784432, 0.09459996718255104, 0.09461874799108616]}}
{"id": "ce2e16a6-ce78-4f63-a50f-eb14a5b47987", "fitness": 0.09400384579586891, "name": "HybridOptimizer", "description": "Enhance exploitation by reducing the crossover probability dynamically based on the number of evaluations.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                CR_dynamic = CR * (1 - evaluations / self.budget)  # Dynamic crossover reduction\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 70, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11391640688683136, 0.11391578202612651, 0.11391039454603546, 0.0734808904564368, 0.07348062040881898, 0.07347875009462046, 0.09461862297243706, 0.09461819329469268, 0.09461495147682097]}}
{"id": "6c68ed70-ebef-47e8-97ad-b7f4c8f84df0", "fitness": 0.09400037704582703, "name": "HybridOptimizer", "description": "Implement adaptive parameters for differential weight and crossover probability in HybridOptimizer to enhance exploration and exploitation balance, improving convergence rate.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Initial Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptive F and CR\n                F = 0.5 + 0.5 * (1 - evaluations / self.budget)\n                CR = 0.6 + 0.4 * (evaluations / self.budget)\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 71, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11391087119716958, 0.11390356719793593, 0.11391229460992514, 0.07347890998305817, 0.07347626098001869, 0.07347942448079925, 0.09461523244630321, 0.0946107228529286, 0.09461610966430478]}}
{"id": "846090f0-7bd1-4968-8da4-1469ae061be1", "fitness": 0.09399567140322118, "name": "HybridOptimizer", "description": "Augment the HybridOptimizer by incorporating an adaptive differential weight and crossover probability, dynamically adjusting exploration and exploitation during the optimization process.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Initial Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n\n            # Adaptive parameters\n            F = 0.5 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            CR = 0.7 + 0.2 * np.sin(np.pi * evaluations / self.budget)\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 72, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11390643284375168, 0.11389022406928229, 0.11390856709167896, 0.07347732753307534, 0.07347149026480504, 0.07347809591916032, 0.09461251963883432, 0.09460255347253754, 0.09461383179586513]}}
{"id": "7723b8cd-ee81-4bb2-bc89-823c307c9ee7", "fitness": 0.0940009418245629, "name": "HybridOptimizer", "description": "Introduce adaptive scaling of mutation factor in the HybridOptimizer for better exploration and exploitation balance.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F + 0.1 * (1 - evaluations / self.budget)  # Adaptive scaling\n                mutant = np.clip(a + adaptive_F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 73, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.1139124661148897, 0.11389772388052832, 0.11391912349314182, 0.07347947979302827, 0.0734741708052683, 0.07348186746629404, 0.09461620848637431, 0.09460714427614436, 0.09462029210539691]}}
{"id": "8336a595-c0ca-422f-a097-ae63c9c95cac", "fitness": 0.09399957490812548, "name": "HybridOptimizer", "description": "Introduce a fitness-based dynamic scaling factor for mutation in Differential Evolution to enhance convergence speed and solution quality.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 - (fitness[i] - min(fitness)) / (max(fitness) - min(fitness) + 1e-9))\n                mutant = np.clip(a + adaptive_F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 74, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11388994191458934, 0.11391934578228191, 0.11390409004982616, 0.07347857382904754, 0.0734826362893527, 0.07348062692202517, 0.09460800367348965, 0.09462186230816394, 0.09461109340435292]}}
{"id": "dec99ffa-50bc-4c7d-aaaa-777c21958981", "fitness": 0.09399976857750768, "name": "AdaptiveHybridOptimizer", "description": "Introduce adaptive parameter control within the enhanced HybridOptimizer to dynamically adjust mutation and crossover rates, improving adaptability and convergence speed.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Initialize adaptive parameters\n        F = 0.8  # Initial differential weight\n        CR = 0.9  # Initial crossover probability\n        F_decay = 0.99  # Decay factor for F\n        CR_increase = 1.01  # Increase factor for CR\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    # Adjust parameters when a successful trial is found\n                    F = min(1.0, F * F_decay)\n                    CR = min(1.0, CR * CR_increase)\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 75, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11389428340518815, 0.11391705250098128, 0.11391261600548253, 0.07347298597622609, 0.07348107271385595, 0.07347954336476747, 0.09460508354129593, 0.09461896914147083, 0.09461631054830089]}}
{"id": "34958d3a-53b6-4973-bc7e-2586a711e985", "fitness": 0.09399741210663666, "name": "HybridOptimizer", "description": "Enhance convergence by introducing adaptive crossover probability and preserving multiple elite solutions for increased diversity.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover with adaptive CR\n                CR = 0.5 + 0.5 * np.random.rand()  # Adaptive crossover probability\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve multiple best solutions\n            elite_count = 2  # Preserve two best solutions\n            best_indices = np.argsort(fitness)[:elite_count]\n            for idx in best_indices:\n                if evaluations < self.budget:\n                    rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                    rand_fitness = func(rand_ind)\n                    evaluations += 1\n\n                    if rand_fitness < fitness[np.argmax(fitness)]:\n                        worst_idx = np.argmax(fitness)\n                        fitness[worst_idx] = rand_fitness\n                        population[worst_idx] = rand_ind\n\n            # Ensure best solutions are retained\n            for idx in best_indices:\n                population[idx] = population[np.argmin(fitness)]\n                fitness[idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 76, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11391589740909991, 0.11388708182776308, 0.1139102200902482, 0.0734807101986017, 0.07347035144263026, 0.07347868222966125, 0.09461831300794687, 0.0946006137653248, 0.09461483898845391]}}
{"id": "6bf63872-75ea-4f11-a605-c49a266d9c13", "fitness": -Infinity, "name": "HybridOptimizer", "description": "Enhance HybridOptimizer by integrating adaptive mutation and dynamic population sizing to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptive Mutation\n                F_adaptive = 0.5 + 0.3 * np.random.rand()\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F_adaptive * (b - c), func.bounds.lb, func.bounds.ub)  # Use adaptive F\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Dynamic population size\n            if evaluations % 50 == 0:  # Adjust population size every 50 evaluations\n                population_size = min(population_size + 1, 20)\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 77, "feedback": "An exception occurred: IndexError('index 10 is out of bounds for axis 0 with size 10').", "error": "IndexError('index 10 is out of bounds for axis 0 with size 10')", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {}}
{"id": "f80332af-c75b-435e-8c29-cc9cc9b0eed6", "fitness": 0.09400705115708352, "name": "HybridOptimizer", "description": "Enhance the elitism strategy by incorporating a memory mechanism that stores and reuses the best solution across iterations.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.best_solution = None\n        self.best_fitness = float('inf')\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n            # Update the global best solution\n            current_best_fitness = min(fitness)\n            if current_best_fitness < self.best_fitness:\n                self.best_fitness = current_best_fitness\n                self.best_solution = population[np.argmin(fitness)]\n\n        # Return the best solution found\n        return self.best_solution", "configspace": "", "generation": 78, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09401 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11392272460429431, 0.11391411951903274, 0.11392039463881998, 0.0734831458791716, 0.07348002725225711, 0.07348231801619287, 0.0946224874862499, 0.09461717671824177, 0.09462106629949141]}}
{"id": "4e3052e2-1f62-490a-bfe4-910f665afb8f", "fitness": 0.09400281145305853, "name": "EnhancedHybridOptimizer", "description": "Implement a multi-phase approach by incorporating local search around the best solution found and adaptive mutation scaling to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Initial Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            best_solution = population[best_idx]\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptive Mutation based on distance from best solution\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                local_distance = np.linalg.norm(best_solution - population[i])\n                adaptive_F = F * (1 + local_distance / self.dim)\n\n                mutant = np.clip(a + adaptive_F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Local Search around the best solution\n            if evaluations < self.budget:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim) * (func.bounds.ub - func.bounds.lb)\n                local_solution = np.clip(best_solution + perturbation, func.bounds.lb, func.bounds.ub)\n                local_fitness = func(local_solution)\n                evaluations += 1\n\n                if local_fitness < min(fitness):\n                    best_idx = np.argmax(fitness)\n                    population[best_idx] = local_solution\n                    fitness[best_idx] = local_fitness\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 79, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11390932030721534, 0.11392167396685426, 0.11390686659371407, 0.07347835545493397, 0.07348272340107709, 0.07347749036411899, 0.09461428288495255, 0.09462179691924155, 0.09461279318541893]}}
{"id": "4911e26e-f8f1-475e-9eeb-4b667d25b508", "fitness": 0.09400412657829776, "name": "HybridOptimizer", "description": "Introduce random reinitialization for the worst-performing individual to potentially explore unexplored regions and improve diversity.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n            # Random reinitialization for worst-performing individual\n            if evaluations < self.budget:\n                worst_idx = np.argmax(fitness)\n                population[worst_idx] = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                fitness[worst_idx] = func(population[worst_idx])\n                evaluations += 1\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 80, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11391574181462782, 0.11390895067965001, 0.11391917438678079, 0.07348065393725889, 0.07347818284410657, 0.07348188254476162, 0.094618217094245, 0.094614015862442, 0.09462032004080712]}}
{"id": "4c2f4303-5266-4f72-9fde-0e37ff8eef25", "fitness": 0.09399903097401811, "name": "HybridOptimizer", "description": "Introduce adaptive mutation scaling in the HybridOptimizer to enhance exploration and convergence speed.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 - evaluations / self.budget)  # Adaptive mutation scaling\n                mutant = np.clip(a + adaptive_F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 81, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11391151778457198, 0.11390278239031626, 0.11390627556501243, 0.07347914313353154, 0.07347597962953056, 0.0734772780432078, 0.09461563035812104, 0.09461024157603248, 0.09461243028583888]}}
{"id": "e74b672c-cd19-4d34-b95c-77c3cf51f9d3", "fitness": 0.09398632142076573, "name": "HybridOptimizer", "description": "Introduce adaptive control of the crossover probability based on population diversity in HybridOptimizer to improve solution exploration and exploitation balance.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Differential Evolution parameters\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability base\n        delta_CR = 0.1  # Adaptation step for crossover probability\n\n        while evaluations < self.budget:\n            # Compute diversity (standard deviation of the population)\n            diversity = np.mean(np.std(population, axis=0))\n\n            # Adapt CR based on diversity\n            if diversity > 0.1:\n                adaptive_CR = min(1.0, CR + delta_CR)  # Increase CR with high diversity\n            else:\n                adaptive_CR = max(0.0, CR - delta_CR)  # Decrease CR with low diversity\n\n            best_idx = np.argmin(fitness)  # Track best solution index\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 82, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09399 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11388673819712836, 0.11388998971956499, 0.11388573603868268, 0.0734702969547717, 0.0734714158993407, 0.07346995109167653, 0.09460047294597951, 0.09460241972685135, 0.09459987221289567]}}
{"id": "532981b5-dc89-4fa4-8a4b-0de5b9585b3b", "fitness": 0.09400705115708713, "name": "AdaptiveHybridOptimizer", "description": "Introduce adaptive parameter tuning in HybridOptimizer to dynamically adjust mutation and crossover rates based on population diversity, enhancing exploration and convergence.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive parameters\n        F_base = 0.8  # Initial differential weight\n        CR_base = 0.9  # Initial crossover probability\n        diversity_threshold = 0.1\n\n        def calculate_diversity():\n            centroid = np.mean(population, axis=0)\n            diversity = np.mean(np.linalg.norm(population - centroid, axis=1))\n            return diversity\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            diversity = calculate_diversity()\n\n            if diversity < diversity_threshold:\n                F = F_base * (1 + np.random.rand() * 0.5)  # Increase exploration\n                CR = CR_base * (1 - np.random.rand() * 0.5)  # Decrease exploitation\n            else:\n                F = F_base\n                CR = CR_base\n            \n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 83, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09401 with standard deviation 0.01651.", "error": "", "parent_ids": ["ae435c33-40b5-413f-9bd1-29743579a44a"], "operator": null, "metadata": {"aucs": [0.11392272460430508, 0.11391411951903274, 0.11392039463881998, 0.07348314587918237, 0.07348002725225711, 0.07348231801619287, 0.09462248748626079, 0.09461717671824177, 0.09462106629949141]}}
{"id": "90886aa4-2890-4638-bb55-583fa9634a53", "fitness": 0.09400775628417779, "name": "AdaptiveHybridOptimizer", "description": "Introduce a dynamic population size adjustment based on convergence speed to enhance the exploration-exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive parameters\n        F_base = 0.8  # Initial differential weight\n        CR_base = 0.9  # Initial crossover probability\n        diversity_threshold = 0.1\n\n        def calculate_diversity():\n            centroid = np.mean(population, axis=0)\n            diversity = np.mean(np.linalg.norm(population - centroid, axis=1))\n            return diversity\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            diversity = calculate_diversity()\n\n            if diversity < diversity_threshold:\n                F = F_base * (1 + np.random.rand() * 0.5)  # Increase exploration\n                CR = CR_base * (1 - np.random.rand() * 0.5)  # Decrease exploitation\n            else:\n                F = F_base\n                CR = CR_base\n            \n            # Dynamic population adjustment\n            if evaluations % 50 == 0 and evaluations > 0:\n                convergence_rate = np.std(fitness) / np.mean(fitness)\n                if convergence_rate < 0.01:\n                    population = np.vstack((population, np.random.uniform(func.bounds.lb, func.bounds.ub, (5, self.dim))))\n                    fitness = np.concatenate((fitness, [func(ind) for ind in population[-5:]]))\n                    evaluations += 5\n                    population_size += 5\n                   \n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 84, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09401 with standard deviation 0.01651.", "error": "", "parent_ids": ["532981b5-dc89-4fa4-8a4b-0de5b9585b3b"], "operator": null, "metadata": {"aucs": [0.11392272460429431, 0.11391772048506288, 0.11392001788206385, 0.0734831458791716, 0.07348131205531383, 0.07348218359678693, 0.0946224874862499, 0.0946193786437457, 0.0946208359249111]}}
{"id": "b1a83fe6-0229-46cd-a6b0-7e75f3bfdede", "fitness": 0.09400422781369736, "name": "AdaptiveHybridOptimizer", "description": "Introduce an adaptive mutation strategy with historical learning to dynamically adjust parameters based on past performance, enhancing convergence speed and solution quality.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive parameters\n        F_base = 0.8  # Initial differential weight\n        CR_base = 0.9  # Initial crossover probability\n        diversity_threshold = 0.1\n\n        def calculate_diversity():\n            centroid = np.mean(population, axis=0)\n            diversity = np.mean(np.linalg.norm(population - centroid, axis=1))\n            return diversity\n\n        historical_success = []\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            diversity = calculate_diversity()\n\n            # Historical learning for F and CR\n            if historical_success:\n                F = np.mean([s['F'] for s in historical_success]) * (1 + np.random.rand() * 0.1)\n                CR = np.mean([s['CR'] for s in historical_success]) * (1 + np.random.rand() * 0.1)\n            else:\n                F = F_base\n                CR = CR_base\n\n            if diversity < diversity_threshold:\n                F *= (1 + np.random.rand() * 0.5)  # Increase exploration\n                CR *= (1 - np.random.rand() * 0.5)  # Decrease exploitation\n\n            # Dynamic population adjustment\n            if evaluations % 50 == 0 and evaluations > 0:\n                convergence_rate = np.std(fitness) / np.mean(fitness)\n                if convergence_rate < 0.01:\n                    new_members = np.random.uniform(func.bounds.lb, func.bounds.ub, (5, self.dim))\n                    population = np.vstack((population, new_members))\n                    new_fitness = np.array([func(ind) for ind in new_members])\n                    fitness = np.concatenate((fitness, new_fitness))\n                    evaluations += 5\n                    population_size += 5\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    historical_success.append({'F': F, 'CR': CR, 'success': True})\n                else:\n                    historical_success.append({'F': F, 'CR': CR, 'success': False})\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 85, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["90886aa4-2890-4638-bb55-583fa9634a53"], "operator": null, "metadata": {"aucs": [0.11391535762912142, 0.11391123488298771, 0.11391773483014689, 0.07348051667610511, 0.0734789979768441, 0.07348137132450838, 0.09461798197919924, 0.09461541275916208, 0.09461944226520125]}}
{"id": "efdff191-6ef0-498a-a9bd-574b45cd960a", "fitness": 0.09400775628417779, "name": "AdaptiveHybridOptimizer", "description": "Improve diversity management by adapting diversity threshold dynamically.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive parameters\n        F_base = 0.8  # Initial differential weight\n        CR_base = 0.9  # Initial crossover probability\n        diversity_threshold = 0.1\n\n        def calculate_diversity():\n            centroid = np.mean(population, axis=0)\n            diversity = np.mean(np.linalg.norm(population - centroid, axis=1))\n            return diversity\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            diversity = calculate_diversity()\n            diversity_threshold = 0.1 + (0.2 * (1 - (evaluations / self.budget)))  # Dynamically adjust threshold\n\n            if diversity < diversity_threshold:\n                F = F_base * (1 + np.random.rand() * 0.5)  # Increase exploration\n                CR = CR_base * (1 - np.random.rand() * 0.5)  # Decrease exploitation\n            else:\n                F = F_base\n                CR = CR_base\n            \n            # Dynamic population adjustment\n            if evaluations % 50 == 0 and evaluations > 0:\n                convergence_rate = np.std(fitness) / np.mean(fitness)\n                if convergence_rate < 0.01:\n                    population = np.vstack((population, np.random.uniform(func.bounds.lb, func.bounds.ub, (5, self.dim))))\n                    fitness = np.concatenate((fitness, [func(ind) for ind in population[-5:]]))\n                    evaluations += 5\n                    population_size += 5\n                   \n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 86, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09401 with standard deviation 0.01651.", "error": "", "parent_ids": ["90886aa4-2890-4638-bb55-583fa9634a53"], "operator": null, "metadata": {"aucs": [0.11392272460429431, 0.11391772048506288, 0.11392001788206385, 0.0734831458791716, 0.07348131205531383, 0.07348218359678693, 0.0946224874862499, 0.0946193786437457, 0.0946208359249111]}}
{"id": "cb89f8ca-b156-4ab1-ba49-45017ba18198", "fitness": 0.09400775628417779, "name": "AdaptiveHybridOptimizer", "description": "Introduce elite propagation when diversity is low to accelerate convergence by exploiting the best solutions.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive parameters\n        F_base = 0.8  # Initial differential weight\n        CR_base = 0.9  # Initial crossover probability\n        diversity_threshold = 0.1\n\n        def calculate_diversity():\n            centroid = np.mean(population, axis=0)\n            diversity = np.mean(np.linalg.norm(population - centroid, axis=1))\n            return diversity\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            diversity = calculate_diversity()\n\n            if diversity < diversity_threshold:\n                F = F_base * (1 + np.random.rand() * 0.5)  # Increase exploration\n                CR = CR_base * (1 - np.random.rand() * 0.5)  # Decrease exploitation\n                # Elite propagation when diversity is low\n                elite = population[best_idx] + np.random.normal(0, 0.1, self.dim)\n                elite = np.clip(elite, func.bounds.lb, func.bounds.ub)\n                elite_fitness = func(elite)\n                evaluations += 1\n                if elite_fitness < fitness[best_idx]:\n                    population[best_idx] = elite\n                    fitness[best_idx] = elite_fitness\n            else:\n                F = F_base\n                CR = CR_base\n            \n            # Dynamic population adjustment\n            if evaluations % 50 == 0 and evaluations > 0:\n                convergence_rate = np.std(fitness) / np.mean(fitness)\n                if convergence_rate < 0.01:\n                    population = np.vstack((population, np.random.uniform(func.bounds.lb, func.bounds.ub, (5, self.dim))))\n                    fitness = np.concatenate((fitness, [func(ind) for ind in population[-5:]]))\n                    evaluations += 5\n                    population_size += 5\n                   \n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 87, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09401 with standard deviation 0.01651.", "error": "", "parent_ids": ["90886aa4-2890-4638-bb55-583fa9634a53"], "operator": null, "metadata": {"aucs": [0.11392272460429431, 0.11391772048506288, 0.11392001788206385, 0.0734831458791716, 0.07348131205531383, 0.07348218359678693, 0.0946224874862499, 0.0946193786437457, 0.0946208359249111]}}
{"id": "20237a39-675c-4d7a-9c65-99cbfa4842a3", "fitness": 0.09400396530843896, "name": "MultiSwarmCooperativeOptimizer", "description": "Implement a multi-swarm cooperative strategy with dynamic regrouping to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass MultiSwarmCooperativeOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_swarms = 3\n        swarm_size = 5\n        population_size = num_swarms * swarm_size\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive parameters\n        F_base = 0.8\n        CR_base = 0.9\n        diversity_threshold = 0.1\n\n        def calculate_diversity(swarm):\n            centroid = np.mean(swarm, axis=0)\n            diversity = np.mean(np.linalg.norm(swarm - centroid, axis=1))\n            return diversity\n\n        while evaluations < self.budget:\n            # Iterate over each swarm\n            for swarm_idx in range(num_swarms):\n                start, end = swarm_idx * swarm_size, (swarm_idx + 1) * swarm_size\n                swarm = population[start:end]\n                swarm_fitness = fitness[start:end]\n\n                best_idx_swarm = np.argmin(swarm_fitness)\n                diversity = calculate_diversity(swarm)\n\n                if diversity < diversity_threshold:\n                    F = F_base * (1 + np.random.rand() * 0.5)\n                    CR = CR_base * (1 - np.random.rand() * 0.5)\n                else:\n                    F = F_base\n                    CR = CR_base\n\n                # Perform DE for the swarm\n                for i in range(swarm_size):\n                    if evaluations >= self.budget:\n                        break\n\n                    idxs = [idx for idx in range(swarm_size) if idx != i]\n                    a, b, c = swarm[np.random.choice(idxs, 3, replace=False)]\n                    mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n\n                    trial = np.where(cross_points, mutant, swarm[i])\n                    trial_fitness = func(trial)\n                    evaluations += 1\n\n                    if trial_fitness < swarm_fitness[i]:\n                        swarm[i] = trial\n                        swarm_fitness[i] = trial_fitness\n\n                # Elitism and update global population\n                best_idx_global = np.argmin(fitness)\n                if evaluations < self.budget:\n                    rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                    rand_fitness = func(rand_ind)\n                    evaluations += 1\n\n                    if rand_fitness < fitness[np.argmax(fitness)]:\n                        worst_idx = np.argmax(fitness)\n                        fitness[worst_idx] = rand_fitness\n                        population[worst_idx] = rand_ind\n\n                population[start:end] = swarm\n                fitness[start:end] = swarm_fitness\n                population[best_idx_global] = population[np.argmin(fitness)]\n                fitness[best_idx_global] = min(fitness)\n\n        # Return best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 88, "feedback": "The algorithm MultiSwarmCooperativeOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["90886aa4-2890-4638-bb55-583fa9634a53"], "operator": null, "metadata": {"aucs": [0.11390625839004409, 0.11391408934474001, 0.11392279426634244, 0.0734772606742864, 0.07348001330483578, 0.07348317458388265, 0.09461240819530459, 0.09461715497981216, 0.09462253403670251]}}
{"id": "c80ac2b4-37e2-407d-9618-f466e6b865ac", "fitness": 0.09400775628417779, "name": "AdaptiveHybridOptimizer", "description": "Add a mechanism to diversify population by randomizing individuals every 100 evaluations to avoid premature convergence.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive parameters\n        F_base = 0.8  # Initial differential weight\n        CR_base = 0.9  # Initial crossover probability\n        diversity_threshold = 0.1\n\n        def calculate_diversity():\n            centroid = np.mean(population, axis=0)\n            diversity = np.mean(np.linalg.norm(population - centroid, axis=1))\n            return diversity\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            diversity = calculate_diversity()\n\n            if diversity < diversity_threshold:\n                F = F_base * (1 + np.random.rand() * 0.5)  # Increase exploration\n                CR = CR_base * (1 - np.random.rand() * 0.5)  # Decrease exploitation\n            else:\n                F = F_base\n                CR = CR_base\n            \n            # Dynamic population adjustment\n            if evaluations % 50 == 0 and evaluations > 0:\n                convergence_rate = np.std(fitness) / np.mean(fitness)\n                if convergence_rate < 0.01:\n                    population = np.vstack((population, np.random.uniform(func.bounds.lb, func.bounds.ub, (5, self.dim))))\n                    fitness = np.concatenate((fitness, [func(ind) for ind in population[-5:]]))\n                    evaluations += 5\n                    population_size += 5\n\n            if evaluations % 100 == 0:  # New diversification mechanism\n                random_indices = np.random.choice(range(population_size), 2, replace=False)\n                for idx in random_indices:\n                    population[idx] = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                    fitness[idx] = func(population[idx])\n                    evaluations += 1\n                   \n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 89, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09401 with standard deviation 0.01651.", "error": "", "parent_ids": ["90886aa4-2890-4638-bb55-583fa9634a53"], "operator": null, "metadata": {"aucs": [0.11392272460429431, 0.11391772048506288, 0.11392001788206385, 0.0734831458791716, 0.07348131205531383, 0.07348218359678693, 0.0946224874862499, 0.0946193786437457, 0.0946208359249111]}}
{"id": "91365396-1e6c-47dc-aabb-339929ba242f", "fitness": -Infinity, "name": "AdaptiveHybridOptimizer", "description": "Enhance convergence by incorporating a dynamic mutation strategy based on historical success rates.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive parameters\n        F_base = 0.8\n        CR_base = 0.9\n        diversity_threshold = 0.1\n        success_rates = np.ones(population_size)  # Track success rates\n\n        def calculate_diversity():\n            centroid = np.mean(population, axis=0)\n            diversity = np.mean(np.linalg.norm(population - centroid, axis=1))\n            return diversity\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)\n            diversity = calculate_diversity()\n\n            if diversity < diversity_threshold:\n                F = F_base * (1 + np.random.rand() * 0.5)  \n                CR = CR_base * (1 - np.random.rand() * 0.5)\n            else:\n                F = F_base\n                CR = CR_base\n            \n            if evaluations % 50 == 0 and evaluations > 0:\n                convergence_rate = np.std(fitness) / np.mean(fitness)\n                if convergence_rate < 0.01:\n                    population = np.vstack((population, np.random.uniform(func.bounds.lb, func.bounds.ub, (5, self.dim))))\n                    fitness = np.concatenate((fitness, [func(ind) for ind in population[-5:]]))\n                    evaluations += 5\n                    population_size += 5\n                   \n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                dynamic_F = F * (1 + 0.1 * (1 - success_rates[i]))  # Dynamic mutation strength\n                mutant = np.clip(a + dynamic_F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    success_rates[i] *= 1.1  # Increase success rate\n                else:\n                    success_rates[i] *= 0.9  # Decrease success rate\n\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 90, "feedback": "An exception occurred: IndexError('index 10 is out of bounds for axis 0 with size 10').", "error": "IndexError('index 10 is out of bounds for axis 0 with size 10')", "parent_ids": ["90886aa4-2890-4638-bb55-583fa9634a53"], "operator": null, "metadata": {}}
{"id": "bc849aef-89ad-4186-8b74-ef8dbf455fc5", "fitness": 0.09400719771800983, "name": "ImprovedAdaptiveHybridOptimizer", "description": "Introduce a feedback mechanism to adaptively adjust mutation and crossover parameters based on individual performance to enhance convergence.", "code": "import numpy as np\n\nclass ImprovedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive parameters with feedback mechanism\n        F_base = 0.8  # Initial differential weight\n        CR_base = 0.9  # Initial crossover probability\n        diversity_threshold = 0.1\n\n        def calculate_diversity():\n            centroid = np.mean(population, axis=0)\n            diversity = np.mean(np.linalg.norm(population - centroid, axis=1))\n            return diversity\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            diversity = calculate_diversity()\n            \n            # Adjust parameters based on diversity\n            if diversity < diversity_threshold:\n                F = F_base * (1 + np.random.rand() * 0.5)  # Increase exploration\n                CR = CR_base * (1 - np.random.rand() * 0.5)  # Decrease exploitation\n            else:\n                F = F_base\n                CR = CR_base\n            \n            # Dynamic population and parameter adjustment\n            if evaluations % 50 == 0 and evaluations > 0:\n                convergence_rate = np.std(fitness) / np.mean(fitness)\n                if convergence_rate < 0.01:\n                    new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, (5, self.dim))\n                    new_fitness = np.array([func(ind) for ind in new_population])\n                    evaluations += 5\n                    population = np.vstack((population, new_population))\n                    fitness = np.concatenate((fitness, new_fitness))\n                    population_size += 5\n\n                # Feedback mechanism for adaptive F and CR\n                improvement = np.std(fitness) / (np.mean(fitness) + 1e-9)\n                F *= (1.0 + improvement)\n                CR *= (1.0 - improvement)\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 91, "feedback": "The algorithm ImprovedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09401 with standard deviation 0.01651.", "error": "", "parent_ids": ["90886aa4-2890-4638-bb55-583fa9634a53"], "operator": null, "metadata": {"aucs": [0.11392272460429431, 0.1139150091148382, 0.11392001633913129, 0.0734831458791716, 0.07348021444329556, 0.07348218194900158, 0.09462308150357401, 0.09461757252783476, 0.09462083310094715]}}
{"id": "a19bb9d1-f4df-4c21-8992-6d985e88623a", "fitness": 0.0940000721851331, "name": "EnhancedAdaptiveHybridOptimizer", "description": "Incorporate adaptive memory and self-adaptive parameter control to enhance convergence through informed decision making.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive parameters\n        F_memory = [0.8] * 5  # Memory for differential weight\n        CR_memory = [0.9] * 5  # Memory for crossover probability\n        diversity_threshold = 0.1\n        memory_index = 0\n\n        def calculate_diversity():\n            centroid = np.mean(population, axis=0)\n            diversity = np.mean(np.linalg.norm(population - centroid, axis=1))\n            return diversity\n\n        def update_memory(memory, value):\n            memory[memory_index] = value\n            return sum(memory) / len(memory)\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)\n            diversity = calculate_diversity()\n\n            if diversity < diversity_threshold:\n                F = np.random.choice(F_memory) * (1 + np.random.rand() * 0.5)\n                CR = np.random.choice(CR_memory) * (1 - np.random.rand() * 0.5)\n            else:\n                F = np.random.choice(F_memory)\n                CR = np.random.choice(CR_memory)\n            \n            # Dynamic population adjustment\n            if evaluations % 50 == 0 and evaluations > 0:\n                convergence_rate = np.std(fitness) / np.mean(fitness)\n                if convergence_rate < 0.01:\n                    new_members = np.random.uniform(func.bounds.lb, func.bounds.ub, (5, self.dim))\n                    population = np.vstack((population, new_members))\n                    fitness = np.concatenate((fitness, [func(ind) for ind in new_members]))\n                    evaluations += 5\n                    population_size += 5\n                   \n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    # Update memory\n                    F_memory[memory_index] = update_memory(F_memory, F)\n                    CR_memory[memory_index] = update_memory(CR_memory, CR)\n                    memory_index = (memory_index + 1) % len(F_memory)\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["90886aa4-2890-4638-bb55-583fa9634a53"], "operator": null, "metadata": {"aucs": [0.1139031746671596, 0.11391901688375661, 0.11390314283351044, 0.07347616605008822, 0.0734817746358225, 0.07347616022523074, 0.09461052836996275, 0.09462017140245937, 0.09461051459820757]}}
{"id": "259f4c4d-e88e-4acb-93d0-525acbc01687", "fitness": 0.09400152972206778, "name": "AdaptiveHybridOptimizer", "description": "Enhance the crossover mechanism by dynamically adjusting the crossover probability based on population diversity to improve balance between exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive parameters\n        F_base = 0.8  # Initial differential weight\n        CR_base = 0.9  # Initial crossover probability\n        diversity_threshold = 0.1\n\n        def calculate_diversity():\n            centroid = np.mean(population, axis=0)\n            diversity = np.mean(np.linalg.norm(population - centroid, axis=1))\n            return diversity\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            diversity = calculate_diversity()\n\n            if diversity < diversity_threshold:\n                F = F_base * (1 + np.random.rand() * 0.5)  # Increase exploration\n                CR = CR_base * (1 - np.random.rand() * 0.5)  # Decrease exploitation\n            else:\n                F = F_base\n                CR = CR_base * (diversity / (diversity + 1))  # Adjust CR based on diversity\n            \n            # Dynamic population adjustment\n            if evaluations % 50 == 0 and evaluations > 0:\n                convergence_rate = np.std(fitness) / np.mean(fitness)\n                if convergence_rate < 0.01:\n                    population = np.vstack((population, np.random.uniform(func.bounds.lb, func.bounds.ub, (5, self.dim))))\n                    fitness = np.concatenate((fitness, [func(ind) for ind in population[-5:]]))\n                    evaluations += 5\n                    population_size += 5\n                   \n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 93, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["90886aa4-2890-4638-bb55-583fa9634a53"], "operator": null, "metadata": {"aucs": [0.11390920088113643, 0.11390974438625245, 0.11391304977607297, 0.07347831787227221, 0.07347846609686182, 0.0734796972688776, 0.09461421505275314, 0.09461450126483983, 0.09461657489954356]}}
{"id": "ce5cf8d3-2442-4dae-9b39-9717057db17a", "fitness": 0.09400775628417779, "name": "EnhancedAdaptiveHybridOptimizer", "description": "Improve the algorithm by incorporating adaptive learning rates and a variable mutation scheme to enhance exploration capabilities.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive parameters\n        F_base = 0.8  # Initial differential weight\n        CR_base = 0.9  # Initial crossover probability\n        diversity_threshold = 0.1\n        learning_rate = 0.1  # Adaptive learning rate\n\n        def calculate_diversity():\n            centroid = np.mean(population, axis=0)\n            diversity = np.mean(np.linalg.norm(population - centroid, axis=1))\n            return diversity\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            diversity = calculate_diversity()\n\n            if diversity < diversity_threshold:\n                F = F_base * (1 + np.random.rand() * 0.5)  # Increase exploration\n                CR = CR_base * (1 - np.random.rand() * 0.5)  # Decrease exploitation\n            else:\n                F = F_base\n                CR = CR_base\n            \n            # Dynamic population adjustment\n            if evaluations % 50 == 0 and evaluations > 0:\n                convergence_rate = np.std(fitness) / np.mean(fitness)\n                if convergence_rate < 0.01:\n                    additional_population_size = max(5, int(learning_rate * population_size))\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, (additional_population_size, self.dim))\n                    new_fitness = np.array([func(ind) for ind in new_individuals])\n                    population = np.vstack((population, new_individuals))\n                    fitness = np.concatenate((fitness, new_fitness))\n                    evaluations += additional_population_size\n                    population_size += additional_population_size\n                    learning_rate *= 0.9  # Decay learning rate to adapt slower\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation with variable scheme\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 94, "feedback": "The algorithm EnhancedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09401 with standard deviation 0.01651.", "error": "", "parent_ids": ["90886aa4-2890-4638-bb55-583fa9634a53"], "operator": null, "metadata": {"aucs": [0.11392272460429431, 0.11391772048506288, 0.11392001788206385, 0.0734831458791716, 0.07348131205531383, 0.07348218359678693, 0.0946224874862499, 0.0946193786437457, 0.0946208359249111]}}
{"id": "7eaa54d9-197e-43af-b2d9-0163c584a940", "fitness": 0.09400177161551851, "name": "AdaptiveHybridOptimizer", "description": "Enhance adaptive mechanisms by introducing random elitism to improve convergence diversity.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive parameters\n        F_base = 0.8  # Initial differential weight\n        CR_base = 0.9  # Initial crossover probability\n        diversity_threshold = 0.1\n\n        def calculate_diversity():\n            centroid = np.mean(population, axis=0)\n            diversity = np.mean(np.linalg.norm(population - centroid, axis=1))\n            return diversity\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            diversity = calculate_diversity()\n\n            if diversity < diversity_threshold:\n                F = F_base * (1 + np.random.rand() * 0.5)  # Increase exploration\n                CR = CR_base * (1 - np.random.rand() * 0.5)  # Decrease exploitation\n            else:\n                F = F_base\n                CR = CR_base\n            \n            # Dynamic population adjustment\n            if evaluations % 50 == 0 and evaluations > 0:\n                convergence_rate = np.std(fitness) / np.mean(fitness)\n                if convergence_rate < 0.01:\n                    population = np.vstack((population, np.random.uniform(func.bounds.lb, func.bounds.ub, (5, self.dim))))\n                    fitness = np.concatenate((fitness, [func(ind) for ind in population[-5:]]))\n                    evaluations += 5\n                    population_size += 5\n                   \n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Random elitism: Occasionally replace a random individual with the best\n            if evaluations < self.budget:\n                rand_replace_idx = np.random.randint(population_size)\n                population[rand_replace_idx] = population[best_idx]\n                fitness[rand_replace_idx] = fitness[best_idx]\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 95, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["90886aa4-2890-4638-bb55-583fa9634a53"], "operator": null, "metadata": {"aucs": [0.11391198673327585, 0.11390495950812962, 0.1139175676898575, 0.07347836793703688, 0.07347675856811287, 0.07348008868174738, 0.09461739339382946, 0.09461157507459983, 0.09461724695307716]}}
{"id": "d09f1907-b0f2-4bd1-8941-827a7b7c32d8", "fitness": 0.09400270742728017, "name": "AdaptiveHybridOptimizer", "description": "Enhance mutation strategy by introducing dynamic scaling to improve exploration.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive parameters\n        F_base = 0.8  # Initial differential weight\n        CR_base = 0.9  # Initial crossover probability\n        diversity_threshold = 0.1\n\n        def calculate_diversity():\n            centroid = np.mean(population, axis=0)\n            diversity = np.mean(np.linalg.norm(population - centroid, axis=1))\n            return diversity\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            diversity = calculate_diversity()\n\n            if diversity < diversity_threshold:\n                F = F_base * (1 + np.random.rand() * 0.5)  # Increase exploration\n                CR = CR_base * (1 - np.random.rand() * 0.5)  # Decrease exploitation\n            else:\n                F = F_base\n                CR = CR_base\n            \n            # Dynamic population adjustment\n            if evaluations % 50 == 0 and evaluations > 0:\n                convergence_rate = np.std(fitness) / np.mean(fitness)\n                if convergence_rate < 0.01:\n                    population = np.vstack((population, np.random.uniform(func.bounds.lb, func.bounds.ub, (5, self.dim))))\n                    fitness = np.concatenate((fitness, [func(ind) for ind in population[-5:]]))\n                    evaluations += 5\n                    population_size += 5\n                   \n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_dynamic = F * (1 + np.sin(np.pi * evaluations / self.budget))  # Dynamic scaling\n                mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 96, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["90886aa4-2890-4638-bb55-583fa9634a53"], "operator": null, "metadata": {"aucs": [0.1139082768764762, 0.11391184789540965, 0.11391725957827403, 0.0734779840950025, 0.07347921670771151, 0.0734811992289881, 0.0946136458047303, 0.09461578761669243, 0.09461914904223678]}}
{"id": "44113f11-4acc-4366-9214-7680d38f1abf", "fitness": 0.09399182447700925, "name": "AdaptiveHybridOptimizer", "description": "Introduce self-tuning of the crossover probability to dynamically adjust based on diversity, enhancing convergence efficiency.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive parameters\n        F_base = 0.8  # Initial differential weight\n        CR_base = 0.9  # Initial crossover probability\n        diversity_threshold = 0.1\n\n        def calculate_diversity():\n            centroid = np.mean(population, axis=0)\n            diversity = np.mean(np.linalg.norm(population - centroid, axis=1))\n            return diversity\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            diversity = calculate_diversity()\n\n            if diversity < diversity_threshold:\n                F = F_base * (1 + np.random.rand() * 0.5)  # Increase exploration\n                CR = CR_base * (1 - np.random.rand() * 0.5)  # Decrease exploitation\n            else:\n                F = F_base\n                CR = CR_base * (1 + 0.1 * (diversity / diversity_threshold))  # Adjust CR based on diversity\n            \n            # Dynamic population adjustment\n            if evaluations % 50 == 0 and evaluations > 0:\n                convergence_rate = np.std(fitness) / np.mean(fitness)\n                if convergence_rate < 0.01:\n                    population = np.vstack((population, np.random.uniform(func.bounds.lb, func.bounds.ub, (5, self.dim))))\n                    fitness = np.concatenate((fitness, [func(ind) for ind in population[-5:]]))\n                    evaluations += 5\n                    population_size += 5\n                   \n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 97, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09399 with standard deviation 0.01651.", "error": "", "parent_ids": ["90886aa4-2890-4638-bb55-583fa9634a53"], "operator": null, "metadata": {"aucs": [0.11390403861747633, 0.11389785020188137, 0.11388573603868268, 0.07347647090934717, 0.07347422112432656, 0.07346995109167653, 0.09461105315246021, 0.09460722694433676, 0.09459987221289567]}}
{"id": "e64985e3-d814-4f2b-9333-fb116bd24c7e", "fitness": 0.09400374704867874, "name": "EnhancedAdaptiveHybridOptimizer", "description": "Enhance adaptive parameter tuning and introduce a local search step to improve convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive parameters\n        F_base = 0.8\n        CR_base = 0.9\n        diversity_threshold = 0.1\n\n        def calculate_diversity():\n            centroid = np.mean(population, axis=0)\n            return np.mean(np.linalg.norm(population - centroid, axis=1))\n\n        def local_search(individual):\n            step_size = (func.bounds.ub - func.bounds.lb) * 0.01\n            neighbor = np.clip(individual + np.random.uniform(-step_size, step_size), func.bounds.lb, func.bounds.ub)\n            return neighbor\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)\n            diversity = calculate_diversity()\n\n            if diversity < diversity_threshold:\n                F = F_base * (1 + np.random.rand() * 0.5)\n                CR = CR_base * (1 - np.random.rand() * 0.5)\n            else:\n                F = F_base\n                CR = CR_base\n            \n            if evaluations % 50 == 0 and evaluations > 0:\n                convergence_rate = np.std(fitness) / np.mean(fitness)\n                if convergence_rate < 0.01:\n                    new_inds = np.random.uniform(func.bounds.lb, func.bounds.ub, (5, self.dim))\n                    new_fitness = np.array([func(ind) for ind in new_inds])\n                    evaluations += 5\n                    population = np.vstack((population, new_inds))\n                    fitness = np.concatenate((fitness, new_fitness))\n                    population_size += 5\n                   \n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            if evaluations < self.budget:\n                local_idx = np.random.randint(population_size)\n                local_candidate = local_search(population[local_idx])\n                local_fitness = func(local_candidate)\n                evaluations += 1\n\n                if local_fitness < fitness[local_idx]:\n                    population[local_idx] = local_candidate\n                    fitness[local_idx] = local_fitness\n\n            best_idx = np.argmin(fitness)\n            population[best_idx] = population[best_idx]\n            fitness[best_idx] = min(fitness)\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 98, "feedback": "The algorithm EnhancedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09400 with standard deviation 0.01651.", "error": "", "parent_ids": ["90886aa4-2890-4638-bb55-583fa9634a53"], "operator": null, "metadata": {"aucs": [0.11391619900775274, 0.11391551413678425, 0.11391041864201501, 0.07348081729112466, 0.07348052482834755, 0.07347875782993729, 0.09461849689757629, 0.09461802948605968, 0.09461496531851121]}}
{"id": "af5f0ea5-dc43-4650-af55-a968e7b732bf", "fitness": 0.09400705115708352, "name": "EnhancedAdaptiveHybridOptimizer", "description": "Introduce adaptive mutation strategies and diversity-driven reinitialization to enhance convergence and exploration balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize population\n        population_size = 10\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n\n        # Adaptive parameters\n        F_base = 0.8  # Initial differential weight\n        CR_base = 0.9  # Initial crossover probability\n        diversity_threshold = 0.1\n\n        def calculate_diversity():\n            centroid = np.mean(population, axis=0)\n            diversity = np.mean(np.linalg.norm(population - centroid, axis=1))\n            return diversity\n\n        while evaluations < self.budget:\n            best_idx = np.argmin(fitness)  # Track best solution index\n            diversity = calculate_diversity()\n\n            if diversity < diversity_threshold:\n                F = F_base * (1 + np.random.rand() * 0.5)  # Increase exploration\n                CR = CR_base * (1 - np.random.rand() * 0.5)  # Decrease exploitation\n                # Re-initialize part of the population for diversity\n                reinit_indices = np.random.choice(population_size, size=2, replace=False)\n                for idx in reinit_indices:\n                    new_individual = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                    population[idx] = new_individual\n                    fitness[idx] = func(new_individual)\n                    evaluations += 1 if evaluations < self.budget else 0\n            else:\n                F = F_base\n                CR = CR_base\n            \n            # Dynamic mutation strategy\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Elitism: Preserve best solution\n            if evaluations < self.budget:\n                rand_ind = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                rand_fitness = func(rand_ind)\n                evaluations += 1\n\n                if rand_fitness < fitness[np.argmax(fitness)]:\n                    worst_idx = np.argmax(fitness)\n                    fitness[worst_idx] = rand_fitness\n                    population[worst_idx] = rand_ind\n\n            # Ensure best solution is retained\n            population[best_idx] = population[np.argmin(fitness)]\n            fitness[best_idx] = min(fitness)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 99, "feedback": "The algorithm EnhancedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09401 with standard deviation 0.01651.", "error": "", "parent_ids": ["90886aa4-2890-4638-bb55-583fa9634a53"], "operator": null, "metadata": {"aucs": [0.11392272460429431, 0.11391411951903274, 0.11392039463881998, 0.0734831458791716, 0.07348002725225711, 0.07348231801619287, 0.0946224874862499, 0.09461717671824177, 0.09462106629949141]}}
