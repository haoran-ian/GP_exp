{"id": "d83aceba-e77d-41be-a2d9-14deb78580da", "fitness": 0.053770298217453175, "name": "HybridDELocalSearchOptimizer", "description": "A novel metaheuristic algorithm combining Differential Evolution with Local Search to exploit both global exploration and local refinement for efficient optimization of black box functions.", "code": "import numpy as np\n\nclass HybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.7\n        self.local_search_perturbation = 0.1\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                else:\n                    trial_population[i] = population[i]\n\n                # Local Search\n                if evals < self.budget:\n                    local_trial = trial + self.local_search_perturbation * np.random.randn(self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05377 with standard deviation 0.00285.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.04851325121309613, 0.05388229050317539, 0.052154194402423304, 0.05135930122075871, 0.057050112876547376, 0.05521751060746427, 0.0520253367929957, 0.057794350078631096, 0.055936336261986574]}}
{"id": "eedbe37b-1905-4501-b5b4-5ffb7b90eee8", "fitness": 0.057209368702053495, "name": "EnhancedHybridDELocalSearchOptimizer", "description": "An enhanced metaheuristic algorithm integrating Adaptive Differential Evolution and Stochastic Local Search to dynamically adjust exploration and exploitation, optimizing black box functions more efficiently.", "code": "import numpy as np\n\nclass EnhancedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Adaptive Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Stochastic Local Search\n                if evals < self.budget:\n                    local_trial = trial + self.local_search_perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 1, "feedback": "The algorithm EnhancedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05721 with standard deviation 0.00317.", "error": "", "parent_ids": ["d83aceba-e77d-41be-a2d9-14deb78580da"], "operator": null, "metadata": {"aucs": [0.05206686491999468, 0.0581541917611037, 0.05416775595807277, 0.05512831814929908, 0.061614857260963274, 0.05736189251476198, 0.055846747961487586, 0.062430991138836656, 0.058112698653961736]}}
{"id": "2a20574d-265c-4768-a5c2-43339841ff6e", "fitness": 0.05654117985024129, "name": "EnhancedHybridDELocalSearchOptimizer", "description": "Enhanced metaheuristic algorithm with a refined local search perturbation factor to balance exploration and exploitation more effectively.", "code": "import numpy as np\n\nclass EnhancedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.1  # Adjusted perturbation factor\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Adaptive Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Stochastic Local Search\n                if evals < self.budget:\n                    local_trial = trial + self.local_search_perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 2, "feedback": "The algorithm EnhancedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05654 with standard deviation 0.00232.", "error": "", "parent_ids": ["eedbe37b-1905-4501-b5b4-5ffb7b90eee8"], "operator": null, "metadata": {"aucs": [0.05393471572355524, 0.05609015679985774, 0.05244043909936147, 0.05714068656266125, 0.05940560025738428, 0.05552678021137292, 0.0578949283119109, 0.06018600563856935, 0.056251306047498506]}}
{"id": "35a9df28-6ef8-49e4-8c46-3444093017c4", "fitness": 0.058392149523595824, "name": "EnhancedHybridDELocalSearchOptimizer", "description": "A refined optimization algorithm improving Adaptive Differential Evolution by adjusting mutation factor and local search perturbation based on fitness variance, enhancing convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Adaptive Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Stochastic Local Search\n                if evals < self.budget:\n                    local_trial = trial + (self.local_search_perturbation + 0.01 * fitness_variance) * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05839 with standard deviation 0.00315.", "error": "", "parent_ids": ["eedbe37b-1905-4501-b5b4-5ffb7b90eee8"], "operator": null, "metadata": {"aucs": [0.05469230654560797, 0.05938290881826702, 0.05369366691976907, 0.05792855801019958, 0.06292571516161405, 0.05685573611727146, 0.058689801292920674, 0.06376197435276088, 0.05759867849395173]}}
{"id": "8722d4a8-ae7a-4cba-a345-109ed135a16a", "fitness": 0.05698145183016264, "name": "EnhancedHybridDELocalSearchOptimizer", "description": "Enhanced Adaptive Differential Evolution with refined local search perturbation to improve convergence.", "code": "import numpy as np\n\nclass EnhancedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Adaptive Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Stochastic Local Search\n                if evals < self.budget:\n                    local_trial = trial + (self.local_search_perturbation + 0.02 * fitness_variance) * np.random.normal(size=self.dim) # Changed line\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05698 with standard deviation 0.00333.", "error": "", "parent_ids": ["35a9df28-6ef8-49e4-8c46-3444093017c4"], "operator": null, "metadata": {"aucs": [0.051823506361465044, 0.058267258840086544, 0.0536461733116933, 0.05486920348282254, 0.061736241672484926, 0.0568051432431288, 0.055583782563007667, 0.06255442925379773, 0.05754732774297722]}}
{"id": "2131c558-c0ee-4bb9-a195-8edcde7537f9", "fitness": 0.05878463555802966, "name": "EnhancedHybridDELocalSearchOptimizer", "description": "Enhanced mutation factor adaptation in the DE algorithm to boost convergence.", "code": "import numpy as np\n\nclass EnhancedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Adaptive Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)  # Changed line\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Stochastic Local Search\n                if evals < self.budget:\n                    local_trial = trial + (self.local_search_perturbation + 0.01 * fitness_variance) * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05878 with standard deviation 0.00344.", "error": "", "parent_ids": ["35a9df28-6ef8-49e4-8c46-3444093017c4"], "operator": null, "metadata": {"aucs": [0.05258208555028043, 0.056980538319672425, 0.0592998281350422, 0.05567722511933959, 0.06036154487971834, 0.06287655098323985, 0.05640389348252628, 0.06115813666889225, 0.06372191688355555]}}
{"id": "a41e971e-7728-4a17-9de0-63641e6c9977", "fitness": 0.05422848247680523, "name": "EnhancedHybridDELocalSearchOptimizer", "description": "Introduce adaptive crossover probability based on fitness improvement to further enhance convergence.", "code": "import numpy as np\n\nclass EnhancedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Adaptive Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)  # Changed line\n                    self.crossover_probability = min(1.0, self.crossover_probability + self.adaptation_rate * 0.1)  # Added line\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Stochastic Local Search\n                if evals < self.budget:\n                    local_trial = trial + (self.local_search_perturbation + 0.01 * fitness_variance) * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05423 with standard deviation 0.00270.", "error": "", "parent_ids": ["2131c558-c0ee-4bb9-a195-8edcde7537f9"], "operator": null, "metadata": {"aucs": [0.04910594713823202, 0.05384253258476679, 0.05291305806779967, 0.05198526844016338, 0.05700763101255191, 0.05602971158077208, 0.0526593531791586, 0.0577511997035729, 0.05676164058422972]}}
{"id": "9e0e07f3-0904-43eb-a786-9d2a2a7f51ce", "fitness": 0.054717650326105045, "name": "EnhancedHybridDELocalSearchOptimizer", "description": "Incorporating dynamic population resizing and elitism to enhance convergence speed and solution quality in DE.", "code": "import numpy as np\n\nclass EnhancedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n        self.dynamic_population_shrink = 0.95\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            num_elites = max(1, int(self.elitism_rate * population_size))\n            elites = population[np.argsort(fitness)[:num_elites]]\n\n            for i in range(num_elites, population_size):\n                if evals >= self.budget:\n                    break\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                if evals < self.budget:\n                    local_trial = trial + (self.local_search_perturbation + 0.01 * fitness_variance) * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            trial_population[:num_elites] = elites\n            fitness[:num_elites] = [func(ind) for ind in elites]\n            population = trial_population\n            population_size = int(population_size * self.dynamic_population_shrink)\n            population = population[:population_size]\n            fitness = fitness[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05472 with standard deviation 0.00455.", "error": "", "parent_ids": ["2131c558-c0ee-4bb9-a195-8edcde7537f9"], "operator": null, "metadata": {"aucs": [0.051402773135513447, 0.05780106501642601, 0.04804423845773409, 0.05442476638587712, 0.061229853658117506, 0.05086260858654301, 0.05513361056869037, 0.06203800956134564, 0.0515219275646982]}}
{"id": "88030c8c-5d2e-48a3-ae42-a0796319e899", "fitness": 0.05317764158734627, "name": "EnhancedHybridDELocalSearchOptimizer", "description": "Enhance early convergence by dynamically adjusting the crossover probability based on fitness improvement.", "code": "import numpy as np\n\nclass EnhancedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Adaptive Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                    self.crossover_probability = min(1.0, self.crossover_probability + self.adaptation_rate * 0.5)  # Changed line\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Stochastic Local Search\n                if evals < self.budget:\n                    local_trial = trial + (self.local_search_perturbation + 0.01 * fitness_variance) * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05318 with standard deviation 0.00267.", "error": "", "parent_ids": ["2131c558-c0ee-4bb9-a195-8edcde7537f9"], "operator": null, "metadata": {"aucs": [0.04944416728962009, 0.05383223084231559, 0.04957100478020782, 0.052343572157354745, 0.056996669166707536, 0.05248318015007947, 0.05302253440104798, 0.05774007666218084, 0.05316533883660235]}}
{"id": "fd6b6d28-fd81-4620-bbfa-6850c8aab15a", "fitness": 0.05398399132617829, "name": "EnhancedHybridDELocalSearchOptimizer", "description": "Add an adaptive crossover probability strategy to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Adaptive Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover\n                self.crossover_probability = min(1.0, max(0.1, self.crossover_probability + self.adaptation_rate * (fitness[i] / np.mean(fitness) - 0.5)))  # Changed line\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Stochastic Local Search\n                if evals < self.budget:\n                    local_trial = trial + (self.local_search_perturbation + 0.01 * fitness_variance) * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 9, "feedback": "The algorithm EnhancedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05398 with standard deviation 0.00244.", "error": "", "parent_ids": ["2131c558-c0ee-4bb9-a195-8edcde7537f9"], "operator": null, "metadata": {"aucs": [0.051755883127770796, 0.05383223084231559, 0.04957100478020782, 0.05479880366415213, 0.056996669166707536, 0.05248318015007947, 0.05551273470558804, 0.05774007666218084, 0.05316533883660235]}}
{"id": "8dec97da-2714-4831-8eec-4544727a8638", "fitness": 0.05834607121586945, "name": "EnhancedHybridDELocalSearchOptimizer", "description": "Enhanced adaptation rate calculation for the mutation factor to improve convergence stability.", "code": "import numpy as np\n\nclass EnhancedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Adaptive Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * fitness_variance * 0.2)  # Changed line\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Stochastic Local Search\n                if evals < self.budget:\n                    local_trial = trial + (self.local_search_perturbation + 0.01 * fitness_variance) * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 10, "feedback": "The algorithm EnhancedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05835 with standard deviation 0.00318.", "error": "", "parent_ids": ["2131c558-c0ee-4bb9-a195-8edcde7537f9"], "operator": null, "metadata": {"aucs": [0.05237411447564322, 0.058133447576613984, 0.05712640356550547, 0.055451075289324026, 0.06158708488157827, 0.06053244580086259, 0.05617322286803628, 0.06240138081548341, 0.06133546566977777]}}
{"id": "4b45cc58-4574-448d-bb5d-12a0a3f3457c", "fitness": 0.054073576067451966, "name": "EnhancedHybridDELocalSearchOptimizer", "description": "Enhanced DE with adaptive mutation and crossover probability for improved convergence.", "code": "import numpy as np\n\nclass EnhancedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Adaptive Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                    self.crossover_probability = min(1.0, self.crossover_probability + 0.05)  # Changed line\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Stochastic Local Search\n                if evals < self.budget:\n                    local_trial = trial + (self.local_search_perturbation + 0.01 * fitness_variance) * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 11, "feedback": "The algorithm EnhancedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05407 with standard deviation 0.00259.", "error": "", "parent_ids": ["2131c558-c0ee-4bb9-a195-8edcde7537f9"], "operator": null, "metadata": {"aucs": [0.04921076487143783, 0.05383223084231559, 0.05237575406613382, 0.05209636543004037, 0.056996669166707536, 0.05545747874813245, 0.052771976110902785, 0.05774007666218084, 0.05618086870921646]}}
{"id": "e2075948-ea33-427b-91ff-cb501a6b23c4", "fitness": 0.05909348538386297, "name": "EnhancedHybridDELocalSearchOptimizer", "description": "Introduced dynamic crossover probability adaptation based on fitness diversity to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Adaptive Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5  # Changed line\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Stochastic Local Search\n                if evals < self.budget:\n                    local_trial = trial + (self.local_search_perturbation + 0.01 * fitness_variance) * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05909 with standard deviation 0.00182.", "error": "", "parent_ids": ["2131c558-c0ee-4bb9-a195-8edcde7537f9"], "operator": null, "metadata": {"aucs": [0.0563370371044436, 0.05690061261877377, 0.05652886089683862, 0.05968594176394049, 0.06026917573494295, 0.059894594444125926, 0.06047485818591547, 0.061062575997280044, 0.06068771170850584]}}
{"id": "4a8082a0-ae11-4d89-8114-ed75c8741d90", "fitness": 0.0528055746423473, "name": "EnhancedHybridDELocalSearchOptimizer", "description": "Introduced a dynamic scaling mutation factor based on diversity and implemented a convergence speedup mechanism to enhance exploitation capabilities.", "code": "import numpy as np\n\nclass EnhancedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.convergence_speedup = 0.1\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            diversity = np.mean(np.std(population, axis=0))\n            \n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Dynamic Scaling Mutation Factor\n                adjusted_mutation_factor = self.mutation_factor * (1 + diversity)\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + adjusted_mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation and convergence speedup\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n                    \n                if evals > self.budget / 2:  # Apply convergence speedup in the second half of evaluations\n                    self.local_search_perturbation *= self.convergence_speedup\n\n                # Stochastic Local Search\n                if evals < self.budget:\n                    local_trial = trial + (self.local_search_perturbation + 0.01 * fitness_variance) * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 13, "feedback": "The algorithm EnhancedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05281 with standard deviation 0.00273.", "error": "", "parent_ids": ["e2075948-ea33-427b-91ff-cb501a6b23c4"], "operator": null, "metadata": {"aucs": [0.04852136304828136, 0.05352022808391732, 0.04973882557378673, 0.05136894962904781, 0.056664819390635834, 0.05265686004442671, 0.052035375286396124, 0.05740338292637348, 0.053340367798260346]}}
{"id": "1e0c69d9-631e-41b1-bea0-9337884bc1df", "fitness": 0.05750902891618015, "name": "EnhancedHybridDELocalSearchOptimizer", "description": "Enhanced fitness variance-based crossover and selective local search to refine exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Adaptive Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.7 + (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.2  # Changed line\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Stochastic Local Search\n                if evals < self.budget:\n                    if np.random.rand() < 0.5:  # Changed line\n                        local_trial = trial + (self.local_search_perturbation + 0.01 * fitness_variance) * np.random.normal(size=self.dim)\n                        local_trial = np.clip(local_trial, lb, ub)\n                        local_fitness = func(local_trial)\n                        evals += 1\n                        if local_fitness < fitness[i]:\n                            trial_population[i] = local_trial\n                            fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 14, "feedback": "The algorithm EnhancedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05751 with standard deviation 0.00225.", "error": "", "parent_ids": ["e2075948-ea33-427b-91ff-cb501a6b23c4"], "operator": null, "metadata": {"aucs": [0.054923626130111036, 0.05681510026162229, 0.05350980805173, 0.058171883094247145, 0.06017636566857243, 0.05666905294106206, 0.05893601516564906, 0.060967947034262004, 0.057411461898365324]}}
{"id": "6d32029a-4abe-4d2f-81a8-817eccd58802", "fitness": 0.05987877496233683, "name": "RefinedHybridDELocalSearchOptimizer", "description": "Introduced fitness-weighted mutation and adaptive local search perturbation to enhance convergence speed and solution precision.", "code": "import numpy as np\n\nclass RefinedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * fitness_variance\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 15, "feedback": "The algorithm RefinedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05988 with standard deviation 0.00335.", "error": "", "parent_ids": ["e2075948-ea33-427b-91ff-cb501a6b23c4"], "operator": null, "metadata": {"aucs": [0.05474375738675352, 0.06100713904950206, 0.05624845698573988, 0.05798459731938188, 0.06468070960936245, 0.05958146850299517, 0.05874700052329063, 0.0655494999066798, 0.0603663453773261]}}
{"id": "c86d81fc-3257-4034-ab26-7a53976a999a", "fitness": 0.05799192301326756, "name": "AdaptiveArchiveDEOptimizer", "description": "Introduced adaptive archive-based mutation strategy and dynamic exploration-exploitation balance to enhance robustness and solution quality.", "code": "import numpy as np\n\nclass AdaptiveArchiveDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.archive_size = self.population_size // 5\n        self.archive = []\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Archive-based Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if self.archive:\n                    archive_idx = np.random.choice(len(self.archive))\n                    c = self.archive[archive_idx]\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Dynamic Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    self.archive.append(population[i].copy())\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Maintain archive size\n                if len(self.archive) > self.archive_size:\n                    self.archive.pop(0)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * fitness_variance\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 16, "feedback": "The algorithm AdaptiveArchiveDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05799 with standard deviation 0.00226.", "error": "", "parent_ids": ["6d32029a-4abe-4d2f-81a8-817eccd58802"], "operator": null, "metadata": {"aucs": [0.05422873834554054, 0.05737215430323783, 0.05501908821412038, 0.057423241598828056, 0.06079390814558028, 0.05827466264143333, 0.05817407281232123, 0.061600843676455685, 0.059040597381890714]}}
{"id": "2eda54aa-c8b0-4b10-be08-8cfe68831a3a", "fitness": 0.059817755248379255, "name": "EnhancedHybridDELocalSearchOptimizer", "description": "Enhanced the algorithm by integrating a self-adaptive mutation strategy and more robust local search to boost exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.base_mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.mutation_decay = 0.99\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n        mutation_factors = np.full(self.population_size, self.base_mutation_factor)\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutation_factor = mutation_factors[i] * self.mutation_decay\n                mutant = np.clip(a + weight * mutation_factor * (b - c), lb, ub)\n\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    mutation_factors[i] = min(1.0, mutation_factors[i] + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    mutation_factors[i] = max(0.1, mutation_factors[i] * 0.9)\n\n                if evals < self.budget:\n                    perturbation_scale = self.local_search_perturbation + 0.01 * fitness_variance\n                    local_trial = trial + perturbation_scale * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05982 with standard deviation 0.00309.", "error": "", "parent_ids": ["6d32029a-4abe-4d2f-81a8-817eccd58802"], "operator": null, "metadata": {"aucs": [0.05388331489564713, 0.05983527841377301, 0.057615008505404686, 0.058376139648562475, 0.06344038775293859, 0.061148251288182465, 0.05780763373223585, 0.06429226661808107, 0.061961516380588]}}
{"id": "43f11cf0-3d31-4ca1-bfd8-0a20ab9ba4fc", "fitness": -Infinity, "name": "EnhancedHybridDELocalSearchOptimizer", "description": "Enhanced exploration with diversity maintenance through dynamic niche creation and adaptive mutation scaling.", "code": "import numpy as np\n\nclass EnhancedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.niche_radius = 0.1 * (np.max(func.bounds.ub) - np.min(func.bounds.lb))\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * fitness_variance\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            # Dynamic Niche Creation\n            for i in range(self.population_size):\n                niche_count = sum(np.linalg.norm(population[i] - population[j]) < self.niche_radius for j in range(self.population_size))\n                if niche_count < 3:  # Encourage diversity\n                    trial_population[i] = np.random.uniform(lb, ub, self.dim)\n                    fitness[i] = func(trial_population[i])\n                    evals += 1\n                    if evals >= self.budget:\n                        break\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 18, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_ids": ["6d32029a-4abe-4d2f-81a8-817eccd58802"], "operator": null, "metadata": {}}
{"id": "31e2b46b-2852-462d-bdce-901283c767b4", "fitness": 0.058407323613631955, "name": "RefinedHybridDELocalSearchOptimizer", "description": "Refined mutation control by dynamic mutation factor and enhanced local search perturbation.", "code": "import numpy as np\n\nclass RefinedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                dynamic_mutation_factor = self.mutation_factor * (1 + 0.1 * fitness_variance)\n                mutant = np.clip(a + weight * dynamic_mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.02 * fitness_variance\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 19, "feedback": "The algorithm RefinedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05841 with standard deviation 0.00243.", "error": "", "parent_ids": ["6d32029a-4abe-4d2f-81a8-817eccd58802"], "operator": null, "metadata": {"aucs": [0.05666818088825132, 0.05752219985220053, 0.05410931041646594, 0.06005317053532311, 0.06093372788897833, 0.05652574152218792, 0.060851253026707575, 0.06173761540625755, 0.0572647129863153]}}
{"id": "3079a807-aff6-4dba-9ffa-a5fb8fb43218", "fitness": 0.05987877496233683, "name": "RefinedHybridDELocalSearchOptimizer", "description": "Enhanced robustness through variable population size adjustment based on fitness diversity.", "code": "import numpy as np\n\nclass RefinedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            if fitness_variance < 1e-3:  # Adjust population size based on diversity\n                self.population_size = max(5, self.population_size - 1)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * fitness_variance\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 20, "feedback": "The algorithm RefinedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05988 with standard deviation 0.00335.", "error": "", "parent_ids": ["6d32029a-4abe-4d2f-81a8-817eccd58802"], "operator": null, "metadata": {"aucs": [0.05474375738675352, 0.06100713904950206, 0.05624845698573988, 0.05798459731938188, 0.06468070960936245, 0.05958146850299517, 0.05874700052329063, 0.0655494999066798, 0.0603663453773261]}}
{"id": "35d8c567-2fe8-4728-a533-58324f219ebe", "fitness": 0.05800871634355763, "name": "EnhancedCrowdingDEOptimizer", "description": "Enhanced crowding-based replacement strategy and learning rate adaptation to balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedCrowdingDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.learning_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                distances = np.array([np.linalg.norm(population[i] - population[j]) for j in indices])\n                nearest_idx = np.argmin(distances)\n                a, b, c = population[indices]\n                weight = (fitness[indices[nearest_idx]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with crowding-based replacement\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search with learning rate adjustment\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * fitness_variance\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n                    self.local_search_perturbation = max(0.01, self.local_search_perturbation * (1 - self.learning_rate))\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 21, "feedback": "The algorithm EnhancedCrowdingDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05801 with standard deviation 0.00372.", "error": "", "parent_ids": ["6d32029a-4abe-4d2f-81a8-817eccd58802"], "operator": null, "metadata": {"aucs": [0.052503156649165605, 0.05435320425891421, 0.05981290141076334, 0.05556361948369504, 0.057555675652859906, 0.0634072043872922, 0.05632317191024838, 0.0583029832987485, 0.0642565300403315]}}
{"id": "67c7b90e-ce28-4ab3-9402-acd50fd5def8", "fitness": 0.0587521627203965, "name": "EnhancedDEAdaptiveOptimizer", "description": "Enhanced differential evolution with dynamic mutation factor scaling based on success history and adaptive crossover strategy to improve convergence and robustness.", "code": "import numpy as np\n\nclass EnhancedDEAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.success_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            successful_mutations = 0\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Differential Evolution Mutation with Success History\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with Success Rate Influence\n                crossover_rate = self.crossover_probability + self.success_rate * (1 - self.crossover_probability)\n                crossover = np.random.rand(self.dim) < crossover_rate\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                    successful_mutations += 1\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - self.success_rate)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            # Update success rate\n            self.success_rate = successful_mutations / self.population_size\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 22, "feedback": "The algorithm EnhancedDEAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05875 with standard deviation 0.00441.", "error": "", "parent_ids": ["6d32029a-4abe-4d2f-81a8-817eccd58802"], "operator": null, "metadata": {"aucs": [0.0616548774271779, 0.053580725471177426, 0.0535324607027069, 0.06539904712270195, 0.05672915410303636, 0.05668862864186419, 0.06628570674649281, 0.057468653994820706, 0.0574302102735903]}}
{"id": "ad3f621f-e598-4f0c-a28d-ac6ff4d9b8d0", "fitness": 0.05809631657984829, "name": "AdaptiveQuantumDEOptimizer", "description": "An Adaptive Quantum-inspired Differential Evolution with Dynamic Quantum Rotation and Influence-based Local Search to Enhance Exploration-Exploitation Balance.", "code": "import numpy as np\n\nclass AdaptiveQuantumDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.quantum_rotation_rate = np.pi / 4\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Quantum-inspired Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                theta = self.quantum_rotation_rate * weight\n                quantum_factor = np.cos(theta) + 1j * np.sin(theta)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c) * quantum_factor.real, lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                    self.quantum_rotation_rate = min(np.pi / 2, self.quantum_rotation_rate + 0.01)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n                    self.quantum_rotation_rate = max(np.pi / 8, self.quantum_rotation_rate - 0.01)\n\n                # Influence-based Stochastic Local Search\n                if evals < self.budget:\n                    influence = np.exp(-fitness_variance)\n                    perturbation = self.local_search_perturbation + 0.01 * fitness_variance * influence\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 23, "feedback": "The algorithm AdaptiveQuantumDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05810 with standard deviation 0.00387.", "error": "", "parent_ids": ["6d32029a-4abe-4d2f-81a8-817eccd58802"], "operator": null, "metadata": {"aucs": [0.05322817870895036, 0.06031761283946191, 0.05375198847332341, 0.05636904327505432, 0.06393189636699548, 0.05631938371391032, 0.057106960594823675, 0.06478563475540522, 0.05705615049070989]}}
{"id": "39adf9f6-bb54-4b32-9add-07d231334661", "fitness": 0.058310868176583375, "name": "EnhancedHybridDELocalSearchOptimizer", "description": "Introduced dynamic population resizing and fitness-based adaptive mutation strategy to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.min_population_size = 4 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            \n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Dynamic Crossover probability\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection and mutation factor adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * fitness_variance\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            # Dynamic population size adjustment\n            population_size = max(self.min_population_size, int(population_size * 0.98))\n            population = trial_population[:population_size]\n            fitness = fitness[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 24, "feedback": "The algorithm EnhancedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05831 with standard deviation 0.00219.", "error": "", "parent_ids": ["6d32029a-4abe-4d2f-81a8-817eccd58802"], "operator": null, "metadata": {"aucs": [0.05553573048283522, 0.05744165875965723, 0.054524739377067255, 0.05883978793646405, 0.06094835169331603, 0.05774166480469434, 0.05961789271913165, 0.061649946772620035, 0.058498041043464566]}}
{"id": "218db0ea-5f1e-4d77-898f-ed3585bd3436", "fitness": 0.05533107232428255, "name": "AdvancedHybridDECooperativeOptimizer", "description": "Enhanced adaptation mechanisms and introduced cooperative coevolution to improve population diversity and convergence in high-dimensional spaces.", "code": "import numpy as np\n\nclass AdvancedHybridDECooperativeOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n        subcomponent_size = max(1, self.dim // 4)\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * fitness_variance\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n                # Cooperative Coevolution\n                if evals < self.budget:\n                    subcomponents = np.array_split(np.arange(self.dim), self.dim // subcomponent_size)\n                    for subcomponent in subcomponents:\n                        sub_trial = trial.copy()\n                        sub_trial[subcomponent] = trial[subcomponent] + np.random.normal(scale=0.1, size=len(subcomponent))\n                        sub_trial = np.clip(sub_trial, lb, ub)\n                        sub_fitness = func(sub_trial)\n                        evals += 1\n                        if sub_fitness < fitness[i]:\n                            trial_population[i] = sub_trial\n                            fitness[i] = sub_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 25, "feedback": "The algorithm AdvancedHybridDECooperativeOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05533 with standard deviation 0.00276.", "error": "", "parent_ids": ["6d32029a-4abe-4d2f-81a8-817eccd58802"], "operator": null, "metadata": {"aucs": [0.05365221356431271, 0.055190729162094665, 0.0501730922615119, 0.05681483144769084, 0.058447580850234826, 0.05311955551975056, 0.057557973580043775, 0.059213661069988044, 0.053810013462915585]}}
{"id": "b8055357-114d-4853-ae97-a9c43b778ed5", "fitness": 0.05842532630907707, "name": "RefinedHybridDELocalSearchOptimizer", "description": "Enhanced convergence by integrating a dynamic mutation factor based on population diversity and fitness improvement.", "code": "import numpy as np\n\nclass RefinedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                else:\n                    trial_population[i] = population[i]\n\n                # Dynamic Mutation Factor Adjustment\n                self.mutation_factor = 0.5 + 0.5 * (np.mean(fitness) - trial_fitness) / (1e-9 + np.std(fitness))\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * fitness_variance\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 26, "feedback": "The algorithm RefinedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05843 with standard deviation 0.00430.", "error": "", "parent_ids": ["6d32029a-4abe-4d2f-81a8-817eccd58802"], "operator": null, "metadata": {"aucs": [0.05193082664552162, 0.06017383995633363, 0.053272638667137606, 0.061911435950664395, 0.06086633613435777, 0.05633527493621593, 0.05783976090281728, 0.06671801648991482, 0.05677980709873054]}}
{"id": "1f5e275f-bca1-42ca-ba25-c72605ac4fa8", "fitness": 0.056438976157132274, "name": "EnhancedDynamicDELocalSearchOptimizer", "description": "Integrated dynamic inertia weighting for mutation and stochastic gradient descent-based local search to enhance convergence and robustness.", "code": "import numpy as np\n\nclass EnhancedDynamicDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.inertia_weight = 0.9\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Dynamic inertia-weighted Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                self.inertia_weight = 0.5 + 0.4 * np.random.rand()\n                mutant = np.clip(a + self.inertia_weight * weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Gradient-based Stochastic Local Search\n                if evals < self.budget:\n                    grad_perturbation = np.random.normal(size=self.dim)\n                    grad_trial = trial - self.local_search_perturbation * grad_perturbation\n                    grad_trial = np.clip(grad_trial, lb, ub)\n                    grad_fitness = func(grad_trial)\n                    evals += 1\n                    if grad_fitness < fitness[i]:\n                        trial_population[i] = grad_trial\n                        fitness[i] = grad_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 27, "feedback": "The algorithm EnhancedDynamicDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05644 with standard deviation 0.00352.", "error": "", "parent_ids": ["6d32029a-4abe-4d2f-81a8-817eccd58802"], "operator": null, "metadata": {"aucs": [0.051352544338773676, 0.05813809768976974, 0.05266435160055971, 0.054424480398191255, 0.06159713640499209, 0.055776588031264174, 0.05507705358264425, 0.062412879342145455, 0.05650765402585012]}}
{"id": "291ded01-10e4-4a66-9633-252ffd0580f5", "fitness": 0.057575560311040865, "name": "EnhancedHybridDELocalSearchOptimizer", "description": "Enhanced Fitness-Weighted Mutation and Dynamic Search Intensification to Improve Convergence and Precision.", "code": "import numpy as np\n\nclass EnhancedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Enhanced Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                scaling_factor = 0.5 + 0.5 * np.random.rand()  # Introduce randomness in scaling\n                mutant = np.clip(a + weight * scaling_factor * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Dynamic Search Intensification\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * fitness_variance\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 28, "feedback": "The algorithm EnhancedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05758 with standard deviation 0.00298.", "error": "", "parent_ids": ["6d32029a-4abe-4d2f-81a8-817eccd58802"], "operator": null, "metadata": {"aucs": [0.05196017863267621, 0.05619570351707337, 0.05727176609463336, 0.05501873978574234, 0.0595150756716063, 0.06068982908770859, 0.05573653532223488, 0.06029638428664996, 0.061495830401042784]}}
{"id": "cbd12bbc-02c3-4303-9a38-7ad70dce60cc", "fitness": 0.05722982448231975, "name": "EnhancedMemoryHybridDE", "description": "Enhanced hybrid optimization approach using dynamic memory-based strategy to adjust mutation and crossover based on historical success rates.", "code": "import numpy as np\n\nclass EnhancedMemoryHybridDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.success_memory = []\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            success_rate = np.mean(self.success_memory[-10:]) if len(self.success_memory) >= 10 else 0.5\n\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Dynamic Memory-Based Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                factor_adjustment = 0.1 + 0.9 * success_rate\n                mutant = np.clip(a + factor_adjustment * weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with historical success influence\n                dynamic_cp = self.crossover_probability * (0.5 + 0.5 * success_rate)\n                crossover = np.random.rand(self.dim) < dynamic_cp\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.success_memory.append(1)\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.success_memory.append(0)\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * fitness_variance\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 29, "feedback": "The algorithm EnhancedMemoryHybridDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05723 with standard deviation 0.00284.", "error": "", "parent_ids": ["6d32029a-4abe-4d2f-81a8-817eccd58802"], "operator": null, "metadata": {"aucs": [0.05189034084952393, 0.05697467527167388, 0.05557640914809958, 0.05494610465690619, 0.060350936452134185, 0.05887228287225876, 0.055663228831710154, 0.061146279032221584, 0.05964816322634947]}}
{"id": "5d0c3c2c-7fb9-488b-b2db-ecb52a2ae7ec", "fitness": 0.05963254698844583, "name": "RefinedHybridDELocalSearchOptimizer", "description": "Integrated fitness-weighted mutation and adaptive local search perturbation with enhanced trial acceptance strategy.", "code": "import numpy as np\n\nclass RefinedHybridDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n                \n                # Augmenting with enhanced trial acceptance strategy\n                trial_fitness = func(trial)\n                evals += 1\n                acceptance_threshold = 0.95  # Enhanced trial acceptance condition\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i] * acceptance_threshold:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * fitness_variance\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 30, "feedback": "The algorithm RefinedHybridDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05963 with standard deviation 0.00243.", "error": "", "parent_ids": ["6d32029a-4abe-4d2f-81a8-817eccd58802"], "operator": null, "metadata": {"aucs": [0.055040984431937834, 0.057765929939989524, 0.05734584318273139, 0.05829896167320747, 0.0611949901918063, 0.06117255113099174, 0.061886828848004005, 0.061471844742934656, 0.06251498875440953]}}
{"id": "4fc995e8-93d0-4caf-8353-5186defed30b", "fitness": 0.05831618729552051, "name": "DynamicPopulationHybridOptimizer", "description": "Integrated a dynamic population resizing strategy and fitness-indexed crossover mechanism for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass DynamicPopulationHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Fitness-indexed Crossover\n                crossover = np.random.rand(self.dim) < (self.crossover_probability * (1 - fitness[i] / (1e-9 + np.max(fitness))))\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * fitness_variance\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                self.population_size = max(4, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:self.population_size]\n                trial_population = trial_population[:self.population_size]\n                fitness = fitness[:self.population_size]\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 31, "feedback": "The algorithm DynamicPopulationHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05832 with standard deviation 0.00235.", "error": "", "parent_ids": ["6d32029a-4abe-4d2f-81a8-817eccd58802"], "operator": null, "metadata": {"aucs": [0.05386194844341541, 0.059252872776857535, 0.05498317749214443, 0.0592826278664933, 0.06064432791562746, 0.058869541208327436, 0.057178039900166455, 0.06121904521154997, 0.059554104845102596]}}
{"id": "185c00e2-cd98-4058-9a5f-4a0afe263633", "fitness": 0.05486374846028738, "name": "EnhancedCMADEOptimizer", "description": "Enhanced Differential Evolution with dynamic covariance matrix adaptation for improved diversity and exploration.", "code": "import numpy as np\n\nclass EnhancedCMADEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.cov_matrix = np.eye(dim)  # Initialize with identity matrix\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(self.population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation with Covariance Matrix Adaptation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                direction = np.dot(np.random.multivariate_normal(np.zeros(self.dim), self.cov_matrix), b - c)\n                mutant = np.clip(a + weight * self.mutation_factor * direction, lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with covariance adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                    # Update covariance matrix\n                    delta = trial - population[i]\n                    self.cov_matrix += np.outer(delta, delta) / self.population_size\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * fitness_variance\n                    local_trial = trial + perturbation * np.random.multivariate_normal(np.zeros(self.dim), self.cov_matrix)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n                        # Update covariance matrix\n                        delta = local_trial - trial_population[i]\n                        self.cov_matrix += np.outer(delta, delta) / self.population_size\n\n            population[:] = trial_population\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedCMADEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05486 with standard deviation 0.00259.", "error": "", "parent_ids": ["6d32029a-4abe-4d2f-81a8-817eccd58802"], "operator": null, "metadata": {"aucs": [0.051883399149354026, 0.05412086775846203, 0.050275874750507654, 0.0566358506652872, 0.05730369515245237, 0.05270341469008433, 0.05564918496471194, 0.05805159129236359, 0.05714985771936332]}}
{"id": "85a383c1-3124-4614-bd65-4e8aa55d69b7", "fitness": 0.06168841590800209, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Enhanced exploration-exploitation balance using dynamic population resizing and a variance-controlled perturbation mechanism.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 33, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06169 with standard deviation 0.00405.", "error": "", "parent_ids": ["6d32029a-4abe-4d2f-81a8-817eccd58802"], "operator": null, "metadata": {"aucs": [0.05491237143825245, 0.06331661105636421, 0.05917675647103904, 0.058281522584824, 0.06718003272286066, 0.062146001892866454, 0.05896095018466063, 0.06809617733120443, 0.06312531948994693]}}
{"id": "2e2932c9-6110-41f1-affc-115805b69a3f", "fitness": 0.059034707547361176, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Enhanced exploration-exploitation balance using dynamic population resizing, variance-controlled perturbation mechanism, and adaptive local search intensity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search with intensity scaling\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    perturbation *= np.exp(-fitness_variance)  # Added line\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 34, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05903 with standard deviation 0.00456.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.05208567327675684, 0.06166261608383461, 0.05594076623610056, 0.054774141347546546, 0.06539345700046717, 0.05925920685760355, 0.05587822547217247, 0.06627767755897518, 0.06004060409279366]}}
{"id": "ef5c5967-699c-4328-8519-d0af58d16c60", "fitness": 0.05819618819084818, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Enhanced convergence control using adaptive mutation scaling, hyperbolic crossover probability, and chaotic local search perturbation to refine the balance between exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Chaotic DE Mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Hyperbolic Crossover\n                self.crossover_probability = 1.0 - np.tanh(fitness_variance) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with enhanced adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Chaotic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * np.sin(evals)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 35, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05820 with standard deviation 0.00267.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.05796049184901253, 0.0559174692532689, 0.05332608934684291, 0.061422757364214764, 0.059218051121989856, 0.05647349483967612, 0.062239572589522285, 0.059994752360024495, 0.057213014993081734]}}
{"id": "90d4c4f6-367c-4aaa-b67e-eb6fdd703971", "fitness": 0.06168841590800209, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "An enhanced adaptive differential evolution algorithm with dynamic hyperparameter tuning and adaptive local search to improve convergence and performance across multidimensional optimization problems.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.dynamic_mutation_adapt = 0.1\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n            # New Dynamic Mutation Factor Adaptation\n            if evals < self.budget:\n                self.mutation_factor = max(0.1, min(1.0, self.mutation_factor + self.dynamic_mutation_adapt * (fitness_variance - np.mean(fitness_variance))))\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 36, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06169 with standard deviation 0.00405.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.05491237143825245, 0.06331661105636421, 0.05917675647103904, 0.058281522584824, 0.06718003272286066, 0.062146001892866454, 0.05896095018466063, 0.06809617733120443, 0.06312531948994693]}}
{"id": "cf82419c-f460-43f3-92ac-3da1dd366a5c", "fitness": 0.056585814954049116, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Introduce adaptive learning rates and hybrid mutation strategies to enhance convergence speed and robustness in varying landscapes.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.learning_rate = 0.01\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Hybrid Mutation Strategy\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n                else:\n                    mutant = np.clip(a + self.mutation_factor * (a - b) + self.learning_rate * np.random.randn(self.dim), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 37, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05659 with standard deviation 0.00243.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.05217063793866428, 0.056202786482240974, 0.054235247867823744, 0.05523783890849099, 0.05952895050003215, 0.05743718852894464, 0.05595766518010037, 0.06031204704131965, 0.05818997213882526]}}
{"id": "3815925e-54ed-4db0-8dd7-75701bbfbb16", "fitness": 0.05994599037039505, "name": "ImprovedAdaptiveDELocalSearchOptimizer", "description": "Improved exploration-exploitation trade-off using dynamic mutation factor adaptation and fitness diversity-based population resizing.", "code": "import numpy as np\n\nclass ImprovedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = max(0.01, self.local_search_perturbation * (1 - fitness_variance))\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing based on Fitness Diversity\n            if evals < self.budget:\n                diversity = np.std(fitness) / (1e-9 + np.mean(fitness))\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget) * (1 + diversity)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 38, "feedback": "The algorithm ImprovedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05995 with standard deviation 0.00361.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.05415207333773109, 0.059427187121744995, 0.05595799078823804, 0.058564982727946524, 0.06481254709997597, 0.05880126155671106, 0.0626988135097295, 0.06568212064611878, 0.05941693654535951]}}
{"id": "25dc771e-0aba-4ee6-8767-32244b040feb", "fitness": 0.06072123310290863, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Enhanced exploration-exploitation balance using dynamic population resizing, variance-controlled perturbation mechanism, and improved adaptive mutation factor adjustment.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.2)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 39, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06072 with standard deviation 0.00258.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.0561736129826681, 0.060212168958452605, 0.05787174566681241, 0.05949052726779891, 0.06382161374618378, 0.06147433675016334, 0.06027495490861767, 0.0646744411817819, 0.06249769646369896]}}
{"id": "ff27f811-e925-421f-b59f-f19dd88295eb", "fitness": 0.05879049569584053, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Improved adaptive differential evolution using dynamic population control and enhanced greedy local search for better convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Enhanced Greedy Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation * (1 + 0.1 * (1 - fitness_variance))\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05879 with standard deviation 0.00274.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.057810038733766, 0.05756811472912238, 0.0533988222321603, 0.06130855822281345, 0.06069551807715923, 0.05668905213962527, 0.062129575922284075, 0.06181697907753014, 0.05769780212810394]}}
{"id": "ab44005c-97e6-49f3-b706-001eb6c512fa", "fitness": 0.059359163566598414, "name": "ChaosEnhancedDEOptimizer", "description": "Incorporates self-adapting mutation and crossover strategies with chaos-inspired perturbations to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass ChaosEnhancedDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.chaos_factor = 0.3  # Introduce a chaos factor for perturbation\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        def logistic_map(x):\n            return self.chaos_factor * x * (1 - x)\n\n        chaotics = np.random.rand(population_size)\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Chaos-enhanced Local Search\n                if evals < self.budget:\n                    chaotics[i] = logistic_map(chaotics[i])\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * chaotics[i] * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 41, "feedback": "The algorithm ChaosEnhancedDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05936 with standard deviation 0.00212.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.058503971491987605, 0.056391642495702055, 0.05650809712368776, 0.06168722802369464, 0.057734990587095636, 0.05957601017424219, 0.06282113666720568, 0.060520299353123974, 0.060489096182646196]}}
{"id": "92e71361-a65d-4d3c-a834-f4abd67d2852", "fitness": 0.061423590290672764, "name": "EnhancedClusteringAdaptiveDELocalSearchOptimizer", "description": "Improved adaptive exploration-exploitation using population clustering and feedback-driven parameter tuning for enhanced convergence.", "code": "import numpy as np\n\nclass EnhancedClusteringAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n\n            # Population Clustering for Improved Diversity\n            cluster_labels = self.cluster_population(population, k=max(2, population_size // 20))\n            \n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Choose cluster-based mutation strategy\n                cluster_indices = np.where(cluster_labels == cluster_labels[i])[0]\n                if len(cluster_indices) < 3:\n                    cluster_indices = np.arange(population_size)\n\n                indices = np.random.choice(cluster_indices, 3, replace=False)\n                a, b, c = population[indices]\n                \n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with feedback-driven adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]\n\n    def cluster_population(self, population, k):\n        from sklearn.cluster import KMeans\n        kmeans = KMeans(n_clusters=k, random_state=0)\n        return kmeans.fit_predict(population)", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedClusteringAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06142 with standard deviation 0.00286.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.059399585968288204, 0.061271170592893, 0.05602004146624606, 0.06296363107146996, 0.0640286532501404, 0.05934150137635619, 0.06380549849078088, 0.06585848338106615, 0.06012374701881407]}}
{"id": "762a7b7d-2ab1-4498-8282-a84fea3cd856", "fitness": -Infinity, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Enhanced balance of exploration and exploitation using adaptive population dynamics, with strategic perturbations and parameter tuning based on fitness diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            dynamic_population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight_factor = np.exp(-fitness[i] / (1e-9 + max_fitness_diff))\n                mutant = np.clip(a + weight_factor * self.mutation_factor * (b - c), lb, ub)\n\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population[:dynamic_population_size]\n            fitness = fitness[:dynamic_population_size]\n            population_size = dynamic_population_size\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 43, "feedback": "An exception occurred: ValueError('could not broadcast input array from shape (90,10) into shape (100,10)').", "error": "ValueError('could not broadcast input array from shape (90,10) into shape (100,10)')", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {}}
{"id": "564949b7-2709-4e53-901f-5ab549e87ffe", "fitness": 0.05927407190862713, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Optimized exploration-exploitation through slight adjustment of mutation factor adaptation for enhanced convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.05)  # Adjusted\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05927 with standard deviation 0.00410.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.05583939073847044, 0.0613702734906072, 0.05258822555007381, 0.05885493083793947, 0.06507485422727499, 0.05652934012904154, 0.05998429495286228, 0.06595139386333637, 0.05727394338803804]}}
{"id": "ee51732c-81dc-46da-b777-2c88cf6ed681", "fitness": 0.058236249688630025, "name": "RefinedAdaptiveDELocalSearchOptimizer", "description": "Integrating a dynamic self-adaptive mutation strategy and intermittent local search for enhanced convergence.", "code": "import numpy as np\n\nclass RefinedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            adaptive_mutation_base = np.mean(fitness) * 0.2  # New adaptive mutation base\n            \n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Dynamic Self-Adaptive Mutation Strategy\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                adaptive_mutation_factor = self.mutation_factor + weight * adaptive_mutation_base\n                mutant = np.clip(a + adaptive_mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Intermittent Local Search\n                if evals < self.budget and np.random.rand() < 0.3:  # 30% chance for local search\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 45, "feedback": "The algorithm RefinedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05824 with standard deviation 0.00239.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.05694116832619667, 0.057713224925115436, 0.053264103347320124, 0.058700175418537115, 0.061486413670135964, 0.05879541765998586, 0.05647188349146659, 0.06125683616501876, 0.05949702419389369]}}
{"id": "94420745-21f4-4f06-983f-d88a7e743e92", "fitness": 0.05850405679277268, "name": "RefinedAdaptiveDELocalSearchOptimizer", "description": "Leveraging adaptive differential evolution with a multi-phase search strategy and adaptive scaling to optimize exploration and exploitation.", "code": "import numpy as np\n\nclass RefinedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.scale_factor = 0.5\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            \n            # Adaptive scaling factor based on fitness diversity\n            self.scale_factor = 0.5 + 0.5 * (1 - fitness_variance / (1e-9 + np.max(fitness_variance)))\n            \n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation with adaptive scaling\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.scale_factor * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 46, "feedback": "The algorithm RefinedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05850 with standard deviation 0.00320.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.05237269701209446, 0.05848512403053796, 0.057606193795276206, 0.05613285164093407, 0.06285084726124335, 0.0592014491428271, 0.05617921328973663, 0.06262341358599699, 0.06108472137630738]}}
{"id": "562c410c-b5d2-4da8-af26-7974a56fe785", "fitness": 0.0596717689423382, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Enhanced adaptive exploration-exploitation with self-adaptive mutation and crossover rates, leveraging fitness variance for dynamic strategies.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            normalized_variance = fitness_variance / (1e-9 + max_fitness_diff)\n            \n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Self-adaptive Crossover Probability\n                self.crossover_probability = 0.7 + 0.3 * normalized_variance\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation * (1 + normalized_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 47, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05967 with standard deviation 0.00561.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.06325134078776917, 0.05807447191329829, 0.050954220857356236, 0.06711454123180172, 0.05867594571341295, 0.053626700793179216, 0.06803068205417839, 0.06235486882284602, 0.05496314830720184]}}
{"id": "07217d0e-ab62-4c1c-b4e8-834cbd8860c3", "fitness": 0.05990350844354664, "name": "EnhancedOppositionDELocalSearchOptimizer", "description": "Integrating opposition-based learning and adaptive restart mechanisms to enhance diversity and resilience in dynamic environments.", "code": "import numpy as np\n\nclass EnhancedOppositionDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.restart_threshold = 0.001\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n            # Opposition-based Learning\n            if evals < self.budget:\n                opposition_population = lb + ub - population\n                opposition_fitness = np.array([func(ind) for ind in opposition_population])\n                evals += population_size\n                improved_indices = opposition_fitness < fitness\n                population[improved_indices] = opposition_population[improved_indices]\n                fitness[improved_indices] = opposition_fitness[improved_indices]\n\n            # Adaptive Restart Mechanism\n            if fitness_variance < self.restart_threshold:\n                population = np.random.uniform(lb, ub, (population_size, self.dim))\n                fitness = np.array([func(ind) for ind in population])\n                evals += population_size\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 48, "feedback": "The algorithm EnhancedOppositionDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05990 with standard deviation 0.00484.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.05132425956733577, 0.06013447203273392, 0.06057899437706449, 0.054348848907918135, 0.06374629412231914, 0.06423748405125107, 0.05505846141179427, 0.0645998924104878, 0.06510286911101515]}}
{"id": "35bd5ade-7702-40fd-845e-b9a0e4d3934a", "fitness": 0.057667968007607665, "name": "HybridizedAdaptiveDELocalSearchOptimizer", "description": "Hybridized Adaptive Differential Evolution with Explorative Opposite-based Population Initialization and Perturbation-based Local Search.", "code": "import numpy as np\n\nclass HybridizedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.1\n        self.opposite_learning_rate = 0.3\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        opposite_population = lb + ub - population\n        population = np.vstack((population, opposite_population))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size * 2\n\n        while evals < self.budget:\n            if evals >= self.budget:\n                break\n\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Crossover operation\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Perturbation-based Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing and Opposite-based Reset\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                if np.random.rand() < self.opposite_learning_rate:\n                    opposite_population = lb + ub - population[:population_size]\n                    population = np.vstack((population[:population_size], opposite_population))\n                    fitness = np.array([func(ind) for ind in population])\n                    evals += population_size\n                else:\n                    population = population[:population_size]\n                    fitness = fitness[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 49, "feedback": "The algorithm HybridizedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05767 with standard deviation 0.00210.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.055126033634399385, 0.05394657474047626, 0.05663041151889714, 0.05838264110968572, 0.0571186959889074, 0.060000280228945435, 0.05914872552649375, 0.05786398430709683, 0.06079436501356705]}}
{"id": "0ee048f5-847e-46b3-bd3b-48711703a2c7", "fitness": 0.06112534210102748, "name": "ImprovedAdaptiveDELocalSearchOptimizer", "description": "Enhanced mutation and adaptation strategy with dynamic exploration-exploitation balance and constrained perturbation for improved diversity and convergence.", "code": "import numpy as np\n\nclass ImprovedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            # Enhanced exploration-exploitation balance\n            adapt_mutation_factor = self.mutation_factor * (1 + 0.1 * (1 - fitness_variance))\n            adapt_local_perturbation = self.local_search_perturbation * (1 - 0.5 * fitness_variance)\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Enhanced Differential Evolution Mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * adapt_mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Enhanced Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = adapt_local_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 50, "feedback": "The algorithm ImprovedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06113 with standard deviation 0.00565.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.052897558651289533, 0.06508567773932328, 0.05734831427219722, 0.05601951031020824, 0.0690832225482293, 0.06054132604161633, 0.05675291824039763, 0.07003257674482666, 0.06236697436115912]}}
{"id": "b0f4e175-9929-4176-b332-72bcb0c2ff5a", "fitness": 0.06162520661186263, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "An adaptive differential evolution optimizer with strategic diversity enhancement through selective restart based on stagnation detection and fitness landscape analysis.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.stagnation_threshold = 15\n        self.restart_diversification = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n        no_improvement_count = 0\n        best_fitness_so_far = np.min(fitness)\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                    no_improvement_count = 0\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n                    no_improvement_count += 1\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n                        no_improvement_count = 0\n\n            population[:] = trial_population\n\n            # Check for stagnation and perform selective restart\n            current_best_fitness = np.min(fitness)\n            if current_best_fitness < best_fitness_so_far:\n                best_fitness_so_far = current_best_fitness\n                no_improvement_count = 0\n            elif no_improvement_count >= self.stagnation_threshold:\n                population = np.random.uniform(lb, ub, (population_size, self.dim))\n                fitness = np.array([func(ind) for ind in population])\n                evals += population_size\n                no_improvement_count = 0\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 51, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06163 with standard deviation 0.00403.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.05491237143825245, 0.06331661105636421, 0.05917675647103904, 0.058281522584824, 0.06718003272286066, 0.062146001892866454, 0.05896095018466063, 0.06809617733120443, 0.06255643582469184]}}
{"id": "f80abc69-2963-4e02-a23f-6ece9029024e", "fitness": 0.05899592759390916, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Enhanced exploitation through adaptive local search and multi-dimensional scaling for improved convergence speed and precision.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Enhanced Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n                    # Multi-dimensional Scaling for precision adjustment\n                    scale_factor = 1 + 0.5 * (1 - fitness_variance)\n                    scaled_trial = trial * scale_factor\n                    scaled_trial = np.clip(scaled_trial, lb, ub)\n                    scaled_fitness = func(scaled_trial)\n                    evals += 1\n                    if scaled_fitness < fitness[i]:\n                        trial_population[i] = scaled_trial\n                        fitness[i] = scaled_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 52, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05900 with standard deviation 0.00330.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.059972352174961996, 0.05576089166982423, 0.05371295991184544, 0.06360191061865628, 0.059057007124701966, 0.05688033291642791, 0.06452117493781662, 0.05983273557799429, 0.057623983412953717]}}
{"id": "dad12746-03f1-4e5b-a29c-33c5fbb5c981", "fitness": 0.058362922990751374, "name": "RefinedAdaptiveDELocalSearchOptimizer", "description": "Adaptive multi-strategy mutation and selection enhance convergence by diversifying exploration with adaptive local search under constrained budget.", "code": "import numpy as np\n\nclass RefinedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.fitness_threshold = 0.1\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Multi-strategy Differential Evolution Mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                if fitness_variance > self.fitness_threshold:\n                    mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n                else:\n                    mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with enhanced adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Improved Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing with enhanced mechanism\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 53, "feedback": "The algorithm RefinedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05836 with standard deviation 0.00238.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.05755249833923326, 0.056247528873606956, 0.05389118606415644, 0.06097911709785564, 0.059571180236881016, 0.057068724495109135, 0.06178702242583911, 0.060353552732107296, 0.0578154966519735]}}
{"id": "d4186b13-b4af-4570-af17-ab015875161a", "fitness": 0.05826010969288891, "name": "RefinedAdaptiveDELocalSearchOptimizer", "description": "Improved exploration-exploitation balance with adaptive Gaussian perturbation and self-adaptive mutation factor adjustment.", "code": "import numpy as np\n\nclass RefinedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search with Gaussian Perturbation\n                if evals < self.budget:\n                    perturbation_scale = self.local_search_perturbation * (1 + np.exp(-fitness_variance))\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    local_trial = trial + perturbation\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 54, "feedback": "The algorithm RefinedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05826 with standard deviation 0.00327.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.052058631537844846, 0.058324373723465994, 0.057516614120108445, 0.05507919061769362, 0.06086017632545093, 0.060483595904210485, 0.055858071192476655, 0.0626286256026356, 0.06153170821211362]}}
{"id": "be7ffcfc-7677-4bea-8cc3-c84b88a7b206", "fitness": 0.055566870558882125, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Improved dynamic adaptation by introducing a learning rate for mutation factor and crossover adjustment.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.learning_rate = 0.01  # Added a learning rate for dynamic adjustment\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Fitness-weighted Differential Evolution Mutation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1 * self.learning_rate)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate * self.learning_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 55, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05557 with standard deviation 0.00251.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.0515742485576981, 0.05469262711374423, 0.05230658233940866, 0.05455712868246709, 0.059022498018860725, 0.05586345436358986, 0.05530568885542375, 0.058700689799014216, 0.05807891729973247]}}
{"id": "83bbdc18-6159-4bc3-ab68-54c5fcbcf28c", "fitness": 0.06364219382675013, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Enhanced exploration-exploitation with multi-strategy mutation and adaptive elitism to improve convergence and diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 56, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06364 with standard deviation 0.00333.", "error": "", "parent_ids": ["85a383c1-3124-4614-bd65-4e8aa55d69b7"], "operator": null, "metadata": {"aucs": [0.057977288367430435, 0.06422987888368858, 0.05985298588989474, 0.06350555371361644, 0.0681725462421029, 0.06340903426717448, 0.0622639056500659, 0.06910857545595461, 0.064259975970823]}}
{"id": "3bfe1a46-d15f-4c99-a7a3-2449cb7e27e8", "fitness": 0.0584018051188676, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Introduced a simple decay mechanism to dynamically adjust local search perturbation based on the number of evaluations, enhancing fine-tuning capabilities over time.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = (self.local_search_perturbation + 0.01 * (1 - fitness_variance)) * (1 - evals / self.budget)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 57, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05840 with standard deviation 0.00209.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.056023404197015236, 0.057469379803020804, 0.05433678755685567, 0.058860973406652195, 0.059772195571115905, 0.05908673060592584, 0.06013599951648574, 0.061689168904778824, 0.05824160650795818]}}
{"id": "05d2d352-0a2a-46e5-b171-e949d883e006", "fitness": 0.06364219382675013, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Improve convergence by adjusting the adaptation rate dynamically based on fitness improvement.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 58, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06364 with standard deviation 0.00333.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.057977288367430435, 0.06422987888368858, 0.05985298588989474, 0.06350555371361644, 0.0681725462421029, 0.06340903426717448, 0.0622639056500659, 0.06910857545595461, 0.064259975970823]}}
{"id": "535491cd-d680-4c90-a8f2-16eeeb0f8da5", "fitness": 0.062068028744854, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Enhanced exploration-exploitation with multi-strategy mutation, adaptive elitism, and revised local search perturbation to improve convergence and diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.02 * (1 - fitness_variance)  # Changed line\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 59, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06207 with standard deviation 0.00335.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.05808911865564592, 0.06298458802239193, 0.056839675690637415, 0.06156603940527616, 0.06681907302291501, 0.061182924990106025, 0.062368852830743005, 0.06772797164205813, 0.06103401444391243]}}
{"id": "0a4a61c9-300c-4c2c-9eff-13c07dbad164", "fitness": 0.054618643155416535, "name": "EnhancedMultiPhaseOptimizer", "description": "Enhanced multi-phase mutation with diversity-aware elitism and adaptive perturbation to balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedMultiPhaseOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # Elitism to retain best solutions\n        self.diversity_threshold = 0.1  # Threshold for diversity-aware adjustments\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Diversity-aware elitism\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-phase Differential Evolution Mutation\n                indices = np.random.choice(population_size, 5, replace=False)\n                if np.random.rand() < 0.5:\n                    a, b, c = population[indices[:3]]\n                else:\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 60, "feedback": "The algorithm EnhancedMultiPhaseOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05462 with standard deviation 0.00181.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.052181718761193374, 0.05356339595573689, 0.051869638684044705, 0.054718767880706065, 0.05678665104257541, 0.05412576185465512, 0.05596620047902068, 0.05744995867825231, 0.054905695062564264]}}
{"id": "f7fa4180-3e2d-4c4d-b82f-44e51a903ccb", "fitness": 0.057676804678752275, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Improved exploration-exploitation balance by enhancing mutation diversity and refined adaptive elitism for better convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.15  # Increased elitism rate\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c) + np.random.normal(0, 0.1, self.dim), lb, ub)  # Added noise for diversity\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05768 with standard deviation 0.00204.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.05516752469348907, 0.055629083094517306, 0.0540193303538834, 0.0584337035325605, 0.059173299417661585, 0.05976097879396036, 0.05920092500872254, 0.059684963792728474, 0.058021433421247215]}}
{"id": "acfd2af0-ddb0-488a-9313-d1ccf49a5db7", "fitness": 0.06364219382675013, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Introduced an elitism pressure adjustment by modifying the elitism rate based on the current fitness variance to enhance convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size * (1 + 0.1 * fitness_variance))  # Adjusted line\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 62, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06364 with standard deviation 0.00333.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.057977288367430435, 0.06422987888368858, 0.05985298588989474, 0.06350555371361644, 0.0681725462421029, 0.06340903426717448, 0.0622639056500659, 0.06910857545595461, 0.064259975970823]}}
{"id": "d39e68de-c836-499f-be73-5b5b971d80f6", "fitness": 0.058714815469035155, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Introduce adaptive elitism rate adjustment based on fitness variance to fine-tune exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n            # Adjust elitism rate based on fitness variance\n            self.elitism_rate = min(0.2, 0.1 + 0.1 * (1 - fitness_variance / (np.var(fitness) + 1e-9)))\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 63, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05871 with standard deviation 0.00248.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.05424678342169198, 0.058257355755163, 0.05580348695198545, 0.0581750694739257, 0.061731551005814955, 0.05954376698832653, 0.05820074750238491, 0.06255085401895555, 0.05992372410306834]}}
{"id": "b12dea00-646b-4c32-b26b-fa6c31496960", "fitness": 0.06117729416016329, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Introduce dynamic mutation factor adjustment based on fitness improvement rate to enhance convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor * 0.95)  # Dynamic adjustment\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 64, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06118 with standard deviation 0.00252.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.05744615700781541, 0.05951811579794264, 0.06098570628439259, 0.060217723739789064, 0.06314196162328134, 0.05825167887477489, 0.061492795888081986, 0.06393842322031351, 0.06560308500507817]}}
{"id": "10b3e2c2-3c88-4320-a4e5-d157a191c04b", "fitness": 0.06364219382675013, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Enhanced exploration-exploitation with improved adaptation and elitism for better convergence in diverse tasks. ", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 65, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06364 with standard deviation 0.00333.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.057977288367430435, 0.06422987888368858, 0.05985298588989474, 0.06350555371361644, 0.0681725462421029, 0.06340903426717448, 0.0622639056500659, 0.06910857545595461, 0.064259975970823]}}
{"id": "2704acf5-d075-4976-a019-eead4d27e33d", "fitness": 0.05632908492160604, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "A slight refinement with adaptive elitism rate to balance diversity and convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.05  # Modified from 0.1 to 0.05 for better balance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 66, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05633 with standard deviation 0.00237.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.0525206110032318, 0.054998665329157315, 0.053041841869920225, 0.057284181834755254, 0.060228333917556776, 0.055989990514181076, 0.05703475534185254, 0.058819079750084824, 0.05704430473371458]}}
{"id": "ae9a3e30-258f-46d1-8784-7cbe34c225e4", "fitness": 0.05675705564963984, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Incorporate adaptive parameter control and chaotic local search to enhance convergence and diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n        chaotic_sequence = self._generate_chaotic_sequence(self.budget)\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + fitness_variance)) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + chaotic_sequence[evals] * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]\n\n    def _generate_chaotic_sequence(self, length):\n        sequence = np.empty(length)\n        sequence[0] = np.random.rand()\n        for i in range(1, length):\n            sequence[i] = 4 * sequence[i - 1] * (1 - sequence[i - 1])\n        return sequence", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05676 with standard deviation 0.00188.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.05381265696680382, 0.055104281738892036, 0.05385810383799994, 0.05694030929697236, 0.05835338752211572, 0.05777302130626971, 0.05779814582013265, 0.05911753157230071, 0.058056062785271645]}}
{"id": "17b355c4-ac93-427c-ada5-4d8b1e621085", "fitness": 0.06239107236704025, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Introduced fitness-based dynamic scaling for local search perturbation to improve solution refinement.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance / (1e-9 + np.max(fitness)))\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 68, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06239 with standard deviation 0.00526.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.055290454903416886, 0.06951250677715992, 0.05738567103826886, 0.061753113309939045, 0.06947936190368476, 0.057981468516267176, 0.058838304159853694, 0.06871402128280824, 0.06256474941196366]}}
{"id": "92503be3-fbe7-438f-8d6d-b43a3847f3ed", "fitness": 0.05705966862470567, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Enhanced Adaptive DE with Dynamic Mutation Control and Stochastic Gradient-Based Local Search for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                indices = np.random.permutation(population_size)[:5]\n                a, b, c, d, e = population[indices]\n                a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    gradient_direction = np.sign(np.random.normal(size=self.dim))\n                    local_trial = trial + perturbation * gradient_direction\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 69, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05706 with standard deviation 0.00233.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.05645325773156051, 0.05421990963371881, 0.05280528332474099, 0.059832528947252106, 0.05731591170069106, 0.05647133726917841, 0.06062936420118059, 0.05816101261824291, 0.05764841219578565]}}
{"id": "2cc551cf-984f-4173-991a-d21d848f2c2a", "fitness": 0.06137201434875009, "name": "HybridAdaptiveDECrossPollinationOptimizer", "description": "Hybrid adaptive differential evolution with cross-pollination and dynamic feedback mechanism to enhance convergence speed and solution accuracy.", "code": "import numpy as np\n\nclass HybridAdaptiveDECrossPollinationOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # Elitism to retain top performers\n        self.feedback_rate = 0.05  # New parameter for dynamic feedback\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n                if np.random.rand() < self.feedback_rate:\n                    peer = population[np.random.randint(0, population_size)]\n                    cross_pollen = trial + self.feedback_rate * (peer - trial)\n                    cross_pollen = np.clip(cross_pollen, lb, ub)\n                    cross_fitness = func(cross_pollen)\n                    evals += 1\n                    if cross_fitness < fitness[i]:\n                        trial_population[i] = cross_pollen\n                        fitness[i] = cross_fitness\n\n            population[:] = trial_population\n\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 70, "feedback": "The algorithm HybridAdaptiveDECrossPollinationOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06137 with standard deviation 0.00338.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.0552675692516873, 0.06197183476124857, 0.05878422195553745, 0.05926604305107219, 0.06571210911112901, 0.06229648600921711, 0.059327139879907675, 0.06659727762418755, 0.0631254474947639]}}
{"id": "5d461fba-c4f8-4b76-8cec-cd7d0d477e44", "fitness": 0.05765952804599962, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Enhanced exploration-exploitation with multi-strategy mutation, adaptive elitism, and dynamic crossover adjustment to improve convergence and diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5 + 0.05\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 71, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05766 with standard deviation 0.00206.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.05392903566422835, 0.05586900854346577, 0.05595787575122535, 0.0565760757105086, 0.059170902388475, 0.05960065176407092, 0.057861629669055836, 0.05994801141481576, 0.060022561508150996]}}
{"id": "4fcc8899-180f-4e97-af25-178aca6ec558", "fitness": 0.060471354582785554, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Introduced a weighted elitism mechanism to enhance selection pressure and promote convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    # Introducing weighted elitism\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]] * (1 - self.elitism_rate) + population[i] * self.elitism_rate\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 72, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06047 with standard deviation 0.00408.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.055139675718358094, 0.06273252934296547, 0.05601169376453763, 0.057701579536894476, 0.06654384677346337, 0.05937363622805958, 0.0591785580187435, 0.06744690788793184, 0.06011376397411605]}}
{"id": "64a29594-7b6e-4911-8f20-69cdfd74cba1", "fitness": 0.06364219382675013, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Introduced adaptive crossover probability scaling based on current population diversity to enhance convergence balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                diversity = np.std(population, axis=0).mean()  # Compute diversity\n                self.crossover_probability = 0.9 - (diversity / (1e-9 + np.max(diversity))) * 0.5  # Changed line\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 73, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06364 with standard deviation 0.00333.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.057977288367430435, 0.06422987888368858, 0.05985298588989474, 0.06350555371361644, 0.0681725462421029, 0.06340903426717448, 0.0622639056500659, 0.06910857545595461, 0.064259975970823]}}
{"id": "500d25f7-61a3-4f54-b550-b41c69ba4781", "fitness": 0.05870543477917753, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Incorporates dynamic parameter adjustment and opposition-based learning to enhance adaptation and convergence in black box optimization.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            # Opposition-based Learning\n            if evals < self.budget:\n                opposition_population = lb + ub - population\n                opposition_fitness = np.array([func(ind) for ind in opposition_population])\n                evals += population_size\n                for j in range(population_size):\n                    if opposition_fitness[j] < fitness[j]:\n                        trial_population[j] = opposition_population[j]\n                        fitness[j] = opposition_fitness[j]\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05871 with standard deviation 0.00208.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.05763349353835334, 0.055679677313294484, 0.05534605138375159, 0.061081474765647736, 0.058964270995373935, 0.05862070230879857, 0.06189494340373647, 0.05973705354541825, 0.05939124575822341]}}
{"id": "9b7e18f8-d0c6-4f15-9e67-bc7feb057cc9", "fitness": 0.06240994756424242, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Introduced adaptive dynamic adjustment of local search perturbation and mutation factor for improved balance between exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation * (0.9 + 0.1 * (1 - fitness_variance))\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 75, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06241 with standard deviation 0.00318.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.059200262148964566, 0.05719519635090109, 0.06285209613736176, 0.06278191669986577, 0.06030296746895836, 0.06669887213773029, 0.06359306568091372, 0.061368767997196816, 0.06769638345628937]}}
{"id": "5966e293-e1c6-4999-9eee-3f4cc88a414b", "fitness": 0.06003267907000785, "name": "RefinedAdaptiveDELocalSearchOptimizer", "description": "Introduce adaptive mutation scaling and dynamic population strategies to further enhance convergence and exploration balance.  ", "code": "import numpy as np\n\nclass RefinedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n            adaptation_factor = np.tanh(fitness_variance)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                indices = np.random.choice(population_size, 5, replace=False)\n                a, b, c, d, e = population[indices]\n                mutant_strategy = np.random.rand() < 0.5\n                a = a + adaptation_factor * self.mutation_factor * (b - c + d - e) if not mutant_strategy else a\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation * (1 + adaptation_factor)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 76, "feedback": "The algorithm RefinedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06003 with standard deviation 0.00314.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.055528878431026274, 0.06066895956213292, 0.05658890739048228, 0.05870237275312762, 0.06512185505525125, 0.058786508411642124, 0.059585165418297836, 0.06518725987400575, 0.060124204734104625]}}
{"id": "abe610ee-d6d1-4f0a-9469-4a778a9f21d5", "fitness": 0.057070113011428836, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Improved mutation strategy with enhanced random selection and adapted crossover probability.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                indices = np.random.choice(population_size, 5, replace=False)  # Changed from 3 to 5\n                a, b, c, d, e = population[indices]\n                a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 77, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05707 with standard deviation 0.00204.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.05389161214691007, 0.05584452289744868, 0.054274444357767404, 0.0573088429347447, 0.05983831060967493, 0.05651625740075328, 0.057835028827127655, 0.059923874285723655, 0.058198123642709154]}}
{"id": "5bfca237-ef5c-4e9d-977f-ff13d1a6f2c2", "fitness": 0.061263564502346934, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Introduce adaptive elitism rate based on fitness variance to dynamically balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size * (1 + fitness_variance))  # Modified line\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06126 with standard deviation 0.00296.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.05594699232598088, 0.06114801331314346, 0.05875340208875768, 0.05908356699420492, 0.06483292013597575, 0.0628680141313549, 0.060047907888033825, 0.06570507346630439, 0.06298619017736662]}}
{"id": "0b9f1d11-ebe7-4bc6-a038-8807d18ca103", "fitness": 0.05621995666326131, "name": "HybridAdaptiveDEOptimizer", "description": "Incorporate hybrid local-global search with adaptive population resizing and elite preservation to enhance convergence efficiency and solution quality.", "code": "import numpy as np\n\nclass HybridAdaptiveDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            # Retain top performers using elitism\n            elite_indices = np.argsort(fitness)[:elitism_count]\n            trial_population[:elitism_count] = population[elite_indices]\n\n            for i in range(elitism_count, population_size):\n                if evals >= self.budget:\n                    break\n\n                # Multi-strategy DE mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive crossover\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection and adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Local and global hybrid search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    global_search = (np.random.rand() > 0.5)\n                    if global_search:\n                        random_point = np.random.uniform(lb, ub, self.dim)\n                        trial_population[i] = random_point\n                        trial_fitness = func(random_point)\n                        evals += 1\n                        if trial_fitness < fitness[i]:\n                            fitness[i] = trial_fitness\n                    else:\n                        local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                        local_trial = np.clip(local_trial, lb, ub)\n                        local_fitness = func(local_trial)\n                        evals += 1\n                        if local_fitness < fitness[i]:\n                            trial_population[i] = local_trial\n                            fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Adaptive Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 79, "feedback": "The algorithm HybridAdaptiveDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05622 with standard deviation 0.00218.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.052277080643649776, 0.05546087853811821, 0.053835377433294807, 0.05534991901705244, 0.05873159421430896, 0.057004077049615964, 0.05607109491712803, 0.059500990548001065, 0.05774859760818252]}}
{"id": "71d42e07-cde0-44c8-8755-7108690a502b", "fitness": 0.05845650138968615, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Enhanced mutation and crossover strategies to improve exploratory capabilities and convergence accuracy.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.6  # Changed from 0.5 to 0.6\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                self.crossover_probability = 0.85 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5  # Changed from 0.9 to 0.85\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 80, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05846 with standard deviation 0.00193.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.054741969748587804, 0.05650743889429788, 0.05655676269812182, 0.05896425350418022, 0.05987561028513111, 0.05964783520685646, 0.058741064566627865, 0.06063917833343535, 0.06043439926993688]}}
{"id": "87ddf0bd-a6ca-4ead-9d97-bfcf43ad6b69", "fitness": 0.05975943736955031, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Enhanced exploration-exploitation with adaptive strategy adjustment for mutation factor based on success rate.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n        success_counter = np.zeros(population_size)  # Track success rate\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    success_counter[i] += 1  # Increment success counter\n                else:\n                    trial_population[i] = population[i]\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            # Adjust mutation factor based on success rate\n            self.mutation_factor = np.clip(self.mutation_factor + self.adaptation_rate * (np.mean(success_counter) - 0.5), 0.1, 1.0)\n            success_counter.fill(0)  # Reset success counter\n            \n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05976 with standard deviation 0.00195.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.05801389182125938, 0.058058418311876525, 0.057600225040569275, 0.06147137450297757, 0.06155687934512055, 0.05790037592797437, 0.06228686028586472, 0.062288628758387254, 0.05865828233192316]}}
{"id": "d30cc7bd-c040-450f-82c1-08d03feba3e0", "fitness": 0.05743621263238144, "name": "ImprovedAdaptiveDELocalSearchOptimizer", "description": "Improved dynamic parameter adaptation and local search intensity tuning to enhance convergence and robustness in diverse optimization landscapes.", "code": "import numpy as np\n\nclass ImprovedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n        self.mutation_factor_decay = 0.99  # New parameter for mutation factor decay\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor * self.mutation_factor_decay)\n\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 82, "feedback": "The algorithm ImprovedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05744 with standard deviation 0.00281.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.052495112543850087, 0.057685597624456686, 0.05500619670251683, 0.05608366067657444, 0.061114071958576544, 0.056967301730837416, 0.05630611049038492, 0.06192230119391706, 0.05934556077031894]}}
{"id": "b6f7411e-f39b-41bb-af38-67f1e8923c42", "fitness": 0.06364219382675013, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Introducing adaptive elitism rate for better diversity and convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n                \n            # Adaptive elitism rate\n            self.elitism_rate = min(0.3, self.elitism_rate + 0.01 * np.var(fitness))\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06364 with standard deviation 0.00333.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.057977288367430435, 0.06422987888368858, 0.05985298588989474, 0.06350555371361644, 0.0681725462421029, 0.06340903426717448, 0.0622639056500659, 0.06910857545595461, 0.064259975970823]}}
{"id": "615b660b-68ac-4da1-8100-002a03e011c3", "fitness": 0.0618557543728193, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Incorporate cooperative co-evolution and adaptive learning rates to bolster convergence and maintain exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n        self.learning_rate = 0.1  # New parameter for adaptive learning\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + self.learning_rate * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 84, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06186 with standard deviation 0.00349.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.0557490453656343, 0.059648458410810234, 0.06236474047450724, 0.05860045751556886, 0.06322358393837368, 0.06615465580460667, 0.05984043366268543, 0.0640679238230265, 0.06705249036016081]}}
{"id": "31ae39db-6ccf-4775-a0db-123219296f58", "fitness": 0.06012844879867385, "name": "EnhancedMemoryAdaptiveDEOptimizer", "description": "Implement a dynamic memory-based adaptive mutation strategy and crossover probability adjustment to enhance convergence and diversity in challenging optimization landscapes.", "code": "import numpy as np\n\nclass EnhancedMemoryAdaptiveDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n        self.memory_factor = np.full(self.initial_population_size, self.mutation_factor)\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.memory_factor[i] * (b - c) + self.memory_factor[i] * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.memory_factor[i] * (b - c), lb, ub)\n\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.memory_factor[i] = min(1.0, self.memory_factor[i] + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.memory_factor[i] = max(0.1, self.memory_factor[i] - self.adaptation_rate)\n\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n                self.memory_factor = self.memory_factor[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedMemoryAdaptiveDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06013 with standard deviation 0.00222.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.056451354048597246, 0.05694294055646587, 0.059544915638962026, 0.05987167260359594, 0.060250299803080165, 0.0627825161036153, 0.0605824766037657, 0.06110823991056369, 0.06362162391941872]}}
{"id": "c6f5f4bf-5681-4c9d-a4c8-9e37aebb846f", "fitness": 0.06364219382675013, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Improved convergence by incorporating a refined crossover probability scaling factor based on fitness variance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-6 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 86, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06364 with standard deviation 0.00333.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.057977288367430435, 0.06422987888368858, 0.05985298588989474, 0.06350555371361644, 0.0681725462421029, 0.06340903426717448, 0.0622639056500659, 0.06910857545595461, 0.064259975970823]}}
{"id": "ab68e335-a9ed-4767-aff9-a72d111e0f5b", "fitness": 0.061263564502346934, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Introduced a dynamic elitism rate based on fitness variance to further improve performance balance between exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            # Dynamic elitism rate\n            elitism_count = int(self.elitism_rate * (1 + fitness_variance) * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06126 with standard deviation 0.00296.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.05594699232598088, 0.06114801331314346, 0.05875340208875768, 0.05908356699420492, 0.06483292013597575, 0.0628680141313549, 0.060047907888033825, 0.06570507346630439, 0.06298619017736662]}}
{"id": "2a44c23d-7703-4afb-9587-0b51ce239558", "fitness": 0.062068028744854, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Enhanced exploration-exploitation with multi-strategy mutation, adaptive elitism, and improved local search perturbation for better convergence.  ", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.02 * (1 - fitness_variance)  # Adjusted perturbation\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 88, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06207 with standard deviation 0.00335.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.05808911865564592, 0.06298458802239193, 0.056839675690637415, 0.06156603940527616, 0.06681907302291501, 0.061182924990106025, 0.062368852830743005, 0.06772797164205813, 0.06103401444391243]}}
{"id": "6c16d11f-ada3-4d34-ac6f-c904f4264abd", "fitness": 0.055545503420456534, "name": "RefinedEnhancedDiffStrategyOptimizer", "description": "Refined Enhanced Differentiated Strategy with Adaptive Multimodal Mutation and Elitism Balance for Diverse Convergence.", "code": "import numpy as np\n\nclass RefinedEnhancedDiffStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 12 * dim  # Slightly increased to improve diversity\n        self.mutation_factor = 0.6\n        self.crossover_probability = 0.85\n        self.local_search_perturbation = 0.04  # Reduced to focus on global search initially\n        self.adaptation_rate = 0.15  # Adjusted for smoother mutation factor adaptation\n        self.elitism_rate = 0.15  # Increased to ensure better preservation of top solutions\n        self.dynamic_fitness_weight = 0.5  # Additional mechanism to balance exploration-exploitation\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n                \n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Enhanced Multi-strategy Differential Evolution Mutation\n                mutation_strategy = np.random.rand()\n                if mutation_strategy < 0.3:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant = a + self.mutation_factor * (b - c)\n                elif mutation_strategy < 0.6:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    mutant = a + self.mutation_factor * (b - c + d - e)\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    mutant = (a + b + c - d - e) / 3\n\n                mutant = np.clip(mutant, lb, ub)\n                \n                # Adaptive Crossover with dynamic adjustment\n                adaptive_cp = 0.8 + self.dynamic_fitness_weight * (fitness_variance / (1e-9 + np.max(fitness_variance)))\n                crossover = np.random.rand(self.dim) < adaptive_cp\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.2)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation * (1 + np.log1p(fitness_variance))\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 89, "feedback": "The algorithm RefinedEnhancedDiffStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05555 with standard deviation 0.00230.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.05123435715733593, 0.0535306854023474, 0.054874851400344316, 0.0542448123476017, 0.0566759391020244, 0.058111169181010114, 0.054950830487675684, 0.05741466425354791, 0.058872221452221374]}}
{"id": "c8d6d743-b4fd-4652-9fc7-c3220bd02257", "fitness": 0.05758067205540467, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Enhanced adaptive elitism and perturbation tuning for improved diversification and faster convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.15  # Adjusted elitism rate\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.4  # Adjusted factor\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.02 * (1 - fitness_variance)  # Adjusted perturbation\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 90, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05758 with standard deviation 0.00290.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.05574923871164228, 0.05767863984126165, 0.0528625071893678, 0.05904858877750352, 0.060768492481640846, 0.054107556535855195, 0.0598251838778997, 0.06193981669150839, 0.05624602439196269]}}
{"id": "2b7645af-318a-430f-b87a-322fa30b6a1f", "fitness": 0.06287712338941662, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Enhanced exploration-exploitation with refined mutation scaling strategy and dynamic crossover adaptation to improve convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1  # New parameter for elitism\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + 0.6 * (b - c) + 0.6 * (d - e)  # Changed mutation factor here\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.85 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.4  # Adjusted crossover probability\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 91, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06288 with standard deviation 0.00294.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.05995066322233478, 0.06249758498149105, 0.05692093897600958, 0.06355562978811824, 0.06629701673404431, 0.06254231595832194, 0.06440759524393558, 0.06719726368808321, 0.0625251019124109]}}
{"id": "94a6fe9c-b883-4f5e-8b14-58adc0215765", "fitness": 0.06364219382675013, "name": "EnhancedAdaptiveDELocalSearchOptimizer", "description": "Leveraging dynamic adaptation and strategic elitism to optimize convergence efficiency and solution quality in Differential Evolution.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + fitness_variance)) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n\n            population[:] = trial_population\n\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedAdaptiveDELocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06364 with standard deviation 0.00333.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.057977288367430435, 0.06422987888368858, 0.05985298588989474, 0.06350555371361644, 0.0681725462421029, 0.06340903426717448, 0.0622639056500659, 0.06910857545595461, 0.064259975970823]}}
{"id": "7909e2db-f2ec-4e0a-8d19-9460b87c64a4", "fitness": 0.057390803104320556, "name": "ImprovedAdaptiveHybridDEOptimizer", "description": "Introduce self-adaptive parameter control, hybrid local search, and elitist retention to enhance convergence speed and solution quality.", "code": "import numpy as np\n\nclass ImprovedAdaptiveHybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n        self.c1, self.c2 = 0.5, 0.5  # Coefficients for hybrid local search\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n\n        while evals < self.budget:\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n            trial_population = np.copy(population)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Self-adaptive Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Hybrid Local Search incorporating stochastic and guided search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n                    else:\n                        # Guided search component\n                        global_best = population[np.argmin(fitness)]\n                        guided_trial = trial + self.c1 * np.random.rand() * (global_best - trial) + self.c2 * np.random.rand() * (local_trial - trial)\n                        guided_trial = np.clip(guided_trial, lb, ub)\n                        guided_fitness = func(guided_trial)\n                        evals += 1\n                        if guided_fitness < fitness[i]:\n                            trial_population[i] = guided_trial\n                            fitness[i] = guided_fitness\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 93, "feedback": "The algorithm ImprovedAdaptiveHybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05739 with standard deviation 0.00246.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.0545872263017968, 0.05707922955253675, 0.05295889281162214, 0.05783801876992589, 0.06046381411252899, 0.056911223399402355, 0.058602992448516855, 0.06126121310913413, 0.05681461743342109]}}
{"id": "947148ee-9c7f-4f69-974b-e178d000ae91", "fitness": 0.05710695751194022, "name": "HybridAdaptiveDEOptimizer", "description": "Hybridized Differential Evolution with adaptive learning rate, elite preservation, and dynamic exploration to enhance convergence speed and robustness.", "code": "import numpy as np\n\nclass HybridAdaptiveDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n        self.learning_rate = 0.05  # New parameter for adaptive learning rate\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n        best_fitness = np.min(fitness)\n\n        while evals < self.budget:\n            trial_population = population.copy()\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < 0.5:\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + fitness_variance)) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation and learning\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    best_fitness = min(best_fitness, trial_fitness)\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n\n                # Adaptive Stochastic Local Search with Learning Rate\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim) * self.learning_rate\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n                        best_fitness = min(best_fitness, local_fitness)\n\n            population[:] = trial_population\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 94, "feedback": "The algorithm HybridAdaptiveDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05711 with standard deviation 0.00249.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.05416601061259152, 0.056451602182361116, 0.05365900522973677, 0.055248388664297265, 0.06129631118134149, 0.05680325645095663, 0.05835214300541025, 0.06057368897209703, 0.057412211308669914]}}
{"id": "09ddb637-94f0-422e-9e99-5d8ee176cf33", "fitness": 0.06382246598699846, "name": "AgeChaoticAdaptiveDEOptimizer", "description": "Introduce age-based selection and chaotic map-based parameter adaptation to enhance exploration and convergence speed.", "code": "import numpy as np\n\nclass AgeChaoticAdaptiveDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n        self.ages = np.zeros(self.initial_population_size)\n\n    def chaotic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n        chaos = 0.7\n        \n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    self.ages[i] += 1\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation\n                if np.random.rand() < self.chaotic_map(chaos):\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                    self.ages[i] = 0\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n                    self.ages[i] += 1\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n                        self.ages[i] = 0\n\n            population[:] = trial_population\n\n            # Age-based selection: remove oldest individuals\n            oldest_indices = np.argsort(self.ages)[-int(0.1 * population_size):]\n            for idx in oldest_indices:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                population[idx] = new_individual\n                fitness[idx] = new_fitness\n                self.ages[idx] = 0\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n                self.ages = self.ages[:population_size]\n            chaos = self.chaotic_map(chaos)\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 95, "feedback": "The algorithm AgeChaoticAdaptiveDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06382 with standard deviation 0.00519.", "error": "", "parent_ids": ["83bbdc18-6159-4bc3-ab68-54c5fcbcf28c"], "operator": null, "metadata": {"aucs": [0.05674013424978053, 0.06726001113350122, 0.058920611280637525, 0.06008172472590556, 0.07145079905205753, 0.06300752495801099, 0.060874130913077584, 0.07244875586125932, 0.06361850170875594]}}
{"id": "eef19b68-6f76-497a-bf58-ad16769ba0b1", "fitness": 0.06184574045238874, "name": "AgeChaoticAdaptiveDEOptimizer", "description": "Enhance the mutation strategy with a memory mechanism to adaptively fine-tune exploration and exploitation balance.", "code": "import numpy as np\n\nclass AgeChaoticAdaptiveDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n        self.ages = np.zeros(self.initial_population_size)\n        self.memory = np.full(self.initial_population_size, 0.5)  # Initialize mutation memories\n\n    def chaotic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n        chaos = 0.7\n        \n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    self.ages[i] += 1\n                    continue\n\n                # Multi-strategy Differential Evolution Mutation with memory\n                if np.random.rand() < self.chaotic_map(chaos):\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.memory[i] = self.memory[i] * 0.9 + 0.1 * self.mutation_factor  # Update memory\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.memory[i] * (b - c) + self.memory[i] * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.memory[i] * (b - c), lb, ub)\n\n                # Adaptive Crossover with dynamic adjustment\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                    self.ages[i] = 0\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n                    self.ages[i] += 1\n\n                # Adaptive Stochastic Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n                        self.ages[i] = 0\n\n            population[:] = trial_population\n\n            # Age-based selection: remove oldest individuals\n            oldest_indices = np.argsort(self.ages)[-int(0.1 * population_size):]\n            for idx in oldest_indices:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                population[idx] = new_individual\n                fitness[idx] = new_fitness\n                self.ages[idx] = 0\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n                self.ages = self.ages[:population_size]\n                self.memory = self.memory[:population_size]\n            chaos = self.chaotic_map(chaos)\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 96, "feedback": "The algorithm AgeChaoticAdaptiveDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06185 with standard deviation 0.00564.", "error": "", "parent_ids": ["09ddb637-94f0-422e-9e99-5d8ee176cf33"], "operator": null, "metadata": {"aucs": [0.05623960212085555, 0.06623162556036222, 0.054962830557008346, 0.059310200401821556, 0.07034555664549591, 0.058459414594764336, 0.0603029045685759, 0.0713245046958475, 0.05943502492676733]}}
{"id": "8db60829-e8fe-4fc2-ace4-1d3da3bf1b3a", "fitness": 0.06306251967394277, "name": "EnhancedAgeChaoticDEOptimizer", "description": "Enhance exploration and exploitation balance using dynamic chaotic maps and adaptive memorization of successful strategies.", "code": "import numpy as np\n\nclass EnhancedAgeChaoticDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n        self.ages = np.zeros(self.initial_population_size)\n        self.success_rate_memory = np.zeros(self.initial_population_size)\n        self.memory_decay = 0.9\n\n    def chaotic_map(self, x, chaos_type='logistic'):\n        if chaos_type == 'logistic':\n            return 4 * x * (1 - x)\n        elif chaos_type == 'sinusoidal':\n            return np.abs(np.sin(np.pi * x))\n        return x\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n        chaos_logistic = 0.7\n        chaos_sinusoidal = 0.8\n        \n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    self.ages[i] += 1\n                    continue\n\n                if np.random.rand() < self.chaotic_map(chaos_logistic):\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                chaos_weight = self.chaotic_map(chaos_sinusoidal, 'sinusoidal')\n                weight = chaos_weight * (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.success_rate_memory[i] = self.memory_decay * self.success_rate_memory[i] + (1 - self.memory_decay) * 1\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                    self.ages[i] = 0\n                else:\n                    trial_population[i] = population[i]\n                    self.success_rate_memory[i] = self.memory_decay * self.success_rate_memory[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n                    self.ages[i] += 1\n\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n                        self.ages[i] = 0\n\n            population[:] = trial_population\n\n            oldest_indices = np.argsort(self.ages)[-int(0.1 * population_size):]\n            for idx in oldest_indices:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                population[idx] = new_individual\n                fitness[idx] = new_fitness\n                self.ages[idx] = 0\n\n            population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n            population = population[:population_size]\n            fitness = fitness[:population_size]\n            trial_population = trial_population[:population_size]\n            self.ages = self.ages[:population_size]\n            self.success_rate_memory = self.success_rate_memory[:population_size]\n            chaos_logistic = self.chaotic_map(chaos_logistic)\n            chaos_sinusoidal = self.chaotic_map(chaos_sinusoidal, 'sinusoidal')\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 97, "feedback": "The algorithm EnhancedAgeChaoticDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06306 with standard deviation 0.00452.", "error": "", "parent_ids": ["09ddb637-94f0-422e-9e99-5d8ee176cf33"], "operator": null, "metadata": {"aucs": [0.05599891508169641, 0.06539188201798363, 0.05951838545840782, 0.05972849070826147, 0.06944579618588465, 0.06308354229941127, 0.06006016552996796, 0.07040996075206507, 0.06392553903180676]}}
{"id": "db529a65-bad9-425e-8d30-b0277fac28fa", "fitness": 0.05329504803445354, "name": "SwarmChaoticAdaptiveDEOptimizer", "description": "Integrate swarm intelligence with chaotic adaptation and age-based selection to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass SwarmChaoticAdaptiveDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n        self.ages = np.zeros(self.initial_population_size)\n        self.inertia_weight = 0.7\n        self.cognitive_weight = 1.5\n        self.social_weight = 1.5\n\n    def chaotic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (population_size, self.dim))\n        personal_best = np.copy(population)\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_fitness = np.copy(fitness)\n        evals = population_size\n        global_best_index = np.argmin(fitness)\n        global_best = population[global_best_index]\n        global_best_fitness = fitness[global_best_index]\n        chaos = 0.7\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                # Use elitism to retain top performers\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    self.ages[i] += 1\n                    continue\n\n                # Velocity update using swarm intelligence\n                inertia = self.inertia_weight * velocity[i]\n                cognitive = self.cognitive_weight * np.random.rand(self.dim) * (personal_best[i] - population[i])\n                social = self.social_weight * np.random.rand(self.dim) * (global_best - population[i])\n                velocity[i] = inertia + cognitive + social\n\n                # DE mutation with chaotic map influence\n                if np.random.rand() < self.chaotic_map(chaos):\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                # Adaptive Crossover\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i] + velocity[i])\n\n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evals += 1\n\n                # Selection with adaptation\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                    self.ages[i] = 0\n                    if trial_fitness < personal_best_fitness[i]:\n                        personal_best[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n                        if trial_fitness < global_best_fitness:\n                            global_best = trial\n                            global_best_fitness = trial_fitness\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n                    self.ages[i] += 1\n\n                # Local Search\n                if evals < self.budget:\n                    perturbation = self.local_search_perturbation + 0.01 * (1 - fitness_variance)\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n                        self.ages[i] = 0\n\n            population[:] = trial_population\n\n            # Age-based selection: remove oldest individuals\n            oldest_indices = np.argsort(self.ages)[-int(0.1 * population_size):]\n            for idx in oldest_indices:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                population[idx] = new_individual\n                fitness[idx] = new_fitness\n                self.ages[idx] = 0\n\n            # Dynamic Population Resizing\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n                personal_best = personal_best[:population_size]\n                velocity = velocity[:population_size]\n                personal_best_fitness = personal_best_fitness[:population_size]\n                self.ages = self.ages[:population_size]\n            chaos = self.chaotic_map(chaos)\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 98, "feedback": "The algorithm SwarmChaoticAdaptiveDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05330 with standard deviation 0.00263.", "error": "", "parent_ids": ["09ddb637-94f0-422e-9e99-5d8ee176cf33"], "operator": null, "metadata": {"aucs": [0.051046863060786984, 0.05352022808391732, 0.04862221858775284, 0.05404064252272267, 0.056664819390635834, 0.051473711249655496, 0.0547425116744239, 0.05740338292637348, 0.05214105481381337]}}
{"id": "cf384174-f6f1-4b06-ac8e-28339f65ed2a", "fitness": -Infinity, "name": "EnhancedAgeChaoticDEOptimizer", "description": "Enhance the age-based chaotic DE by incorporating Lvy flights for better exploration and dynamic clustering for adaptive population diversity control.", "code": "import numpy as np\n\nclass EnhancedAgeChaoticDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.local_search_perturbation = 0.05\n        self.adaptation_rate = 0.2\n        self.elitism_rate = 0.1\n        self.ages = np.zeros(self.initial_population_size)\n\n    def chaotic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1 / beta)\n        u = np.random.normal(0, sigma, size=size)\n        v = np.random.normal(0, 1, size=size)\n        step = u / abs(v)**(1 / beta)\n        return step\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = population_size\n        chaos = 0.7\n\n        while evals < self.budget:\n            trial_population = np.empty_like(population)\n            fitness_variance = np.var(fitness)\n            max_fitness_diff = np.max(fitness) - np.min(fitness)\n            elitism_count = int(self.elitism_rate * population_size)\n\n            for i in range(population_size):\n                if evals >= self.budget:\n                    break\n\n                if i < elitism_count:\n                    trial_population[i] = population[np.argsort(fitness)[:elitism_count][i]]\n                    self.ages[i] += 1\n                    continue\n\n                if np.random.rand() < self.chaotic_map(chaos):\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                else:\n                    indices = np.random.permutation(population_size)[:5]\n                    a, b, c, d, e = population[indices]\n                    a = a + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n\n                weight = (fitness[indices[0]] - fitness[i]) / (1e-9 + max_fitness_diff)\n                mutant = np.clip(a + weight * self.mutation_factor * (b - c), lb, ub)\n\n                self.crossover_probability = 0.9 - (fitness_variance / (1e-9 + np.max(fitness_variance))) * 0.5\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                if not np.any(crossover):\n                    crossover[np.random.randint(0, self.dim)] = True\n                trial = np.where(crossover, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evals += 1\n\n                if trial_fitness < fitness[i]:\n                    trial_population[i] = trial\n                    fitness[i] = trial_fitness\n                    self.mutation_factor = min(1.0, self.mutation_factor + self.adaptation_rate * 1.1)\n                    self.ages[i] = 0\n                else:\n                    trial_population[i] = population[i]\n                    self.mutation_factor = max(0.1, self.mutation_factor - self.adaptation_rate)\n                    self.ages[i] += 1\n\n                if evals < self.budget:\n                    perturbation = (self.local_search_perturbation + 0.01 * (1 - fitness_variance) + \n                                    self.levy_flight(self.dim))\n                    local_trial = trial + perturbation * np.random.normal(size=self.dim)\n                    local_trial = np.clip(local_trial, lb, ub)\n                    local_fitness = func(local_trial)\n                    evals += 1\n                    if local_fitness < fitness[i]:\n                        trial_population[i] = local_trial\n                        fitness[i] = local_fitness\n                        self.ages[i] = 0\n\n            population[:] = trial_population\n\n            cluster_centers = np.random.choice(population_size, 3, replace=False)\n            if np.any(np.linalg.norm(population - population[cluster_centers], axis=1) < 1e-5):\n                for idx in cluster_centers:\n                    new_individual = np.random.uniform(lb, ub, self.dim)\n                    new_fitness = func(new_individual)\n                    evals += 1\n                    population[idx] = new_individual\n                    fitness[idx] = new_fitness\n                    self.ages[idx] = 0\n\n            oldest_indices = np.argsort(self.ages)[-int(0.1 * population_size):]\n            for idx in oldest_indices:\n                new_individual = np.random.uniform(lb, ub, self.dim)\n                new_fitness = func(new_individual)\n                evals += 1\n                population[idx] = new_individual\n                fitness[idx] = new_fitness\n                self.ages[idx] = 0\n\n            if evals < self.budget:\n                population_size = max(10, int(self.initial_population_size * (1 - evals / self.budget)))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n                trial_population = trial_population[:population_size]\n                self.ages = self.ages[:population_size]\n            chaos = self.chaotic_map(chaos)\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "configspace": "", "generation": 99, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (100,10) (3,10) ').", "error": "ValueError('operands could not be broadcast together with shapes (100,10) (3,10) ')", "parent_ids": ["09ddb637-94f0-422e-9e99-5d8ee176cf33"], "operator": null, "metadata": {}}
