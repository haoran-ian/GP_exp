{"id": "d94e3fe8-b43e-4422-aa0d-63cc7f2be692", "fitness": 0.060996090483717826, "name": "HybridEvoGradOptimizer", "description": "A hybrid evolutionary-gradient optimization algorithm combining genetic search with gradient-based refinement for efficient exploration and exploitation.", "code": "import numpy as np\n\nclass HybridEvoGradOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.mutation_rate = 0.1\n        self.gradient_alpha = 0.01\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            # Selection\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            # Crossover\n            offspring = []\n            for _ in range(self.population_size // 2):\n                p1, p2 = parents[np.random.choice(len(parents), 2, replace=False)]\n                child = np.where(np.random.rand(self.dim) < 0.5, p1, p2)\n                offspring.append(child)\n\n            # Mutation\n            offspring = np.array(offspring)\n            mutations = np.random.normal(0, self.mutation_rate, offspring.shape)\n            offspring += mutations\n            offspring = np.clip(offspring, lb, ub)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            evaluations += len(offspring)\n\n            # Gradient refinement\n            for i in range(len(offspring)):\n                grad = self._estimate_gradient(func, offspring[i], lb, ub)\n                offspring[i] -= self.gradient_alpha * grad\n                offspring[i] = np.clip(offspring[i], lb, ub)\n                offspring_fitness[i] = func(offspring[i])\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n            # Combine and select the next generation\n            combined_population = np.vstack((parents, offspring))\n            combined_fitness = np.hstack((fitness[parents_idx], offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_idx]\n            fitness = combined_fitness[best_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _estimate_gradient(self, func, x, lb, ub, epsilon=1e-5):\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = np.copy(x)\n            x_minus = np.copy(x)\n            x_plus[i] = min(x[i] + epsilon, ub[i])\n            x_minus[i] = max(x[i] - epsilon, lb[i])\n            grad[i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n        return grad", "configspace": "", "generation": 0, "feedback": "The algorithm HybridEvoGradOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06100 with standard deviation 0.00717.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.07112035370151415, 0.05535070836985678, 0.05651720937978255]}}
{"id": "bca62739-28fb-4c25-be87-a903ed409173", "fitness": 0.0609961825779467, "name": "AdaptiveHybridEvoGradOptimizer", "description": "Adaptive Hybrid Evolutionary-Gradient Optimizer with dynamic mutation scaling and gradient enhancement for improved exploration and convergence.", "code": "import numpy as np\n\nclass AdaptiveHybridEvoGradOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.gradient_alpha = 0.01\n        self.mutation_decay_factor = 0.99  # New: decay factor for adaptive mutation\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        \n        while evaluations < self.budget:\n            # Selection\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            # Crossover\n            offspring = []\n            for _ in range(self.population_size // 2):\n                p1, p2 = parents[np.random.choice(len(parents), 2, replace=False)]\n                child = np.where(np.random.rand(self.dim) < 0.5, p1, p2)\n                offspring.append(child)\n\n            # Mutation with decay\n            offspring = np.array(offspring)\n            mutations = np.random.normal(0, mutation_rate, offspring.shape)\n            offspring += mutations\n            offspring = np.clip(offspring, lb, ub)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            evaluations += len(offspring)\n            mutation_rate *= self.mutation_decay_factor  # New: decay mutation rate\n\n            # Gradient refinement with dynamic adjustment\n            for i in range(len(offspring)):\n                grad = self._estimate_gradient(func, offspring[i], lb, ub)\n                alpha_adjusted = self.gradient_alpha / (1.0 + np.linalg.norm(grad))  # New: dynamic adjustment\n                offspring[i] -= alpha_adjusted * grad\n                offspring[i] = np.clip(offspring[i], lb, ub)\n                offspring_fitness[i] = func(offspring[i])\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n            # Combine and select the next generation\n            combined_population = np.vstack((parents, offspring))\n            combined_fitness = np.hstack((fitness[parents_idx], offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_idx]\n            fitness = combined_fitness[best_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _estimate_gradient(self, func, x, lb, ub, epsilon=1e-5):\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = np.copy(x)\n            x_minus = np.copy(x)\n            x_plus[i] = min(x[i] + epsilon, ub[i])\n            x_minus[i] = max(x[i] - epsilon, lb[i])\n            grad[i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n        return grad", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveHybridEvoGradOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06100 with standard deviation 0.00717.", "error": "", "parent_ids": ["d94e3fe8-b43e-4422-aa0d-63cc7f2be692"], "operator": null, "metadata": {"aucs": [0.07112062920088258, 0.05535070836985678, 0.05651721016310074]}}
{"id": "74dcc6ac-98c1-46c1-94e9-9511cc9deac0", "fitness": 0.05999792980500603, "name": "AdaptiveHybridEvoGradOptimizer", "description": "Adaptive Hybrid Evolutionary-Gradient Optimizer with self-adaptive learning rate and enhanced dynamic population size adjustment for robust exploration and convergence.", "code": "import numpy as np\n\nclass AdaptiveHybridEvoGradOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, 5 * dim)\n        self.initial_mutation_rate = 0.1\n        self.gradient_alpha = 0.01\n        self.mutation_decay_factor = 0.99\n        self.learning_rate_decay = 0.995  # New: decay for learning rate\n        self.dynamic_population_increase = 0.1  # New: dynamic population adjustment rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        gradient_alpha = self.gradient_alpha\n\n        while evaluations < self.budget:\n            # Selection\n            selected_indices = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[selected_indices]\n\n            # Crossover\n            offspring = []\n            for _ in range(self.population_size // 2):\n                p1, p2 = parents[np.random.choice(len(parents), 2, replace=False)]\n                child = np.where(np.random.rand(self.dim) < 0.5, p1, p2)\n                offspring.append(child)\n\n            # Mutation with decay\n            offspring = np.array(offspring)\n            mutations = np.random.normal(0, mutation_rate, offspring.shape)\n            offspring += mutations\n            offspring = np.clip(offspring, lb, ub)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            evaluations += len(offspring)\n\n            # Adjust mutation rate and learning rate\n            mutation_rate *= self.mutation_decay_factor\n            gradient_alpha *= self.learning_rate_decay\n\n            # Gradient refinement with dynamic learning rate\n            for i in range(len(offspring)):\n                grad = self._estimate_gradient(func, offspring[i], lb, ub)\n                offspring[i] -= gradient_alpha * grad\n                offspring[i] = np.clip(offspring[i], lb, ub)\n                offspring_fitness[i] = func(offspring[i])\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n            # Combine and select the next generation\n            combined_population = np.vstack((parents, offspring))\n            combined_fitness = np.hstack((fitness[selected_indices], offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Dynamic population size increase\n            if evaluations < self.budget // 2:\n                self.population_size = int(self.population_size * (1 + self.dynamic_population_increase))\n                self.population_size = min(self.population_size, self.budget - evaluations)\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _estimate_gradient(self, func, x, lb, ub, epsilon=1e-5):\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = np.copy(x)\n            x_minus = np.copy(x)\n            x_plus[i] = min(x[i] + epsilon, ub[i])\n            x_minus[i] = max(x[i] - epsilon, lb[i])\n            grad[i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n        return grad", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveHybridEvoGradOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06000 with standard deviation 0.00687.", "error": "", "parent_ids": ["bca62739-28fb-4c25-be87-a903ed409173"], "operator": null, "metadata": {"aucs": [0.06970798379289145, 0.05535070836985678, 0.05493509725226986]}}
{"id": "d6e6ac50-57c3-4509-997b-5e137ffa6c20", "fitness": 0.060994471284463635, "name": "AdaptiveHybridEvoGradOptimizer", "description": "Enhanced Adaptive Hybrid Evolutionary-Gradient Optimizer with increased initial mutation rate and precise gradient estimation.", "code": "import numpy as np\n\nclass AdaptiveHybridEvoGradOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.15  # Changed: Increased initial mutation rate\n        self.gradient_alpha = 0.01\n        self.mutation_decay_factor = 0.99\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        \n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                p1, p2 = parents[np.random.choice(len(parents), 2, replace=False)]\n                child = np.where(np.random.rand(self.dim) < 0.5, p1, p2)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            mutations = np.random.normal(0, mutation_rate, offspring.shape)\n            offspring += mutations\n            offspring = np.clip(offspring, lb, ub)\n\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            evaluations += len(offspring)\n            mutation_rate *= self.mutation_decay_factor  \n\n            for i in range(len(offspring)):\n                grad = self._estimate_gradient(func, offspring[i], lb, ub, epsilon=1e-6)  # Changed: refined gradient estimation\n                alpha_adjusted = self.gradient_alpha / (1.0 + np.linalg.norm(grad))\n                offspring[i] -= alpha_adjusted * grad\n                offspring[i] = np.clip(offspring[i], lb, ub)\n                offspring_fitness[i] = func(offspring[i])\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n            combined_population = np.vstack((parents, offspring))\n            combined_fitness = np.hstack((fitness[parents_idx], offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_idx]\n            fitness = combined_fitness[best_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _estimate_gradient(self, func, x, lb, ub, epsilon=1e-5):\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = np.copy(x)\n            x_minus = np.copy(x)\n            x_plus[i] = min(x[i] + epsilon, ub[i])\n            x_minus[i] = max(x[i] - epsilon, lb[i])\n            grad[i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n        return grad", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveHybridEvoGradOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06099 with standard deviation 0.00717.", "error": "", "parent_ids": ["bca62739-28fb-4c25-be87-a903ed409173"], "operator": null, "metadata": {"aucs": [0.07111597017781535, 0.05535070836985678, 0.05651673530571877]}}
{"id": "726bf32b-9595-4085-9846-1642142c9c41", "fitness": 0.060480593403102266, "name": "EnhancedAdaptiveHybridEvoGradOptimizer", "description": "Enhanced Adaptive Hybrid Evolutionary-Gradient Optimizer with stochastic selection, fitness diversity maintenance, and adaptive mutation scaling for robust exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridEvoGradOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.gradient_alpha = 0.01\n        self.mutation_decay_factor = 0.99\n        self.diversity_threshold = 1e-4\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        \n        while evaluations < self.budget:\n            # Stochastic Selection with Diversity Maintenance\n            sorted_idx = np.argsort(fitness)\n            parents_idx = sorted_idx[:self.population_size // 2]\n            if np.std(fitness) < self.diversity_threshold:\n                parents_idx = np.random.choice(sorted_idx, self.population_size // 2, replace=False)\n            parents = population[parents_idx]\n\n            # Crossover\n            offspring = []\n            for _ in range(self.population_size // 2):\n                p1, p2 = parents[np.random.choice(len(parents), 2, replace=False)]\n                child = np.where(np.random.rand(self.dim) < 0.5, p1, p2)\n                offspring.append(child)\n\n            # Mutation with adaptive scaling\n            offspring = np.array(offspring)\n            scale_factor = np.random.rand() * mutation_rate\n            mutations = np.random.normal(0, scale_factor, offspring.shape)\n            offspring += mutations\n            offspring = np.clip(offspring, lb, ub)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            evaluations += len(offspring)\n            mutation_rate *= self.mutation_decay_factor\n\n            # Gradient refinement with dynamic adjustment\n            for i in range(len(offspring)):\n                grad = self._estimate_gradient(func, offspring[i], lb, ub)\n                alpha_adjusted = self.gradient_alpha / (1.0 + np.linalg.norm(grad))\n                offspring[i] -= alpha_adjusted * grad\n                offspring[i] = np.clip(offspring[i], lb, ub)\n                offspring_fitness[i] = func(offspring[i])\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n            # Combine and select the next generation\n            combined_population = np.vstack((parents, offspring))\n            combined_fitness = np.hstack((fitness[parents_idx], offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_idx]\n            fitness = combined_fitness[best_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _estimate_gradient(self, func, x, lb, ub, epsilon=1e-5):\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = np.copy(x)\n            x_minus = np.copy(x)\n            x_plus[i] = min(x[i] + epsilon, ub[i])\n            x_minus[i] = max(x[i] - epsilon, lb[i])\n            grad[i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n        return grad", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedAdaptiveHybridEvoGradOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06048 with standard deviation 0.00755.", "error": "", "parent_ids": ["bca62739-28fb-4c25-be87-a903ed409173"], "operator": null, "metadata": {"aucs": [0.07115597458718015, 0.05535070836985678, 0.05493509725226986]}}
{"id": "094c3edd-421c-46dd-8d43-785ee6c9f0be", "fitness": 0.06168137651156499, "name": "AdaptiveHybridEvoGradOptimizer", "description": "Adaptive Hybrid Evolutionary-Gradient Optimizer with dynamic mutation scaling, gradient enhancement, and selective gradient estimation for improved exploration and convergence.", "code": "import numpy as np\n\nclass AdaptiveHybridEvoGradOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.gradient_alpha = 0.01\n        self.mutation_decay_factor = 0.99  # New: decay factor for adaptive mutation\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        \n        while evaluations < self.budget:\n            # Selection\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            # Crossover\n            offspring = []\n            for _ in range(self.population_size // 2):\n                p1, p2 = parents[np.random.choice(len(parents), 2, replace=False)]\n                child = np.where(np.random.rand(self.dim) < 0.5, p1, p2)\n                offspring.append(child)\n\n            # Mutation with decay\n            offspring = np.array(offspring)\n            mutations = np.random.normal(0, mutation_rate, offspring.shape)\n            offspring += mutations\n            offspring = np.clip(offspring, lb, ub)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            evaluations += len(offspring)\n            mutation_rate *= self.mutation_decay_factor  # New: decay mutation rate\n\n            # Gradient refinement with dynamic adjustment\n            for i in range(len(offspring)):\n                grad = self._estimate_gradient(func, offspring[i], lb, ub) if np.random.rand() < 0.5 else np.zeros(self.dim)  # Change: selective gradient estimation\n                alpha_adjusted = self.gradient_alpha / (1.0 + np.linalg.norm(grad))  # New: dynamic adjustment\n                offspring[i] -= alpha_adjusted * grad\n                offspring[i] = np.clip(offspring[i], lb, ub)\n                offspring_fitness[i] = func(offspring[i])\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n            # Combine and select the next generation\n            combined_population = np.vstack((parents, offspring))\n            combined_fitness = np.hstack((fitness[parents_idx], offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_idx]\n            fitness = combined_fitness[best_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _estimate_gradient(self, func, x, lb, ub, epsilon=1e-5):\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = np.copy(x)\n            x_minus = np.copy(x)\n            x_plus[i] = min(x[i] + epsilon, ub[i])\n            x_minus[i] = max(x[i] - epsilon, lb[i])\n            grad[i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n        return grad", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveHybridEvoGradOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06168 with standard deviation 0.00742.", "error": "", "parent_ids": ["bca62739-28fb-4c25-be87-a903ed409173"], "operator": null, "metadata": {"aucs": [0.07210010115361443, 0.05535070836985678, 0.05759332001122375]}}
{"id": "9f657683-7164-4137-904c-78d2641b020f", "fitness": 0.061918772862286975, "name": "AdaptiveHybridEvoGradOptimizer", "description": "Enhanced Adaptive Hybrid Evolutionary-Gradient Optimizer with adaptive crossover probability and sporadic restart mechanism for enhanced exploration and robustness.", "code": "import numpy as np\n\nclass AdaptiveHybridEvoGradOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.gradient_alpha = 0.01\n        self.mutation_decay_factor = 0.99\n        self.adaptive_crossover_prob = 0.5  # New: initial adaptive crossover probability\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        \n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                p1, p2 = parents[np.random.choice(len(parents), 2, replace=False)]\n                child = np.where(np.random.rand(self.dim) < self.adaptive_crossover_prob, p1, p2)  # Modified: adaptive crossover\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            mutations = np.random.normal(0, mutation_rate, offspring.shape)\n            offspring += mutations\n            offspring = np.clip(offspring, lb, ub)\n\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            evaluations += len(offspring)\n            mutation_rate *= self.mutation_decay_factor\n\n            for i in range(len(offspring)):\n                grad = self._estimate_gradient(func, offspring[i], lb, ub) if np.random.rand() < 0.5 else np.zeros(self.dim)\n                alpha_adjusted = self.gradient_alpha / (1.0 + np.linalg.norm(grad))\n                offspring[i] -= alpha_adjusted * grad\n                offspring[i] = np.clip(offspring[i], lb, ub)\n                offspring_fitness[i] = func(offspring[i])\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n            combined_population = np.vstack((parents, offspring))\n            combined_fitness = np.hstack((fitness[parents_idx], offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_idx]\n            fitness = combined_fitness[best_idx]\n\n            if evaluations % 100 == 0:  # New: sporadic restart mechanism\n                self.adaptive_crossover_prob *= 0.95  # New: adjust crossover probability\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _estimate_gradient(self, func, x, lb, ub, epsilon=1e-5):\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = np.copy(x)\n            x_minus = np.copy(x)\n            x_plus[i] = min(x[i] + epsilon, ub[i])\n            x_minus[i] = max(x[i] - epsilon, lb[i])\n            grad[i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n        return grad", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveHybridEvoGradOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06192 with standard deviation 0.00722.", "error": "", "parent_ids": ["094c3edd-421c-46dd-8d43-785ee6c9f0be"], "operator": null, "metadata": {"aucs": [0.07209832116448345, 0.05607182854813353, 0.05758616887424395]}}
{"id": "3d820429-36aa-4e07-9f8a-2efa53a1ba43", "fitness": 0.061356349572804114, "name": "AdaptiveHybridEvoGradOptimizer", "description": "Optimized Adaptive Hybrid Evo-Grad Optimizer with enhanced gradient estimation and adaptive mutation rate for improved convergence.", "code": "import numpy as np\n\nclass AdaptiveHybridEvoGradOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.gradient_alpha = 0.01\n        self.mutation_decay_factor = 0.99\n        self.adaptive_crossover_prob = 0.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        \n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                p1, p2 = parents[np.random.choice(len(parents), 2, replace=False)]\n                child = np.where(np.random.rand(self.dim) < self.adaptive_crossover_prob, p1, p2)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            mutations = np.random.normal(0, mutation_rate, offspring.shape)\n            offspring += mutations\n            offspring = np.clip(offspring, lb, ub)\n\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            evaluations += len(offspring)\n            mutation_rate *= self.mutation_decay_factor\n\n            for i in range(len(offspring)):\n                grad = self._estimate_gradient(func, offspring[i], lb, ub) if np.random.rand() < 0.7 else np.zeros(self.dim)  # Changed probability for gradient use\n                alpha_adjusted = self.gradient_alpha / (1.0 + np.linalg.norm(grad))\n                offspring[i] -= alpha_adjusted * grad\n                offspring[i] = np.clip(offspring[i], lb, ub)\n                offspring_fitness[i] = func(offspring[i])\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n            combined_population = np.vstack((parents, offspring))\n            combined_fitness = np.hstack((fitness[parents_idx], offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_idx]\n            fitness = combined_fitness[best_idx]\n\n            if evaluations % 100 == 0:\n                self.adaptive_crossover_prob *= 0.95\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _estimate_gradient(self, func, x, lb, ub, epsilon=1e-5):\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            delta = epsilon * (ub[i] - lb[i])  # Adjusted gradient estimation scale\n            x_plus = np.copy(x)\n            x_minus = np.copy(x)\n            x_plus[i] = min(x[i] + delta, ub[i])\n            x_minus[i] = max(x[i] - delta, lb[i])\n            grad[i] = (func(x_plus) - func(x_minus)) / (2 * delta)\n        return grad", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveHybridEvoGradOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06136 with standard deviation 0.00735.", "error": "", "parent_ids": ["9f657683-7164-4137-904c-78d2641b020f"], "operator": null, "metadata": {"aucs": [0.07171294242116788, 0.05535070836985678, 0.05700539792738768]}}
{"id": "a1c96ff7-d8c9-43a2-91d9-0da1f1066a91", "fitness": 0.06218447375006068, "name": "DynamicAdaptiveEvoGradOptimizer", "description": "Dynamic Adaptive EvoGrad Optimizer with enhanced crossover and mutation adaptation for improved convergence and diversity.", "code": "import numpy as np\n\nclass DynamicAdaptiveEvoGradOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.gradient_alpha = 0.01\n        self.mutation_decay_factor = 0.99\n        self.adaptive_crossover_prob = 0.5\n        self.crossover_decay_factor = 0.95\n        self.evaluation_chunk = 50\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        \n        def dynamic_adaptation(evals):\n            if evals % self.evaluation_chunk == 0:\n                self.adaptive_crossover_prob *= self.crossover_decay_factor\n                return True\n            return False\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                p1, p2 = parents[np.random.choice(len(parents), 2, replace=False)]\n                child = np.where(np.random.rand(self.dim) < self.adaptive_crossover_prob, p1, p2)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            mutations = np.random.normal(0, mutation_rate, offspring.shape)\n            offspring += mutations\n            offspring = np.clip(offspring, lb, ub)\n\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            evaluations += len(offspring)\n            mutation_rate *= self.mutation_decay_factor\n\n            for i in range(len(offspring)):\n                grad = self._estimate_gradient(func, offspring[i], lb, ub) if np.random.rand() < 0.5 else np.zeros(self.dim)\n                alpha_adjusted = self.gradient_alpha / (1.0 + np.linalg.norm(grad))\n                offspring[i] -= alpha_adjusted * grad\n                offspring[i] = np.clip(offspring[i], lb, ub)\n                offspring_fitness[i] = func(offspring[i])\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n            combined_population = np.vstack((parents, offspring))\n            combined_fitness = np.hstack((fitness[parents_idx], offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_idx]\n            fitness = combined_fitness[best_idx]\n\n            if dynamic_adaptation(evaluations):\n                print(f\"Dynamic adaptation at evaluation {evaluations}. Crossover probability: {self.adaptive_crossover_prob:.4f}\")\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _estimate_gradient(self, func, x, lb, ub, epsilon=1e-5):\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = np.copy(x)\n            x_minus = np.copy(x)\n            x_plus[i] = min(x[i] + epsilon, ub[i])\n            x_minus[i] = max(x[i] - epsilon, lb[i])\n            grad[i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n        return grad", "configspace": "", "generation": 8, "feedback": "The algorithm DynamicAdaptiveEvoGradOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06218 with standard deviation 0.00703.", "error": "", "parent_ids": ["9f657683-7164-4137-904c-78d2641b020f"], "operator": null, "metadata": {"aucs": [0.0721173680110776, 0.05684988436486049, 0.05758616887424395]}}
{"id": "a21882fe-b6af-4055-8b18-8bd79d872a4d", "fitness": 0.061681494583793205, "name": "GradientInformedEvoGradOptimizer", "description": "Gradient-Informed EvoGrad Optimizer with dynamic selection pressure and adaptive mutation rates for enhanced exploration and convergence.", "code": "import numpy as np\n\nclass GradientInformedEvoGradOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.min_mutation_rate = 0.01\n        self.gradient_alpha = 0.01\n        self.crossover_prob = 0.5\n        self.evaluation_chunk = 50\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        \n        def adjust_mutation_rate(mutation_rate, evals):\n            if evals % self.evaluation_chunk == 0:\n                return max(mutation_rate * 0.9, self.min_mutation_rate)\n            return mutation_rate\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                p1, p2 = parents[np.random.choice(len(parents), 2, replace=False)]\n                child = np.where(np.random.rand(self.dim) < self.crossover_prob, p1, p2)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            mutation_rate = self.initial_mutation_rate\n            mutations = np.random.normal(0, mutation_rate, offspring.shape)\n            offspring += mutations\n            offspring = np.clip(offspring, lb, ub)\n\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            evaluations += len(offspring)\n            mutation_rate = adjust_mutation_rate(mutation_rate, evaluations)\n\n            for i in range(len(offspring)):\n                grad = self._estimate_gradient(func, offspring[i], lb, ub) if np.random.rand() < 0.5 else np.zeros(self.dim)\n                alpha_adjusted = self.gradient_alpha / (1.0 + np.linalg.norm(grad))\n                offspring[i] -= alpha_adjusted * grad\n                offspring[i] = np.clip(offspring[i], lb, ub)\n                offspring_fitness[i] = func(offspring[i])\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n            combined_population = np.vstack((parents, offspring))\n            combined_fitness = np.hstack((fitness[parents_idx], offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_idx]\n            fitness = combined_fitness[best_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _estimate_gradient(self, func, x, lb, ub, epsilon=1e-5):\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = np.copy(x)\n            x_minus = np.copy(x)\n            x_plus[i] = min(x[i] + epsilon, ub[i])\n            x_minus[i] = max(x[i] - epsilon, lb[i])\n            grad[i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n        return grad", "configspace": "", "generation": 9, "feedback": "The algorithm GradientInformedEvoGradOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06168 with standard deviation 0.00742.", "error": "", "parent_ids": ["a1c96ff7-d8c9-43a2-91d9-0da1f1066a91"], "operator": null, "metadata": {"aucs": [0.07210056498076978, 0.05535070836985678, 0.05759321040075305]}}
{"id": "5f0cd694-105a-4b22-b7c1-0801ecfdeb42", "fitness": 0.05409266008672642, "name": "QuantumInspiredDynamicAdaptiveEvoGradOptimizer", "description": "Quantum-Inspired Dynamic Adaptive EvoGrad Optimizer with quantum superposition state sampling and adaptive parameter tuning for enhanced exploration and convergence.", "code": "import numpy as np\n\nclass QuantumInspiredDynamicAdaptiveEvoGradOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.gradient_alpha = 0.01\n        self.mutation_decay_factor = 0.99\n        self.adaptive_crossover_prob = 0.5\n        self.crossover_decay_factor = 0.95\n        self.evaluation_chunk = 50\n        self.quantum_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        \n        def dynamic_adaptation(evals):\n            if evals % self.evaluation_chunk == 0:\n                self.adaptive_crossover_prob *= self.crossover_decay_factor\n                self.quantum_prob *= 0.98\n                return True\n            return False\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                if np.random.rand() < self.quantum_prob:\n                    weights = np.random.uniform(-1, 1, (2, self.dim))\n                    superposed = np.tanh(weights[0] * parents[np.random.randint(len(parents))] +\n                                         weights[1] * parents[np.random.randint(len(parents))])\n                    child = (superposed + 1) / 2 * (ub - lb) + lb\n                else:\n                    p1, p2 = parents[np.random.choice(len(parents), 2, replace=False)]\n                    child = np.where(np.random.rand(self.dim) < self.adaptive_crossover_prob, p1, p2)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            mutations = np.random.normal(0, mutation_rate, offspring.shape)\n            offspring += mutations\n            offspring = np.clip(offspring, lb, ub)\n\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            evaluations += len(offspring)\n            mutation_rate *= self.mutation_decay_factor\n\n            for i in range(len(offspring)):\n                grad = self._estimate_gradient(func, offspring[i], lb, ub) if np.random.rand() < 0.5 else np.zeros(self.dim)\n                alpha_adjusted = self.gradient_alpha / (1.0 + np.linalg.norm(grad))\n                offspring[i] -= alpha_adjusted * grad\n                offspring[i] = np.clip(offspring[i], lb, ub)\n                offspring_fitness[i] = func(offspring[i])\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n            combined_population = np.vstack((parents, offspring))\n            combined_fitness = np.hstack((fitness[parents_idx], offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_idx]\n            fitness = combined_fitness[best_idx]\n\n            if dynamic_adaptation(evaluations):\n                print(f\"Dynamic adaptation at evaluation {evaluations}. Crossover probability: {self.adaptive_crossover_prob:.4f}, Quantum probability: {self.quantum_prob:.4f}\")\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _estimate_gradient(self, func, x, lb, ub, epsilon=1e-5):\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = np.copy(x)\n            x_minus = np.copy(x)\n            x_plus[i] = min(x[i] + epsilon, ub[i])\n            x_minus[i] = max(x[i] - epsilon, lb[i])\n            grad[i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n        return grad", "configspace": "", "generation": 10, "feedback": "The algorithm QuantumInspiredDynamicAdaptiveEvoGradOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05409 with standard deviation 0.00333.", "error": "", "parent_ids": ["a1c96ff7-d8c9-43a2-91d9-0da1f1066a91"], "operator": null, "metadata": {"aucs": [0.04953441988169527, 0.05535070836985678, 0.057392852008627204]}}
{"id": "a4b9f035-14fd-4504-9951-8bc6632dff57", "fitness": 0.062194494489283124, "name": "DynamicAdaptiveEvoGradOptimizer", "description": "Enhanced mutation strategy with a minor increase in mutation rate for improved exploration.", "code": "import numpy as np\n\nclass DynamicAdaptiveEvoGradOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.15  # Changed from 0.1 to 0.15\n        self.gradient_alpha = 0.01\n        self.mutation_decay_factor = 0.99\n        self.adaptive_crossover_prob = 0.5\n        self.crossover_decay_factor = 0.95\n        self.evaluation_chunk = 50\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        \n        def dynamic_adaptation(evals):\n            if evals % self.evaluation_chunk == 0:\n                self.adaptive_crossover_prob *= self.crossover_decay_factor\n                return True\n            return False\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                p1, p2 = parents[np.random.choice(len(parents), 2, replace=False)]\n                child = np.where(np.random.rand(self.dim) < self.adaptive_crossover_prob, p1, p2)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            mutations = np.random.normal(0, mutation_rate, offspring.shape)\n            offspring += mutations\n            offspring = np.clip(offspring, lb, ub)\n\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            evaluations += len(offspring)\n            mutation_rate *= self.mutation_decay_factor\n\n            for i in range(len(offspring)):\n                grad = self._estimate_gradient(func, offspring[i], lb, ub) if np.random.rand() < 0.5 else np.zeros(self.dim)\n                alpha_adjusted = self.gradient_alpha / (1.0 + np.linalg.norm(grad))\n                offspring[i] -= alpha_adjusted * grad\n                offspring[i] = np.clip(offspring[i], lb, ub)\n                offspring_fitness[i] = func(offspring[i])\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n            combined_population = np.vstack((parents, offspring))\n            combined_fitness = np.hstack((fitness[parents_idx], offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_idx]\n            fitness = combined_fitness[best_idx]\n\n            if dynamic_adaptation(evaluations):\n                print(f\"Dynamic adaptation at evaluation {evaluations}. Crossover probability: {self.adaptive_crossover_prob:.4f}\")\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _estimate_gradient(self, func, x, lb, ub, epsilon=1e-5):\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = np.copy(x)\n            x_minus = np.copy(x)\n            x_plus[i] = min(x[i] + epsilon, ub[i])\n            x_minus[i] = max(x[i] - epsilon, lb[i])\n            grad[i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n        return grad", "configspace": "", "generation": 11, "feedback": "The algorithm DynamicAdaptiveEvoGradOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06219 with standard deviation 0.00704.", "error": "", "parent_ids": ["a1c96ff7-d8c9-43a2-91d9-0da1f1066a91"], "operator": null, "metadata": {"aucs": [0.07214605370849225, 0.05685726054753182, 0.057580169211825294]}}
{"id": "b24b843f-2659-40bc-86fb-eeacac5bb310", "fitness": 0.06219472208409923, "name": "DynamicAdaptiveEvoGradOptimizer", "description": "Incorporate a dynamic mutation rate based on convergence to improve exploration-exploitation balance.", "code": "import numpy as np\n\nclass DynamicAdaptiveEvoGradOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.15\n        self.gradient_alpha = 0.01\n        self.mutation_decay_factor = 0.99\n        self.adaptive_crossover_prob = 0.5\n        self.crossover_decay_factor = 0.95\n        self.evaluation_chunk = 50\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        \n        def dynamic_adaptation(evals):\n            if evals % self.evaluation_chunk == 0:\n                self.adaptive_crossover_prob *= self.crossover_decay_factor\n            return evals % (self.evaluation_chunk * 2) == 0\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                p1, p2 = parents[np.random.choice(len(parents), 2, replace=False)]\n                child = np.where(np.random.rand(self.dim) < self.adaptive_crossover_prob, p1, p2)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            mutations = np.random.normal(0, mutation_rate, offspring.shape)\n            mutation_rate *= self.mutation_decay_factor if evaluations > self.budget // 2 else 1.01\n            offspring += mutations\n            offspring = np.clip(offspring, lb, ub)\n\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            evaluations += len(offspring)\n\n            for i in range(len(offspring)):\n                grad = self._estimate_gradient(func, offspring[i], lb, ub) if np.random.rand() < 0.5 else np.zeros(self.dim)\n                alpha_adjusted = self.gradient_alpha / (1.0 + np.linalg.norm(grad))\n                offspring[i] -= alpha_adjusted * grad\n                offspring[i] = np.clip(offspring[i], lb, ub)\n                offspring_fitness[i] = func(offspring[i])\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n            combined_population = np.vstack((parents, offspring))\n            combined_fitness = np.hstack((fitness[parents_idx], offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_idx]\n            fitness = combined_fitness[best_idx]\n\n            if dynamic_adaptation(evaluations):\n                print(f\"Dynamic adaptation at evaluation {evaluations}. Crossover probability: {self.adaptive_crossover_prob:.4f}\")\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _estimate_gradient(self, func, x, lb, ub, epsilon=1e-5):\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = np.copy(x)\n            x_minus = np.copy(x)\n            x_plus[i] = min(x[i] + epsilon, ub[i])\n            x_minus[i] = max(x[i] - epsilon, lb[i])\n            grad[i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n        return grad", "configspace": "", "generation": 12, "feedback": "The algorithm DynamicAdaptiveEvoGradOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06219 with standard deviation 0.00704.", "error": "", "parent_ids": ["a4b9f035-14fd-4504-9951-8bc6632dff57"], "operator": null, "metadata": {"aucs": [0.07214700784607375, 0.056857329690542135, 0.05757982871568179]}}
{"id": "f7141f0c-d754-4bf9-b4d4-c63d2cfd96cb", "fitness": 0.069290805025413, "name": "HybridDynamicDiffEvoOptimizer", "description": "Utilize a hybrid dynamic mutation and differential evolution to enhance convergence speed and solution precision.", "code": "import numpy as np\n\nclass HybridDynamicDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.mutation_decay_factor = 0.995\n        self.adaptive_crossover_prob = 0.7\n        self.crossover_decay_factor = 0.98\n        self.evaluation_chunk = 100\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        \n        def dynamic_adaptation(evals):\n            if evals % self.evaluation_chunk == 0:\n                self.adaptive_crossover_prob *= self.crossover_decay_factor\n            return evals % (self.evaluation_chunk * 2) == 0\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                mutant_vector = np.clip(x0 + mutation_rate * (x1 - x2), lb, ub)\n                child = np.where(np.random.rand(self.dim) < self.adaptive_crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            mutation_rate *= self.mutation_decay_factor\n\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            evaluations += len(offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_idx]\n            fitness = combined_fitness[best_idx]\n\n            if dynamic_adaptation(evaluations):\n                print(f\"Dynamic adaptation at evaluation {evaluations}. Crossover probability: {self.adaptive_crossover_prob:.4f}\")\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 13, "feedback": "The algorithm HybridDynamicDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06929 with standard deviation 0.00319.", "error": "", "parent_ids": ["b24b843f-2659-40bc-86fb-eeacac5bb310"], "operator": null, "metadata": {"aucs": [0.07183650533956021, 0.07124388326879205, 0.06479202646788673]}}
{"id": "6182e437-81a1-448e-b64c-dae5b7368b84", "fitness": 0.069290805025413, "name": "HybridDynamicDiffEvoOptimizer", "description": "Introduce a selection strategy that promotes diversity by preferring diverse solutions, enhancing exploration.", "code": "import numpy as np\n\nclass HybridDynamicDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.mutation_decay_factor = 0.995\n        self.adaptive_crossover_prob = 0.7\n        self.crossover_decay_factor = 0.98\n        self.evaluation_chunk = 100\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        \n        def dynamic_adaptation(evals):\n            if evals % self.evaluation_chunk == 0:\n                self.adaptive_crossover_prob *= self.crossover_decay_factor\n            return evals % (self.evaluation_chunk * 2) == 0\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                mutant_vector = np.clip(x0 + mutation_rate * (x1 - x2), lb, ub)\n                child = np.where(np.random.rand(self.dim) < self.adaptive_crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            mutation_rate *= self.mutation_decay_factor\n\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            evaluations += len(offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            diversity_score = np.std(combined_population, axis=0)\n            selection_metric = combined_fitness - diversity_score.sum() * 0.01  # Line changed for diversity promotion\n            best_idx = np.argsort(selection_metric)[:self.population_size]\n            population = combined_population[best_idx]\n            fitness = combined_fitness[best_idx]\n\n            if dynamic_adaptation(evaluations):\n                print(f\"Dynamic adaptation at evaluation {evaluations}. Crossover probability: {self.adaptive_crossover_prob:.4f}\")\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 14, "feedback": "The algorithm HybridDynamicDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06929 with standard deviation 0.00319.", "error": "", "parent_ids": ["f7141f0c-d754-4bf9-b4d4-c63d2cfd96cb"], "operator": null, "metadata": {"aucs": [0.07183650533956021, 0.07124388326879205, 0.06479202646788673]}}
{"id": "0deaca4b-4449-4fad-a987-1d9fe850c50a", "fitness": 0.069290805025413, "name": "HybridDynamicDiffEvoOptimizer", "description": "Introduce adaptive population size dynamics to improve convergence efficiency.", "code": "import numpy as np\n\nclass HybridDynamicDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.mutation_decay_factor = 0.995\n        self.adaptive_crossover_prob = 0.7\n        self.crossover_decay_factor = 0.98\n        self.evaluation_chunk = 100\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        \n        def dynamic_adaptation(evals):\n            if evals % self.evaluation_chunk == 0:\n                self.adaptive_crossover_prob *= self.crossover_decay_factor\n            return evals % (self.evaluation_chunk * 2) == 0\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                mutant_vector = np.clip(x0 + mutation_rate * (x1 - x2), lb, ub)\n                child = np.where(np.random.rand(self.dim) < self.adaptive_crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            mutation_rate *= self.mutation_decay_factor\n\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            evaluations += len(offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_idx]\n            fitness = combined_fitness[best_idx]\n\n            if dynamic_adaptation(evaluations):\n                print(f\"Dynamic adaptation at evaluation {evaluations}. Crossover probability: {self.adaptive_crossover_prob:.4f}\")\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 15, "feedback": "The algorithm HybridDynamicDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06929 with standard deviation 0.00319.", "error": "", "parent_ids": ["f7141f0c-d754-4bf9-b4d4-c63d2cfd96cb"], "operator": null, "metadata": {"aucs": [0.07183650533956021, 0.07124388326879205, 0.06479202646788673]}}
{"id": "e9dbb877-d29c-4315-8e42-cbdab2ea81f9", "fitness": 0.07842447043022105, "name": "HybridDynamicDiffEvoOptimizer", "description": "Enhance mutation adaptability by integrating a learning rate based on population diversity.", "code": "import numpy as np\n\nclass HybridDynamicDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.mutation_decay_factor = 0.995\n        self.adaptive_crossover_prob = 0.7\n        self.crossover_decay_factor = 0.98\n        self.evaluation_chunk = 100\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        \n        def dynamic_adaptation(evals):\n            if evals % self.evaluation_chunk == 0:\n                self.adaptive_crossover_prob *= self.crossover_decay_factor\n            return evals % (self.evaluation_chunk * 2) == 0\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim  # Change: Add diversity factor to mutation rate\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                child = np.where(np.random.rand(self.dim) < self.adaptive_crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            mutation_rate *= self.mutation_decay_factor\n\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            evaluations += len(offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_idx]\n            fitness = combined_fitness[best_idx]\n\n            if dynamic_adaptation(evaluations):\n                print(f\"Dynamic adaptation at evaluation {evaluations}. Crossover probability: {self.adaptive_crossover_prob:.4f}\")\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 16, "feedback": "The algorithm HybridDynamicDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07842 with standard deviation 0.00659.", "error": "", "parent_ids": ["f7141f0c-d754-4bf9-b4d4-c63d2cfd96cb"], "operator": null, "metadata": {"aucs": [0.0875071927398633, 0.07207492648350222, 0.07569129206729763]}}
{"id": "98ab3129-55b0-4a10-8d1b-8ff9924034fa", "fitness": 0.07913802800463114, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Introduce adaptively controlled mutation and crossover rates based on historical performance trends to enhance convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n        improvement_trend = []\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                recent_improvement = np.mean([np.min(offspring_fitness) < np.min(fitness)])\n                improvement_trend.append(recent_improvement)\n                if len(improvement_trend) > 1:\n                    recent_trend = improvement_trend[-1] - improvement_trend[-2]\n                    mutation_rate = max(0.01, mutation_rate + self.mutation_rate_adaptation_factor * recent_trend)\n                    crossover_prob = min(1.0, max(0.3, crossover_prob + self.crossover_rate_adaptation_factor * recent_trend))\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07914 with standard deviation 0.00187.", "error": "", "parent_ids": ["e9dbb877-d29c-4315-8e42-cbdab2ea81f9"], "operator": null, "metadata": {"aucs": [0.08008367380518522, 0.08079766831849422, 0.07653274189021397]}}
{"id": "6b882b62-5d67-4e4d-b94c-4edd610c7894", "fitness": 0.09183160316357697, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Introduce a memory mechanism to track and reuse historically best mutations to improve convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None  # New attribute to store the best mutation\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n        improvement_trend = []\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:  # Use the best mutation if available\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                recent_improvement = np.mean([np.min(offspring_fitness) < np.min(fitness)])\n                improvement_trend.append(recent_improvement)\n                if len(improvement_trend) > 1:\n                    recent_trend = improvement_trend[-1] - improvement_trend[-2]\n                    mutation_rate = max(0.01, mutation_rate + self.mutation_rate_adaptation_factor * recent_trend)\n                    crossover_prob = min(1.0, max(0.3, crossover_prob + self.crossover_rate_adaptation_factor * recent_trend))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]  # Update the best mutation\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09183 with standard deviation 0.00249.", "error": "", "parent_ids": ["98ab3129-55b0-4a10-8d1b-8ff9924034fa"], "operator": null, "metadata": {"aucs": [0.08878906901691785, 0.09488563502178693, 0.09182010545202612]}}
{"id": "e8c14f10-4678-4c5b-a884-974feb40257f", "fitness": -Infinity, "name": "EnhancedNichingDiffEvoOptimizer", "description": "Integrate a diversity-preserving technique by incorporating niching methods to maintain multiple promising solutions across the search space.", "code": "import numpy as np\n\nclass EnhancedNichingDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.niche_radius = 0.1 * (dim ** 0.5)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n        improvement_trend = []\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            sorted_indices = np.argsort(combined_fitness)\n            niche_filter = np.ones(len(combined_population), dtype=bool)\n\n            for idx in sorted_indices:\n                if niche_filter[idx]:\n                    # Disable further selection in this niche\n                    in_niche = np.linalg.norm(combined_population - combined_population[idx], axis=1) < self.niche_radius\n                    niche_filter[in_niche] = False\n                    niche_filter[idx] = True\n            \n            best_idx = sorted_indices[niche_filter][:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                recent_improvement = np.mean([np.min(offspring_fitness) < np.min(fitness)])\n                improvement_trend.append(recent_improvement)\n                if len(improvement_trend) > 1:\n                    recent_trend = improvement_trend[-1] - improvement_trend[-2]\n                    mutation_rate = max(0.01, mutation_rate + self.mutation_rate_adaptation_factor * recent_trend)\n                    crossover_prob = min(1.0, max(0.3, crossover_prob + self.crossover_rate_adaptation_factor * recent_trend))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 19, "feedback": "An exception occurred: ValueError(\"Cannot take a larger sample than population when 'replace=False'\").", "error": "ValueError(\"Cannot take a larger sample than population when 'replace=False'\")", "parent_ids": ["6b882b62-5d67-4e4d-b94c-4edd610c7894"], "operator": null, "metadata": {}}
{"id": "dc50d874-48ce-418c-b72b-aef9623a9f03", "fitness": 0.09037126090223546, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Enhance mutation strategy by considering a weighted combination of historically best mutations to improve exploration.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None  # New attribute to store the best mutation\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n        improvement_trend = []\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:  # Use the best mutation if available\n                    mutant_vector = (0.7 * mutant_vector + 0.3 * self.best_mutation)  # Weighted combination\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                recent_improvement = np.mean([np.min(offspring_fitness) < np.min(fitness)])\n                improvement_trend.append(recent_improvement)\n                if len(improvement_trend) > 1:\n                    recent_trend = improvement_trend[-1] - improvement_trend[-2]\n                    mutation_rate = max(0.01, mutation_rate + self.mutation_rate_adaptation_factor * recent_trend)\n                    crossover_prob = min(1.0, max(0.3, crossover_prob + self.crossover_rate_adaptation_factor * recent_trend))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]  # Update the best mutation\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 20, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09037 with standard deviation 0.00242.", "error": "", "parent_ids": ["6b882b62-5d67-4e4d-b94c-4edd610c7894"], "operator": null, "metadata": {"aucs": [0.08896493540545758, 0.09377294294723315, 0.08837590435401566]}}
{"id": "813fda0b-dcb5-49a8-a752-0cde70ec8374", "fitness": -Infinity, "name": "AdaptiveMemoryDiffEvoOptimizer", "description": "Incorporate a memory-based mechanism to favor historical mutations with superior performance while dynamically adapting crossover and mutation rates based on performance trends.", "code": "import numpy as np\n\nclass AdaptiveMemoryDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation_memory = []  # Store multiple historical best mutations\n        self.memory_size = 5  # Memory size for storing best mutations\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n        improvement_trend = []\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                \n                if len(self.best_mutation_memory) > 0:  # Use the memory of the best mutations\n                    historical_best = np.random.choice(self.best_mutation_memory)\n                    mutant_vector = (mutant_vector + historical_best) / 2.0\n                \n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                recent_improvement = np.mean([np.min(offspring_fitness) < np.min(fitness)])\n                improvement_trend.append(recent_improvement)\n                if len(improvement_trend) > 1:\n                    recent_trend = improvement_trend[-1] - improvement_trend[-2]\n                    mutation_rate = max(0.01, mutation_rate + self.mutation_rate_adaptation_factor * recent_trend)\n                    crossover_prob = min(1.0, max(0.3, crossover_prob + self.crossover_rate_adaptation_factor * recent_trend))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            best_offspring = offspring[best_offspring_idx]\n            if len(self.best_mutation_memory) < self.memory_size or offspring_fitness[best_offspring_idx] < func(self.best_mutation_memory[0]):\n                if len(self.best_mutation_memory) >= self.memory_size:\n                    self.best_mutation_memory.pop(0)  # Maintain memory size\n                self.best_mutation_memory.append(best_offspring)  # Update the memory\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 21, "feedback": "An exception occurred: ValueError('a must be 1-dimensional').", "error": "ValueError('a must be 1-dimensional')", "parent_ids": ["6b882b62-5d67-4e4d-b94c-4edd610c7894"], "operator": null, "metadata": {}}
{"id": "8c18c772-9f65-4529-ab3d-e58209d776f6", "fitness": 0.09245076753829355, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Introduce adaptive parameter tuning with a memory mechanism incorporating historical best solutions and trial success rates to enhance convergence and exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30  # Length of memory for successful trial rates\n        self.success_memory = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n        improvement_trend = []\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                mutation_rate = max(0.01, mutation_rate + self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 22, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09245 with standard deviation 0.00096.", "error": "", "parent_ids": ["6b882b62-5d67-4e4d-b94c-4edd610c7894"], "operator": null, "metadata": {"aucs": [0.091309568259203, 0.0936519565441345, 0.09239077781154315]}}
{"id": "565fe249-0130-411a-b411-c3ebb6eae390", "fitness": 0.0907131297763993, "name": "ImprovedAdaptiveDiffEvoOptimizer", "description": "Incorporate a diversity preservation mechanism and dynamic adaptation of population size to balance exploration and exploitation, enhancing performance under varying conditions.", "code": "import numpy as np\n\nclass ImprovedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.diversity_threshold = 0.1  # Threshold for diversity preservation\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            # Adjust population size dynamically\n            population_diversity = np.mean(np.std(population, axis=0))\n            if population_diversity < self.diversity_threshold:\n                population_size = min(self.max_population_size, population_size + 1)\n            else:\n                population_size = max(self.min_population_size, population_size - 1)\n\n            parents_idx = np.argsort(fitness)[:population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                mutation_rate = max(0.01, mutation_rate + self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 23, "feedback": "The algorithm ImprovedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09071 with standard deviation 0.00273.", "error": "", "parent_ids": ["8c18c772-9f65-4529-ab3d-e58209d776f6"], "operator": null, "metadata": {"aucs": [0.08717621533086117, 0.09381267209102084, 0.09115050190731588]}}
{"id": "6c5dd1fa-dc0a-41c2-ad11-ae4b0ffd1b19", "fitness": 0.09381030948941844, "name": "EnhancedDynamicPopDiffEvoOptimizer", "description": "Introduce adaptive population resizing, where population size dynamically adjusts based on convergence speed and diversity, to enhance exploration-exploitation balance and improve convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                mutation_rate = max(0.01, mutation_rate + self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 24, "feedback": "The algorithm EnhancedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09381 with standard deviation 0.00212.", "error": "", "parent_ids": ["8c18c772-9f65-4529-ab3d-e58209d776f6"], "operator": null, "metadata": {"aucs": [0.091309568259203, 0.09648255666829364, 0.09363880354075871]}}
{"id": "badc64bf-299b-4a89-b795-03c24db18b15", "fitness": 0.09173206498735113, "name": "EnhancedDynamicPopDiffEvoOptimizer", "description": "Introduce a convergence-based dynamic mutation rate to enhance adaptability and fine-tuning during optimization.", "code": "import numpy as np\n\nclass EnhancedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                mutation_rate = max(0.01, mutation_rate + self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n                # New dynamic mutation rate adaptation\n                mutation_rate *= (1.1 if average_success_rate > 0.75 else 0.9)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 25, "feedback": "The algorithm EnhancedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09173 with standard deviation 0.00356.", "error": "", "parent_ids": ["6c5dd1fa-dc0a-41c2-ad11-ae4b0ffd1b19"], "operator": null, "metadata": {"aucs": [0.08690326214027355, 0.09538117994160356, 0.09291175288017628]}}
{"id": "32635909-21e7-490e-af19-0d0abecf96b5", "fitness": 0.09177250665731862, "name": "EnhancedDynamicPopDiffEvoOptimizer", "description": "Introduce a success-based mutation rate reduction mechanism to further enhance adaptability and convergence speed.", "code": "import numpy as np\n\nclass EnhancedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                mutation_rate = max(0.01, mutation_rate + self.mutation_rate_adaptation_factor * (2.0 * average_success_rate - 1.0))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 26, "feedback": "The algorithm EnhancedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09177 with standard deviation 0.00356.", "error": "", "parent_ids": ["6c5dd1fa-dc0a-41c2-ad11-ae4b0ffd1b19"], "operator": null, "metadata": {"aucs": [0.08692292530688706, 0.09534850092309277, 0.09304609374197603]}}
{"id": "b8744ff2-4884-47b6-a45b-38f6885a7956", "fitness": 0.09381030948941844, "name": "AdaptiveLearningRateDiffEvoOptimizer", "description": "Introduce adaptive learning rates based on past performance and environmental changes to dynamically adjust exploration and exploitation in the optimization process.", "code": "import numpy as np\n\nclass AdaptiveLearningRateDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.learning_rate_adaptation_factor = 0.1\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                mutation_rate = max(0.01, mutation_rate + self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n                \n                # Adapt learning rate\n                learning_rate_adjustment = self.learning_rate_adaptation_factor * (0.5 - average_success_rate)\n                self.learning_rate = min(1.0, max(0.01, self.learning_rate + learning_rate_adjustment))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 27, "feedback": "The algorithm AdaptiveLearningRateDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09381 with standard deviation 0.00212.", "error": "", "parent_ids": ["6c5dd1fa-dc0a-41c2-ad11-ae4b0ffd1b19"], "operator": null, "metadata": {"aucs": [0.091309568259203, 0.09648255666829364, 0.09363880354075871]}}
{"id": "4282f1a6-aa8f-4cbe-9ae0-9b57207aee68", "fitness": 0.09267708209260273, "name": "EnhancedDynamicPopDiffEvoOptimizer", "description": "Improve convergence by adapting crossover probability using diversity measure and adding elitism to preserve the best individual.", "code": "import numpy as np\n\nclass EnhancedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            # Introducing elitism\n            best_global_idx = np.argmin(fitness)\n            best_individual = population[best_global_idx]\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adapt crossover_prob using diversity\n                diversity = np.std(population)\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.crossover_rate_adaptation_factor * (diversity - 0.5)))\n\n                mutation_rate = max(0.01, mutation_rate + self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n\n                # Adapt population size based on success rate and diversity\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n            # Preserve the best individual to ensure elitism\n            population[best_global_idx] = best_individual\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 28, "feedback": "The algorithm EnhancedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09268 with standard deviation 0.00286.", "error": "", "parent_ids": ["6c5dd1fa-dc0a-41c2-ad11-ae4b0ffd1b19"], "operator": null, "metadata": {"aucs": [0.09045821958159483, 0.09671638695348428, 0.09085663974272906]}}
{"id": "6602a082-456f-4694-b915-d97f70ef6e82", "fitness": -Infinity, "name": "EnhancedDynamicPopDiffEvoOptimizer", "description": "Integrate individual-based mutation adaptation by tracking each individual's success rate to fine-tune mutation rates, enhancing convergence speed.", "code": "import numpy as np\n\nclass EnhancedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.individual_success_rates = np.zeros(self.population_size)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit, pop_indices):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            for idx in best_idx:\n                if idx >= self.population_size:\n                    actual_idx = idx - self.population_size\n                    self.individual_success_rates[pop_indices[actual_idx]] = 1\n                else:\n                    self.individual_success_rates[idx] = 0\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            offspring_indices = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n                offspring_indices.append(np.random.randint(0, self.population_size))\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness, offspring_indices)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                mutation_rate = max(0.01, mutation_rate + self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 29, "feedback": "An exception occurred: IndexError('index 50 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 50 is out of bounds for axis 0 with size 50')", "parent_ids": ["6c5dd1fa-dc0a-41c2-ad11-ae4b0ffd1b19"], "operator": null, "metadata": {}}
{"id": "d0d75b5b-eae1-4f23-93bf-1a617eecd253", "fitness": 0.09191309332163429, "name": "EnhancedDynamicPopDiffEvoOptimizer", "description": "Enhance the algorithm by including a dynamic adjustment of crossover probability based on population diversity to improve convergence and exploration.", "code": "import numpy as np\n\nclass EnhancedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                mutation_rate = max(0.01, mutation_rate + self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n                # Dynamic crossover adjustment based on diversity\n                if diversity > 0.5:\n                    crossover_prob += 0.1\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 30, "feedback": "The algorithm EnhancedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09191 with standard deviation 0.00254.", "error": "", "parent_ids": ["6c5dd1fa-dc0a-41c2-ad11-ae4b0ffd1b19"], "operator": null, "metadata": {"aucs": [0.08875780860750992, 0.09497503697263676, 0.0920064343847562]}}
{"id": "f5f98d0b-bf49-4978-a83d-fe369a06429e", "fitness": 0.0938291872206165, "name": "EnhancedDynamicPopDiffEvoOptimizer", "description": "Enhance the dynamic population resizing strategy by incorporating a novel adaptive learning rate mechanism that adjusts based on historical performance trends to optimize exploration-exploitation balance and improve convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 31, "feedback": "The algorithm EnhancedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09383 with standard deviation 0.00214.", "error": "", "parent_ids": ["6c5dd1fa-dc0a-41c2-ad11-ae4b0ffd1b19"], "operator": null, "metadata": {"aucs": [0.09130956440758253, 0.09654339398054723, 0.09363460327371975]}}
{"id": "f32b8a05-e8aa-4095-b83f-7414f2100221", "fitness": -Infinity, "name": "EnhancedDynamicPopDiffEvoOptimizer", "description": "Enhance diversification using a novel adaptive group leader-based differential mutation strategy to dynamically balance exploration and exploitation based on group performance trends.", "code": "import numpy as np\n\nclass EnhancedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.group_diversity_factor = 0.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            # Group-based leader selection\n            group_size = self.population_size // 5\n            leaders_idx = [np.argmin(fitness[i:i+group_size]) + i for i in range(0, self.population_size, group_size)]\n            leaders = population[leaders_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(leaders), 3, replace=False)\n                x0, x1, x2 = leaders[idxs[0]], leaders[idxs[1]], leaders[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 32, "feedback": "An exception occurred: ValueError('attempt to get argmin of an empty sequence').", "error": "ValueError('attempt to get argmin of an empty sequence')", "parent_ids": ["f5f98d0b-bf49-4978-a83d-fe369a06429e"], "operator": null, "metadata": {}}
{"id": "e2bd2e3a-f56c-4317-8386-01ddfeda52e0", "fitness": 0.0915899837108493, "name": "EnhancedDynamicPopDiffEvoOptimizer", "description": "Enhance the diversity control by integrating a dynamic mutation rate strategy that adapts based on real-time population diversity metrics to improve convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                # Dynamically adjust mutation rate based on population diversity\n                mutation_rate = max(0.01, mutation_rate + diversity_factor * self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                \n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 33, "feedback": "The algorithm EnhancedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09159 with standard deviation 0.00477.", "error": "", "parent_ids": ["f5f98d0b-bf49-4978-a83d-fe369a06429e"], "operator": null, "metadata": {"aucs": [0.0849003768002462, 0.09571598370136947, 0.0941535906309322]}}
{"id": "2e001bd0-66e5-4e33-9f62-4aa3d92abcc3", "fitness": 0.07869269332821134, "name": "EnhancedDynamicControlDiffEvoOptimizer", "description": "Introduce a dynamic control strategy for mutation and crossover that adjusts based on the convergence speed and diversity, enhancing convergence and solution quality by maintaining a balance between exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedDynamicControlDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.mutation_rate = self.initial_mutation_rate\n        self.crossover_prob = self.initial_crossover_prob\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + self.mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                child = np.where(np.random.rand(self.dim) < self.crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adjust mutation and crossover rates dynamically\n                convergence_speed = np.abs(fitness.min() - fitness.mean())\n                self.mutation_rate = max(0.01, self.mutation_rate * (1 + (0.5 - average_success_rate) * 0.1))\n                self.crossover_prob = min(1.0, max(0.3, self.crossover_prob * (1 + (0.5 - convergence_speed) * 0.1)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 34, "feedback": "The algorithm EnhancedDynamicControlDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07869 with standard deviation 0.00637.", "error": "", "parent_ids": ["f5f98d0b-bf49-4978-a83d-fe369a06429e"], "operator": null, "metadata": {"aucs": [0.08619942446419382, 0.07062305592801188, 0.07925559959242834]}}
{"id": "acbd71f4-0d68-493e-899f-4c6dd3f03bd8", "fitness": 0.08807671199721852, "name": "HybridCrossoverDiffEvoOptimizer", "description": "Introduce a hybrid crossover mechanism combining differential and simulated annealing strategies to enhance exploration and convergence rates in dynamic population differential evolution.", "code": "import numpy as np\n\nclass HybridCrossoverDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n\n                # Hybrid crossover: Differential + Simulated Annealing\n                crossover_mask = np.random.rand(self.dim) < crossover_prob\n                annealing_mask = np.random.rand(self.dim) < np.exp(-1.0 / self.temperature)\n                child = np.where(crossover_mask, mutant_vector, x0)\n                child = np.where(annealing_mask, mutant_vector, child)\n\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            # Cooling schedule for simulated annealing\n            self.temperature *= self.cooling_rate\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 35, "feedback": "The algorithm HybridCrossoverDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08808 with standard deviation 0.00331.", "error": "", "parent_ids": ["f5f98d0b-bf49-4978-a83d-fe369a06429e"], "operator": null, "metadata": {"aucs": [0.0923749291610374, 0.08754323481673298, 0.08431197201388518]}}
{"id": "427879f4-efe4-4191-8186-d74ba05386fd", "fitness": 0.09313052891017415, "name": "EnhancedDynamicPopDiffEvoOptimizer", "description": "Enhance mutation strategy by incorporating success history to adaptively select parent vectors, improving exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                if len(self.success_memory) > 0 and np.random.rand() < np.mean(self.success_memory):\n                    idxs = np.random.choice(np.arange(0, len(parents)), 3, replace=False)  # Select from successful parents\n                else:\n                    idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 36, "feedback": "The algorithm EnhancedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09313 with standard deviation 0.00130.", "error": "", "parent_ids": ["f5f98d0b-bf49-4978-a83d-fe369a06429e"], "operator": null, "metadata": {"aucs": [0.09242491043812728, 0.09495083264210824, 0.09201584365028692]}}
{"id": "6de33bd1-38e5-4096-9a70-e7bf1e6c9d3b", "fitness": 0.07886860940902089, "name": "EnhancedDynamicPopDiffEvoOptimizer", "description": "Introduce a more nuanced adjustment mechanism for mutation and crossover rates based on diversity metric to enhance exploration capabilities.", "code": "import numpy as np\n\nclass EnhancedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n                # Adjust mutation and crossover rates more dynamically\n                if diversity < 0.05:\n                    mutation_rate *= (1 + diversity_factor)\n                    crossover_prob *= (1 + diversity_factor)\n                elif diversity > 0.2:\n                    mutation_rate /= (1 + diversity_factor)\n                    crossover_prob /= (1 + diversity_factor)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 37, "feedback": "The algorithm EnhancedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07887 with standard deviation 0.00734.", "error": "", "parent_ids": ["f5f98d0b-bf49-4978-a83d-fe369a06429e"], "operator": null, "metadata": {"aucs": [0.08883627035562758, 0.07137876584466296, 0.07639079202677213]}}
{"id": "46d02d59-cf72-4f83-b7e2-61f7590c5d71", "fitness": 0.09221728293273601, "name": "EnhancedDynamicPopDiffEvoOptimizer", "description": "Refine the adaptive learning rate and adjust the crossover mechanism to enhance exploration-exploitation balance and efficiency.", "code": "import numpy as np\n\nclass EnhancedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.05, self.adaptive_learning_rate + 0.5 * self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + 1.5 * self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 38, "feedback": "The algorithm EnhancedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09222 with standard deviation 0.00313.", "error": "", "parent_ids": ["f5f98d0b-bf49-4978-a83d-fe369a06429e"], "operator": null, "metadata": {"aucs": [0.09119823683323092, 0.09645912741641482, 0.08899448454856229]}}
{"id": "1e79f01f-36d3-46f7-b394-b1ef1771bf24", "fitness": 0.09394960711800719, "name": "EnhancedDynamicPopDiffEvoOptimizer", "description": "Improve diversity and exploration by introducing stochastic elements in the selection process for adaptive learning and crossover rates.", "code": "import numpy as np\n\nclass EnhancedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 39, "feedback": "The algorithm EnhancedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09395 with standard deviation 0.00203.", "error": "", "parent_ids": ["f5f98d0b-bf49-4978-a83d-fe369a06429e"], "operator": null, "metadata": {"aucs": [0.09150166487517686, 0.09647483786830013, 0.09387231861054457]}}
{"id": "76014b64-1a31-45f7-9463-998bf7153924", "fitness": 0.09321909920320744, "name": "EnhancedDynamicPopDiffEvoOptimizer", "description": "Enhance the population diversity and adapt mutation rate by including a noise factor to mimic stochastic environmental influences.", "code": "import numpy as np\n\nclass EnhancedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                noise_factor = np.random.normal(0, 0.1, self.dim)\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2) + noise_factor, lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09322 with standard deviation 0.00030.", "error": "", "parent_ids": ["1e79f01f-36d3-46f7-b394-b1ef1771bf24"], "operator": null, "metadata": {"aucs": [0.09295594200075497, 0.09363939892489126, 0.0930619566839761]}}
{"id": "04df5f6c-5329-431c-9051-365ffe7494a2", "fitness": 0.09200538710025281, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Introduce adaptive learning through historical performance to dynamically adjust mutation and crossover parameters, enhancing exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 50\n        self.mutation_step_size = 0.02\n        self.crossover_step_size = 0.02\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_step_size * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_step_size * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 41, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09201 with standard deviation 0.00221.", "error": "", "parent_ids": ["1e79f01f-36d3-46f7-b394-b1ef1771bf24"], "operator": null, "metadata": {"aucs": [0.08967603154064763, 0.09498037721760866, 0.09135975254250217]}}
{"id": "e260e5d4-f372-4b95-9835-01ed925b1908", "fitness": 0.06782367697067837, "name": "EnhancedDynamicPopDiffEvoOptimizer", "description": "Enhance the mutation diversity by adjusting the diversity factor and introducing a more intelligent selection for the best mutation vector.", "code": "import numpy as np\n\nclass EnhancedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.var(population) / self.dim  # Changed np.std to np.var\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06782 with standard deviation 0.00316.", "error": "", "parent_ids": ["1e79f01f-36d3-46f7-b394-b1ef1771bf24"], "operator": null, "metadata": {"aucs": [0.0646168029838613, 0.07212010171098504, 0.06673412621718877]}}
{"id": "3b419164-5a59-44e6-814b-c4f6a6275db6", "fitness": 0.09394960711800719, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Combine adaptive parameter control with differential evolution using feedback from diversity measures and success rates to refine exploration and exploitation balance.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 5 * dim\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - 1)\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + 1)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 43, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09395 with standard deviation 0.00203.", "error": "", "parent_ids": ["1e79f01f-36d3-46f7-b394-b1ef1771bf24"], "operator": null, "metadata": {"aucs": [0.09150166487517686, 0.09647483786830013, 0.09387231861054457]}}
{"id": "9b7b647e-a34f-40de-9803-ede14c781271", "fitness": 0.09448702251187006, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Introduce a dynamic exploration-exploitation balance by leveraging a fluctuating population size and mutation factor based on real-time success and diversity measurements.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 44, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09449 with standard deviation 0.00212.", "error": "", "parent_ids": ["1e79f01f-36d3-46f7-b394-b1ef1771bf24"], "operator": null, "metadata": {"aucs": [0.09150166487517686, 0.09616667523141864, 0.09579272742901468]}}
{"id": "39dcaa74-4d16-4394-8f02-31d38305bd87", "fitness": 0.09448702251187006, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Introduce an adaptive success rate threshold for population and mutation adjustments based on historical performance.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.adaptive_success_threshold = 0.5  # New adaptive success threshold\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - self.adaptive_success_threshold))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - self.adaptive_success_threshold)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > self.adaptive_success_threshold and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < self.adaptive_success_threshold - 0.2:  # Adjusted threshold for population size\n                    self.population_size = min(self.max_population_size, self.population_size + int((self.adaptive_success_threshold - 0.2 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 45, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09449 with standard deviation 0.00212.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.09150166487517686, 0.09616667523141864, 0.09579272742901468]}}
{"id": "09e77a5f-80ee-4190-b495-4d816bb5d3af", "fitness": 0.09448702251187006, "name": "EnhancedDynamicPopDiffEvoOptimizer", "description": "Enhance dynamic population adjustment by integrating a multi-phase adaptive mechanism for mutation and crossover rates, with strategic prioritization of high-diversity and high-impact evolution phases.", "code": "import numpy as np\n\nclass EnhancedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.diversity_threshold = 0.1\n        self.high_impact_threshold = 0.3\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Enhanced Adaptive population size based on success rate, impact, and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < self.diversity_threshold:\n                    # Low diversity and high success - reduce population to focus on exploitation\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < self.high_impact_threshold:\n                    # Low success rate - increase population to explore more\n                    self.population_size = min(self.max_population_size, self.population_size + int((self.high_impact_threshold - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 46, "feedback": "The algorithm EnhancedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09449 with standard deviation 0.00212.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.09150166487517686, 0.09616667523141864, 0.09579272742901468]}}
{"id": "0ed1127a-356b-44e0-82df-65c759cea075", "fitness": 0.08586464043841728, "name": "RefinedClusteredDiffEvoOptimizer", "description": "Introduce an adaptive population clustering mechanism and diversity-driven mutation strategies to enhance exploration-exploitation balance dynamically based on real-time performance.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass RefinedClusteredDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.cluster_count = 3\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            # Cluster-based selection\n            kmeans = KMeans(n_clusters=self.cluster_count, random_state=0).fit(population)\n            clusters = [population[kmeans.labels_ == i] for i in range(self.cluster_count)]\n            parents = np.vstack([np.random.permutation(cluster)[:max(1, len(cluster)//2)] for cluster in clusters])\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate adaptation\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Cluster-based diversity management\n                diversity = np.mean([np.std(cluster) for cluster in clusters])\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 47, "feedback": "The algorithm RefinedClusteredDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08586 with standard deviation 0.00451.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.0889107091262411, 0.07948731690143584, 0.08919589528757488]}}
{"id": "ee171785-0c7c-446d-a118-347a44aa62ac", "fitness": 0.09056188808318777, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Introduce adaptive scaling of mutation based on population diversity to improve convergence speed and accuracy.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adaptive_mutation_factor = mutation_rate * (1 + diversity_factor)  # <-- Modified line\n                mutant_vector = np.clip(x0 + adaptive_mutation_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 48, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09056 with standard deviation 0.00232.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.09201210563179085, 0.08729391456911373, 0.09237964404865873]}}
{"id": "04e1fa50-2e14-4226-8406-096726d03ccb", "fitness": 0.09448702251187006, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Enhance adaptive dynamics by incorporating a success rate threshold to dynamically fine-tune mutation, crossover rates, and population size, promoting robust convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.success_rate_threshold = 0.6\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Enhanced adaptation based on success rate threshold\n                diversity = np.std(population)\n                if average_success_rate > self.success_rate_threshold and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 49, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09449 with standard deviation 0.00212.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.09150166487517686, 0.09616667523141864, 0.09579272742901468]}}
{"id": "b46b6f05-076d-4a4f-9bf8-54e180151974", "fitness": 0.09448702251187006, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Introduce a random restart mechanism to escape local optima by reinitializing the population when progress stagnates for a set number of evaluations.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.stagnation_threshold = 200  # New threshold for stagnation\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n        stagnation_counter = 0  # Stagnation counter initialized\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            # Check for stagnation\n            if np.min(offspring_fitness) >= np.min(fitness):\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n\n            if stagnation_counter >= self.stagnation_threshold:\n                # If stagnation detected, reinitialize the population\n                population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n                fitness = np.array([func(ind) for ind in population])\n                stagnation_counter = 0\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 50, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09449 with standard deviation 0.00212.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.09150166487517686, 0.09616667523141864, 0.09579272742901468]}}
{"id": "13788ed0-f70d-49fd-84ec-1741ccf90eb9", "fitness": 0.08732708064159, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Refine exploration by incorporating a weighted mutation strategy that adapts based on historical success rates.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                weighted_mutation_rate = mutation_rate * (1.0 + self.adaptive_learning_rate)\n                mutant_vector = np.clip(x0 + weighted_mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 51, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08733 with standard deviation 0.00380.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.08691427134156848, 0.08289047704672659, 0.09217649353647495]}}
{"id": "c5c16050-acab-49eb-b5aa-8208ce77deba", "fitness": -Infinity, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Introduce adaptive offspring size based on performance trends for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            # CHANGE: Adaptive offspring size\n            offspring_size = max(self.min_population_size // 2, int(self.population_size // 2 * (1 + np.mean(self.success_memory))))\n            offspring = []\n            for _ in range(offspring_size):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 52, "feedback": "An exception occurred: ValueError('cannot convert float NaN to integer').", "error": "ValueError('cannot convert float NaN to integer')", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {}}
{"id": "3a36f68b-f612-4f26-8cc6-85f0b01ed60e", "fitness": 0.06660301847538663, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Enhance adaptive exploration-exploitation by incorporating a time-varying learning rate, diversity-guided mutation, and an adaptive crossover mechanism with success-driven population resizing.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.mutation_rate = 0.5\n        self.crossover_prob = 0.9\n        self.evaluation_chunk = 100\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.diversity_threshold = 1e-3\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                \n                # Time-varying learning rate\n                time_factor = np.exp(-evaluations / float(self.budget))\n                adaptive_mutation = self.mutation_rate * time_factor\n                mutant_vector = np.clip(x0 + adaptive_mutation * (x1 - x2), lb, ub)\n                child = np.where(np.random.rand(self.dim) < self.crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                # Adjust mutation rate and crossover probability adaptively\n                self.mutation_rate = max(0.01, self.mutation_rate + self.adaptive_learning_rate * (average_success_rate - 0.5))\n                self.crossover_prob = min(1.0, max(0.3, self.crossover_prob + self.adaptive_learning_rate * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < self.diversity_threshold:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 53, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06660 with standard deviation 0.00674.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.07524141251478578, 0.06576510500230848, 0.058802537909065644]}}
{"id": "32ec9c0d-0e3a-476c-8f96-f2adc7e2f05e", "fitness": 0.09284865094626533, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Introduce an adaptive adjustment of the crossover probability based on the diversity of the current population to balance exploration and exploitation.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                diversity = np.std(population)\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5) + diversity * 0.1))\n\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 54, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09285 with standard deviation 0.00319.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.09046931856996454, 0.09735925013026903, 0.09071738413856245]}}
{"id": "a984eb82-8542-4500-ad33-a6fff2c40130", "fitness": 0.09367853690821644, "name": "EnhancedChaoticDynamicPopDiffEvoOptimizer", "description": "Enhance exploration-exploitation with adaptive population size and chaotic map-influenced mutation rate for improved convergence and diversity balance.", "code": "import numpy as np\n\nclass EnhancedChaoticDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.chaos_factor = 0.7\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                # Incorporate chaotic map for mutation rate adaptation\n                chaos_term = np.random.uniform(0, 1)\n                chaos_term = self.chaos_factor * chaos_term * (1 - chaos_term)\n                mutation_rate = max(0.01, mutation_rate + chaos_term + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 55, "feedback": "The algorithm EnhancedChaoticDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09368 with standard deviation 0.00212.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.0920694174316653, 0.09667803695716581, 0.09228815633581822]}}
{"id": "ec5b9ddf-20ab-401d-b077-7c4be6bda025", "fitness": 0.09442486483476276, "name": "EnhancedDynamicPopDiffEvoOptimizer", "description": "Enhance adaptability by incorporating a historical success rate model to dynamically adjust mutation, crossover rates, and population diversity based on convergence behavior.", "code": "import numpy as np\n\nclass EnhancedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.success_memory_length = 50\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.best_mutation = None\n        self.history_success_rate = []\n        self.max_history_length = 100\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Historical success rate model\n                self.history_success_rate.append(average_success_rate)\n                if len(self.history_success_rate) > self.max_history_length:\n                    self.history_success_rate.pop(0)\n\n                # Trend analysis for adaptive learning rate\n                if len(self.history_success_rate) > 1:\n                    trend = self.history_success_rate[-1] - self.history_success_rate[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adaptive population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < self.diversity_threshold:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 56, "feedback": "The algorithm EnhancedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09442 with standard deviation 0.00221.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.09130946690727326, 0.09617240016643935, 0.09579272743057565]}}
{"id": "8bf79c94-c950-4b41-93d7-40ae12f38fea", "fitness": 0.09182336740014667, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Refine the offspring generation process using a weighted average between current best and mutant vectors for improved convergence.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (0.7 * mutant_vector + 0.3 * self.best_mutation)  # Changed line\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 57, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09182 with standard deviation 0.00272.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.09190670556930647, 0.09511774574540688, 0.08844565088572665]}}
{"id": "73e630c6-93a2-4d8c-9168-2723ec22d9d8", "fitness": 0.09404390990176141, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Introduce a self-adaptive parameter control strategy by fine-tuning mutation and crossover rates based on recent performance trends.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.6))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 58, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09404 with standard deviation 0.00351.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.0890899471384814, 0.09686325548935748, 0.09617852707744534]}}
{"id": "99140d40-4162-4a8c-8e8a-4a0e2ed58477", "fitness": -Infinity, "name": "EnhancedMultiPhaseDiffEvoOptimizer", "description": "Enhance convergence through multi-phase strategy by implementing a local search intensification step after initial diversification and incorporating a clustering mechanism for population management.", "code": "import numpy as np\nfrom scipy.spatial.distance import cdist\n\nclass EnhancedMultiPhaseDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.intensification_threshold = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        def local_search(x):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate = np.clip(x + perturbation, lb, ub)\n            return candidate if func(candidate) < func(x) else x\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            if success_rate < self.intensification_threshold:\n                distances = cdist(population, population)\n                np.fill_diagonal(distances, np.inf)\n                nearest_neighbors = np.argsort(distances, axis=1)[:, :5]\n                for i in range(len(population)):\n                    if np.random.rand() < 0.5:\n                        neighbors = population[nearest_neighbors[i]]\n                        cluster_center = np.mean(neighbors, axis=0)\n                        for neighbor in neighbors:\n                            population[i] = local_search(neighbor + 0.1 * (cluster_center - neighbor))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 59, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'success_rate' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'success_rate' referenced before assignment\")", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {}}
{"id": "e4076342-adae-4efb-b0d1-daae977eed10", "fitness": 0.09176744131044294, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Enhance mutation strategy by incorporating historical best solutions for guided exploration.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.best_solution_overall = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_solution_overall is not None:\n                    mutant_vector = (mutant_vector + self.best_solution_overall) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n            if self.best_solution_overall is None or np.min(fitness) < func(self.best_solution_overall):\n                self.best_solution_overall = population[np.argmin(fitness)]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 60, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09177 with standard deviation 0.00290.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.09142353758574295, 0.09548064771274256, 0.08839813863284329]}}
{"id": "5a7847ae-bd03-442c-8d54-e504955e42be", "fitness": 0.07790090236512011, "name": "EnhancedMultiScaleDiffEvoOptimizer", "description": "Enhance the exploration-exploitation balance in differential evolution by introducing a dynamic multi-scale mutation strategy and adaptive crossover probability based on fitness trends and population diversity.", "code": "import numpy as np\n\nclass EnhancedMultiScaleDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.scales = [0.5, 1.0, 1.5]  # Multi-scale mutation factors\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx]\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                \n                # Enhanced multi-scale mutation strategy\n                scale = np.random.choice(self.scales)\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + scale * mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum([func(o) < f for o, f in zip(offspring, fitness[:len(offspring)])])\n                success_rate = successful_trials / len(offspring)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedMultiScaleDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07790 with standard deviation 0.00232.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.07993555082514381, 0.07910947533601709, 0.07465768093419944]}}
{"id": "1ff274c6-2af3-4086-8083-c4c4df9d19a5", "fitness": 0.09364757084801427, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Improve convergence by introducing adaptive control on mutation rate and crossover probability based on fitness improvement variance.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.15  # Adjusted mutation rate\n        self.initial_crossover_prob = 0.6  # Adjusted crossover probability\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.1  # Adjusted adaptation factor\n        self.crossover_rate_adaptation_factor = 0.08  # Adjusted adaptation factor\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.15  # Adjusted learning rate factor\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 62, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09365 with standard deviation 0.00087.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.09252404948055726, 0.09463358269927125, 0.09378508036421429]}}
{"id": "b5a54877-54ce-4053-8d2e-9ae77c65dac1", "fitness": 0.09399297017980028, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Introduce adaptive crossover probability adjustment based on real-time diversity and success trends. ", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5) + np.random.uniform(-0.01, 0.01)))  # Added random noise\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 63, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09399 with standard deviation 0.00181.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.09159550816715911, 0.09440735163839187, 0.09597605073384985]}}
{"id": "310e3d53-2379-4783-8a5f-d305badb6a18", "fitness": 0.08588111829659728, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Enhance evolutionary dynamics by introducing a weighted average approach for crossover to maintain genetic diversity.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, (mutant_vector + x0) / 2, x0)  # Changed crossover to weighted average\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 64, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08588 with standard deviation 0.00613.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.07973684883206567, 0.09425098810996524, 0.08365551794776094]}}
{"id": "f71a10d0-bc41-4327-9975-a723da1813d1", "fitness": -Infinity, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Enhance mutation diversity by adding an exploration boost when success rate is low.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n            # Add exploration boost to mutation rate if success is low\n            if average_success_rate < 0.2:\n                mutation_rate += 0.01\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 65, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'average_success_rate' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'average_success_rate' referenced before assignment\")", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {}}
{"id": "2430a9ae-bacb-4406-aca3-0f71ac615859", "fitness": 0.09132099373723368, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Introduce momentum-based adjustment in mutation rate for better convergence stability.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.momentum = 0.9  # Adding momentum term\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                # Incorporate momentum into mutation rate adaptation\n                mutation_rate_change = self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5)\n                mutation_rate = max(0.01, mutation_rate + mutation_rate_change + self.momentum * mutation_rate_change)\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 66, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09132 with standard deviation 0.00160.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.08912781737749653, 0.09290201218036098, 0.09193315165384353]}}
{"id": "7fb5683f-6a04-45c8-a4b2-74e534c924c0", "fitness": 0.09195480529429534, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Enhance dynamic adaptation by incorporating oscillating mutation and crossover rates to improve exploration-exploitation balance.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.oscillation_factor = 0.2  # New parameter for oscillation\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                # Adaptive learning rate based on historical performance trends\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                # Introduce oscillation to mutation rate and crossover probability\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5) +\n                                    self.oscillation_factor * np.sin(evaluations / 50.0))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5) +\n                                  self.oscillation_factor * np.cos(evaluations / 50.0)))\n\n                # Adapt population size based on success rate and diversity\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 67, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09195 with standard deviation 0.00056.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.09162720903344423, 0.09274395164184002, 0.09149325520760176]}}
{"id": "88587fb1-581a-4f20-8095-d9b6c8d00a70", "fitness": 0.08843591795244125, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Enhance the mutation diversity and adapt crossover strategies dynamically to improve exploration-exploitation balance.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.15  # Changed from 0.1\n        self.initial_crossover_prob = 0.8  # Changed from 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.07  # Changed from 0.05\n        self.crossover_rate_adaptation_factor = 0.05  # Changed from 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.15  # Changed from 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2 + np.random.normal(0, 0.1, size=self.dim)), lb, ub)  # Added random noise\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation) / 2.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 68, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08844 with standard deviation 0.00541.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.08951037194720568, 0.08133373161576318, 0.09446365029435488]}}
{"id": "9490e37a-b63e-4291-9f6c-da0fe99e1dc6", "fitness": 0.09486369697145314, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Incorporate a memory-based mutation strategy to enhance convergence by leveraging historical best solutions.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * diversity_factor * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0  # Change here\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 69, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09486 with standard deviation 0.00139.", "error": "", "parent_ids": ["9b7b647e-a34f-40de-9803-ede14c781271"], "operator": null, "metadata": {"aucs": [0.09431765377724499, 0.09676936254086743, 0.09350407459624699]}}
{"id": "930b546d-882d-4d76-8f4c-ddcf232f71ad", "fitness": 0.09413850449916288, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Introduce adaptive differential weights and dynamic population resizing based on success diversity to enhance convergence speed and solution accuracy.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adaptive_weight = 0.5 + np.random.rand() * 0.5  # Change here\n                mutant_vector = np.clip(x0 + adaptive_weight * diversity_factor * (x1 - x2), lb, ub)  # Change here\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3 or diversity > 0.2:  # Change here\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 70, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09414 with standard deviation 0.00071.", "error": "", "parent_ids": ["9490e37a-b63e-4291-9f6c-da0fe99e1dc6"], "operator": null, "metadata": {"aucs": [0.09505940171813554, 0.09401750374005235, 0.09333860803930072]}}
{"id": "8602521c-b071-4b0d-9ff1-04ad89b98307", "fitness": 0.09487210379767681, "name": "RefinedDynamicPopDiffEvoOptimizer", "description": "Introduce gradual reduction of mutation influence based on adaptive learning rate to enhance precision near convergence.", "code": "import numpy as np\n\nclass RefinedDynamicPopDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                # Change here to use adaptive_learning_rate to scale mutation effect\n                mutant_vector = np.clip(x0 + mutation_rate * self.adaptive_learning_rate * diversity_factor * (x1 - x2), lb, ub)  \n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 71, "feedback": "The algorithm RefinedDynamicPopDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09487 with standard deviation 0.00138.", "error": "", "parent_ids": ["9490e37a-b63e-4291-9f6c-da0fe99e1dc6"], "operator": null, "metadata": {"aucs": [0.0943360356821793, 0.09676936254648971, 0.09351091316436144]}}
{"id": "3cc0971f-c4b1-4a5e-9698-187a97abfed5", "fitness": 0.09541760524177267, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Implement diversity-enhanced adaptive differential evolution with feedback-driven mutation scaling for better convergence control in dynamic environments.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 72, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09542 with standard deviation 0.00277.", "error": "", "parent_ids": ["8602521c-b071-4b0d-9ff1-04ad89b98307"], "operator": null, "metadata": {"aucs": [0.09173319586917583, 0.0984295617107267, 0.09609005814541549]}}
{"id": "01b4dbdb-4018-45d9-8e2d-684c525bd00a", "fitness": 0.09450360266563258, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Fine-tune the scaling factor and mutation strategy in the adaptive differential evolution to enhance convergence speed without compromising diversity.  ", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor \n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.median(population, axis=0)) / 3.0  \n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 73, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09450 with standard deviation 0.00295.", "error": "", "parent_ids": ["3cc0971f-c4b1-4a5e-9698-187a97abfed5"], "operator": null, "metadata": {"aucs": [0.09220785625196593, 0.0986637891906611, 0.09263916255427074]}}
{"id": "29fabe62-21bc-4fd5-8d34-2156a6004916", "fitness": 0.09511930945977909, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Implement a multi-strategy adaptive differential evolution with dynamic population resizing and stochastic gradient-inspired mutation to enhance adaptability and convergence efficiency. ", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.gradient_factor = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                gradient_term = self.gradient_factor * (x0 - np.mean(population, axis=0))\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2 + gradient_term), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend + np.random.uniform(-0.01, 0.01))\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09512 with standard deviation 0.00290.", "error": "", "parent_ids": ["3cc0971f-c4b1-4a5e-9698-187a97abfed5"], "operator": null, "metadata": {"aucs": [0.09126974543956778, 0.09826448308682678, 0.0958236998529427]}}
{"id": "b8173871-576c-4849-8dea-8372e7f6345a", "fitness": 0.09562500212741047, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Integrate a self-adaptive mechanism for dynamically adjusting mutation and crossover rates based on recent improvement trends to enhance convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 75, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09563 with standard deviation 0.00250.", "error": "", "parent_ids": ["3cc0971f-c4b1-4a5e-9698-187a97abfed5"], "operator": null, "metadata": {"aucs": [0.09235538652608921, 0.0984295617107267, 0.09609005814541549]}}
{"id": "5048e5a0-88fe-4c4c-a631-f77d1b7c4895", "fitness": 0.09552754688632963, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Introduce a dynamic population resizing mechanism based on convergence stability and diversity to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n                else:\n                    # Dynamic adjustment based on stability and diversity\n                    if np.std(self.success_memory[-5:]) < 0.02:\n                        self.population_size = max(self.min_population_size, self.population_size - 1)\n                    elif diversity > 0.2:\n                        self.population_size = min(self.max_population_size, self.population_size + 1)\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 76, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09553 with standard deviation 0.00263.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.09206302080284667, 0.0984295617107267, 0.09609005814541549]}}
{"id": "c1ad1e50-b704-4938-a656-070114cd9a37", "fitness": 0.09562500212741047, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Introducing dynamic scaling based on offspring diversity to improve exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                offspring_diversity = np.std(offspring)  # Added line\n                if average_success_rate > 0.5 and offspring_diversity < 0.1:  # Modified line\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 77, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09563 with standard deviation 0.00250.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.09235538652608921, 0.0984295617107267, 0.09609005814541549]}}
{"id": "63d3e480-f92f-4e04-befc-8a94ab63815d", "fitness": 0.09259116336708206, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Fine-tune the adaptation of mutation and crossover rates by integrating historical diversity feedback for more robust convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.diversity_memory = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                current_diversity = np.std(population)\n                self.diversity_memory.append(current_diversity)\n                if len(self.diversity_memory) > self.success_memory_length:\n                    self.diversity_memory.pop(0)\n                avg_diversity = np.mean(self.diversity_memory)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5) * (1 + avg_diversity))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5) * (1 + avg_diversity)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09259 with standard deviation 0.00343.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.08799964048640974, 0.09624875209686012, 0.09352509751797633]}}
{"id": "54f28f8f-92a6-48bc-a9a6-8dde69707e5d", "fitness": 0.09266863852259133, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Introduce a crowding mechanism to maintain diversity and a dynamic scaling factor to improve exploration-exploitation balance in the EnhancedAdaptiveDiffEvoOptimizer.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.crowding_factor = 0.1\n        self.dynamic_scaling_factor = 0.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.dynamic_scaling_factor * diversity_factor\n                crowding_distance = np.sum(np.abs(x0 - population), axis=1)\n                close_indices = np.argsort(crowding_distance)[:3]\n                x1, x2 = population[close_indices[1]], population[close_indices[2]]\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 79, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09267 with standard deviation 0.00236.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.0903813893379124, 0.09591379493235708, 0.09171073129750451]}}
{"id": "9a0d2f3e-74cc-415b-ade1-325eb7fa9078", "fitness": 0.0752816869730153, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Improved adaptive mechanisms with increased population diversity and dynamic learning rate adjustment for enhanced convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                adapt_scaling = 1 + self.adaptive_learning_rate * np.std(population) / self.dim\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if np.random.rand() < 0.1:\n                    mutant_vector += np.random.normal(0, 0.1, self.dim)\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 80, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07528 with standard deviation 0.00551.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.08203897608027022, 0.0685434734022724, 0.07526261143650326]}}
{"id": "770defac-cf75-464d-ac50-d9a8df3624e3", "fitness": 0.09556009168394097, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Introduce a dynamic population resizing strategy based on fitness diversity and success history to maintain exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n                elif diversity > 0.2:\n                    self.population_size = min(self.max_population_size, self.population_size + int(diversity * self.population_size))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09556 with standard deviation 0.00259.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.09216065519568073, 0.0984295617107267, 0.09609005814541549]}}
{"id": "e4d36a84-60ab-471f-9e39-7a80ac7adf0f", "fitness": 0.09562500212741047, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Utilize dynamic population resizing and adaptive learning rate based on convergence diversity to enhance solution quality and robustness in diverse optimization landscapes.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 82, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09563 with standard deviation 0.00250.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.09235538652608921, 0.0984295617107267, 0.09609005814541549]}}
{"id": "49505d2b-3bee-4002-a718-446c8a62f3bd", "fitness": 0.09474316305856519, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Introduce a dynamic diversity preservation mechanism to balance exploration and exploitation, leveraging adaptive learning rates for both mutation and crossover, and incorporating an elitist strategy to enhance convergence reliability.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.elite_fraction = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            elite_count = int(self.elite_fraction * self.population_size)\n            elite_idx = np.argsort(fitness)[:elite_count]\n            elites = population[elite_idx]\n\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n            population[:elite_count] = elites\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09474 with standard deviation 0.00208.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.09205581992810907, 0.09713747122039351, 0.09503619802719299]}}
{"id": "5f830a6d-168d-4d94-8a8c-1949dbfcaf6b", "fitness": 0.09467474083388132, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Introduce a memory-based amplification factor to improve mutation effectiveness in dynamic environments.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.memory_amplification_factor = 1.2  # New parameter\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor * self.memory_amplification_factor  # Modified line\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 84, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09467 with standard deviation 0.00169.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.09445035975631022, 0.09684969299156776, 0.09272416975376596]}}
{"id": "06149af8-3802-4f10-9026-2a202b9ccd50", "fitness": 0.0948244465327122, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Implement a more precise success-memory-based adaptation for mutation and crossover rates, enhancing both exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.6))  # Adjusted success threshold\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.4)))  # Adjusted success threshold\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09482 with standard deviation 0.00336.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.0905543752786876, 0.09877281758785506, 0.09514614673159394]}}
{"id": "0a2d3cf2-d2aa-436d-8318-6d2eb20df4cc", "fitness": 0.09372017374435022, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Enhance exploration by adapting mutation strategy to include a random perturbation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2) + np.random.normal(0, 0.1, self.dim), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 86, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09372 with standard deviation 0.00071.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.09452984279146814, 0.09280100176329609, 0.09382967667828646]}}
{"id": "28322cd1-3f48-4c5f-8d3f-8a80474ce995", "fitness": 0.09562500212741047, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Enhance the adaptive differential evolution strategy by incorporating a dynamic scaling factor based on the diversity and success rate, along with a competitive selection strategy to boost exploration and exploitation trade-offs.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09563 with standard deviation 0.00250.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.09235538652608921, 0.0984295617107267, 0.09609005814541549]}}
{"id": "464a228e-d858-4e29-98c0-5b9bf9d2dfd7", "fitness": 0.07079551672273705, "name": "DynamicPopAdaptiveDiffEvoOptimizer", "description": "Introduce a dynamic population sizing strategy and incorporate fitness diversity monitoring to stabilize convergence and enhance solution discovery in varying landscapes.", "code": "import numpy as np\n\nclass DynamicPopAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.diversity_threshold = 1e-5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.6 and diversity < self.diversity_threshold:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.4:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.4 - average_success_rate) * self.population_size + 1))\n\n                if evaluations + self.population_size > self.budget:\n                    self.population_size = self.budget - evaluations\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 88, "feedback": "The algorithm DynamicPopAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07080 with standard deviation 0.00050.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.07131087267966807, 0.07096482107888735, 0.07011085640965575]}}
{"id": "4f61f91b-7180-4ccd-b57b-ca092245f480", "fitness": 0.09562319391956915, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Introduce adaptive population size control through diversity-monitoring and adaptive differential weights to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.2 # Updated for faster adaptation\n        self.adaptive_learning_rate = 1.0\n        self.diversity_threshold = 0.2 # New parameter for diversity control\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                differential_weight = mutation_rate * adapt_scaling # Adjusted mutation weight\n                mutant_vector = np.clip(x0 + differential_weight * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < self.diversity_threshold: # Adjusted condition for diversity\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 89, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09562 with standard deviation 0.00250.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.09234996190256528, 0.0984295617107267, 0.09609005814541549]}}
{"id": "81f7dbf2-8cad-4735-a37d-ebd627a281e2", "fitness": 0.09101176598685501, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Enhanced mutation strategy by introducing dynamic scaling based on fitness variance to accelerate convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                fitness_variance = np.var(fitness) / (np.mean(fitness) + 1e-8)  # New line\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * fitness_variance * (x1 - x2), lb, ub)  # Modified line\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 90, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09101 with standard deviation 0.00326.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.09515015639311142, 0.08718244596200075, 0.09070269560545285]}}
{"id": "536676c7-5c09-4548-a1bb-db1ccf8e834e", "fitness": 0.09562500159463701, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Slightly increased the learning rate factor for mutation and crossover rate adaptation to enhance responsiveness to convergence trends.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.12  # Changed from 0.1 to 0.12\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 91, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09563 with standard deviation 0.00250.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.09235538492776885, 0.0984295617107267, 0.09609005814541549]}}
{"id": "916b41ca-9239-435e-8351-f6d84298df86", "fitness": 0.09511903183358561, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Introduce a diversity preservation mechanism to ensure a balanced exploration-exploitation trade-off in the optimization process.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def diversity_preservation(self, population, lb, ub):\n        # Ensure diversity by adding a small random noise to the population\n        diversity_factor = 0.01 * (ub - lb)\n        noisy_population = population + np.random.uniform(-diversity_factor, diversity_factor, population.shape)\n        return np.clip(noisy_population, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n\n            # Apply diversity preservation\n            offspring = self.diversity_preservation(offspring, lb, ub)\n\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09512 with standard deviation 0.00310.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.09182326153317721, 0.09926490866854798, 0.09426892529903164]}}
{"id": "88839069-7299-41f3-8f63-388edc064397", "fitness": 0.09562500212741047, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Introduce a dynamic scaling factor adjustment based on average success rate to better balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 93, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09563 with standard deviation 0.00250.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.09235538652608921, 0.0984295617107267, 0.09609005814541549]}}
{"id": "9dc2db8c-d64d-4c11-a163-531c2b80f74e", "fitness": 0.09391223813567666, "name": "DynamicPopResizingDiffEvoOptimizer", "description": "Incorporate a dynamic population resizing mechanism based on convergence metrics to enhance exploration and exploitation trade-offs.", "code": "import numpy as np\n\nclass DynamicPopResizingDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 20 * dim  # Adjusted for more flexibility\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.dynamic_resizing_factor = 0.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, int(self.population_size * (1 - self.dynamic_resizing_factor)))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, int(self.population_size * (1 + self.dynamic_resizing_factor)))\n\n                if self.population_size < self.min_population_size:\n                    self.population_size = self.min_population_size\n                elif self.population_size > self.max_population_size:\n                    self.population_size = self.max_population_size\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 94, "feedback": "The algorithm DynamicPopResizingDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09391 with standard deviation 0.00302.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.09037227432906914, 0.09775163088366123, 0.0936128091942996]}}
{"id": "06f27524-24b4-47fe-a93e-37a8776c90a9", "fitness": 0.09430668211134026, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Introduce a multi-scale approach with dynamic regrouping to enhance exploration and exploitation by varying mutation strategies based on population diversity and convergence trends.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.scale_levels = [0.5, 1, 2]  # Different scales for mutation strategies\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                scale_choice = np.random.choice(self.scale_levels)\n                mutant_vector = np.clip(x0 + scale_choice * mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 95, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09431 with standard deviation 0.00237.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.09166536106477141, 0.0974077080105773, 0.09384697725867208]}}
{"id": "37d64bff-d670-4e07-bfb4-5a248ae5d088", "fitness": 0.09556536385075469, "name": "MultiScaleAdaptiveDiffEvoOptimizer", "description": "Leverage multi-scale adaptive strategies to enhance convergence by dynamically tuning population diversity and learning rates based on population dispersion and historical success metrics.", "code": "import numpy as np\n\nclass MultiScaleAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.mutation_rate = 0.1\n        self.crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.population_size = self.initial_population_size\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.adaptive_learning_rate = 1.0\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + self.mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < self.crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + trend)\n\n                self.mutation_rate = max(0.01, self.mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                self.crossover_prob = min(1.0, max(0.3, self.crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < self.diversity_threshold:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 96, "feedback": "The algorithm MultiScaleAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09557 with standard deviation 0.00258.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.09217647169612186, 0.0984295617107267, 0.09609005814541549]}}
{"id": "9a4c43f6-31c6-4be9-9a5d-76faa90e942c", "fitness": 0.07227548234847818, "name": "HybridArchiveDiffEvoOptimizer", "description": "Introduce a hybridized strategy combining adaptive differential evolution with a memory-enriched archive of elite solutions to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass HybridArchiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.archive = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n                # Archive management\n                best_offspring_idx = np.argmin(offspring_fitness)\n                if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                    self.best_mutation = offspring[best_offspring_idx]\n                self.archive.append(self.best_mutation)\n                if len(self.archive) > 50:\n                    self.archive.pop(0)\n\n                # Hybridization step\n                if len(self.archive) > 5:\n                    archive_sample = np.array(self.archive)\n                    archive_mean = np.mean(archive_sample, axis=0)\n                    archive_diverse = np.std(archive_sample, axis=0)\n                    for ind in population:\n                        if np.random.rand() < 0.3:\n                            ind = ind + 0.1 * (archive_mean - ind) * archive_diverse\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 97, "feedback": "The algorithm HybridArchiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07228 with standard deviation 0.01200.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.08176981457352284, 0.05535070836985678, 0.0797059241020549]}}
{"id": "0b8ac134-38e9-49ec-a483-0f2842a3877e", "fitness": 0.09535954681321883, "name": "EnhancedAdaptiveDiffEvoOptimizer", "description": "Introduce a dynamic adaptation mechanism for population size based on diversity and success trends to enhance exploration and exploitation balance in differential evolution.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.initial_mutation_rate = 0.1\n        self.initial_crossover_prob = 0.7\n        self.evaluation_chunk = 100\n        self.mutation_rate_adaptation_factor = 0.05\n        self.crossover_rate_adaptation_factor = 0.03\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        mutation_rate = self.initial_mutation_rate\n        crossover_prob = self.initial_crossover_prob\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor\n                mutant_vector = np.clip(x0 + mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                mutation_rate = max(0.01, mutation_rate + self.adaptive_learning_rate * self.mutation_rate_adaptation_factor * (average_success_rate - 0.5))\n                crossover_prob = min(1.0, max(0.3, crossover_prob + self.adaptive_learning_rate * self.crossover_rate_adaptation_factor * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(0.2 * self.population_size + 1))\n                elif average_success_rate < 0.3 or diversity > 0.2:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 2))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 98, "feedback": "The algorithm EnhancedAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09536 with standard deviation 0.00194.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.0928363356053612, 0.09756695825624351, 0.09567534657805177]}}
{"id": "562b6cc5-91bb-4137-afd3-b2f9bf1a5cc0", "fitness": 0.09181230555982023, "name": "ChaoticAdaptiveDiffEvoOptimizer", "description": "Introduce chaotic sequences and adaptive population resizing to enhance exploration and exploitation balance dynamically.", "code": "import numpy as np\n\nclass ChaoticAdaptiveDiffEvoOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5 * dim, 50)\n        self.mutation_rate = 0.1\n        self.crossover_prob = 0.7\n        self.evaluation_chunk = 50\n        self.min_population_size = dim\n        self.max_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.learning_rate_factor = 0.1\n        self.adaptive_learning_rate = 1.0\n        self.best_mutation = None\n        self.success_memory_length = 30\n        self.success_memory = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        \n        chaotic_sequence = np.random.rand(self.budget)\n        chaotic_sequence = 4 * chaotic_sequence * (1 - chaotic_sequence)  # Logistic map for chaos\n\n        def evaluate_and_select(pop, fit):\n            offspring_fitness = np.array([func(ind) for ind in pop])\n            combined_population = np.vstack((population, pop))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_idx = np.argsort(combined_fitness)[:self.population_size]\n            return combined_population[best_idx], combined_fitness[best_idx], offspring_fitness\n\n        while evaluations < self.budget:\n            parents_idx = np.argsort(fitness)[:self.population_size // 2]\n            parents = population[parents_idx]\n\n            offspring = []\n            for _ in range(self.population_size // 2):\n                idxs = np.random.choice(len(parents), 3, replace=False)\n                x0, x1, x2 = parents[idxs[0]], parents[idxs[1]], parents[idxs[2]]\n                diversity_factor = np.std(population) / self.dim\n                adapt_scaling = 1 + self.adaptive_learning_rate * diversity_factor * chaotic_sequence[evaluations % self.budget]\n                mutant_vector = np.clip(x0 + self.mutation_rate * adapt_scaling * (x1 - x2), lb, ub)\n                if self.best_mutation is not None:\n                    mutant_vector = (mutant_vector + self.best_mutation + np.mean(population, axis=0)) / 3.0\n                child = np.where(np.random.rand(self.dim) < self.crossover_prob, mutant_vector, x0)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            evaluations += len(offspring)\n            population, fitness, offspring_fitness = evaluate_and_select(offspring, fitness)\n\n            if evaluations % self.evaluation_chunk == 0:\n                successful_trials = np.sum(offspring_fitness < fitness[:len(offspring_fitness)])\n                success_rate = successful_trials / len(offspring_fitness)\n                self.success_memory.append(success_rate)\n                if len(self.success_memory) > self.success_memory_length:\n                    self.success_memory.pop(0)\n                average_success_rate = np.mean(self.success_memory)\n\n                if len(self.success_memory) > 1:\n                    trend = self.success_memory[-1] - self.success_memory[-2]\n                    self.adaptive_learning_rate = max(0.1, self.adaptive_learning_rate + self.learning_rate_factor * trend)\n\n                self.mutation_rate = max(0.01, self.mutation_rate + self.adaptive_learning_rate * 0.05 * (average_success_rate - 0.5))\n                self.crossover_prob = min(1.0, max(0.3, self.crossover_prob + self.adaptive_learning_rate * 0.03 * (average_success_rate - 0.5)))\n\n                diversity = np.std(population)\n                if average_success_rate > 0.5 and diversity < 0.1:\n                    self.population_size = max(self.min_population_size, self.population_size - int(diversity * self.population_size + 1))\n                elif average_success_rate < 0.3:\n                    self.population_size = min(self.max_population_size, self.population_size + int((0.3 - average_success_rate) * self.population_size + 1))\n\n            best_offspring_idx = np.argmin(offspring_fitness)\n            if self.best_mutation is None or offspring_fitness[best_offspring_idx] < func(self.best_mutation):\n                self.best_mutation = offspring[best_offspring_idx]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]", "configspace": "", "generation": 99, "feedback": "The algorithm ChaoticAdaptiveDiffEvoOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09181 with standard deviation 0.00293.", "error": "", "parent_ids": ["b8173871-576c-4849-8dea-8372e7f6345a"], "operator": null, "metadata": {"aucs": [0.0956432446325034, 0.088543669975363, 0.09125000207159428]}}
