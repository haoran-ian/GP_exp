{"id": "c26949a3-7495-42e1-a23d-5cc9ecafed6b", "fitness": -Infinity, "name": "AQPSO", "description": "Introducing an Adaptive Quantum-inspired Particle Swarm Optimization (AQPSO) that dynamically adjusts parameters and leverages quantum principles for enhanced exploration and exploitation in black-box optimization.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        for _ in range(self.budget):\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Calculate quantum-inspired movement\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = phi * self.velocities[i] + self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) + \\\n                                     self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i])\n                self.positions[i] += self.velocities[i]\n                \n                # Handle position bounds\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                # Evaluate the function\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 0, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ubuntu/GP_Compare/LLaMEA/llamea/llamea.py\", line 188, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ubuntu/GP_Compare/LLaMEA/llamea/llamea.py\", line 247, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"scripts/5_algorithm_generation.py\", line 131, in evaluate_gp_func\n    algorithm(problem)\n  File \"<string>\", line 29, in __call__\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'float'\n.", "error": "TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\")Traceback (most recent call last):\n  File \"/home/ubuntu/GP_Compare/LLaMEA/llamea/llamea.py\", line 188, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ubuntu/GP_Compare/LLaMEA/llamea/llamea.py\", line 247, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"scripts/5_algorithm_generation.py\", line 131, in evaluate_gp_func\n    algorithm(problem)\n  File \"<string>\", line 29, in __call__\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'float'\n", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "62a4f16a-32a3-4df1-955a-54362b575c86", "fitness": 0.02151290581144664, "name": "AQPSO", "description": "Enhanced Adaptive Quantum-inspired Particle Swarm Optimization (EAQPSO) with dynamic inertia weight and boundary handling for improved exploration and exploitation.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9  # Added dynamic inertia weight\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)  # Initialized\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        for _ in range(self.budget):\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Adjust inertia weight dynamically\n                self.inertia_weight = 0.5 + np.random.rand() / 2\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                # Handle position bounds\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                # Evaluate the function\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                # Update personal and global bests\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 1, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02151 with standard deviation 0.00000.", "error": "", "parent_ids": ["c26949a3-7495-42e1-a23d-5cc9ecafed6b"], "operator": null, "metadata": {"aucs": [0.021513721400816865, 0.021512970423878564, 0.02151202560964449]}}
{"id": "d767a882-c3c1-49cd-946f-76092e0ecd8e", "fitness": 0.026386624384807395, "name": "AQPSO", "description": "Improved Adaptive Quantum-inspired Particle Swarm Optimization (AQPSO) with stochastic restarts and crowding distance mechanism to enhance diversity and convergence.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5  # 1 change: added restart mechanism\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0:\n                # 2 change: stochastic restarts to improve diversity\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))            \n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.inertia_weight = 0.5 + np.random.rand() / 2\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 2, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02639 with standard deviation 0.00368.", "error": "", "parent_ids": ["62a4f16a-32a3-4df1-955a-54362b575c86"], "operator": null, "metadata": {"aucs": [0.030327367006532602, 0.02147947599239186, 0.027353030155497726]}}
{"id": "9f2efb26-6c56-461d-81a2-f2cbc5f61bf4", "fitness": 0.025230293762402083, "name": "AQPSO", "description": "Introduced a non-linear decreasing inertia weight to balance exploration and exploitation dynamically.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))            \n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.inertia_weight = 0.9 - 0.4 * (self.evaluations / self.budget)\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 3, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02523 with standard deviation 0.00364.", "error": "", "parent_ids": ["d767a882-c3c1-49cd-946f-76092e0ecd8e"], "operator": null, "metadata": {"aucs": [0.02151503400419641, 0.02399536989835982, 0.030180477384650017]}}
{"id": "ae27f1e7-68df-4f1a-8c9c-12810d200a73", "fitness": 0.024995319227042123, "name": "AQPSO", "description": "Introduced adaptive inertia weight adjusted based on improvement rate to balance exploration and exploitation dynamically.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5  # 1 change: added restart mechanism\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0:\n                # 2 change: stochastic restarts to improve diversity\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))            \n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Adaptive inertia weight based on improvement rate\n                if self.evaluations > 0:\n                    improvement_rate = (self.global_best_score - min(self.best_scores)) / self.global_best_score\n                    self.inertia_weight = 0.5 + 0.4 * improvement_rate\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 4, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02500 with standard deviation 0.00392.", "error": "", "parent_ids": ["d767a882-c3c1-49cd-946f-76092e0ecd8e"], "operator": null, "metadata": {"aucs": [0.02151503400419641, 0.023005785325284633, 0.030465138351645327]}}
{"id": "ce2e6e7d-7e1b-4e2b-9793-eb4b279c9d7a", "fitness": 0.026621314219118286, "name": "AQPSO", "description": "Introduced a dynamic adjustment of the cognitive coefficient (c1) using cosine annealing to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.inertia_weight = 0.5 + np.random.rand() / 2\n\n                # 1 change: dynamic adjustment of c1 using cosine annealing \n                self.c1 = 2.0 * (1 + np.cos(np.pi * self.evaluations / self.budget)) / 2\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 5, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02662 with standard deviation 0.00317.", "error": "", "parent_ids": ["d767a882-c3c1-49cd-946f-76092e0ecd8e"], "operator": null, "metadata": {"aucs": [0.030364794221766078, 0.02261361135966955, 0.02688553707591923]}}
{"id": "680db54a-7a49-493e-9661-3390c70af3f3", "fitness": 0.02618645492758816, "name": "AQPSO", "description": "Enhanced exploration by introducing adaptive swarm size based on budget usage.  ", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n                \n            # Change: Adaptive swarm size based on budget usage\n            self.swarm_size = max(10, int(30 * (1 - self.evaluations / self.budget)))\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.inertia_weight = 0.5 + np.random.rand() / 2\n\n                self.c1 = 2.0 * (1 + np.cos(np.pi * self.evaluations / self.budget)) / 2\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 6, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02619 with standard deviation 0.00366.", "error": "", "parent_ids": ["ce2e6e7d-7e1b-4e2b-9793-eb4b279c9d7a"], "operator": null, "metadata": {"aucs": [0.030399092750674428, 0.02147947599239186, 0.026680796039698196]}}
{"id": "bd402951-4d66-4fd3-8a1f-44a501ed77e3", "fitness": 0.02811010459506254, "name": "AQPSO", "description": "Introduced a dynamic adjustment of the social coefficient (c2) using cosine annealing to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.inertia_weight = 0.5 + np.random.rand() / 2\n\n                # 1 change: dynamic adjustment of c1 using cosine annealing \n                self.c1 = 2.0 * (1 + np.cos(np.pi * self.evaluations / self.budget)) / 2\n                # 2 change: dynamic adjustment of c2 using cosine annealing \n                self.c2 = 2.0 * (1 + np.cos(np.pi * self.evaluations / self.budget)) / 2\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 7, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02811 with standard deviation 0.00253.", "error": "", "parent_ids": ["ce2e6e7d-7e1b-4e2b-9793-eb4b279c9d7a"], "operator": null, "metadata": {"aucs": [0.030688497168888773, 0.02467701688084445, 0.028964799735454405]}}
{"id": "a53b7e88-98c3-483c-8ebf-0be97e54a214", "fitness": 0.02811010459506254, "name": "AQPSO", "description": "Enhance AQPSO by introducing an adaptive restart mechanism based on stagnation detection for improved exploration.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        stagnation_count = 0  # Counter for stagnation detection\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 or stagnation_count >= self.swarm_size * 2:  # Adaptive restart\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n                stagnation_count = 0  # Reset stagnation counter\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.inertia_weight = 0.5 + np.random.rand() / 2\n\n                # Dynamic adjustment of c1 using cosine annealing \n                self.c1 = 2.0 * (1 + np.cos(np.pi * self.evaluations / self.budget)) / 2\n                # Dynamic adjustment of c2 using cosine annealing \n                self.c2 = 2.0 * (1 + np.cos(np.pi * self.evaluations / self.budget)) / 2\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                    stagnation_count = 0  # Reset stagnation if improvement\n                else:\n                    stagnation_count += 1\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 8, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02811 with standard deviation 0.00253.", "error": "", "parent_ids": ["bd402951-4d66-4fd3-8a1f-44a501ed77e3"], "operator": null, "metadata": {"aucs": [0.030688497168888773, 0.02467701688084445, 0.028964799735454405]}}
{"id": "72227f32-61d5-4b4e-9a06-f0f67cb244f6", "fitness": 0.027540708413364867, "name": "AQPSO", "description": "Introduced a restart mechanism with random best position selection to enhance exploration in AQPSO.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n                self.global_best_position = self.best_positions[np.random.randint(0, self.swarm_size)].copy()  # Change: Random best position selection during restart\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.inertia_weight = 0.5 + np.random.rand() / 2\n\n                # 1 change: dynamic adjustment of c1 using cosine annealing \n                self.c1 = 2.0 * (1 + np.cos(np.pi * self.evaluations / self.budget)) / 2\n                # 2 change: dynamic adjustment of c2 using cosine annealing \n                self.c2 = 2.0 * (1 + np.cos(np.pi * self.evaluations / self.budget)) / 2\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 9, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02754 with standard deviation 0.00129.", "error": "", "parent_ids": ["bd402951-4d66-4fd3-8a1f-44a501ed77e3"], "operator": null, "metadata": {"aucs": [0.026517189844941336, 0.026745795646734183, 0.029359139748419083]}}
{"id": "a69b38b1-1fe8-438c-bbe2-f8dc1f0576dc", "fitness": 0.030639504844021004, "name": "AQPSO", "description": "Implement an adaptive velocity update mechanism where inertia weight is dynamically adjusted based on particle's personal best improvement.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Change 1: dynamic inertia weight based on personal best improvement\n                if self.best_scores[i] < np.inf:\n                    self.inertia_weight = 0.5 + 0.4 * (1 - (self.best_scores[i] / (self.global_best_score + 1e-10)))\n\n                self.c1 = 2.0 * (1 + np.cos(np.pi * self.evaluations / self.budget)) / 2\n                self.c2 = 2.0 * (1 + np.cos(np.pi * self.evaluations / self.budget)) / 2\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 10, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03064 with standard deviation 0.00253.", "error": "", "parent_ids": ["bd402951-4d66-4fd3-8a1f-44a501ed77e3"], "operator": null, "metadata": {"aucs": [0.03042558332133627, 0.0276524151714983, 0.03384051603922844]}}
{"id": "cd60d2cd-dc43-42a6-894c-b5ecb983af63", "fitness": 0.03230364364805701, "name": "AQPSO", "description": "Incrementally adjust the cognitive and social coefficients dynamically based on the evaluations, aiming for a balanced exploration-exploitation trade-off.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Change 1: dynamic inertia weight based on personal best improvement\n                if self.best_scores[i] < np.inf:\n                    self.inertia_weight = 0.5 + 0.4 * (1 - (self.best_scores[i] / (self.global_best_score + 1e-10)))\n\n                # Change 2: new dynamic update for c1 and c2\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio  # Decreases over time\n                self.c2 = 0.5 + progress_ratio  # Increases over time\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 11, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03230 with standard deviation 0.00372.", "error": "", "parent_ids": ["a69b38b1-1fe8-438c-bbe2-f8dc1f0576dc"], "operator": null, "metadata": {"aucs": [0.029628104803496202, 0.029712842288141417, 0.037569983852533406]}}
{"id": "5479558f-a535-4ad4-810b-144bf99e9e51", "fitness": 0.021510101997618197, "name": "AQPSO", "description": "Incorporate a dynamic restart mechanism based on swarm stagnation to enhance the adaptive balancing of exploration and exploitation.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and np.allclose(self.positions, self.global_best_position, atol=1e-3):\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))  # Dynamic restart if swarm stagnates\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Change 1: dynamic inertia weight based on personal best improvement\n                if self.best_scores[i] < np.inf:\n                    self.inertia_weight = 0.5 + 0.4 * (1 - (self.best_scores[i] / (self.global_best_score + 1e-10)))\n\n                # Change 2: new dynamic update for c1 and c2\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio  # Decreases over time\n                self.c2 = 0.5 + progress_ratio  # Increases over time\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 12, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02151 with standard deviation 0.00000.", "error": "", "parent_ids": ["cd60d2cd-dc43-42a6-894c-b5ecb983af63"], "operator": null, "metadata": {"aucs": [0.021510895937162933, 0.02150901442262465, 0.021510395633067003]}}
{"id": "0675f7b4-6897-4f1b-947b-34faf499672a", "fitness": 0.030221808401511452, "name": "AQPSO", "description": "Enhance AQPSO by introducing adaptive swarm size and velocity perturbation to promote diversity and convergence.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.swarm_size = self.initial_swarm_size\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n                # Change 1: Adaptive swarm size reduction\n                self.swarm_size = max(10, int(self.initial_swarm_size * (1 - self.evaluations / self.budget)))\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                if self.best_scores[i] < np.inf:\n                    self.inertia_weight = 0.5 + 0.4 * (1 - (self.best_scores[i] / (self.global_best_score + 1e-10)))\n\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                phi = np.random.uniform(0, 1, self.dim)\n                # Change 2: Velocity perturbation for diversity\n                perturbation = np.random.normal(0, 0.01, self.velocities[i].shape)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]) +\n                                      perturbation)\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 13, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03022 with standard deviation 0.00087.", "error": "", "parent_ids": ["cd60d2cd-dc43-42a6-894c-b5ecb983af63"], "operator": null, "metadata": {"aucs": [0.02933606180489723, 0.031399992374113506, 0.02992937102552362]}}
{"id": "bf4bf973-0974-4daa-ad8c-e47447371324", "fitness": 0.029418490579642558, "name": "AQPSO", "description": "Implement adaptive neighborhood-based learning to enhance information sharing, dynamically balancing exploration and exploitation based on neighborhood improvements.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.neighborhood_size = 5  # Number of neighbors to consider\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Dynamic inertia weight based on personal best improvement\n                if self.best_scores[i] < np.inf:\n                    self.inertia_weight = 0.5 + 0.4 * (1 - (self.best_scores[i] / (self.global_best_score + 1e-10)))\n\n                # Dynamic update for c1 and c2\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio  # Decreases over time\n                self.c2 = 0.5 + progress_ratio  # Increases over time\n\n                # Neighborhood-based learning\n                neighbors = np.random.choice(self.swarm_size, self.neighborhood_size, replace=False)\n                neighborhood_best_position = min(neighbors, key=lambda n: self.best_scores[n])\n                phi = np.random.uniform(0, 1, self.dim)\n                \n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.best_positions[neighborhood_best_position] - self.positions[i]))\n                \n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 14, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02942 with standard deviation 0.00147.", "error": "", "parent_ids": ["cd60d2cd-dc43-42a6-894c-b5ecb983af63"], "operator": null, "metadata": {"aucs": [0.027720873115903655, 0.03131505706926052, 0.029219541553763495]}}
{"id": "b4fa1020-a5b9-4ef7-a792-cf2aa4ef5964", "fitness": 0.031404792289311555, "name": "AQPSO", "description": "Introduce a small mutation in the velocities to enhance exploration in the AQPSO algorithm.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                if self.best_scores[i] < np.inf:\n                    self.inertia_weight = 0.5 + 0.4 * (1 - (self.best_scores[i] / (self.global_best_score + 1e-10)))\n\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                # Change: Add mutation to velocities\n                self.velocities[i] += 0.05 * np.random.normal(0, 1, self.dim)\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 15, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03140 with standard deviation 0.00450.", "error": "", "parent_ids": ["cd60d2cd-dc43-42a6-894c-b5ecb983af63"], "operator": null, "metadata": {"aucs": [0.02866823422590048, 0.027794778873943216, 0.03775136376809096]}}
{"id": "9af8b756-96e3-4971-ba5c-1393e60c5b5b", "fitness": 0.0, "name": "AQPSO", "description": "Enhance the inertia weight formula by considering the diversity of the swarm to maintain exploration alongside personal best improvement.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Enhanced inertia weight considering diversity in the swarm\n                diversity = np.mean(np.std(self.positions, axis=0))\n                self.inertia_weight = 0.5 + 0.4 * (1 - (self.best_scores[i] / (self.global_best_score + 1e-10))) * (1 + diversity)\n\n                # Change 2: new dynamic update for c1 and c2\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio  # Decreases over time\n                self.c2 = 0.5 + progress_ratio  # Increases over time\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 16, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.00000 with standard deviation 0.00000.", "error": "", "parent_ids": ["cd60d2cd-dc43-42a6-894c-b5ecb983af63"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.0]}}
{"id": "c28b6e89-0b1c-4b37-8035-93d961c2c035", "fitness": -Infinity, "name": "AQPSO", "description": "Introduce slight randomness to swarm size at each restart to enhance exploration capability.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0:\n                self.swarm_size += np.random.randint(-1, 2)  # Slight randomness to swarm size\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                if self.best_scores[i] < np.inf:\n                    self.inertia_weight = 0.5 + 0.4 * (1 - (self.best_scores[i] / (self.global_best_score + 1e-10)))\n\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 17, "feedback": "An exception occurred: IndexError('index 30 is out of bounds for axis 0 with size 30').", "error": "IndexError('index 30 is out of bounds for axis 0 with size 30')", "parent_ids": ["cd60d2cd-dc43-42a6-894c-b5ecb983af63"], "operator": null, "metadata": {}}
{"id": "85c08d59-71ec-44d2-9ce2-8cefe66500bd", "fitness": 0.03230364364805701, "name": "AQPSO", "description": "Introduce adaptive acceleration coefficients and a restart mechanism based on stagnation detection for enhanced convergence.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.stagnation_counter = 0  # Added for stagnation detection\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 or self.stagnation_counter > restart_threshold:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n                self.stagnation_counter = 0  # Reset stagnation counter\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                if self.best_scores[i] < np.inf:\n                    self.inertia_weight = 0.5 + 0.4 * (1 - (self.best_scores[i] / (self.global_best_score + 1e-10)))\n\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio  # Decreases over time\n                self.c2 = 0.5 + progress_ratio  # Increases over time\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                    self.stagnation_counter = 0  # Reset stagnation counter\n                else:\n                    self.stagnation_counter += 1  # Increment stagnation counter\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.stagnation_counter = 0  # Reset stagnation counter\n                \n        return self.global_best_position", "configspace": "", "generation": 18, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03230 with standard deviation 0.00372.", "error": "", "parent_ids": ["cd60d2cd-dc43-42a6-894c-b5ecb983af63"], "operator": null, "metadata": {"aucs": [0.029628104803496202, 0.029712842288141417, 0.037569983852533406]}}
{"id": "21dbad2d-f694-46ec-82f0-8a1a48915854", "fitness": 0.031844837555097594, "name": "AQPSO", "description": "Introduce a linear decrease in the inertia weight over time to enhance global exploration initially and local exploitation later.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Change 1: dynamic inertia weight based on personal best improvement\n                # if self.best_scores[i] < np.inf:\n                #     self.inertia_weight = 0.5 + 0.4 * (1 - (self.best_scores[i] / (self.global_best_score + 1e-10)))\n                self.inertia_weight = 0.9 - (0.5 * (self.evaluations / self.budget))  # Linear decrease\n\n                # Change 2: new dynamic update for c1 and c2\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio  # Decreases over time\n                self.c2 = 0.5 + progress_ratio  # Increases over time\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 19, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03184 with standard deviation 0.00304.", "error": "", "parent_ids": ["cd60d2cd-dc43-42a6-894c-b5ecb983af63"], "operator": null, "metadata": {"aucs": [0.029682691806949157, 0.029712926956222296, 0.03613889390212133]}}
{"id": "e657b7ba-bf2b-4c1c-a846-2e7439b9c919", "fitness": 0.025207991241226486, "name": "AdaptiveAQPSO", "description": "Adaptively adjust learning rates using a feedback mechanism based on the divergence of position updates to enhance convergence.", "code": "import numpy as np\n\nclass AdaptiveAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.prev_positions = self.positions.copy()\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Change 1: dynamic inertia weight based on divergence\n                divergence = np.linalg.norm(self.positions[i] - self.prev_positions[i])\n                self.inertia_weight = 0.5 + 0.4 * (1 - divergence / (np.sqrt(self.dim) + 1e-10))\n\n                # Change 2: adaptive update for c1 and c2 based on divergence\n                self.c1 = 2.0 + 0.5 * (1 - divergence / (np.sqrt(self.dim) + 1e-10))\n                self.c2 = 0.5 + 1.5 * (divergence / (np.sqrt(self.dim) + 1e-10))\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n            \n            self.prev_positions = self.positions.copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 20, "feedback": "The algorithm AdaptiveAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02521 with standard deviation 0.00389.", "error": "", "parent_ids": ["cd60d2cd-dc43-42a6-894c-b5ecb983af63"], "operator": null, "metadata": {"aucs": [0.021515042625041447, 0.023526730694944042, 0.03058220040369397]}}
{"id": "03d8a06d-84bb-4e92-9d8e-f647870b93a4", "fitness": 0.0321742316277008, "name": "AQPSO", "description": "Fine-tune the inertia weight's adaptation method to improve convergence speed.", "code": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Change 1: dynamic inertia weight based on personal best improvement\n                if self.best_scores[i] < np.inf:\n                    self.inertia_weight = 0.4 + 0.5 * (1 - (self.best_scores[i] / (self.global_best_score + 1e-10)))\n\n                # Change 2: new dynamic update for c1 and c2\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio  # Decreases over time\n                self.c2 = 0.5 + progress_ratio  # Increases over time\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 21, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03217 with standard deviation 0.00352.", "error": "", "parent_ids": ["cd60d2cd-dc43-42a6-894c-b5ecb983af63"], "operator": null, "metadata": {"aucs": [0.029653482637030604, 0.029712842288141417, 0.03715636995793037]}}
{"id": "debc718a-3eb1-4b61-8ac5-9ff2fc74806d", "fitness": 0.037129872883580196, "name": "ImprovedAQPSO", "description": "Introduce a diversity-preserving mechanism by probabilistically replacing some particles with random solutions, maintaining diversity and avoiding premature convergence.", "code": "import numpy as np\n\nclass ImprovedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.weights = np.random.rand(self.swarm_size, self.dim)\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Dynamic inertia weight based on personal best improvement\n                if self.best_scores[i] < np.inf:\n                    self.inertia_weight = 0.5 + 0.4 * (1 - (self.best_scores[i] / (self.global_best_score + 1e-10)))\n\n                # Dynamic update for c1 and c2\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                # Introduce diversity-preservation mechanism\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 22, "feedback": "The algorithm ImprovedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03713 with standard deviation 0.00349.", "error": "", "parent_ids": ["cd60d2cd-dc43-42a6-894c-b5ecb983af63"], "operator": null, "metadata": {"aucs": [0.0410817770898152, 0.03772121691489572, 0.03258662464602968]}}
{"id": "fe6a14ea-e074-4165-a5f4-b8d73d48107e", "fitness": 0.04862613182059413, "name": "RefinedAQPSO", "description": "Introduce adaptive dynamic parameter control and local search intensification to balance exploration and exploitation, enhancing convergence speed and solution quality.", "code": "import numpy as np\n\nclass RefinedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Adaptive inertia weight based on convergence\n                success_rate = np.mean(self.best_scores < np.inf)\n                self.inertia_weight = 0.4 + 0.5 * success_rate\n\n                # Dynamic update for c1 and c2\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                # Introduce diversity-preservation mechanism\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                # Local search intensification\n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.05, 0.05, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 23, "feedback": "The algorithm RefinedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04863 with standard deviation 0.01029.", "error": "", "parent_ids": ["debc718a-3eb1-4b61-8ac5-9ff2fc74806d"], "operator": null, "metadata": {"aucs": [0.06096342538516042, 0.049149081215447366, 0.0357658888611746]}}
{"id": "1971e8db-35a0-4a86-89f8-d6f3c4853e6e", "fitness": 0.04862393480964341, "name": "RefinedAQPSO", "description": "Refine adaptive control by integrating a sigmoid-function-based inertia weight and improving local search by adjusting position perturbation.", "code": "import numpy as np\n\nclass RefinedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Adaptive inertia weight based on convergence\n                success_rate = np.mean(self.best_scores < np.inf)\n                # Change 1: Use a sigmoid function for inertia weight adaptation\n                self.inertia_weight = 0.4 + 0.5 / (1 + np.exp(-10 * (success_rate - 0.5)))\n\n                # Dynamic update for c1 and c2\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                # Introduce diversity-preservation mechanism\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                # Local search intensification\n                if np.random.rand() < self.local_search_prob:\n                    # Change 2: Adjust position perturbation for local search\n                    local_pos = self.positions[i] + np.random.uniform(-0.1, 0.1, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 24, "feedback": "The algorithm RefinedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04862 with standard deviation 0.01024.", "error": "", "parent_ids": ["fe6a14ea-e074-4165-a5f4-b8d73d48107e"], "operator": null, "metadata": {"aucs": [0.06084193330750043, 0.049248872593682935, 0.035780998527746855]}}
{"id": "5f6800d2-2481-4514-adf2-76853a251902", "fitness": 0.021510203372775056, "name": "EnhancedAQPSO", "description": "Introduce stochastic gradient estimation and adaptive exploration probability to improve convergence and solution quality.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.exploration_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Adaptive inertia weight based on convergence\n                success_rate = np.mean(self.best_scores < np.inf)\n                self.inertia_weight = 0.4 + 0.5 * success_rate\n\n                # Dynamic update for c1 and c2\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                # Exploration enhancement using stochastic gradient estimation\n                if np.random.rand() < self.exploration_prob:\n                    gradient_estimate = np.random.normal(0, 0.1, self.dim)\n                    self.positions[i] += gradient_estimate\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                # Local search intensification\n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.05, 0.05, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 25, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02151 with standard deviation 0.00000.", "error": "", "parent_ids": ["fe6a14ea-e074-4165-a5f4-b8d73d48107e"], "operator": null, "metadata": {"aucs": [0.021509677378339642, 0.02151039976128022, 0.02151053297870531]}}
{"id": "ac88a9aa-00ad-4741-9c2a-5d7fe3470bcf", "fitness": 0.03039734295850653, "name": "EnhancedAQPSO", "description": "Enhance exploration by integrating a Lvy flight mechanism and improve exploitation using a dimension-wise adaptive local search strategy.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n\n    def levy_flight(self, size, alpha=1.5):\n        sigma = (np.math.gamma(1 + alpha) * np.sin(np.pi * alpha / 2) /\n                 (np.math.gamma((1 + alpha) / 2) * alpha * 2**((alpha - 1) / 2)))**(1 / alpha)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        return u / np.abs(v)**(1 / alpha)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                success_rate = np.mean(self.best_scores < np.inf)\n                self.inertia_weight = 0.4 + 0.5 * success_rate\n\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] += self.levy_flight(self.dim) * (ub - lb)\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.local_search_prob:\n                    for d in range(self.dim):\n                        local_pos = self.positions[i].copy()\n                        local_pos[d] += np.random.uniform(-0.05, 0.05)\n                        local_pos = np.clip(local_pos, lb, ub)\n                        local_score = func(local_pos)\n                        self.evaluations += 1\n                        if local_score < self.best_scores[i]:\n                            self.best_scores[i] = local_score\n                            self.best_positions[i] = local_pos.copy()\n                        if local_score < self.global_best_score:\n                            self.global_best_score = local_score\n                            self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n\n        return self.global_best_position", "configspace": "", "generation": 26, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03040 with standard deviation 0.00302.", "error": "", "parent_ids": ["fe6a14ea-e074-4165-a5f4-b8d73d48107e"], "operator": null, "metadata": {"aucs": [0.03071922773838709, 0.03391915287675462, 0.02655364826037787]}}
{"id": "4e2c7374-3be0-4790-8a6b-e112eb1acaa7", "fitness": -Infinity, "name": "RefinedAQPSOPlus", "description": "Integrate adaptive neighborhood search and Levy flight for enhanced exploration, while maintaining dynamic parameter tuning for improved convergence.", "code": "import numpy as np\n\nclass RefinedAQPSOPlus:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.15  # Increased probability\n        self.local_search_prob = 0.05\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                 (np.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        u = np.random.randn(size) * sigma\n        v = np.random.randn(size)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                success_rate = np.mean(self.best_scores < np.inf)\n                self.inertia_weight = 0.4 + 0.5 * success_rate\n\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]) +\n                                      self.levy_flight(self.dim))  # Added Levy flight\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.05, 0.05, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 27, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_ids": ["fe6a14ea-e074-4165-a5f4-b8d73d48107e"], "operator": null, "metadata": {}}
{"id": "a1168d4b-ba5d-4f1e-a6df-2f94a03fdee4", "fitness": 0.043532585600581486, "name": "EnhancedAQPSO", "description": "Enhance RefinedAQPSO by introducing a multi-phase adaptive exploration strategy and crowding-based diversity mechanism, improving convergence and preventing premature stagnation.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.inertia_weight = 0.9\n        self.phase_switch_threshold = self.budget // 3\n        self.c1 = 2.5\n        self.c2 = 0.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        for i in range(self.budget):\n            if self.evaluations >= self.budget:\n                return self.global_best_position\n\n            current_phase = (self.evaluations // self.phase_switch_threshold) % 3\n            if current_phase == 0:\n                self.inertia_weight *= 0.99\n            elif current_phase == 1:\n                self.inertia_weight = 0.4 + 0.5 * np.random.rand()\n            else:\n                self.inertia_weight *= 1.01\n\n            for j in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[j] = (self.inertia_weight * self.velocities[j] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[j] - self.positions[j]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[j]))\n                self.positions[j] += self.velocities[j]\n\n                self.positions[j] = np.clip(self.positions[j], lb, ub)\n\n                # Crowding-based diversity mechanism\n                distance_to_gbest = np.linalg.norm(self.positions[j] - self.global_best_position)\n                if np.random.rand() < min(0.2, max(0, 1 - distance_to_gbest / (ub - lb).mean())):\n                    self.positions[j] = np.random.uniform(lb, ub, self.dim)\n                \n                # Evaluate the fitness\n                score = func(self.positions[j])\n                self.evaluations += 1\n\n                if score < self.best_scores[j]:\n                    self.best_scores[j] = score\n                    self.best_positions[j] = self.positions[j].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[j].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 28, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04353 with standard deviation 0.00299.", "error": "", "parent_ids": ["fe6a14ea-e074-4165-a5f4-b8d73d48107e"], "operator": null, "metadata": {"aucs": [0.040398739419420626, 0.04755627232688975, 0.042642745055434084]}}
{"id": "9670d9c3-1cb9-453b-84a6-5d776459f36f", "fitness": 0.021510083544540426, "name": "EnhancedAQPSO", "description": "Incorporate self-adaptive velocity clamping and a differential mutation-inspired mechanism to enhance diversity and convergence rate.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        velocity_clamp = (ub - lb) * 0.1\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Adaptive inertia weight based on convergence\n                success_rate = np.mean(self.best_scores < np.inf)\n                self.inertia_weight = 0.4 + 0.5 * success_rate\n\n                # Dynamic update for c1 and c2\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                \n                # Velocity clamping\n                self.velocities[i] = np.clip(self.velocities[i], -velocity_clamp, velocity_clamp)\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                # Introduce diversity-preservation mechanism\n                if np.random.rand() < self.diversity_prob:\n                    rand_indices = np.random.choice(self.swarm_size, 3, replace=False)\n                    random_vector = self.positions[rand_indices[0]] + 0.5 * (self.positions[rand_indices[1]] - self.positions[rand_indices[2]])\n                    self.positions[i] = np.clip(random_vector, lb, ub)\n                \n                # Local search intensification\n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.05, 0.05, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 29, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02151 with standard deviation 0.00000.", "error": "", "parent_ids": ["fe6a14ea-e074-4165-a5f4-b8d73d48107e"], "operator": null, "metadata": {"aucs": [0.0215100670574756, 0.02151036485093838, 0.021509818725207297]}}
{"id": "e5d9158f-b564-47ff-8990-89519f373763", "fitness": 0.04581604009226792, "name": "EnhancedAQPSO", "description": "Introduce self-tuning velocity update and enhanced local search leveraging Lvy flights to improve exploration and exploitation dynamics.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Self-tuning parameters\n                success_rate = np.mean(self.best_scores < np.inf)\n                self.inertia_weight = 0.4 + 0.5 * success_rate\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                # Velocity update with self-tuning\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                # Diversity mechanism\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                # Enhanced local search with Lvy flights\n                if np.random.rand() < self.local_search_prob:\n                    step = levy.rvs(size=self.dim)\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 30, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04582 with standard deviation 0.00497.", "error": "", "parent_ids": ["fe6a14ea-e074-4165-a5f4-b8d73d48107e"], "operator": null, "metadata": {"aucs": [0.04496435456352299, 0.04020116294929854, 0.05228260276398222]}}
{"id": "bddaa689-2b41-4871-884b-9b4d908dfad8", "fitness": 0.044471920205623704, "name": "QuantumEnhancedPSO", "description": "Incorporate quantum superposition and dynamic neighborhood structures to enhance exploration and convergence in complex landscapes.", "code": "import numpy as np\n\nclass QuantumEnhancedPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.quantum_prob = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Adaptive inertia weight based on convergence\n                success_rate = np.mean(self.best_scores < np.inf)\n                self.inertia_weight = 0.4 + 0.5 * success_rate\n\n                # Dynamic update for c1 and c2\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                # Quantum superposition exploration\n                if np.random.rand() < self.quantum_prob:\n                    neighbor_indices = np.random.choice(self.swarm_size, 2, replace=False)\n                    neighbor_positions = self.positions[neighbor_indices]\n                    quantum_position = np.mean(neighbor_positions, axis=0) + np.random.randn(self.dim) * 0.1\n                    quantum_position = np.clip(quantum_position, lb, ub)\n                    quantum_score = func(quantum_position)\n                    self.evaluations += 1\n                    if quantum_score < self.best_scores[i]:\n                        self.best_scores[i] = quantum_score\n                        self.best_positions[i] = quantum_position\n                    if quantum_score < self.global_best_score:\n                        self.global_best_score = quantum_score\n                        self.global_best_position = quantum_position\n\n                # Introduce diversity-preservation mechanism\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                # Local search intensification\n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.05, 0.05, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 31, "feedback": "The algorithm QuantumEnhancedPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04447 with standard deviation 0.00273.", "error": "", "parent_ids": ["fe6a14ea-e074-4165-a5f4-b8d73d48107e"], "operator": null, "metadata": {"aucs": [0.040613043587567565, 0.0464843098002945, 0.04631840722900904]}}
{"id": "06990ee1-2352-4a0d-90d7-c49ecff82687", "fitness": 0.044648395436760434, "name": "EnhancedAQPSO", "description": "Enhance RefinedAQPSO by incorporating a non-uniform mutation operator for refined diversity control, adaptive swarm size adjustment, and fitness-based restart strategy to improve convergence and solution robustness.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.current_swarm_size = self.initial_swarm_size\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        fitness_restart_threshold = 0.95\n\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                if self.global_best_score < fitness_restart_threshold:\n                    self.positions = np.random.uniform(lb, ub, (self.initial_swarm_size, self.dim))\n                    self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n                else:\n                    self.current_swarm_size = max(20, self.current_swarm_size - 1)\n\n            for i in range(self.current_swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Adaptive inertia weight and dynamic parameters\n                success_rate = np.mean(self.best_scores < np.inf)\n                self.inertia_weight = 0.4 + 0.5 * success_rate\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                # Non-uniform mutation for enhanced diversity\n                if np.random.rand() < self.diversity_prob:\n                    tau = np.random.uniform(0, 1, self.dim)\n                    self.positions[i] += (ub - lb) * tau * (1 - np.random.rand() ** (1 - self.evaluations / self.budget))\n                \n                # Local search intensification\n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.05, 0.05, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04465 with standard deviation 0.00967.", "error": "", "parent_ids": ["fe6a14ea-e074-4165-a5f4-b8d73d48107e"], "operator": null, "metadata": {"aucs": [0.03756729322271757, 0.03805389041119511, 0.058324002676368614]}}
{"id": "be9c033b-355b-4dee-a25f-1170c414c792", "fitness": 0.021510570819553026, "name": "RefinedAQPSO", "description": "Enhance exploration dynamics by introducing a mutation-based diversity mechanism and adaptive local search frequency.", "code": "import numpy as np\n\nclass RefinedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.15  # Increased probability\n        self.local_search_prob = 0.03  # Adjusted probability\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                # Adaptive inertia weight based on convergence\n                success_rate = np.mean(self.best_scores < np.inf)\n                self.inertia_weight = 0.4 + 0.5 * success_rate\n\n                # Dynamic update for c1 and c2\n                progress_ratio = self.evaluations / self.budget\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                phi = np.random.uniform(0, 1, self.dim)\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                # Introduce diversity-preservation mechanism\n                if np.random.rand() < self.diversity_prob:\n                    mutation = np.random.normal(0, 0.1, self.dim)\n                    self.positions[i] += mutation\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n                \n                # Local search intensification\n                if np.random.rand() < self.local_search_prob + progress_ratio * 0.05:\n                    local_pos = self.positions[i] + np.random.uniform(-0.05, 0.05, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 33, "feedback": "The algorithm RefinedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02151 with standard deviation 0.00000.", "error": "", "parent_ids": ["fe6a14ea-e074-4165-a5f4-b8d73d48107e"], "operator": null, "metadata": {"aucs": [0.021510383018764068, 0.02151063491754568, 0.021510694522349327]}}
{"id": "e9cc5eb4-770e-4ec1-bb6d-4668e1c0c0e9", "fitness": 0.04916903026069053, "name": "EnhancedAQPSO", "description": "Introduce a self-adaptive mechanism for diversity and local search parameters, alongside incorporating a nonlinear inertia weight for improved balance between exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * np.tanh(5 * (1 - progress_ratio))\n\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.05, 0.05, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 34, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04917 with standard deviation 0.00443.", "error": "", "parent_ids": ["fe6a14ea-e074-4165-a5f4-b8d73d48107e"], "operator": null, "metadata": {"aucs": [0.05208100647601033, 0.05251690788449581, 0.04290917642156544]}}
{"id": "99bdafd7-8d92-46c7-82ca-cb55fe089ae7", "fitness": 0.05845189161689116, "name": "EnhancedAQPSO", "description": "Introduce adaptive velocity updates with a differential evolution-inspired mutation strategy for enhanced exploration capabilities.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1  # Added mutation probability\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate  # Adjust mutation probability\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * np.tanh(5 * (1 - progress_ratio))\n\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                if np.random.rand() < self.mutation_prob:  # Apply mutation\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.05, 0.05, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 35, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05845 with standard deviation 0.00398.", "error": "", "parent_ids": ["e9cc5eb4-770e-4ec1-bb6d-4668e1c0c0e9"], "operator": null, "metadata": {"aucs": [0.053464826197270665, 0.06321464729121051, 0.058676201362192315]}}
{"id": "78311930-ad29-4a42-9234-9413f761ea42", "fitness": 0.05845192533066342, "name": "EnhancedAQPSO", "description": "Enhance exploitation through adaptive velocity and improved local search steps for better convergence precision.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * np.tanh(5 * (1 - progress_ratio))\n\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim) # Reduced local search step size\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 36, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05845 with standard deviation 0.00398.", "error": "", "parent_ids": ["99bdafd7-8d92-46c7-82ca-cb55fe089ae7"], "operator": null, "metadata": {"aucs": [0.05346492733858743, 0.06321464729121051, 0.058676201362192315]}}
{"id": "f579b7af-9bb0-409e-b9ef-04bf4e6db692", "fitness": 0.055085733193596886, "name": "EnhancedAQPSO", "description": "Enhance convergence precision by employing adaptive mutation based on progress ratio.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            progress_ratio = self.evaluations / self.budget\n            self.mutation_prob = 0.15 * (1 - progress_ratio)  # Adjusted mutation probability\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.inertia_weight = 0.4 + 0.5 * np.tanh(5 * (1 - progress_ratio))\n\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim) # Reduced local search step size\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 37, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05509 with standard deviation 0.00872.", "error": "", "parent_ids": ["78311930-ad29-4a42-9234-9413f761ea42"], "operator": null, "metadata": {"aucs": [0.042798179787625124, 0.060358076200235145, 0.06210094359293039]}}
{"id": "c5ce50ca-84c3-4b9e-8b87-13e49ffd6a82", "fitness": 0.04791022396384448, "name": "EnhancedAQPSO", "description": "Integrate adaptive inertia and leader selection to enhance convergence in dynamic environments.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.adaptive_leader_prob = 0.3\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.5 + 0.3 * np.sin(2 * np.pi * progress_ratio)\n\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                if np.random.rand() < self.adaptive_leader_prob:\n                    leader_index = np.random.choice(np.arange(self.swarm_size))\n                    self.global_best_position = self.positions[leader_index]\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 38, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04791 with standard deviation 0.00494.", "error": "", "parent_ids": ["78311930-ad29-4a42-9234-9413f761ea42"], "operator": null, "metadata": {"aucs": [0.04157635901955714, 0.048528648700549404, 0.05362566417142689]}}
{"id": "f5ef8de0-07e6-4128-b6d0-96abe6712586", "fitness": 0.02151084802290711, "name": "EnhancedAQPSO", "description": "Introduce multi-phase dynamic adaptation of exploration and exploitation strategies based on swarm diversity and convergence status for improved robustness and precision.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.0\n        self.c2 = 1.5\n        self.inertia_weight = 0.9\n        self.mutation_prob = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        phase_switch = self.budget // 3\n        for _ in range(self.budget):\n            if self.evaluations >= self.budget:\n                return self.global_best_position\n\n            if self.evaluations % phase_switch == 0 and self.evaluations > 0:\n                # Reduce inertia weight and adjust cognitive and social components as algorithm progresses\n                self.inertia_weight = max(0.4, self.inertia_weight - 0.1)\n                self.c1 = max(0.5, self.c1 - 0.5)\n                self.c2 = min(2.5, self.c2 + 0.5)\n\n            for i in range(self.swarm_size):\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n                # Implement local search with varying intensity based on current diversity and success\n                if np.random.rand() < 0.1 + 0.2 * (1 - np.mean(self.best_scores < np.inf)):\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n        return self.global_best_position", "configspace": "", "generation": 39, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02151 with standard deviation 0.00000.", "error": "", "parent_ids": ["78311930-ad29-4a42-9234-9413f761ea42"], "operator": null, "metadata": {"aucs": [0.021513669999627005, 0.02150811675656228, 0.021510757312532047]}}
{"id": "8517c280-4eb3-4214-a64d-9ea5ba499ea3", "fitness": 0.023578471205595086, "name": "RefinedEnhancedAQPSO", "description": "Refine adaptive velocity and local search steps with dynamic learning factors and diversity retention for enhanced convergence.", "code": "import numpy as np\n\nclass RefinedEnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations >= self.budget:\n                return self.global_best_position\n\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n\n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            progress_ratio = self.evaluations / self.budget\n            self.inertia_weight = 0.4 + 0.5 * np.tanh(5 * (1 - progress_ratio))\n            self.c1 = 2.5 - 2 * progress_ratio\n            self.c2 = 0.5 + 2 * progress_ratio\n\n            for i in range(self.swarm_size):\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.01, 0.01, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 40, "feedback": "The algorithm RefinedEnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02358 with standard deviation 0.00186.", "error": "", "parent_ids": ["78311930-ad29-4a42-9234-9413f761ea42"], "operator": null, "metadata": {"aucs": [0.02151402405520708, 0.023202140405776728, 0.02601924915580145]}}
{"id": "9805ea1e-45e1-4d90-8092-aa9b9f3d31ea", "fitness": 0.044970171562287366, "name": "EnhancedAQPSO", "description": "Introduce dynamic swarm size and adaptive mutation rate based on convergence stability to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        swarm_size = self.initial_swarm_size\n        evaluation_interval = self.budget // 10\n        \n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n            \n            progress_ratio = self.evaluations / self.budget\n            self.inertia_weight = 0.4 + 0.5 * np.tanh(5 * (1 - progress_ratio))\n            self.c1 = 2.5 - progress_ratio\n            self.c2 = 0.5 + progress_ratio\n\n            if self.evaluations > 0 and self.evaluations % evaluation_interval == 0:\n                improvement_rate = (self.global_best_score - np.min(self.best_scores)) / self.global_best_score\n                if improvement_rate < 0.01:\n                    swarm_size = min(swarm_size + 10, 100)\n                    self.positions = np.vstack((self.positions, np.random.uniform(lb, ub, (10, self.dim))))\n                    self.velocities = np.vstack((self.velocities, np.random.uniform(-0.1, 0.1, (10, self.dim))))\n                    self.best_positions = np.vstack((self.best_positions, np.random.uniform(lb, ub, (10, self.dim))))\n                    self.best_scores = np.append(self.best_scores, np.full(10, np.inf))\n                self.mutation_prob = 0.1 + 0.2 * (1 - improvement_rate)\n            \n            for i in range(swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 41, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04497 with standard deviation 0.00686.", "error": "", "parent_ids": ["78311930-ad29-4a42-9234-9413f761ea42"], "operator": null, "metadata": {"aucs": [0.05317321985038992, 0.03637251807891895, 0.04536477675755324]}}
{"id": "8f3c7862-e307-4c66-893c-8e75e9b5b749", "fitness": 0.05845192533066342, "name": "EnhancedAQPSO", "description": "Introduce a dynamic restart mechanism to enhance exploration in stagnated scenarios.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                # Dynamically restart the swarm if no improvement\n                if np.all(np.isinf(self.best_scores)):\n                    self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * np.tanh(5 * (1 - progress_ratio))\n\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim) # Reduced local search step size\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05845 with standard deviation 0.00398.", "error": "", "parent_ids": ["78311930-ad29-4a42-9234-9413f761ea42"], "operator": null, "metadata": {"aucs": [0.05346492733858743, 0.06321464729121051, 0.058676201362192315]}}
{"id": "a87b0b03-6e71-45fa-872e-ffe0c1a6f4e9", "fitness": 0.023683279271784447, "name": "EnhancedAQPSO", "description": "Introduce adaptive mutation and dynamic velocity scaling in EnhancedAQPSO to improve exploration-exploitation balance and convergence speed.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.adaptive_mutation_base = 0.2\n        self.c1 = 2.5\n        self.c2 = 0.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * np.tanh(5 * (1 - progress_ratio))\n\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                self.velocities[i] *= (0.5 + np.random.random() * 0.5)  # Dynamic velocity scaling\n\n                if np.random.rand() < self.adaptive_mutation_base * (1 - progress_ratio):\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n\n        return self.global_best_position", "configspace": "", "generation": 43, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02368 with standard deviation 0.00107.", "error": "", "parent_ids": ["78311930-ad29-4a42-9234-9413f761ea42"], "operator": null, "metadata": {"aucs": [0.023387943122814336, 0.025121288597070746, 0.022540606095468263]}}
{"id": "842968f1-7687-4e96-a6e7-c4184a347e15", "fitness": 0.057906759156579425, "name": "EnhancedAQPSO", "description": "Enhance AQPSO by reducing inertia weight decay rate for improved exploration in early iterations.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.6 + 0.5 * np.tanh(5 * (1 - progress_ratio))\n\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim) # Reduced local search step size\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05791 with standard deviation 0.00494.", "error": "", "parent_ids": ["78311930-ad29-4a42-9234-9413f761ea42"], "operator": null, "metadata": {"aucs": [0.05152821270447616, 0.06354977913617843, 0.05864228562908369]}}
{"id": "bc12f723-59db-4511-9d17-54fa0ef9f939", "fitness": 0.05637262959807343, "name": "EnhancedAQPSO", "description": "Introduce a dynamic inertia weight strategy and adaptive learning rates for improved exploration-exploitation balance and performance across varying problem landscapes.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.inertia_weight = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.9 - (0.5 * progress_ratio)  # Dynamic inertia weight strategy\n\n                self.c1 = 2.5 - progress_ratio * 2\n                self.c2 = 0.5 + progress_ratio * 2\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)  # Reduced local search step size\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 45, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05637 with standard deviation 0.00455.", "error": "", "parent_ids": ["78311930-ad29-4a42-9234-9413f761ea42"], "operator": null, "metadata": {"aucs": [0.05141913335123771, 0.0624108764006418, 0.05528787904234078]}}
{"id": "9f96999d-ea0a-4abb-b6e7-081d64bf70af", "fitness": 0.05845192533066342, "name": "EnhancedAQPSO", "description": "Enhance exploration by integrating dynamic boundary adaptation and adaptive swarm population to maintain diversity and prevent premature convergence.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.swarm_size = self.initial_swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        adaptive_bounds = np.array([lb, ub]).T\n        restart_threshold = self.budget // 5\n        \n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(adaptive_bounds[:, 0], adaptive_bounds[:, 1], (self.swarm_size, self.dim))\n                # Dynamically adjust swarm size based on performance\n                performance_metric = (self.global_best_score - np.mean(self.best_scores)) / self.global_best_score\n                self.swarm_size = max(10, int(self.initial_swarm_size * (1 + performance_metric)))\n                self.positions = np.resize(self.positions, (self.swarm_size, self.dim))\n                self.velocities = np.resize(self.velocities, (self.swarm_size, self.dim))\n                self.best_positions = np.resize(self.best_positions, (self.swarm_size, self.dim))\n                self.best_scores = np.resize(self.best_scores, self.swarm_size)\n                self.best_scores.fill(np.inf)\n                \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * np.tanh(5 * (1 - progress_ratio))\n\n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, adaptive_bounds[:, 0], adaptive_bounds[:, 1])\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], adaptive_bounds[:, 0], adaptive_bounds[:, 1])\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(adaptive_bounds[:, 0], adaptive_bounds[:, 1], self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, adaptive_bounds[:, 0], adaptive_bounds[:, 1])\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n\n            # Update adaptive bounds based on current best and worst positions\n            if self.evaluations % restart_threshold == 0:\n                best_pos = self.positions[np.argmin(self.best_scores)]\n                worst_pos = self.positions[np.argmax(self.best_scores)]\n                adaptive_bounds[:, 0] = np.minimum(lb, best_pos - 0.1 * (best_pos - worst_pos))\n                adaptive_bounds[:, 1] = np.maximum(ub, best_pos + 0.1 * (worst_pos - best_pos))\n                \n        return self.global_best_position", "configspace": "", "generation": 46, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05845 with standard deviation 0.00398.", "error": "", "parent_ids": ["78311930-ad29-4a42-9234-9413f761ea42"], "operator": null, "metadata": {"aucs": [0.05346492733858743, 0.06321464729121051, 0.058676201362192315]}}
{"id": "e5aa2867-8fce-4955-a1df-f62092a77493", "fitness": 0.05813950760153715, "name": "EnhancedAQPSO", "description": "Enhance exploration by adapting the inertia weight more dynamically based on diversity and progress ratio.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.3 + 0.6 * np.tanh(5 * (1 - progress_ratio)) + 0.1 * np.sin(2 * np.pi * progress_ratio)\n                \n                self.c1 = 2.5 - progress_ratio\n                self.c2 = 0.5 + progress_ratio\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim) # Reduced local search step size\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 47, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05814 with standard deviation 0.00280.", "error": "", "parent_ids": ["78311930-ad29-4a42-9234-9413f761ea42"], "operator": null, "metadata": {"aucs": [0.054460229925760384, 0.06123639789531454, 0.05872189498353653]}}
{"id": "85169cba-f6ad-4e93-9943-d6078421bcb0", "fitness": 0.059474204293596844, "name": "EnhancedAQPSO", "description": "Hybridize EnhancedAQPSO with chaotic maps for adaptive parameter tuning to improve exploration and exploitation dynamics.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7  # Initial chaotic factor\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 48, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05947 with standard deviation 0.00232.", "error": "", "parent_ids": ["78311930-ad29-4a42-9234-9413f761ea42"], "operator": null, "metadata": {"aucs": [0.05745720039734381, 0.06272063693480556, 0.05824477554864116]}}
{"id": "91d90950-8d33-43cd-ac36-de8d540b795d", "fitness": 0.053590519656891024, "name": "EnhancedAQPSO", "description": "Introduce an adaptive learning mechanism using Lvy flight and neighborhood learning to enhance global search capabilities in EnhancedAQPSO.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7  # Initial chaotic factor\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def levy_flight(self, scale=0.01):\n        u = np.random.normal(0, 1, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / np.power(np.abs(v), 1/3)\n        return scale * step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    update_vector = self.levy_flight()\n                    local_pos = self.positions[i] + update_vector\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 49, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05359 with standard deviation 0.00301.", "error": "", "parent_ids": ["85169cba-f6ad-4e93-9943-d6078421bcb0"], "operator": null, "metadata": {"aucs": [0.04993729448058826, 0.05353387333366899, 0.05730039115641583]}}
{"id": "20a70ec7-d27c-40f4-bca3-168c77e79ed7", "fitness": 0.05702539148792485, "name": "EnhancedAQPSO", "description": "Introduce adaptive chaotic factor scaling to refine parameter variation dynamically.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7  # Initial chaotic factor\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                # Changed line for adaptive chaotic factor scaling\n                scaling_factor = 1 + 0.1 * np.sin(2 * np.pi * progress_ratio)\n\n                self.c1 = (2.5 - progress_ratio * self.chaotic_factor) * scaling_factor\n                self.c2 = (0.5 + progress_ratio * self.chaotic_factor) * scaling_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 50, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05703 with standard deviation 0.00666.", "error": "", "parent_ids": ["85169cba-f6ad-4e93-9943-d6078421bcb0"], "operator": null, "metadata": {"aucs": [0.04805179266263804, 0.06400864948402174, 0.059015732317114766]}}
{"id": "7880e93d-f767-4380-af76-15225e3fcfb3", "fitness": 0.05517754435911489, "name": "EnhancedAQPSO", "description": "Fine-tune EnhancedAQPSO by increasing the mutation probability dynamically to enhance exploration capabilities.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7  # Initial chaotic factor\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.3 * success_rate  # Increased mutation probability dynamics\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 51, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05518 with standard deviation 0.00738.", "error": "", "parent_ids": ["85169cba-f6ad-4e93-9943-d6078421bcb0"], "operator": null, "metadata": {"aucs": [0.045270751497645145, 0.0629614904232837, 0.05730039115641583]}}
{"id": "8b509c21-e134-4fb6-b65c-5fd0a358a821", "fitness": 0.05098029179953637, "name": "EnhancedAQPSO", "description": "Integrate Lvy flight for global exploration and adaptive learning factors for EnhancedAQPSO to balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7  # Initial chaotic factor\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def levy_flight(self, L):\n        u = np.random.normal(0, 1, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / np.abs(v)**(1 / L)\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                # Implementing Lvy flight for global exploration\n                if np.random.rand() < 0.3:  # 30% chance to perform Lvy flight\n                    L = 1.5  # Lvy flight exponent\n                    levy_step = self.levy_flight(L)\n                    self.positions[i] += levy_step\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 52, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05098 with standard deviation 0.00602.", "error": "", "parent_ids": ["85169cba-f6ad-4e93-9943-d6078421bcb0"], "operator": null, "metadata": {"aucs": [0.04366253679094645, 0.05840482166177974, 0.050873516945882935]}}
{"id": "db9fbc9e-af56-4dbf-956d-0a9e721d6dab", "fitness": 0.0497106636364133, "name": "EnhancedAQPSO", "description": "Introduce adaptive learning strategies for mutation and velocity updates to further balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.15  # Adjusted initial diversity probability\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.15  # Adjusted initial mutation probability\n        self.chaotic_factor = 0.7\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.25 * (1 - success_rate)  # Adjusted formula\n            self.local_search_prob = 0.05 + 0.12 * success_rate   # Adjusted formula\n            self.mutation_prob = 0.12 + 0.22 * success_rate       # Adjusted formula\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.9 * (self.positions[r2] - self.positions[r3])  # Adjusted scaling factor\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 53, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04971 with standard deviation 0.00089.", "error": "", "parent_ids": ["85169cba-f6ad-4e93-9943-d6078421bcb0"], "operator": null, "metadata": {"aucs": [0.0488225926432988, 0.04938794558578208, 0.05092145268015902]}}
{"id": "6acf8160-3adf-4384-b857-9ea7b77bd5a9", "fitness": 0.052521878782156674, "name": "EnhancedAQPSOWithLevy", "description": "Integrate adaptive Lvy flight for enhanced exploration and chaotic map-driven parameter tuning to balance global and local search capabilities.", "code": "import numpy as np\n\nclass EnhancedAQPSOWithLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7  # Initial chaotic factor\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def levy_flight(self, scale=0.01):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1 / beta)\n        u = np.random.normal(0, sigma, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / abs(v)**(1 / beta)\n        return scale * step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                if np.random.rand() < 0.3:  # Integrating Lvy flight occasionally\n                    levy_step = self.levy_flight()\n                    self.positions[i] += levy_step\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 54, "feedback": "The algorithm EnhancedAQPSOWithLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05252 with standard deviation 0.00920.", "error": "", "parent_ids": ["85169cba-f6ad-4e93-9943-d6078421bcb0"], "operator": null, "metadata": {"aucs": [0.04250630842913228, 0.0647139022538683, 0.050345425663469445]}}
{"id": "12fffd1d-e10f-4b26-89a1-d637f178ba84", "fitness": 0.053590519656891024, "name": "EnhancedAQPSOLevy", "description": "Integrate Lvy flights with EnhancedAQPSO for enhanced exploration and balance using chaotic maps and adaptive parameter tuning.", "code": "import numpy as np\n\nclass EnhancedAQPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, beta=1.5):\n        sigma_u = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                   (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma_u, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / np.abs(v) ** (1 / beta)\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    levy_step = self.levy_flight()\n                    local_pos = self.positions[i] + levy_step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 55, "feedback": "The algorithm EnhancedAQPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05359 with standard deviation 0.00301.", "error": "", "parent_ids": ["85169cba-f6ad-4e93-9943-d6078421bcb0"], "operator": null, "metadata": {"aucs": [0.04993729448058826, 0.05353387333366899, 0.05730039115641583]}}
{"id": "445932cf-37de-456a-8944-ec2ec02b824d", "fitness": 0.059474204293596844, "name": "EnhancedAQPSO", "description": "Refine EnhancedAQPSO using self-adaptive parameter control and dynamic swarm resizing to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7  # Initial chaotic factor\n        self.shrink_factor = 0.9  # Dynamic swarm resizing factor\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n                self.swarm_size = max(10, int(self.swarm_size * self.shrink_factor))\n\n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 56, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05947 with standard deviation 0.00232.", "error": "", "parent_ids": ["85169cba-f6ad-4e93-9943-d6078421bcb0"], "operator": null, "metadata": {"aucs": [0.05745720039734381, 0.06272063693480556, 0.05824477554864116]}}
{"id": "64f787c2-64e5-4b8e-b2f4-43f1448ca23e", "fitness": 0.057385759711085726, "name": "EnhancedAQPSO", "description": "Introduce adaptive velocity scaling to enhance convergence speed and diversity by modifying the velocity calculation terms.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7  # Initial chaotic factor\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                velocity_scale = 1 + 0.1 * np.sin(2 * np.pi * progress_ratio)  # New adaptive scaling\n                self.velocities[i] = velocity_scale * (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 57, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05739 with standard deviation 0.00687.", "error": "", "parent_ids": ["85169cba-f6ad-4e93-9943-d6078421bcb0"], "operator": null, "metadata": {"aucs": [0.04809657769818099, 0.06450625937246568, 0.05955444206261051]}}
{"id": "faece6de-ef0d-4c70-b378-b972395014cf", "fitness": 0.05370745324702338, "name": "EnhancedAQPSO", "description": "Enhance learning and diversity by dynamically updating swarm size and integrating Gaussian perturbation.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7  # Initial chaotic factor\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                # Adding Gaussian perturbation for enhanced exploration\n                self.positions[i] += np.random.normal(0, 0.005, self.dim)\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 58, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05371 with standard deviation 0.00425.", "error": "", "parent_ids": ["85169cba-f6ad-4e93-9943-d6078421bcb0"], "operator": null, "metadata": {"aucs": [0.05949217685842212, 0.04938925793682314, 0.052240924945824885]}}
{"id": "4e08ac9a-4a0b-4554-acc6-bd176cd66635", "fitness": 0.041615824584752326, "name": "EnhancedAQPSO", "description": "Introduce adaptive inertia weight adjustment based on fitness variance to balance exploration and exploitation dynamically in EnhancedAQPSO.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7  # Initial chaotic factor\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            fitness_variance = np.var(self.best_scores[self.best_scores < np.inf])\n            self.inertia_weight = 0.4 + 0.5 * (1 / (1 + fitness_variance))\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 59, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04162 with standard deviation 0.00571.", "error": "", "parent_ids": ["85169cba-f6ad-4e93-9943-d6078421bcb0"], "operator": null, "metadata": {"aucs": [0.04669488905087393, 0.044519065409373004, 0.03363351929401004]}}
{"id": "ad453c33-abd5-431c-994a-26efa21ced85", "fitness": 0.059474204293596844, "name": "EnhancedAQPSO", "description": "Introduce adaptive random restarts to enhance global exploration and prevent premature convergence.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7  # Initial chaotic factor\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))  # Adaptive restart\n\n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 60, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05947 with standard deviation 0.00232.", "error": "", "parent_ids": ["85169cba-f6ad-4e93-9943-d6078421bcb0"], "operator": null, "metadata": {"aucs": [0.05745720039734381, 0.06272063693480556, 0.05824477554864116]}}
{"id": "2d61a488-f060-42a3-abcc-965207945c7d", "fitness": 0.0532250270986833, "name": "EnhancedAQPSO", "description": "EnhancedAQPSO with dynamic restart threshold for better exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7  # Initial chaotic factor\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = max(10, self.budget // 10)  # Changed this line\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05323 with standard deviation 0.00965.", "error": "", "parent_ids": ["85169cba-f6ad-4e93-9943-d6078421bcb0"], "operator": null, "metadata": {"aucs": [0.039995968860927156, 0.06272063693480556, 0.056958475500317185]}}
{"id": "57ea7cc5-110a-46a1-9eb7-d7ef3532c6f8", "fitness": 0.05937734659906968, "name": "EnhancedAQPSO_Levy", "description": "Introduce a Lvy flight-based perturbation to enhance exploration capabilities and utilize adaptive chaotic factors for improved convergence speed and robustness.", "code": "import numpy as np\n\nclass EnhancedAQPSO_Levy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = np.random.rand()  # Initial chaotic factor\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def levy_flight(self, step_size=0.01):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1 / beta)\n        u = np.random.normal(0, sigma, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (abs(v) ** (1 / beta))\n        return step_size * step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                if np.random.rand() < 0.05:  # Levy flight perturbation probability\n                    self.positions[i] += self.levy_flight()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 62, "feedback": "The algorithm EnhancedAQPSO_Levy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05938 with standard deviation 0.00773.", "error": "", "parent_ids": ["85169cba-f6ad-4e93-9943-d6078421bcb0"], "operator": null, "metadata": {"aucs": [0.0518996226630315, 0.07001437130556243, 0.056218045828615115]}}
{"id": "e87ec928-6541-4fc0-af65-7552ba661a6b", "fitness": 0.046528841168046085, "name": "EnhancedAQPSO", "description": "Introduce adaptive velocity clamping and Gaussian mutation for robust convergence and enhanced exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7\n        self.velocity_clamp = 0.1  # New adaptive velocity clamping factor\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        restart_threshold = self.budget // 5\n        for _ in range(self.budget):\n            if self.evaluations % restart_threshold == 0 and self.evaluations > 0:\n                self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n                \n                # Add Gaussian mutation\n                if np.random.rand() < 0.05:\n                    self.positions[i] += np.random.normal(0, 0.1, self.dim)\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                # Apply adaptive velocity clamping\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                \n        return self.global_best_position", "configspace": "", "generation": 63, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04653 with standard deviation 0.00654.", "error": "", "parent_ids": ["85169cba-f6ad-4e93-9943-d6078421bcb0"], "operator": null, "metadata": {"aucs": [0.04812643988458121, 0.037834415020465806, 0.053625668599091236]}}
{"id": "c58dc91f-6ca0-4774-b476-6ae55d33468e", "fitness": 0.04985533391266017, "name": "EnhancedAQPSO", "description": "Introduce adaptive mutation control and a dynamic restart mechanism in EnhancedAQPSO to enhance convergence speed and robustness.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        dynamic_restart_freq = max(self.budget // 10, 1)\n\n        for _ in range(self.budget):\n            if self.evaluations % dynamic_restart_freq == 0 and self.evaluations > 0:\n                improvement_rate = (self.global_best_score - np.min(self.best_scores)) / self.global_best_score\n                if improvement_rate < 0.01:  # If little improvement, restart positions\n                    self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n\n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * (1 - success_rate)\n\n            for i in range(self.swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.local_search_prob:\n                    local_pos = self.positions[i] + np.random.uniform(-0.02, 0.02, self.dim)\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n\n        return self.global_best_position", "configspace": "", "generation": 64, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04986 with standard deviation 0.00207.", "error": "", "parent_ids": ["85169cba-f6ad-4e93-9943-d6078421bcb0"], "operator": null, "metadata": {"aucs": [0.05270034478373942, 0.049020563440306586, 0.04784509351393451]}}
{"id": "f1a9b0c1-6e4d-40a5-bedd-33ce776be16b", "fitness": 0.06094358469993481, "name": "EnhancedAQPSO", "description": "Introduce dynamic swarm size and Lvy flight-based local search to enhance exploration and exploitation balance in EnhancedAQPSO.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.current_swarm_size = self.initial_swarm_size\n        self.positions = np.random.uniform(0, 1, (self.current_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.current_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.current_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        for _ in range(self.budget):\n            if self.evaluations % (self.budget // 10) == 0 and self.evaluations > 0:\n                self.current_swarm_size = np.random.randint(20, 40)\n                self.positions = np.random.uniform(lb, ub, (self.current_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (self.current_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(self.current_swarm_size, np.inf)\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.current_swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.current_swarm_size, 3, replace=False)\n                    mutant = self.positions[r1] + 0.8 * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.local_search_prob:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n\n        return self.global_best_position", "configspace": "", "generation": 65, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06094 with standard deviation 0.00541.", "error": "", "parent_ids": ["85169cba-f6ad-4e93-9943-d6078421bcb0"], "operator": null, "metadata": {"aucs": [0.06629484862360002, 0.05353387333366899, 0.06300203214253541]}}
{"id": "656b2b29-5b0e-41dd-8aca-0ac200b0de5c", "fitness": 0.06288957588273662, "name": "EnhancedAQPSO", "description": "Introduce adaptive chaotic mutation scale to enhance diversity in EnhancedAQPSO.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.current_swarm_size = self.initial_swarm_size\n        self.positions = np.random.uniform(0, 1, (self.current_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.current_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.current_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        for _ in range(self.budget):\n            if self.evaluations % (self.budget // 10) == 0 and self.evaluations > 0:\n                self.current_swarm_size = np.random.randint(20, 40)\n                self.positions = np.random.uniform(lb, ub, (self.current_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (self.current_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(self.current_swarm_size, np.inf)\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.current_swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.current_swarm_size, 3, replace=False)\n                    scale = 0.8 * self.chaotic_factor  # Changed line: adaptive chaotic mutation scale\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.local_search_prob:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n\n        return self.global_best_position", "configspace": "", "generation": 66, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06289 with standard deviation 0.00283.", "error": "", "parent_ids": ["f1a9b0c1-6e4d-40a5-bedd-33ce776be16b"], "operator": null, "metadata": {"aucs": [0.06629484862360002, 0.05937184688207442, 0.06300203214253541]}}
{"id": "dd34ff2c-2dae-4c4a-85c5-6636c35b2d04", "fitness": 0.0503906758623757, "name": "EnhancedAQPSO", "description": "Introduce dynamic adaptive learning coefficients and nonlinear inertia weight to improve convergence in EnhancedAQPSO.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.current_swarm_size = self.initial_swarm_size\n        self.positions = np.random.uniform(0, 1, (self.current_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.current_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.current_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1_initial = 2.5\n        self.c2_initial = 0.5\n        self.c1_final = 0.5\n        self.c2_final = 2.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        for _ in range(self.budget):\n            if self.evaluations % (self.budget // 10) == 0 and self.evaluations > 0:\n                self.current_swarm_size = np.random.randint(20, 40)\n                self.positions = np.random.uniform(lb, ub, (self.current_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (self.current_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(self.current_swarm_size, np.inf)\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.current_swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.9 - 0.5 * progress_ratio  # Nonlinear inertia weight\n\n                # Dynamic adaptive learning coefficients\n                self.c1 = self.c1_initial * (1 - progress_ratio) + self.c1_final * progress_ratio\n                self.c2 = self.c2_initial * (1 - progress_ratio) + self.c2_final * progress_ratio\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.current_swarm_size, 3, replace=False)\n                    scale = 0.8 * self.chaotic_factor\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.local_search_prob:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n\n        return self.global_best_position", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05039 with standard deviation 0.00512.", "error": "", "parent_ids": ["656b2b29-5b0e-41dd-8aca-0ac200b0de5c"], "operator": null, "metadata": {"aucs": [0.0576328363010874, 0.04691726933036133, 0.04662192195567838]}}
{"id": "68d5a3a4-4bb0-4e3b-814e-d1f3a8bf6c88", "fitness": 0.06312384083809301, "name": "EnhancedAQPSO", "description": "Introduce an adaptive velocity damping factor to stabilize convergence in EnhancedAQPSO.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.current_swarm_size = self.initial_swarm_size\n        self.positions = np.random.uniform(0, 1, (self.current_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.current_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.current_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        for _ in range(self.budget):\n            if self.evaluations % (self.budget // 10) == 0 and self.evaluations > 0:\n                self.current_swarm_size = np.random.randint(20, 40)\n                self.positions = np.random.uniform(lb, ub, (self.current_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (self.current_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(self.current_swarm_size, np.inf)\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.current_swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.current_swarm_size, 3, replace=False)\n                    scale = 0.8 * self.chaotic_factor  # Changed line: adaptive chaotic mutation scale\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor  # Changed line: adaptive velocity damping factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.local_search_prob:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n\n        return self.global_best_position", "configspace": "", "generation": 68, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06312 with standard deviation 0.00302.", "error": "", "parent_ids": ["656b2b29-5b0e-41dd-8aca-0ac200b0de5c"], "operator": null, "metadata": {"aucs": [0.06672590628940123, 0.05933554567023469, 0.0633100705546431]}}
{"id": "e06203af-67fb-4b8c-a75b-ee2ccaa78a5f", "fitness": 0.06312384083809301, "name": "EnhancedAQPSO", "description": "Increase swarm size upper limit to 50 for better exploration.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.current_swarm_size = self.initial_swarm_size\n        self.positions = np.random.uniform(0, 1, (self.current_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.current_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.current_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        for _ in range(self.budget):\n            if self.evaluations % (self.budget // 10) == 0 and self.evaluations > 0:\n                self.current_swarm_size = np.random.randint(20, 50)  # Changed line: increased upper limit of swarm size\n                self.positions = np.random.uniform(lb, ub, (self.current_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (self.current_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(self.current_swarm_size, np.inf)\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.current_swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.current_swarm_size, 3, replace=False)\n                    scale = 0.8 * self.chaotic_factor  # Changed line: adaptive chaotic mutation scale\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor  # Changed line: adaptive velocity damping factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.local_search_prob:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n\n        return self.global_best_position", "configspace": "", "generation": 69, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06312 with standard deviation 0.00302.", "error": "", "parent_ids": ["68d5a3a4-4bb0-4e3b-814e-d1f3a8bf6c88"], "operator": null, "metadata": {"aucs": [0.06672590628940123, 0.05933554567023469, 0.0633100705546431]}}
{"id": "73af6455-2613-487b-a5ce-7e32e8b23718", "fitness": 0.057615778882703604, "name": "EnhancedAQPSO", "description": "Introduce a dynamic chaotic factor to improve exploration in EnhancedAQPSO.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.current_swarm_size = self.initial_swarm_size\n        self.positions = np.random.uniform(0, 1, (self.current_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.current_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.current_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        for _ in range(self.budget):\n            if self.evaluations % (self.budget // 10) == 0 and self.evaluations > 0:\n                self.current_swarm_size = np.random.randint(20, 40)\n                self.positions = np.random.uniform(lb, ub, (self.current_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (self.current_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(self.current_swarm_size, np.inf)\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * success_rate\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.current_swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                if self.evaluations % (self.budget // 20) == 0: # Changed line: dynamic chaotic factor adjustment\n                    self.chaotic_factor = np.random.rand()\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.current_swarm_size, 3, replace=False)\n                    scale = 0.8 * self.chaotic_factor\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.local_search_prob:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n\n        return self.global_best_position", "configspace": "", "generation": 70, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05762 with standard deviation 0.00218.", "error": "", "parent_ids": ["68d5a3a4-4bb0-4e3b-814e-d1f3a8bf6c88"], "operator": null, "metadata": {"aucs": [0.06023193837260432, 0.05771286880871018, 0.0549025294667963]}}
{"id": "2293b06a-bddb-453d-8abe-f626b63d0242", "fitness": 0.0641766022336497, "name": "EnhancedAQPSO", "description": "Introduce a neighborhood search strategy by adjusting the local search probability based on the current global best score.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.current_swarm_size = self.initial_swarm_size\n        self.positions = np.random.uniform(0, 1, (self.current_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.current_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.current_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        for _ in range(self.budget):\n            if self.evaluations % (self.budget // 10) == 0 and self.evaluations > 0:\n                self.current_swarm_size = np.random.randint(20, 40)\n                self.positions = np.random.uniform(lb, ub, (self.current_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (self.current_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(self.current_swarm_size, np.inf)\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * (1 - self.global_best_score / np.max(self.best_scores))  # Adjusted line\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.current_swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.current_swarm_size, 3, replace=False)\n                    scale = 0.8 * self.chaotic_factor\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.local_search_prob:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n\n        return self.global_best_position", "configspace": "", "generation": 71, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06418 with standard deviation 0.00545.", "error": "", "parent_ids": ["68d5a3a4-4bb0-4e3b-814e-d1f3a8bf6c88"], "operator": null, "metadata": {"aucs": [0.07174146757624922, 0.0616675796134869, 0.059120759511212984]}}
{"id": "efb54f07-a3bf-47e9-9b96-f07bd75e6f6e", "fitness": 0.05657507113340402, "name": "EnhancedAQPSO", "description": "Enhance swarm exploration by dynamically adjusting the chaotic factor based on the success rate.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.current_swarm_size = self.initial_swarm_size\n        self.positions = np.random.uniform(0, 1, (self.current_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.current_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.current_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        for _ in range(self.budget):\n            if self.evaluations % (self.budget // 10) == 0 and self.evaluations > 0:\n                self.current_swarm_size = np.random.randint(20, 40)\n                self.positions = np.random.uniform(lb, ub, (self.current_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (self.current_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(self.current_swarm_size, np.inf)\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * (1 - self.global_best_score / np.max(self.best_scores))\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            # Modify the chaotic factor based on the success rate to enhance exploration\n            self.chaotic_factor = self.logistic_map(self.chaotic_factor * (1 + 0.1 * success_rate))\n\n            for i in range(self.current_swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                progress_ratio = self.evaluations / self.budget\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.current_swarm_size, 3, replace=False)\n                    scale = 0.8 * self.chaotic_factor\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.local_search_prob:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n\n        return self.global_best_position", "configspace": "", "generation": 72, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05658 with standard deviation 0.00255.", "error": "", "parent_ids": ["2293b06a-bddb-453d-8abe-f626b63d0242"], "operator": null, "metadata": {"aucs": [0.05359003310627275, 0.05982654048319347, 0.05630863981074585]}}
{"id": "d710df2d-fce8-41d8-a664-a2edb0a0eb49", "fitness": 0.06395893787962241, "name": "EnhancedAQPSO", "description": "Enhance convergence speed by dynamically adjusting the inertia weight for better exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.current_swarm_size = self.initial_swarm_size\n        self.positions = np.random.uniform(0, 1, (self.current_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.current_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.current_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.diversity_prob = 0.1\n        self.local_search_prob = 0.05\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.mutation_prob = 0.1\n        self.chaotic_factor = 0.7\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        for _ in range(self.budget):\n            if self.evaluations % (self.budget // 10) == 0 and self.evaluations > 0:\n                self.current_swarm_size = np.random.randint(20, 40)\n                self.positions = np.random.uniform(lb, ub, (self.current_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (self.current_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(self.current_swarm_size, np.inf)\n            \n            success_rate = np.mean(self.best_scores < np.inf)\n            self.diversity_prob = 0.1 + 0.2 * (1 - success_rate)\n            self.local_search_prob = 0.05 + 0.1 * (1 - self.global_best_score / np.max(self.best_scores))\n            self.mutation_prob = 0.1 + 0.2 * success_rate\n\n            for i in range(self.current_swarm_size):\n                if self.evaluations >= self.budget:\n                    return self.global_best_position\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                # Adjusted line for dynamic inertia weight\n                self.inertia_weight = 0.9 - 0.5 * progress_ratio\n\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < self.mutation_prob:\n                    r1, r2, r3 = np.random.choice(self.current_swarm_size, 3, replace=False)\n                    scale = 0.8 * self.chaotic_factor\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < self.diversity_prob:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.local_search_prob:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n\n        return self.global_best_position", "configspace": "", "generation": 73, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06396 with standard deviation 0.00556.", "error": "", "parent_ids": ["2293b06a-bddb-453d-8abe-f626b63d0242"], "operator": null, "metadata": {"aucs": [0.07174562371502102, 0.0610089497544527, 0.05912224016939349]}}
{"id": "15ba1d46-b909-465a-a5a0-a68e2af428d4", "fitness": 0.07065753699751469, "name": "EnhancedAQPSO", "description": "Introduce dynamic swarm size adaptation based on improvement rate and a differential evolution-inspired mutation operator to enhance exploration.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = 0.7\n        self.last_improvement = 0\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.1:\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.8 * self.chaotic_factor\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.1:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.05:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n\n        return self.global_best_position", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07066 with standard deviation 0.00253.", "error": "", "parent_ids": ["2293b06a-bddb-453d-8abe-f626b63d0242"], "operator": null, "metadata": {"aucs": [0.07262166006615245, 0.06708706633423289, 0.07226388459215871]}}
{"id": "5f2d4ecf-9e0c-4894-9b25-9352df3ce0d2", "fitness": 0.0694593436056143, "name": "EnhancedAQPSO", "description": "Adjust chaotic factor initialization to improve convergence stability.", "code": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = 0.9  # Adjusted from 0.7 to 0.9\n        self.last_improvement = 0\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.1:\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.8 * self.chaotic_factor\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.1:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.05:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n\n        return self.global_best_position", "configspace": "", "generation": 75, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06946 with standard deviation 0.00232.", "error": "", "parent_ids": ["15ba1d46-b909-465a-a5a0-a68e2af428d4"], "operator": null, "metadata": {"aucs": [0.06728358408377, 0.0684163323487087, 0.07267811438436422]}}
{"id": "e479ba09-8354-4ecb-a553-4ff3631cbfcd", "fitness": 0.08076143611461457, "name": "EnhancedDynamicSwarm", "description": "Incorporate adaptive chaotic factors and memory-based migration to enhance exploration and convergence in dynamic swarm optimization.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.1:\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.8 * self.chaotic_factor\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.1:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.05:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n                \n                if len(self.memory) > 0 and np.random.rand() < 0.05:\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 10:\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 76, "feedback": "The algorithm EnhancedDynamicSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08076 with standard deviation 0.00795.", "error": "", "parent_ids": ["15ba1d46-b909-465a-a5a0-a68e2af428d4"], "operator": null, "metadata": {"aucs": [0.07077325737897444, 0.08128494971764477, 0.09022610124722452]}}
{"id": "ae03c42b-7341-4d1a-b6d0-1906e6264596", "fitness": 0.07736869786718233, "name": "EnhancedDynamicSwarm", "description": "Enhance swarm exploration by integrating a novel mutation strategy based on differential evolution and improved chaotic factor dynamics.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.5 + 0.4 * self.chaotic_factor  # Changed line\n                self.c1 = 2.0 - progress_ratio * self.chaotic_factor  # Changed line\n                self.c2 = 0.7 + progress_ratio * self.chaotic_factor  # Changed line\n\n                if np.random.rand() < 0.2:  # Changed line\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.9 * self.chaotic_factor  # Changed line\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.1:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.05:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n                \n                if len(self.memory) > 0 and np.random.rand() < 0.05:\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 10:\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 77, "feedback": "The algorithm EnhancedDynamicSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07737 with standard deviation 0.01396.", "error": "", "parent_ids": ["e479ba09-8354-4ecb-a553-4ff3631cbfcd"], "operator": null, "metadata": {"aucs": [0.0688671454257953, 0.0661897989679534, 0.09704914920779828]}}
{"id": "97c54538-1514-41f2-8e84-841254e4ae45", "fitness": 0.0808055403020862, "name": "EnhancedDynamicSwarm", "description": "Enhance swarm diversity and convergence by augmenting chaotic dynamics and optimizing memory-based selections.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.1:\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.8 * self.chaotic_factor\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.1:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.05:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n                \n                if len(self.memory) > 0 and np.random.rand() < 0.1:  # Enhanced memory usage probability\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 15:  # Increased memory size\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedDynamicSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08081 with standard deviation 0.00803.", "error": "", "parent_ids": ["e479ba09-8354-4ecb-a553-4ff3631cbfcd"], "operator": null, "metadata": {"aucs": [0.07059347376747072, 0.08159704589156336, 0.09022610124722452]}}
{"id": "3ed83f63-56c5-488e-8d62-2039347dd44a", "fitness": 0.06384074476060626, "name": "EnhancedDynamicSwarmRefined", "description": "Enhance swarm efficiency with improved chaos-driven dynamics, adaptive mutation strategies, and a refined memory mechanism for better global convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarmRefined:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def levy_flight(self, lam=1.3):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            if np.mean(self.best_scores) < np.inf:\n                improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            else:\n                improvement_rate = 0 \n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n\n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.5 + 0.5 * self.chaotic_factor\n                self.c1 = 2.0 - progress_ratio * self.chaotic_factor\n                self.c2 = 1.0 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.2:  # Increased probability for diversity\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.7 * self.chaotic_factor  # Modified scale\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.8 + 0.2 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.2:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.1:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n\n                if len(self.memory) > 0 and np.random.rand() < 0.15:  # Optimized memory reuse\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 20:  # Enhanced memory retention\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 79, "feedback": "The algorithm EnhancedDynamicSwarmRefined got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06384 with standard deviation 0.00778.", "error": "", "parent_ids": ["97c54538-1514-41f2-8e84-841254e4ae45"], "operator": null, "metadata": {"aucs": [0.06068109948337541, 0.07454868336960219, 0.056292451428841206]}}
{"id": "88c4210f-e3bb-4703-ac2e-a16137be6f33", "fitness": 0.07072314058555836, "name": "EnhancedDynamicSwarm", "description": "Improve exploration by refining chaotic factor dynamics and modifying selection strategy.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.1:\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.8 * self.chaotic_factor\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.15:  # Modified probability\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.05:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n                \n                if len(self.memory) > 0 and np.random.rand() < 0.15:  # Increased memory usage probability\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 15:\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 80, "feedback": "The algorithm EnhancedDynamicSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07072 with standard deviation 0.00694.", "error": "", "parent_ids": ["97c54538-1514-41f2-8e84-841254e4ae45"], "operator": null, "metadata": {"aucs": [0.062036505340885784, 0.07902761693533666, 0.07110529948045263]}}
{"id": "66614e05-75da-4651-ba1c-6265b1a736e0", "fitness": 0.0808055403020862, "name": "EnhancedDynamicSwarm", "description": "Optimize global exploration by enhancing chaotic factor adaptation.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.1:\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.8 * self.chaotic_factor\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.1:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.05:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n                \n                if len(self.memory) > 0 and np.random.rand() < 0.1:  # Enhanced memory usage probability\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 15:  # Increased memory size\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedDynamicSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08081 with standard deviation 0.00803.", "error": "", "parent_ids": ["97c54538-1514-41f2-8e84-841254e4ae45"], "operator": null, "metadata": {"aucs": [0.07059347376747072, 0.08159704589156336, 0.09022610124722452]}}
{"id": "d2270f14-619b-4720-b09d-dab39a8fd7bf", "fitness": 0.0710593033761308, "name": "EnhancedDynamicSwarm", "description": "Improve convergence by integrating adaptive memory and hybrid exploration-exploitation strategies.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n\n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.15:  # Increased mutation probability\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.9 * self.chaotic_factor  # Adjusted scaling\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.85 + 0.1 * self.chaotic_factor  # Adjusted damping\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.15:  # Enhanced exploration probability\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.1:  # Enhanced exploitation probability\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n                \n                if len(self.memory) > 0 and np.random.rand() < 0.15:  # Enhanced memory usage probability\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 20:  # Increased memory size\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 82, "feedback": "The algorithm EnhancedDynamicSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07106 with standard deviation 0.01130.", "error": "", "parent_ids": ["97c54538-1514-41f2-8e84-841254e4ae45"], "operator": null, "metadata": {"aucs": [0.05525771014887526, 0.08103168874228073, 0.07688851123723639]}}
{"id": "6a15de3a-d419-43ab-b028-1474dd603924", "fitness": 0.07039765656209145, "name": "RefinedDynamicSwarm", "description": "Introduce a multi-phase adaptive control mechanism to dynamically adjust chaotic influence and memory utilization for superior convergence.", "code": "import numpy as np\n\nclass RefinedDynamicSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n\n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.2:\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.8 * self.chaotic_factor\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.15:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.1:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n                \n                if len(self.memory) > 0 and np.random.rand() < 0.15:  # Enhanced memory usage probability\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 20:  # Increased memory size\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 83, "feedback": "The algorithm RefinedDynamicSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07040 with standard deviation 0.01043.", "error": "", "parent_ids": ["97c54538-1514-41f2-8e84-841254e4ae45"], "operator": null, "metadata": {"aucs": [0.057436823369457435, 0.08297864768212093, 0.07077749863469596]}}
{"id": "9e6c861c-0230-42c0-b357-348e48750a6b", "fitness": 0.08272288685190303, "name": "EnhancedDynamicSwarm", "description": "Integrate a dynamic learning component and adaptive diversity strategy to enhance swarm intelligence and optimize convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.2:  # Increased mutation probability for better exploration\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.6 * self.chaotic_factor  # Adjusted scale for mutation\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.1:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.1:  # Increased probability for Levy flight\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n\n                if len(self.memory) > 0 and np.random.rand() < 0.1:\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 20:  # Further increased memory size for more diverse memory\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 84, "feedback": "The algorithm EnhancedDynamicSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08272 with standard deviation 0.00537.", "error": "", "parent_ids": ["97c54538-1514-41f2-8e84-841254e4ae45"], "operator": null, "metadata": {"aucs": [0.07602319466601482, 0.08297864768212093, 0.08916681820757333]}}
{"id": "ae315dad-23ef-40c1-8210-6c4019b2a96f", "fitness": 0.06380026627141551, "name": "EnhancedDynamicSwarmV2", "description": "Implement a chaotic adaptive memory component with targeted exploration to enhance global convergence and solution accuracy.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarmV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):     \n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n\n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.25:  # Adjusted mutation probability for balanced exploration and exploitation\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.6 * self.chaotic_factor\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.15:  # Adjusted probability for Levy flight\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n\n                if len(self.memory) > 0 and np.random.rand() < 0.15:  # Increased probability for memory usage\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 30:  # Adjusted size of memory to allow more solutions\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedDynamicSwarmV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06380 with standard deviation 0.00291.", "error": "", "parent_ids": ["9e6c861c-0230-42c0-b357-348e48750a6b"], "operator": null, "metadata": {"aucs": [0.05995029388784945, 0.06445460389782165, 0.06699590102857544]}}
{"id": "830ee88b-08ec-4f30-a70f-ee5060fc6961", "fitness": 0.07629734495029306, "name": "EnhancedDynamicSwarm", "description": "Adjusted the progress ratio formula to improve adaptation during convergence, refining swarm movement and exploration balance.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = 1 - np.exp(-5 * self.evaluations / self.budget)  # Modified progress ratio for better adaptation\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.2:  # Increased mutation probability for better exploration\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.6 * self.chaotic_factor  # Adjusted scale for mutation\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.1:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.1:  # Increased probability for Levy flight\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n\n                if len(self.memory) > 0 and np.random.rand() < 0.1:\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 20:  # Further increased memory size for more diverse memory\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 86, "feedback": "The algorithm EnhancedDynamicSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07630 with standard deviation 0.01594.", "error": "", "parent_ids": ["9e6c861c-0230-42c0-b357-348e48750a6b"], "operator": null, "metadata": {"aucs": [0.07913907527833641, 0.055513006940121845, 0.09423995263242091]}}
{"id": "4920372f-1fbc-4476-b151-75278372d66b", "fitness": 0.0680774861550083, "name": "EnhancedDynamicSwarm", "description": "Introduce adaptive velocity constraints and memory consolidation to enhance swarm exploration and convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.2:\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.6 * self.chaotic_factor\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                max_velocity = (ub - lb) * (0.1 + 0.1 * self.chaotic_factor)\n                self.velocities[i] = np.clip(\n                    damping_factor * self.inertia_weight * self.velocities[i] +\n                    self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                    self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]),\n                    -max_velocity, max_velocity\n                )\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.1:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.1:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n\n                if len(self.memory) > 0 and np.random.rand() < 0.1:\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 20:\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedDynamicSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06808 with standard deviation 0.00911.", "error": "", "parent_ids": ["9e6c861c-0230-42c0-b357-348e48750a6b"], "operator": null, "metadata": {"aucs": [0.05956895694931519, 0.06395062166117227, 0.08071287985453746]}}
{"id": "0cfb12c4-7b35-4a88-a10e-b3ca777f10cb", "fitness": 0.06599319020147783, "name": "EnhancedDynamicSwarm", "description": "Utilize hybrid memory-enhanced dynamic swarm strategies for improved exploration and convergence balance.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.25:  # Modified mutation probability\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.7 * self.chaotic_factor  # Adjusted scale for mutation\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.15:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.15:  # Adjusted probability for Levy flight\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n\n                if len(self.memory) > 0 and np.random.rand() < 0.15:\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 25:  # Further adjusted memory size\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 88, "feedback": "The algorithm EnhancedDynamicSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06599 with standard deviation 0.01093.", "error": "", "parent_ids": ["9e6c861c-0230-42c0-b357-348e48750a6b"], "operator": null, "metadata": {"aucs": [0.06246372990399751, 0.05472945921863215, 0.08078638148180384]}}
{"id": "757ed5f3-3798-49df-82a0-bb6c8ec2fc3f", "fitness": 0.07964857060641599, "name": "EnhancedDynamicSwarm", "description": "Slightly adjust chaotic factor initialization for enhanced exploration.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand() * 0.5  # Adjusted initialization for better exploration\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.2:  # Increased mutation probability for better exploration\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.6 * self.chaotic_factor  # Adjusted scale for mutation\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.1:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.1:  # Increased probability for Levy flight\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n\n                if len(self.memory) > 0 and np.random.rand() < 0.1:\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 20:  # Further increased memory size for more diverse memory\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 89, "feedback": "The algorithm EnhancedDynamicSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07965 with standard deviation 0.00850.", "error": "", "parent_ids": ["9e6c861c-0230-42c0-b357-348e48750a6b"], "operator": null, "metadata": {"aucs": [0.0680179755115844, 0.08284078230561076, 0.0880869540020528]}}
{"id": "b0bad7e5-2729-4283-a996-aa10f12ba756", "fitness": 0.07448744092096264, "name": "QuantumChaoticSwarm", "description": "Introduce adaptive learning parameters and hybridize chaotic dynamics with quantum-inspired exploration to refine swarm intelligence for enhanced optimization performance.", "code": "import numpy as np\n\nclass QuantumChaoticSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.quantum_param = 0.05\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.15:\n                    quantum_step = np.random.normal(0, self.quantum_param, self.dim)\n                    self.positions[i] += quantum_step\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.1: \n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n\n                if len(self.memory) > 0 and np.random.rand() < 0.1:\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 20:\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 90, "feedback": "The algorithm QuantumChaoticSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07449 with standard deviation 0.00042.", "error": "", "parent_ids": ["9e6c861c-0230-42c0-b357-348e48750a6b"], "operator": null, "metadata": {"aucs": [0.07396541834447456, 0.07499220721251032, 0.07450469720590303]}}
{"id": "68d54635-230d-484f-9bb6-a09158a3934e", "fitness": 0.07093687589727797, "name": "EnhancedDynamicSwarm", "description": "Introduce a chaotic mutation strategy and adaptive inertia for enhanced exploration and convergence stability.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.3 * self.chaotic_factor  # Adjusted inertia influence\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.25:  # Enhanced chaotic mutation probability\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.5 * self.chaotic_factor  # Further adjustment for chaos influence\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.1:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.1:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n\n                if len(self.memory) > 0 and np.random.rand() < 0.1:\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 20:\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 91, "feedback": "The algorithm EnhancedDynamicSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07094 with standard deviation 0.01305.", "error": "", "parent_ids": ["9e6c861c-0230-42c0-b357-348e48750a6b"], "operator": null, "metadata": {"aucs": [0.06431398144834166, 0.05933582797404913, 0.08916081826944311]}}
{"id": "510fddef-45f4-473d-abd2-67c9d8b41d8a", "fitness": 0.08272288685190303, "name": "EnhancedDynamicSwarm", "description": "Enhance exploration by refining chaotic factor tuning and adjust mutation scale for improved diversity.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n\n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * (1.5 * self.chaotic_factor)  # Modified scaling for c1\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.2:\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.8 * self.chaotic_factor  # Adjusted scale for mutation\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.1:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.1:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n\n                if len(self.memory) > 0 and np.random.rand() < 0.1:\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 20:\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedDynamicSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08272 with standard deviation 0.00537.", "error": "", "parent_ids": ["9e6c861c-0230-42c0-b357-348e48750a6b"], "operator": null, "metadata": {"aucs": [0.07602319466601482, 0.08297864768212093, 0.08916681820757333]}}
{"id": "b85bf90d-fc2c-44fc-ba2f-2197e6e8913a", "fitness": 0.06528698034998766, "name": "EnhancedDynamicSwarm", "description": "Introduce adaptive chaos and memory reinforcement to enhance exploration and exploitation balance in dynamic swarm intelligence.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.3:  # Increased mutation probability for better exploration\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.6 * self.chaotic_factor  # Adjusted scale for mutation\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.15:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.2:  # Increased probability for Levy flight\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n\n                if len(self.memory) > 0 and np.random.rand() < 0.1:\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 25:  # Further increased memory size for more diverse memory\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 93, "feedback": "The algorithm EnhancedDynamicSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06529 with standard deviation 0.01096.", "error": "", "parent_ids": ["9e6c861c-0230-42c0-b357-348e48750a6b"], "operator": null, "metadata": {"aucs": [0.05782482952562207, 0.057249730042537084, 0.08078638148180384]}}
{"id": "ad3a5b1d-70ac-4218-9400-71a218edab90", "fitness": 0.06796950675816804, "name": "EnhancedDynamicSwarm", "description": "Introduce dynamic mutation and learning rate strategies to enhance PSO convergence and diversity.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.25:  # More frequent dynamic mutation\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.4 * self.chaotic_factor + 0.1  # Adjusted scale for mutation\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.85 + 0.15 * self.chaotic_factor  # More dynamic damping factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.1:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.15:  # Increased probability for Levy flight\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n\n                if len(self.memory) > 0 and np.random.rand() < 0.1:\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 20:\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 94, "feedback": "The algorithm EnhancedDynamicSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06797 with standard deviation 0.01101.", "error": "", "parent_ids": ["9e6c861c-0230-42c0-b357-348e48750a6b"], "operator": null, "metadata": {"aucs": [0.06385583507958115, 0.057025746728236126, 0.08302693846668685]}}
{"id": "09faa8fb-c49d-4679-abb3-b66abee4190f", "fitness": 0.02151201734081402, "name": "EnhancedDynamicSwarmV2", "description": "Introduce adaptive inertia with neighborhood-based learning and historical best memory to improve swarm convergence and exploration balance.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarmV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.7\n        self.memory = []\n\n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        neighborhood_size = 5\n\n        while self.evaluations < self.budget:\n            for i in range(self.positions.shape[0]):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Adaptive inertia weight\n                self.inertia_weight = 0.5 + 0.4 * (self.evaluations / self.budget)\n                \n                # Update velocity using local best in neighborhood\n                neighbors = np.random.choice(self.positions.shape[0], neighborhood_size, replace=False)\n                local_best_position = min(neighbors, key=lambda n: self.best_scores[n])\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.best_positions[local_best_position] - self.positions[i]))\n\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.1:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n\n                # Use historical best memory\n                if len(self.memory) > 0 and np.random.rand() < 0.1:\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 20:\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 95, "feedback": "The algorithm EnhancedDynamicSwarmV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02151 with standard deviation 0.00000.", "error": "", "parent_ids": ["9e6c861c-0230-42c0-b357-348e48750a6b"], "operator": null, "metadata": {"aucs": [0.021512662547386152, 0.02151203897631704, 0.021511350498738868]}}
{"id": "59eb0e99-4740-4ee8-8a6a-4135c4f0e629", "fitness": 0.07081675456572496, "name": "EnhancedDynamicSwarm", "description": "Tweak the mutation probability to enhance exploration during the optimization process.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.25:  # Tweaked mutation probability for improved exploration\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.6 * self.chaotic_factor  # Adjusted scale for mutation\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.1:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.1:  # Increased probability for Levy flight\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n\n                if len(self.memory) > 0 and np.random.rand() < 0.1:\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 20:  # Further increased memory size for more diverse memory\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 96, "feedback": "The algorithm EnhancedDynamicSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07082 with standard deviation 0.01316.", "error": "", "parent_ids": ["9e6c861c-0230-42c0-b357-348e48750a6b"], "operator": null, "metadata": {"aucs": [0.06432394830302868, 0.058959497186572896, 0.08916681820757333]}}
{"id": "a159e8ca-b108-4389-8a16-f90eacddad8c", "fitness": 0.065365674191242, "name": "EnhancedDynamicSwarmV2", "description": "Introduce adaptive chaotic factors and social cognition adjustments to reinforce swarm exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarmV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def adaptive_chaotic_factor(self):\n        return self.logistic_map(self.chaotic_factor)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.adaptive_chaotic_factor()\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.25:\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.8 * self.chaotic_factor\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.15:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.15:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n\n                if len(self.memory) > 0 and np.random.rand() < 0.15:\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 30:\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 97, "feedback": "The algorithm EnhancedDynamicSwarmV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06537 with standard deviation 0.01159.", "error": "", "parent_ids": ["9e6c861c-0230-42c0-b357-348e48750a6b"], "operator": null, "metadata": {"aucs": [0.06246372990399751, 0.05284691118792462, 0.08078638148180384]}}
{"id": "11fb9f22-34e1-4a7c-aa3b-754c133f722f", "fitness": 0.07081675456572496, "name": "EnhancedDynamicSwarm", "description": "Improved swarm diversity by adjusting the mutation probability to enhance exploration.", "code": "import numpy as np\n\nclass EnhancedDynamicSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.5\n        self.c2 = 0.5\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            success_rate = np.mean(self.best_scores < np.inf)\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - success_rate) * 10)\n            if adaptive_swarm_size != self.positions.shape[0]:\n                self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n                self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n                self.best_positions = self.positions.copy()\n                self.best_scores = np.full(adaptive_swarm_size, np.inf)\n        \n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                self.inertia_weight = 0.4 + 0.5 * self.chaotic_factor\n                self.c1 = 2.5 - progress_ratio * self.chaotic_factor\n                self.c2 = 0.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.25:  # Changed mutation probability for better diversity\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.6 * self.chaotic_factor  # Adjusted scale for mutation\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * self.inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.1:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.1:  # Increased probability for Levy flight\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n\n                if len(self.memory) > 0 and np.random.rand() < 0.1:\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 20:  # Further increased memory size for more diverse memory\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 98, "feedback": "The algorithm EnhancedDynamicSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07082 with standard deviation 0.01316.", "error": "", "parent_ids": ["9e6c861c-0230-42c0-b357-348e48750a6b"], "operator": null, "metadata": {"aucs": [0.06432394830302868, 0.058959497186572896, 0.08916681820757333]}}
{"id": "4642e443-732e-4588-b86d-0e5b52f632b6", "fitness": 0.05822324292902936, "name": "EnhancedAdaptiveSwarm", "description": "Enhance swarm intelligence with adaptive learning and dynamic exploration using chaotic maps and cooperative memory exchange for superior convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.positions = np.random.uniform(0, 1, (self.initial_swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.initial_swarm_size, self.dim))\n        self.best_positions = self.positions.copy()\n        self.best_scores = np.full(self.initial_swarm_size, np.inf)\n        self.global_best_position = np.random.uniform(0, 1, self.dim)\n        self.global_best_score = np.inf\n        self.evaluations = 0\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.chaotic_factor = np.random.rand()\n        self.last_improvement = 0\n        self.memory = []\n        self.inertia_weight = 0.9\n\n    def logistic_map(self, x):\n        return 4 * x * (1 - x)\n    \n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 1, self.dim) * (np.sin(np.pi * lam / 2) / np.pi)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / lam))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.evaluations < self.budget:\n            improvement_rate = (self.evaluations - self.last_improvement) / self.budget\n            adaptive_swarm_size = self.initial_swarm_size + int((1 - improvement_rate) * 10)\n            self.positions = np.random.uniform(lb, ub, (adaptive_swarm_size, self.dim))\n            self.velocities = np.random.uniform(-0.1, 0.1, (adaptive_swarm_size, self.dim))\n            self.best_positions = self.positions.copy()\n            self.best_scores = np.full(adaptive_swarm_size, np.inf)\n\n            for i in range(adaptive_swarm_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                self.chaotic_factor = self.logistic_map(self.chaotic_factor)\n                progress_ratio = self.evaluations / self.budget\n                inertia_weight = 0.5 + 0.5 * self.chaotic_factor\n                self.c1 = 1.5 + progress_ratio * self.chaotic_factor\n                self.c2 = 1.5 + progress_ratio * self.chaotic_factor\n\n                if np.random.rand() < 0.3:\n                    r1, r2, r3 = np.random.choice(adaptive_swarm_size, 3, replace=False)\n                    scale = 0.5 * self.chaotic_factor\n                    mutant = self.positions[r1] + scale * (self.positions[r2] - self.positions[r3])\n                    self.positions[i] = np.clip(mutant, lb, ub)\n\n                damping_factor = 0.9 + 0.1 * self.chaotic_factor\n                self.velocities[i] = (damping_factor * inertia_weight * self.velocities[i] +\n                                      self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i]) +\n                                      self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.positions[i]))\n                self.positions[i] += self.velocities[i]\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                if np.random.rand() < 0.1:\n                    self.positions[i] = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < 0.2:\n                    step = self.levy_flight()\n                    local_pos = self.positions[i] + step\n                    local_pos = np.clip(local_pos, lb, ub)\n                    local_score = func(local_pos)\n                    self.evaluations += 1\n                    if local_score < self.best_scores[i]:\n                        self.best_scores[i] = local_score\n                        self.best_positions[i] = local_pos.copy()\n                    if local_score < self.global_best_score:\n                        self.global_best_score = local_score\n                        self.global_best_position = local_pos.copy()\n                        self.last_improvement = self.evaluations\n\n                score = func(self.positions[i])\n                self.evaluations += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.positions[i].copy()\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.positions[i].copy()\n                    self.last_improvement = self.evaluations\n\n                if len(self.memory) > 0 and np.random.rand() < 0.2:\n                    memory_choice = np.random.choice(len(self.memory))\n                    self.positions[i] = self.memory[memory_choice].copy()\n                    self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            self.memory.append(self.global_best_position.copy())\n            if len(self.memory) > 30:\n                self.memory.pop(0)\n\n        return self.global_best_position", "configspace": "", "generation": 99, "feedback": "The algorithm EnhancedAdaptiveSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05822 with standard deviation 0.00852.", "error": "", "parent_ids": ["9e6c861c-0230-42c0-b357-348e48750a6b"], "operator": null, "metadata": {"aucs": [0.06150206545941572, 0.04653713995015163, 0.06663052337752073]}}
